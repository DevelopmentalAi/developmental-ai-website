Timestamp,Email Address,Does your work use coordinate(s) as neural network input(s)?,Title,Nickname (e.g. DeepSDF),"Venue (e.g. CVPR, ICML, SIGGRAPH)","Date (earliest release date, e.g. arXiv v1 date)","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put ""2022"" for this entry, and ""2021"" for the above)",Bibtex Citation,PDF (use ArXiv when possible),Project Webpage (web link),Code Release (Github link),Data Release (link),"Talk/Video (link, e.g. youtube)",Supplement PDF (link),"Supplement video (link, comma separated if multiple exists)",Keywords,Frequency/Positional Encoding,"Geometry proxy (for non-visual computing papers, choose ""N/A"")","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose ""N/A"")",Training time (hr),Rendering time (FPS),Dataset(s) used (e.g. Tanks and Temples),# of input views (e.g. 18 for 18-camera system),Inputs,Lighting,Authors,Bibtex Name,UID,Citation Count,Abstract,Coordinates all at once,"Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)",Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)
,,,AtlasNet: A Papier-Mâché Approach to Learning 3D Surface Generation,AtlasNet,CVPR 2018,2/15/2018,2018,"@inproceedings{groueix2018atlasnet,
  BOOKTITLE = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  ID = {groueix2018atlasnet},
  ENTRYTYPE = {inproceedings},
  AUTHOR = {Thibault Groueix and Matthew Fisher and Vladimir G. Kim and Bryan C. Russell and Mathieu Aubry},
  TITLE = {AtlasNet: A Papier-Mache Approach to Learning 3D Surface Generation},
  EPRINT = {1802.05384v3},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We introduce a method for learning to generate the surface of 3D shapes. Our approach represents a 3D shape as a collection of parametric surface elements and, in contrast to methods generating voxel grids or point clouds, naturally infers a surface representation of the shape. Beyond its novelty, our new shape generation framework, AtlasNet, comes with significant advantages, such as improved precision and generalization capabilities, and the possibility to generate a shape of arbitrary resolution without memory issues. We demonstrate these benefits and compare to strong baselines on the ShapeNet benchmark for two applications: (i) auto-encoding shapes, and (ii) single-view reconstruction from a still image. We also provide results showing its potential for other applications, such as morphing, parametrization, super-resolution, matching, and co-segmentation.},
  YEAR = {2018},
  URL = {http://arxiv.org/abs/1802.05384v3},
  FILE = {1802.05384v3.pdf}
 }",https://arxiv.org/pdf/1802.05384.pdf,http://imagine.enpc.fr/~groueixt/atlasnet/,https://github.com/ThibaultGROUEIX/AtlasNet,,"http://imagine.enpc.fr/~groueixt/atlasnet/atlasnet_slides_spotlight_CVPR.pptx, http://imagine.enpc.fr/~groueixt/atlasnet/atlasnet_poster.pdf",,,"Conditional Neural Field, Sampling, Data-Driven Method",,,,,,,,,,"Thibault Groueix, Matthew Fisher, Vladimir G. Kim, Bryan C. Russell, Mathieu Aubry",groueix2018atlasnet,1,6,"We introduce a method for learning to generate the surface of 3D shapes. Our approach represents a 3D shape as a collection of parametric surface elements and, in contrast to methods generating voxel grids or point clouds, naturally infers a surface representation of the shape. Beyond its novelty, our new shape generation framework, AtlasNet, comes with significant advantages, such as improved precision and generalization capabilities, and the possibility to generate a shape of arbitrary resolution without memory issues. We demonstrate these benefits and compare to strong baselines on the ShapeNet benchmark for two applications: (i) auto-encoding shapes, and (ii) single-view reconstruction from a still image. We also provide results showing its potential for other applications, such as morphing, parametrization, super-resolution, matching, and co-segmentation.",,,
,,,Learning Implicit Fields for Generative Shape Modeling,IM-NET,CVPR 2019,12/6/2018,2019,"@inproceedings{chen2019imnet,
  BOOKTITLE = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  ID = {chen2019imnet},
  ENTRYTYPE = {inproceedings},
  AUTHOR = {Zhiqin Chen and Hao Zhang},
  TITLE = {Learning Implicit Fields for Generative Shape Modeling},
  EPRINT = {1812.02822v5},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.GR},
  ABSTRACT = {We advocate the use of implicit fields for learning generative models of shapes and introduce an implicit field decoder, called IM-NET, for shape generation, aimed at improving the visual quality of the generated shapes. An implicit field assigns a value to each point in 3D space, so that a shape can be extracted as an iso-surface. IM-NET is trained to perform this assignment by means of a binary classifier. Specifically, it takes a point coordinate, along with a feature vector encoding a shape, and outputs a value which indicates whether the point is outside the shape or not. By replacing conventional decoders by our implicit decoder for representation learning (via IM-AE) and shape generation (via IM-GAN), we demonstrate superior results for tasks such as generative shape modeling, interpolation, and single-view 3D reconstruction, particularly in terms of visual quality. Code and supplementary material are available at https://github.com/czq142857/implicit-decoder.},
  YEAR = {2019},
  URL = {http://arxiv.org/abs/1812.02822v5},
  FILE = {1812.02822v5.pdf}
 }",https://arxiv.org/pdf/1812.02822.pdf,https://www.sfu.ca/~zhiqinc/imgan/Readme.html,https://github.com/czq142857/implicit-decoder,,,,,"Generalization, Generative Models, Conditional Neural Field, Data-Driven Method",,Occupancy,,,,,,,,"Zhiqin Chen, Hao Zhang",chen2019imnet,2,324,"We advocate the use of implicit fields for learning generative models of shapes and introduce an implicit field decoder, called IM-NET, for shape generation, aimed at improving the visual quality of the generated shapes. An implicit field assigns a value to each point in 3D space, so that a shape can be extracted as an iso-surface. IM-NET is trained to perform this assignment by means of a binary classifier. Specifically, it takes a point coordinate, along with a feature vector encoding a shape, and outputs a value which indicates whether the point is outside the shape or not. By replacing conventional decoders by our implicit decoder for representation learning (via IM-AE) and shape generation (via IM-GAN), we demonstrate superior results for tasks such as generative shape modeling, interpolation, and single-view 3D reconstruction, particularly in terms of visual quality. Code and supplementary material are available at https://github.com/czq142857/implicit-decoder.",,,
,,,Occupancy Networks: Learning 3D Reconstruction in Function Space,Occupancy Networks,CVPR 2019,12/10/2018,2019,"@inproceedings{mescheder2019occupancynetworks,
  BOOKTITLE = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  ID = {mescheder2019occupancynetworks},
  ENTRYTYPE = {inproceedings},
  AUTHOR = {Lars Mescheder and Michael Oechsle and Michael Niemeyer and Sebastian Nowozin and Andreas Geiger},
  TITLE = {Occupancy Networks: Learning 3D Reconstruction in Function Space},
  EPRINT = {1812.03828v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {With the advent of deep neural networks, learning-based approaches for 3D reconstruction have gained popularity. However, unlike for images, in 3D there is no canonical representation which is both computationally and memory efficient yet allows for representing high-resolution geometry of arbitrary topology. Many of the state-of-the-art learning-based 3D reconstruction approaches can hence only represent very coarse 3D geometry or are limited to a restricted domain. In this paper, we propose Occupancy Networks, a new representation for learning-based 3D reconstruction methods. Occupancy networks implicitly represent the 3D surface as the continuous decision boundary of a deep neural network classifier. In contrast to existing approaches, our representation encodes a description of the 3D output at infinite resolution without excessive memory footprint. We validate that our representation can efficiently encode 3D structure and can be inferred from various kinds of input. Our experiments demonstrate competitive results, both qualitatively and quantitatively, for the challenging tasks of 3D reconstruction from single images, noisy point clouds and coarse discrete voxel grids. We believe that occupancy networks will become a useful tool in a wide variety of learning-based 3D tasks.},
  YEAR = {2019},
  URL = {http://arxiv.org/abs/1812.03828v2},
  FILE = {1812.03828v2.pdf}
 }",https://arxiv.org/pdf/1812.03828.pdf,https://avg.is.tuebingen.mpg.de/publications/occupancy-networks,https://github.com/autonomousvision/occupancy_networks,,https://www.youtube.com/watch?v=w1Qo3bOiPaE,,,"Generalization, Conditional Neural Field, Sampling",Sinusoidal Activation (SIREN),Occupancy,,,,,,,,"Lars Mescheder, Michael Oechsle, Michael Niemeyer, Sebastian Nowozin, Andreas Geiger",mescheder2019occupancynetworks,3,540,"With the advent of deep neural networks, learning-based approaches for 3D reconstruction have gained popularity. However, unlike for images, in 3D there is no canonical representation which is both computationally and memory efficient yet allows for representing high-resolution geometry of arbitrary topology. Many of the state-of-the-art learning-based 3D reconstruction approaches can hence only represent very coarse 3D geometry or are limited to a restricted domain. In this paper, we propose Occupancy Networks, a new representation for learning-based 3D reconstruction methods. Occupancy networks implicitly represent the 3D surface as the continuous decision boundary of a deep neural network classifier. In contrast to existing approaches, our representation encodes a description of the 3D output at infinite resolution without excessive memory footprint. We validate that our representation can efficiently encode 3D structure and can be inferred from various kinds of input. Our experiments demonstrate competitive results, both qualitatively and quantitatively, for the challenging tasks of 3D reconstruction from single images, noisy point clouds and coarse discrete voxel grids. We believe that occupancy networks will become a useful tool in a wide variety of learning-based 3D tasks.",,,
,,,DeepSDF: Learning Continuous Signed Distance Functions for Shape Representation,DeepSDF,CVPR 2019,1/16/2019,2019,"@inproceedings{park2019deepsdf,
  BOOKTITLE = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  ID = {park2019deepsdf},
  ENTRYTYPE = {inproceedings},
  AUTHOR = {Jeong Joon Park and Peter Florence and Julian Straub and Richard Newcombe and Steven Lovegrove},
  TITLE = {DeepSDF: Learning Continuous Signed Distance Functions for Shape Representation},
  EPRINT = {1901.05103v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Computer graphics, 3D computer vision and robotics communities have produced multiple approaches to representing 3D geometry for rendering and reconstruction. These provide trade-offs across fidelity, efficiency and compression capabilities. In this work, we introduce DeepSDF, a learned continuous Signed Distance Function (SDF) representation of a class of shapes that enables high quality shape representation, interpolation and completion from partial and noisy 3D input data. DeepSDF, like its classical counterpart, represents a shape's surface by a continuous volumetric field: the magnitude of a point in the field represents the distance to the surface boundary and the sign indicates whether the region is inside (-) or outside (+) of the shape, hence our representation implicitly encodes a shape's boundary as the zero-level-set of the learned function while explicitly representing the classification of space as being part of the shapes interior or not. While classical SDF's both in analytical or discretized voxel form typically represent the surface of a single shape, DeepSDF can represent an entire class of shapes. Furthermore, we show state-of-the-art performance for learned 3D shape representation and completion while reducing the model size by an order of magnitude compared with previous work.},
  YEAR = {2019},
  URL = {http://arxiv.org/abs/1901.05103v1},
  FILE = {1901.05103v1.pdf}
 }",https://arxiv.org/pdf/1901.05103.pdf,,https://github.com/facebookresearch/DeepSDF,,,,,"Generalization, Generative Models, Conditional Neural Field",Other,SDF,,,,,,,,"Jeong Joon Park, Peter Florence, Julian Straub, Richard Newcombe, Steven Lovegrove",park2019deepsdf,4,593,"Computer graphics, 3D computer vision and robotics communities have produced multiple approaches to representing 3D geometry for rendering and reconstruction. These provide trade-offs across fidelity, efficiency and compression capabilities. In this work, we introduce DeepSDF, a learned continuous Signed Distance Function (SDF) representation of a class of shapes that enables high quality shape representation, interpolation and completion from partial and noisy 3D input data. DeepSDF, like its classical counterpart, represents a shape's surface by a continuous volumetric field: the magnitude of a point in the field represents the distance to the surface boundary and the sign indicates whether the region is inside (-) or outside (+) of the shape, hence our representation implicitly encodes a shape's boundary as the zero-level-set of the learned function while explicitly representing the classification of space as being part of the shapes interior or not. While classical SDF's both in analytical or discretized voxel form typically represent the surface of a single shape, DeepSDF can represent an entire class of shapes. Furthermore, we show state-of-the-art performance for learned 3D shape representation and completion while reducing the model size by an order of magnitude compared with previous work.",,,
,,,Texture Fields: Learning Texture Representations in Function Space,Texture Fields,ICCV 2019,5/17/2019,2019,"@inproceedings{oechsle2019texturefields,
  BOOKTITLE = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
  ID = {oechsle2019texturefields},
  ENTRYTYPE = {inproceedings},
  AUTHOR = {Michael Oechsle and Lars Mescheder and Michael Niemeyer and Thilo Strauss and Andreas Geiger},
  TITLE = {Texture Fields: Learning Texture Representations in Function Space},
  EPRINT = {1905.07259v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {In recent years, substantial progress has been achieved in learning-based reconstruction of 3D objects. At the same time, generative models were proposed that can generate highly realistic images. However, despite this success in these closely related tasks, texture reconstruction of 3D objects has received little attention from the research community and state-of-the-art methods are either limited to comparably low resolution or constrained experimental setups. A major reason for these limitations is that common representations of texture are inefficient or hard to interface for modern deep learning techniques. In this paper, we propose Texture Fields, a novel texture representation which is based on regressing a continuous 3D function parameterized with a neural network. Our approach circumvents limiting factors like shape discretization and parameterization, as the proposed texture representation is independent of the shape representation of the 3D object. We show that Texture Fields are able to represent high frequency texture and naturally blend with modern deep learning techniques. Experimentally, we find that Texture Fields compare favorably to state-of-the-art methods for conditional texture reconstruction of 3D objects and enable learning of probabilistic generative models for texturing unseen 3D models. We believe that Texture Fields will become an important building block for the next generation of generative 3D models.},
  YEAR = {2019},
  URL = {http://arxiv.org/abs/1905.07259v1},
  FILE = {1905.07259v1.pdf}
 }",https://arxiv.org/pdf/1905.07259.pdf,https://autonomousvision.github.io/texture-fields/,https://github.com/autonomousvision/texture_fields,,https://www.youtube.com/watch?v=pbfeE0qmD2E,http://www.cvlibs.net/publications/Oechsle2019ICCV_supplementary.pdf,,"Generalization, Generative Models, Conditional Neural Field, Data-Driven Method",,,,,,,,,,"Michael Oechsle, Lars Mescheder, Michael Niemeyer, Thilo Strauss, Andreas Geiger",oechsle2019texturefields,5,77,"In recent years, substantial progress has been achieved in learning-based reconstruction of 3D objects. At the same time, generative models were proposed that can generate highly realistic images. However, despite this success in these closely related tasks, texture reconstruction of 3D objects has received little attention from the research community and state-of-the-art methods are either limited to comparably low resolution or constrained experimental setups. A major reason for these limitations is that common representations of texture are inefficient or hard to interface for modern deep learning techniques. In this paper, we propose Texture Fields, a novel texture representation which is based on regressing a continuous 3D function parameterized with a neural network. Our approach circumvents limiting factors like shape discretization and parameterization, as the proposed texture representation is independent of the shape representation of the 3D object. We show that Texture Fields are able to represent high frequency texture and naturally blend with modern deep learning techniques. Experimentally, we find that Texture Fields compare favorably to state-of-the-art methods for conditional texture reconstruction of 3D objects and enable learning of probabilistic generative models for texturing unseen 3D models. We believe that Texture Fields will become an important building block for the next generation of generative 3D models.",,,
,,,Controlling Neural Level Sets,,NeurIPS 2019,5/28/2019,2019,"@inproceedings{atzmon2019controlling,
  PUBLISHER = {Curran Associates, Inc.},
  BOOKTITLE = {Advances in Neural Information Processing Systems (NeurIPS)},
  ID = {atzmon2019controlling},
  ENTRYTYPE = {inproceedings},
  AUTHOR = {Matan Atzmon and Niv Haim and Lior Yariv and Ofer Israelov and Haggai Maron and Yaron Lipman},
  TITLE = {Controlling Neural Level Sets},
  EPRINT = {1905.11911v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.LG},
  ABSTRACT = {The level sets of neural networks represent fundamental properties such as decision boundaries of classifiers and are used to model non-linear manifold data such as curves and surfaces. Thus, methods for controlling the neural level sets could find many applications in machine learning. In this paper we present a simple and scalable approach to directly control level sets of a deep neural network. Our method consists of two parts: (i) sampling of the neural level sets, and (ii) relating the samples' positions to the network parameters. The latter is achieved by a sample network that is constructed by adding a single fixed linear layer to the original network. In turn, the sample network can be used to incorporate the level set samples into a loss function of interest. We have tested our method on three different learning tasks: improving generalization to unseen data, training networks robust to adversarial attacks, and curve and surface reconstruction from point clouds. For surface reconstruction, we produce high fidelity surfaces directly from raw 3D point clouds. When training small to medium networks to be robust to adversarial attacks we obtain robust accuracy comparable to state-of-the-art methods.},
  YEAR = {2019},
  URL = {http://arxiv.org/abs/1905.11911v2},
  FILE = {1905.11911v2.pdf}
 }",https://arxiv.org/pdf/1905.11911.pdf,https://github.com/matanatz/ControllingNeuralLevelsets,,http://faust.is.tue.mpg.de/,,,,"Generalization, Fundamentals, Conditional Neural Field, Sampling, Data-Driven Method",,,,,,,,,,"Matan Atzmon, Niv Haim, Lior Yariv, Ofer Israelov, Haggai Maron, Yaron Lipman",atzmon2019controlling,6,30,"The level sets of neural networks represent fundamental properties such as decision boundaries of classifiers and are used to model non-linear manifold data such as curves and surfaces. Thus, methods for controlling the neural level sets could find many applications in machine learning. In this paper we present a simple and scalable approach to directly control level sets of a deep neural network. Our method consists of two parts: (i) sampling of the neural level sets, and (ii) relating the samples' positions to the network parameters. The latter is achieved by a sample network that is constructed by adding a single fixed linear layer to the original network. In turn, the sample network can be used to incorporate the level set samples into a loss function of interest. We have tested our method on three different learning tasks: improving generalization to unseen data, training networks robust to adversarial attacks, and curve and surface reconstruction from point clouds. For surface reconstruction, we produce high fidelity surfaces directly from raw 3D point clouds. When training small to medium networks to be robust to adversarial attacks we obtain robust accuracy comparable to state-of-the-art methods.",,,
,,,Scene Representation Networks: Continuous 3D-Structure-Aware Neural Scene Representations,SRN,CVPR 2020,6/4/2019,2020,"@inproceedings{sitzmann2020srn,
  BOOKTITLE = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  ID = {sitzmann2020srn},
  ENTRYTYPE = {inproceedings},
  AUTHOR = {Vincent Sitzmann and Michael Zollhofer and Gordon Wetzstein},
  TITLE = {Scene Representation Networks: Continuous 3D-Structure-Aware Neural Scene Representations},
  EPRINT = {1906.01618v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Unsupervised learning with generative models has the potential of discovering rich representations of 3D scenes. While geometric deep learning has explored 3D-structure-aware representations of scene geometry, these models typically require explicit 3D supervision. Emerging neural scene representations can be trained only with posed 2D images, but existing methods ignore the three-dimensional structure of scenes. We propose Scene Representation Networks (SRNs), a continuous, 3D-structure-aware scene representation that encodes both geometry and appearance. SRNs represent scenes as continuous functions that map world coordinates to a feature representation of local scene properties. By formulating the image formation as a differentiable ray-marching algorithm, SRNs can be trained end-to-end from only 2D images and their camera poses, without access to depth or shape. This formulation naturally generalizes across scenes, learning powerful geometry and appearance priors in the process. We demonstrate the potential of SRNs by evaluating them for novel view synthesis, few-shot reconstruction, joint shape and appearance interpolation, and unsupervised discovery of a non-rigid face model.},
  YEAR = {2020},
  URL = {http://arxiv.org/abs/1906.01618v2},
  FILE = {1906.01618v2.pdf}
 }",https://arxiv.org/pdf/1906.01618.pdf,https://vsitzmann.github.io/srns/,https://github.com/vsitzmann/scene-representation-networks,https://drive.google.com/drive/folders/1OkYgeRcIcLOFu1ft5mRODWNQaPJ0ps90,"https://www.youtube.com/watch?v=6vMEBWD8O20, https://slideslive.com/38922305/scene-representation-networks-continuous-3dstructureaware-neural-scene-representations",,,"Generalization, Conditional Neural Field, Hypernetwork/Meta-learning",,,,,,,,,,"Vincent Sitzmann, Michael Zollhöfer, Gordon Wetzstein",sitzmann2020srn,7,262,"Unsupervised learning with generative models has the potential of discovering rich representations of 3D scenes. While geometric deep learning has explored 3D-structure-aware representations of scene geometry, these models typically require explicit 3D supervision. Emerging neural scene representations can be trained only with posed 2D images, but existing methods ignore the three-dimensional structure of scenes. We propose Scene Representation Networks (SRNs), a continuous, 3D-structure-aware scene representation that encodes both geometry and appearance. SRNs represent scenes as continuous functions that map world coordinates to a feature representation of local scene properties. By formulating the image formation as a differentiable ray-marching algorithm, SRNs can be trained end-to-end from only 2D images and their camera poses, without access to depth or shape. This formulation naturally generalizes across scenes, learning powerful geometry and appearance priors in the process. We demonstrate the potential of SRNs by evaluating them for novel view synthesis, few-shot reconstruction, joint shape and appearance interpolation, and unsupervised discovery of a non-rigid face model.",,,
,,,Neural Volumes: Learning Dynamic Renderable Volumes from Images,NV,SIGGRAPH 2019,6/18/2019,2019,"@article{lombardi2019nv,
  PUBLISHER = {Association for Computing Machinery},
  JOURNAL = {ACM Transactions on Graphics (TOG)},
  ID = {lombardi2019nv},
  ENTRYTYPE = {article},
  AUTHOR = {Stephen Lombardi and Tomas Simon and Jason Saragih and Gabriel Schwartz and Andreas Lehrmann and Yaser Sheikh},
  TITLE = {Neural Volumes: Learning Dynamic Renderable Volumes from Images},
  EPRINT = {1906.07751v1},
  DOI = {10.1145/3306346.3323020},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.GR},
  ABSTRACT = {Modeling and rendering of dynamic scenes is challenging, as natural scenes often contain complex phenomena such as thin structures, evolving topology, translucency, scattering, occlusion, and biological motion. Mesh-based reconstruction and tracking often fail in these cases, and other approaches (e.g., light field video) typically rely on constrained viewing conditions, which limit interactivity. We circumvent these difficulties by presenting a learning-based approach to representing dynamic objects inspired by the integral projection model used in tomographic imaging. The approach is supervised directly from 2D images in a multi-view capture setting and does not require explicit reconstruction or tracking of the object. Our method has two primary components: an encoder-decoder network that transforms input images into a 3D volume representation, and a differentiable ray-marching operation that enables end-to-end training. By virtue of its 3D representation, our construction extrapolates better to novel viewpoints compared to screen-space rendering techniques. The encoder-decoder architecture learns a latent representation of a dynamic scene that enables us to produce novel content sequences not seen during training. To overcome memory limitations of voxel-based representations, we learn a dynamic irregular grid structure implemented with a warp field during ray-marching. This structure greatly improves the apparent resolution and reduces grid-like artifacts and jagged motion. Finally, we demonstrate how to incorporate surface-based representations into our volumetric-learning framework for applications where the highest resolution is required, using facial performance capture as a case in point.},
  YEAR = {2019},
  NOTE = {ACM Transactions on Graphics (SIGGRAPH 2019) 38, 4, Article 65},
  URL = {http://arxiv.org/abs/1906.07751v1},
  FILE = {1906.07751v1.pdf}
 }",https://arxiv.org/pdf/1906.07751.pdf,https://stephenlombardi.github.io/projects/neuralvolumes/,https://github.com/facebookresearch/neuralvolumes,,"https://youtu.be/JlyGNvbGKB8?t=5347, https://crossminds.ai/video/neural-volumes-learning-dynamic-renderable-volumes-from-images-606f94d175292b321dd0906f/",,,"Dynamic, Conditional Neural Field, Lifting 2D CNN Features to 3D, Hybrid Geometry Parameterization, Coordinate Re-mapping, Voxel Grid",,,,,,,,,,"Stephen Lombardi, Tomas Simon, Jason Saragih, Gabriel Schwartz, Andreas Lehrmann, Yaser Sheikh",lombardi2019nv,8,161,"Modeling and rendering of dynamic scenes is challenging, as natural scenes often contain complex phenomena such as thin structures, evolving topology, translucency, scattering, occlusion, and biological motion. Mesh-based reconstruction and tracking often fail in these cases, and other approaches (e.g., light field video) typically rely on constrained viewing conditions, which limit interactivity. We circumvent these difficulties by presenting a learning-based approach to representing dynamic objects inspired by the integral projection model used in tomographic imaging. The approach is supervised directly from 2D images in a multi-view capture setting and does not require explicit reconstruction or tracking of the object. Our method has two primary components: an encoder-decoder network that transforms input images into a 3D volume representation, and a differentiable ray-marching operation that enables end-to-end training. By virtue of its 3D representation, our construction extrapolates better to novel viewpoints compared to screen-space rendering techniques. The encoder-decoder architecture learns a latent representation of a dynamic scene that enables us to produce novel content sequences not seen during training. To overcome memory limitations of voxel-based representations, we learn a dynamic irregular grid structure implemented with a warp field during ray-marching. This structure greatly improves the apparent resolution and reduces grid-like artifacts and jagged motion. Finally, we demonstrate how to incorporate surface-based representations into our volumetric-learning framework for applications where the highest resolution is required, using facial performance capture as a case in point.",,,
,,,Learning elementary structures for 3D shape generation and matching,,ICCV 2019,8/13/2019,2019,"@inproceedings{deprelle2019learning,
  BOOKTITLE = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
  ID = {deprelle2019learning},
  ENTRYTYPE = {inproceedings},
  AUTHOR = {Theo Deprelle and Thibault Groueix and Matthew Fisher and Vladimir G. Kim and Bryan C. Russell and Mathieu Aubry},
  TITLE = {Learning elementary structures for 3D shape generation and matching},
  EPRINT = {1908.04725v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We propose to represent shapes as the deformation and combination of learnable elementary 3D structures, which are primitives resulting from training over a collection of shape. We demonstrate that the learned elementary 3D structures lead to clear improvements in 3D shape generation and matching. More precisely, we present two complementary approaches for learning elementary structures: (i) patch deformation learning and (ii) point translation learning. Both approaches can be extended to abstract structures of higher dimensions for improved results. We evaluate our method on two tasks: reconstructing ShapeNet objects and estimating dense correspondences between human scans (FAUST inter challenge). We show 16% improvement over surface deformation approaches for shape reconstruction and outperform FAUST inter challenge state of the art by 6%.},
  YEAR = {2019},
  URL = {http://arxiv.org/abs/1908.04725v2},
  FILE = {1908.04725v2.pdf}
 }",https://arxiv.org/pdf/1908.04725.pdf,,,,,,,"Surface Reconstruction, Conditional Neural Field",,Atlas,,,,,,,,"Theo Deprelle, Thibault Groueix, Matthew Fisher, Vladimir G. Kim, Bryan C. Russell, Mathieu Aubry",deprelle2019learning,9,61,"We propose to represent shapes as the deformation and combination of learnable elementary 3D structures, which are primitives resulting from training over a collection of shape. We demonstrate that the learned elementary 3D structures lead to clear improvements in 3D shape generation and matching. More precisely, we present two complementary approaches for learning elementary structures: (i) patch deformation learning and (ii) point translation learning. Both approaches can be extended to abstract structures of higher dimensions for improved results. We evaluate our method on two tasks: reconstructing ShapeNet objects and estimating dense correspondences between human scans (FAUST inter challenge). We show 16% improvement over surface deformation approaches for shape reconstruction and outperform FAUST inter challenge state of the art by 6%.",,,
,,,Reconstructing continuous distributions of 3D protein structure from cryo-EM images,cryoDRGN,ICLR 2020,9/11/2019,2020,"@inproceedings{zhong2020cryodrgn,
  BOOKTITLE = {International Conference on Learning Representations},
  ID = {zhong2020cryodrgn},
  ENTRYTYPE = {inproceedings},
  AUTHOR = {Ellen D. Zhong and Tristan Bepler and Joseph H. Davis and Bonnie Berger},
  TITLE = {Reconstructing continuous distributions of 3D protein structure from cryo-EM images},
  EPRINT = {1909.05215v3},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {q-bio.QM},
  ABSTRACT = {Cryo-electron microscopy (cryo-EM) is a powerful technique for determining the structure of proteins and other macromolecular complexes at near-atomic resolution. In single particle cryo-EM, the central problem is to reconstruct the three-dimensional structure of a macromolecule from $10^{4-7}$ noisy and randomly oriented two-dimensional projections. However, the imaged protein complexes may exhibit structural variability, which complicates reconstruction and is typically addressed using discrete clustering approaches that fail to capture the full range of protein dynamics. Here, we introduce a novel method for cryo-EM reconstruction that extends naturally to modeling continuous generative factors of structural heterogeneity. This method encodes structures in Fourier space using coordinate-based deep neural networks, and trains these networks from unlabeled 2D cryo-EM images by combining exact inference over image orientation with variational inference for structural heterogeneity. We demonstrate that the proposed method, termed cryoDRGN, can perform ab initio reconstruction of 3D protein complexes from simulated and real 2D cryo-EM image data. To our knowledge, cryoDRGN is the first neural network-based approach for cryo-EM reconstruction and the first end-to-end method for directly reconstructing continuous ensembles of protein structures from cryo-EM images.},
  YEAR = {2020},
  NOTE = {International Conference on Learning Representations (ICLR), 2020},
  URL = {http://arxiv.org/abs/1909.05215v3},
  FILE = {1909.05215v3.pdf}
 }",https://arxiv.org/pdf/1909.05215.pdf,http://cb.csail.mit.edu/cb/cryodrgn/,https://github.com/zhonge/cryodrgn,,https://www.youtube.com/watch?v=zd6YcUyDhPE,,,"Beyond Visual Computing, Alternative Imaging, Physics-Informed Neural Networks/Science & Engineering, Conditional Neural Field, Lifting 2D CNN Features to 3D, Data-Driven Method",Fourier Feature (NeRF),Electron density,,,,,,,,"Ellen D. Zhong, Tristan Bepler, Joseph H. Davis, Bonnie Berger",zhong2020cryodrgn,10,,"Cryo-electron microscopy (cryo-EM) is a powerful technique for determining the structure of proteins and other macromolecular complexes at near-atomic resolution. In single particle cryo-EM, the central problem is to reconstruct the three-dimensional structure of a macromolecule from $10^{4-7}$ noisy and randomly oriented two-dimensional projections. However, the imaged protein complexes may exhibit structural variability, which complicates reconstruction and is typically addressed using discrete clustering approaches that fail to capture the full range of protein dynamics. Here, we introduce a novel method for cryo-EM reconstruction that extends naturally to modeling continuous generative factors of structural heterogeneity. This method encodes structures in Fourier space using coordinate-based deep neural networks, and trains these networks from unlabeled 2D cryo-EM images by combining exact inference over image orientation with variational inference for structural heterogeneity. We demonstrate that the proposed method, termed cryoDRGN, can perform ab initio reconstruction of 3D protein complexes from simulated and real 2D cryo-EM image data. To our knowledge, cryoDRGN is the first neural network-based approach for cryo-EM reconstruction and the first end-to-end method for directly reconstructing continuous ensembles of protein structures from cryo-EM images.",No,,
,,,Local Deep Implicit Functions for 3D Shape,LDIF,CVPR 2020,12/12/2019,2020,"@inproceedings{genova2020ldif,
  BOOKTITLE = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  ID = {genova2020ldif},
  ENTRYTYPE = {inproceedings},
  AUTHOR = {Kyle Genova and Forrester Cole and Avneesh Sud and Aaron Sarna and Thomas Funkhouser},
  TITLE = {Local Deep Implicit Functions for 3D Shape},
  EPRINT = {1912.06126v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {The goal of this project is to learn a 3D shape representation that enables accurate surface reconstruction, compact storage, efficient computation, consistency for similar shapes, generalization across diverse shape categories, and inference from depth camera observations. Towards this end, we introduce Local Deep Implicit Functions (LDIF), a 3D shape representation that decomposes space into a structured set of learned implicit functions. We provide networks that infer the space decomposition and local deep implicit functions from a 3D mesh or posed depth image. During experiments, we find that it provides 10.3 points higher surface reconstruction accuracy (F-Score) than the state-of-the-art (OccNet), while requiring fewer than 1 percent of the network parameters. Experiments on posed depth image completion and generalization to unseen classes show 15.8 and 17.8 point improvements over the state-of-the-art, while producing a structured 3D representation for each input with consistency across diverse shape collections.},
  YEAR = {2020},
  URL = {http://arxiv.org/abs/1912.06126v2},
  FILE = {1912.06126v2.pdf}
 }",https://arxiv.org/pdf/1912.06126.pdf,https://ldif.cs.princeton.edu/,https://github.com/google/ldif,,https://www.youtube.com/watch?v=3RAITzNWVJs,,,"Human (Body), Conditional Neural Field, Hybrid Geometry Parameterization, Data-Driven Method",,Occupancy,,,,,,,,"Kyle Genova, Forrester Cole, Avneesh Sud, Aaron Sarna, Thomas Funkhouser",genova2020ldif,11,70,"The goal of this project is to learn a 3D shape representation that enables accurate surface reconstruction, compact storage, efficient computation, consistency for similar shapes, generalization across diverse shape categories, and inference from depth camera observations. Towards this end, we introduce Local Deep Implicit Functions (LDIF), a 3D shape representation that decomposes space into a structured set of learned implicit functions. We provide networks that infer the space decomposition and local deep implicit functions from a 3D mesh or posed depth image. During experiments, we find that it provides 10.3 points higher surface reconstruction accuracy (F-Score) than the state-of-the-art (OccNet), while requiring fewer than 1 percent of the network parameters. Experiments on posed depth image completion and generalization to unseen classes show 15.8 and 17.8 point improvements over the state-of-the-art, while producing a structured 3D representation for each input with consistency across diverse shape collections.",,,
,,,Differentiable Volumetric Rendering: Learning Implicit 3D Representations without 3D Supervision,DVR,CVPR 2020,12/16/2019,2020,"@inproceedings{niemeyer2020dvr,
  BOOKTITLE = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  ID = {niemeyer2020dvr},
  ENTRYTYPE = {inproceedings},
  AUTHOR = {Michael Niemeyer and Lars Mescheder and Michael Oechsle and Andreas Geiger},
  TITLE = {Differentiable Volumetric Rendering: Learning Implicit 3D Representations without 3D Supervision},
  EPRINT = {1912.07372v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Learning-based 3D reconstruction methods have shown impressive results. However, most methods require 3D supervision which is often hard to obtain for real-world datasets. Recently, several works have proposed differentiable rendering techniques to train reconstruction models from RGB images. Unfortunately, these approaches are currently restricted to voxel- and mesh-based representations, suffering from discretization or low resolution. In this work, we propose a differentiable rendering formulation for implicit shape and texture representations. Implicit representations have recently gained popularity as they represent shape and texture continuously. Our key insight is that depth gradients can be derived analytically using the concept of implicit differentiation. This allows us to learn implicit shape and texture representations directly from RGB images. We experimentally show that our single-view reconstructions rival those learned with full 3D supervision. Moreover, we find that our method can be used for multi-view 3D reconstruction, directly resulting in watertight meshes.},
  YEAR = {2020},
  URL = {http://arxiv.org/abs/1912.07372v2},
  FILE = {1912.07372v2.pdf}
 }",https://arxiv.org/pdf/1912.07372.pdf,https://www.youtube.com/watch?v=U_jIN3qWVEw,https://github.com/autonomousvision/differentiable_volumetric_rendering,,https://www.youtube.com/watch?v=U_jIN3qWVEw,http://www.cvlibs.net/publications/Niemeyer2020CVPR_supplementary.pdf,https://www.youtube.com/watch?v=lcub1KH-mmk,Conditional Neural Field,,Occupancy,,,,,,,,"Michael Niemeyer, Lars Mescheder, Michael Oechsle, Andreas Geiger",niemeyer2020dvr,12,142,"Learning-based 3D reconstruction methods have shown impressive results. However, most methods require 3D supervision which is often hard to obtain for real-world datasets. Recently, several works have proposed differentiable rendering techniques to train reconstruction models from RGB images. Unfortunately, these approaches are currently restricted to voxel- and mesh-based representations, suffering from discretization or low resolution. In this work, we propose a differentiable rendering formulation for implicit shape and texture representations. Implicit representations have recently gained popularity as they represent shape and texture continuously. Our key insight is that depth gradients can be derived analytically using the concept of implicit differentiation. This allows us to learn implicit shape and texture representations directly from RGB images. We experimentally show that our single-view reconstructions rival those learned with full 3D supervision. Moreover, we find that our method can be used for multi-view 3D reconstruction, directly resulting in watertight meshes.",,,
,,,Semantic Implicit Neural Scene Representations With Semi-Supervised Training,,3DV 2020,3/28/2020,2020,"@article{kohli2020semantic,
  ID = {kohli2020semantic},
  ENTRYTYPE = {article},
  AUTHOR = {Amit Kohli and Vincent Sitzmann and Gordon Wetzstein},
  TITLE = {Semantic Implicit Neural Scene Representations With Semi-Supervised Training},
  EPRINT = {2003.12673v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {The recent success of implicit neural scene representations has presented a viable new method for how we capture and store 3D scenes. Unlike conventional 3D representations, such as point clouds, which explicitly store scene properties in discrete, localized units, these implicit representations encode a scene in the weights of a neural network which can be queried at any coordinate to produce these same scene properties. Thus far, implicit representations have primarily been optimized to estimate only the appearance and/or 3D geometry information in a scene. We take the next step and demonstrate that an existing implicit representation (SRNs) is actually multi-modal; it can be further leveraged to perform per-point semantic segmentation while retaining its ability to represent appearance and geometry. To achieve this multi-modal behavior, we utilize a semi-supervised learning strategy atop the existing pre-trained scene representation. Our method is simple, general, and only requires a few tens of labeled 2D segmentation masks in order to achieve dense 3D semantic segmentation. We explore two novel applications for this semantically aware implicit neural scene representation: 3D novel view and semantic label synthesis given only a single input RGB image or 2D label mask, as well as 3D interpolation of appearance and semantics.},
  YEAR = {2020},
  MONTH = {Mar},
  URL = {http://arxiv.org/abs/2003.12673v2},
  FILE = {2003.12673v2.pdf}
 }",https://arxiv.org/pdf/2003.12673.pdf,http://www.computationalimaging.org/publications/semantic-srn/,,,https://www.youtube.com/watch?v=iVubC_ymE5w,,,"Generative Models, Conditional Neural Field",Fourier Feature (NeRF),,,,,,,,,"Amit Kohli, Vincent Sitzmann, Gordon Wetzstein",kohli2020semantic,13,7,"The recent success of implicit neural scene representations has presented a viable new method for how we capture and store 3D scenes. Unlike conventional 3D representations, such as point clouds, which explicitly store scene properties in discrete, localized units, these implicit representations encode a scene in the weights of a neural network which can be queried at any coordinate to produce these same scene properties. Thus far, implicit representations have primarily been optimized to estimate only the appearance and/or 3D geometry information in a scene. We take the next step and demonstrate that an existing implicit representation (SRNs) is actually multi-modal; it can be further leveraged to perform per-point semantic segmentation while retaining its ability to represent appearance and geometry. To achieve this multi-modal behavior, we utilize a semi-supervised learning strategy atop the existing pre-trained scene representation. Our method is simple, general, and only requires a few tens of labeled 2D segmentation masks in order to achieve dense 3D semantic segmentation. We explore two novel applications for this semantically aware implicit neural scene representation: 3D novel view and semantic label synthesis given only a single input RGB image or 2D label mask, as well as 3D interpolation of appearance and semantics.",,,
,,,DualSDF: Semantic Shape Manipulation using a Two-Level Representation,DualSDF,CVPR 2020,4/6/2020,2020,"@inproceedings{hao2020dualsdf,
  BOOKTITLE = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  ID = {hao2020dualsdf},
  ENTRYTYPE = {inproceedings},
  AUTHOR = {Zekun Hao and Hadar Averbuch-Elor and Noah Snavely and Serge Belongie},
  TITLE = {DualSDF: Semantic Shape Manipulation using a Two-Level Representation},
  EPRINT = {2004.02869v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We are seeing a Cambrian explosion of 3D shape representations for use in machine learning. Some representations seek high expressive power in capturing high-resolution detail. Other approaches seek to represent shapes as compositions of simple parts, which are intuitive for people to understand and easy to edit and manipulate. However, it is difficult to achieve both fidelity and interpretability in the same representation. We propose DualSDF, a representation expressing shapes at two levels of granularity, one capturing fine details and the other representing an abstracted proxy shape using simple and semantically consistent shape primitives. To achieve a tight coupling between the two representations, we use a variational objective over a shared latent space. Our two-level model gives rise to a new shape manipulation technique in which a user can interactively manipulate the coarse proxy shape and see the changes instantly mirrored in the high-resolution shape. Moreover, our model actively augments and guides the manipulation towards producing semantically meaningful shapes, making complex manipulations possible with minimal user input.},
  YEAR = {2020},
  URL = {http://arxiv.org/abs/2004.02869v1},
  FILE = {2004.02869v1.pdf}
 }",https://arxiv.org/pdf/2004.02869.pdf,https://www.cs.cornell.edu/~hadarelor/dualsdf/,https://github.com/zekunhao1995/DualSDF,,https://www.youtube.com/watch?v=pAszEMLd5Xk,,https://www.youtube.com/watch?v=u40ZwDINz0A,"Editable, Conditional Neural Field, Hybrid Geometry Parameterization, Data-Driven Method",Fourier Feature (NeRF),SDF,,,,,,,,"Zekun Hao, Hadar Averbuch-Elor, Noah Snavely, Serge Belongie",hao2020dualsdf,14,21,"We are seeing a Cambrian explosion of 3D shape representations for use in machine learning. Some representations seek high expressive power in capturing high-resolution detail. Other approaches seek to represent shapes as compositions of simple parts, which are intuitive for people to understand and easy to edit and manipulate. However, it is difficult to achieve both fidelity and interpretability in the same representation. We propose DualSDF, a representation expressing shapes at two levels of granularity, one capturing fine details and the other representing an abstracted proxy shape using simple and semantically consistent shape primitives. To achieve a tight coupling between the two representations, we use a variational objective over a shared latent space. Our two-level model gives rise to a new shape manipulation technique in which a user can interactively manipulate the coarse proxy shape and see the changes instantly mirrored in the high-resolution shape. Moreover, our model actively augments and guides the manipulation towards producing semantically meaningful shapes, making complex manipulations possible with minimal user input.",,,
,,,ARCH: Animatable Reconstruction of Clothed Humans,ARCH,CVPR 2020,4/8/2020,2020,"@inproceedings{huang2020arch,
  BOOKTITLE = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  ID = {huang2020arch},
  ENTRYTYPE = {inproceedings},
  AUTHOR = {Zeng Huang and Yuanlu Xu and Christoph Lassner and Hao Li and Tony Tung},
  TITLE = {ARCH: Animatable Reconstruction of Clothed Humans},
  EPRINT = {2004.04572v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.GR},
  ABSTRACT = {In this paper, we propose ARCH (Animatable Reconstruction of Clothed Humans), a novel end-to-end framework for accurate reconstruction of animation-ready 3D clothed humans from a monocular image. Existing approaches to digitize 3D humans struggle to handle pose variations and recover details. Also, they do not produce models that are animation ready. In contrast, ARCH is a learned pose-aware model that produces detailed 3D rigged full-body human avatars from a single unconstrained RGB image. A Semantic Space and a Semantic Deformation Field are created using a parametric 3D body estimator. They allow the transformation of 2D/3D clothed humans into a canonical space, reducing ambiguities in geometry caused by pose variations and occlusions in training data. Detailed surface geometry and appearance are learned using an implicit function representation with spatial local features. Furthermore, we propose additional per-pixel supervision on the 3D reconstruction using opacity-aware differentiable rendering. Our experiments indicate that ARCH increases the fidelity of the reconstructed humans. We obtain more than 50% lower reconstruction errors for standard metrics compared to state-of-the-art methods on public datasets. We also show numerous qualitative examples of animated, high-quality reconstructed avatars unseen in the literature so far.},
  YEAR = {2020},
  URL = {http://arxiv.org/abs/2004.04572v2},
  FILE = {2004.04572v2.pdf}
 }",https://arxiv.org/pdf/2004.04572.pdf,https://vgl.ict.usc.edu/Research/ARCH/,,,https://www.youtube.com/watch?v=DG3QNMcmTvo,,,"Human (Body), Conditional Neural Field, Lifting 2D CNN Features to 3D, Voxel Grid, Feature volume, Coordinate Re-mapping, Data-Driven Method",Fourier Feature (NeRF),Occupancy,,,,,,,,"Zeng Huang, Yuanlu Xu, Christoph Lassner, Hao Li, Tony Tung",huang2020arch,15,,"In this paper, we propose ARCH (Animatable Reconstruction of Clothed Humans), a novel end-to-end framework for accurate reconstruction of animation-ready 3D clothed humans from a monocular image. Existing approaches to digitize 3D humans struggle to handle pose variations and recover details. Also, they do not produce models that are animation ready. In contrast, ARCH is a learned pose-aware model that produces detailed 3D rigged full-body human avatars from a single unconstrained RGB image. A Semantic Space and a Semantic Deformation Field are created using a parametric 3D body estimator. They allow the transformation of 2D/3D clothed humans into a canonical space, reducing ambiguities in geometry caused by pose variations and occlusions in training data. Detailed surface geometry and appearance are learned using an implicit function representation with spatial local features. Furthermore, we propose additional per-pixel supervision on the 3D reconstruction using opacity-aware differentiable rendering. Our experiments indicate that ARCH increases the fidelity of the reconstructed humans. We obtain more than 50% lower reconstruction errors for standard metrics compared to state-of-the-art methods on public datasets. We also show numerous qualitative examples of animated, high-quality reconstructed avatars unseen in the literature so far.",,,
,,,Geo-PIFu: Geometry and Pixel Aligned Implicit Functions for Single-view Human Reconstruction,Geo-PIFu,NeurIPS 2020,6/15/2020,2020,"@inproceedings{he2020geopifu,
  PUBLISHER = {Curran Associates, Inc.},
  BOOKTITLE = {Advances in Neural Information Processing Systems (NeurIPS)},
  ID = {he2020geopifu},
  ENTRYTYPE = {inproceedings},
  AUTHOR = {Tong He and John Collomosse and Hailin Jin and Stefano Soatto},
  TITLE = {Geo-PIFu: Geometry and Pixel Aligned Implicit Functions for Single-view Human Reconstruction},
  EPRINT = {2006.08072v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We propose Geo-PIFu, a method to recover a 3D mesh from a monocular color image of a clothed person. Our method is based on a deep implicit function-based representation to learn latent voxel features using a structure-aware 3D U-Net, to constrain the model in two ways: first, to resolve feature ambiguities in query point encoding, second, to serve as a coarse human shape proxy to regularize the high-resolution mesh and encourage global shape regularity. We show that, by both encoding query points and constraining global shape using latent voxel features, the reconstruction we obtain for clothed human meshes exhibits less shape distortion and improved surface details compared to competing methods. We evaluate Geo-PIFu on a recent human mesh public dataset that is $10 \times$ larger than the private commercial dataset used in PIFu and previous derivative work. On average, we exceed the state of the art by $42.7\%$ reduction in Chamfer and Point-to-Surface Distances, and $19.4\%$ reduction in normal estimation errors.},
  YEAR = {2020},
  URL = {http://arxiv.org/abs/2006.08072v2},
  FILE = {2006.08072v2.pdf}
 }",https://arxiv.org/pdf/2006.08072.pdf,,https://github.com/simpleig/Geo-PIFu,,,,,"Human (Body), Conditional Neural Field, Lifting 2D CNN Features to 3D, Voxel Grid, Feature volume",Fourier Feature (NeRF),Occupancy,,,,,,,,"Tong He, John Collomosse, Hailin Jin, Stefano Soatto",he2020geopifu,16,,"We propose Geo-PIFu, a method to recover a 3D mesh from a monocular color image of a clothed person. Our method is based on a deep implicit function-based representation to learn latent voxel features using a structure-aware 3D U-Net, to constrain the model in two ways: first, to resolve feature ambiguities in query point encoding, second, to serve as a coarse human shape proxy to regularize the high-resolution mesh and encourage global shape regularity. We show that, by both encoding query points and constraining global shape using latent voxel features, the reconstruction we obtain for clothed human meshes exhibits less shape distortion and improved surface details compared to competing methods. We evaluate Geo-PIFu on a recent human mesh public dataset that is $10 \times$ larger than the private commercial dataset used in PIFu and previous derivative work. On average, we exceed the state of the art by $42.7\%$ reduction in Chamfer and Point-to-Surface Distances, and $19.4\%$ reduction in normal estimation errors.",,Yes,
,,,GRAF: Generative Radiance Fields for 3D-Aware Image Synthesis,GRAF,NeurIPS 2020,7/5/2020,2020,"@inproceedings{schwarz2020graf,
  PUBLISHER = {Curran Associates, Inc.},
  BOOKTITLE = {Advances in Neural Information Processing Systems (NeurIPS)},
  ID = {schwarz2020graf},
  ENTRYTYPE = {inproceedings},
  AUTHOR = {Katja Schwarz and Yiyi Liao and Michael Niemeyer and Andreas Geiger},
  TITLE = {GRAF: Generative Radiance Fields for 3D-Aware Image Synthesis},
  EPRINT = {2007.02442v4},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {While 2D generative adversarial networks have enabled high-resolution image synthesis, they largely lack an understanding of the 3D world and the image formation process. Thus, they do not provide precise control over camera viewpoint or object pose. To address this problem, several recent approaches leverage intermediate voxel-based representations in combination with differentiable rendering. However, existing methods either produce low image resolution or fall short in disentangling camera and scene properties, e.g., the object identity may vary with the viewpoint. In this paper, we propose a generative model for radiance fields which have recently proven successful for novel view synthesis of a single scene. In contrast to voxel-based representations, radiance fields are not confined to a coarse discretization of the 3D space, yet allow for disentangling camera and scene properties while degrading gracefully in the presence of reconstruction ambiguity. By introducing a multi-scale patch-based discriminator, we demonstrate synthesis of high-resolution images while training our model from unposed 2D images alone. We systematically analyze our approach on several challenging synthetic and real-world datasets. Our experiments reveal that radiance fields are a powerful representation for generative image synthesis, leading to 3D consistent models that render with high fidelity.},
  YEAR = {2020},
  NOTE = {Advances in Neural Information Processing Systems, NeurIPS 2020},
  URL = {http://arxiv.org/abs/2007.02442v4},
  FILE = {2007.02442v4.pdf}
 }",https://arxiv.org/pdf/2007.02442.pdf,,https://github.com/autonomousvision/graf,,https://www.youtube.com/watch?v=akQf7WaCOHo,,,"Generalization, Generative Models, Conditional Neural Field, Lifting 2D CNN Features to 3D, Data-Driven Method",Fourier Feature (NeRF),,,,,,,,,"Katja Schwarz, Yiyi Liao, Michael Niemeyer, Andreas Geiger",schwarz2020graf,17,61,"While 2D generative adversarial networks have enabled high-resolution image synthesis, they largely lack an understanding of the 3D world and the image formation process. Thus, they do not provide precise control over camera viewpoint or object pose. To address this problem, several recent approaches leverage intermediate voxel-based representations in combination with differentiable rendering. However, existing methods either produce low image resolution or fall short in disentangling camera and scene properties, e.g., the object identity may vary with the viewpoint. In this paper, we propose a generative model for radiance fields which have recently proven successful for novel view synthesis of a single scene. In contrast to voxel-based representations, radiance fields are not confined to a coarse discretization of the 3D space, yet allow for disentangling camera and scene properties while degrading gracefully in the presence of reconstruction ambiguity. By introducing a multi-scale patch-based discriminator, we demonstrate synthesis of high-resolution images while training our model from unposed 2D images alone. We systematically analyze our approach on several challenging synthetic and real-world datasets. Our experiments reveal that radiance fields are a powerful representation for generative image synthesis, leading to 3D consistent models that render with high fidelity.",,,
,,,Coupling Explicit and Implicit Surface Representations for Generative 3D Modeling,HybridNet,ECCV 2020,7/20/2020,2020,"@inproceedings{poursaeed2020hybridnet,
  BOOKTITLE = {Proceedings of the European Conference on Computer Vision (ECCV)},
  ID = {poursaeed2020hybridnet},
  ENTRYTYPE = {inproceedings},
  AUTHOR = {Omid Poursaeed and Matthew Fisher and Noam Aigerman and Vladimir G. Kim},
  TITLE = {Coupling Explicit and Implicit Surface Representations for Generative 3D Modeling},
  EPRINT = {2007.10294v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We propose a novel neural architecture for representing 3D surfaces, which harnesses two complementary shape representations: (i) an explicit representation via an atlas, i.e., embeddings of 2D domains into 3D; (ii) an implicit-function representation, i.e., a scalar function over the 3D volume, with its levels denoting surfaces. We make these two representations synergistic by introducing novel consistency losses that ensure that the surface created from the atlas aligns with the level-set of the implicit function. Our hybrid architecture outputs results which are superior to the output of the two equivalent single-representation networks, yielding smoother explicit surfaces with more accurate normals, and a more accurate implicit occupancy function. Additionally, our surface reconstruction step can directly leverage the explicit atlas-based representation. This process is computationally efficient, and can be directly used by differentiable rasterizers, enabling training our hybrid representation with image-based losses.},
  YEAR = {2020},
  URL = {http://arxiv.org/abs/2007.10294v2},
  FILE = {2007.10294v2.pdf}
 }",https://arxiv.org/pdf/2007.10294.pdf,https://omidpoursaeed.github.io/publication/hybrid/,,,https://drive.google.com/file/d/1wwnp6HlDdfYw19__ESxWdTQfmc8Bf--v/view,https://omidpoursaeed.github.io/pdf/HybridNet_Supp.pdf,,"Surface Reconstruction, Conditional Neural Field, Hybrid Geometry Parameterization",,"Occupancy, Atlas",,,,,,,,"Omid Poursaeed, Matthew Fisher, Noam Aigerman, Vladimir G. Kim",poursaeed2020hybridnet,18,,"We propose a novel neural architecture for representing 3D surfaces, which harnesses two complementary shape representations: (i) an explicit representation via an atlas, i.e., embeddings of 2D domains into 3D; (ii) an implicit-function representation, i.e., a scalar function over the 3D volume, with its levels denoting surfaces. We make these two representations synergistic by introducing novel consistency losses that ensure that the surface created from the atlas aligns with the level-set of the implicit function. Our hybrid architecture outputs results which are superior to the output of the two equivalent single-representation networks, yielding smoother explicit surfaces with more accurate normals, and a more accurate implicit occupancy function. Additionally, our surface reconstruction step can directly leverage the explicit atlas-based representation. This process is computationally efficient, and can be directly used by differentiable rasterizers, enabling training our hybrid representation with image-based losses.",No,,
,,,Continuous Object Representation Networks: Novel View Synthesis without Target View Supervision,CORN,NeurIPS 2020,7/30/2020,2020,"@inproceedings{hani2020corn,
  PUBLISHER = {Curran Associates, Inc.},
  BOOKTITLE = {Advances in Neural Information Processing Systems (NeurIPS)},
  ID = {hani2020corn},
  ENTRYTYPE = {inproceedings},
  AUTHOR = {Nicolai Hani and Selim Engin and Jun-Jee Chao and Volkan Isler},
  TITLE = {Continuous Object Representation Networks: Novel View Synthesis without Target View Supervision},
  EPRINT = {2007.15627v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Novel View Synthesis (NVS) is concerned with synthesizing views under camera viewpoint transformations from one or multiple input images. NVS requires explicit reasoning about 3D object structure and unseen parts of the scene to synthesize convincing results. As a result, current approaches typically rely on supervised training with either ground truth 3D models or multiple target images. We propose Continuous Object Representation Networks (CORN), a conditional architecture that encodes an input image's geometry and appearance that map to a 3D consistent scene representation. We can train CORN with only two source images per object by combining our model with a neural renderer. A key feature of CORN is that it requires no ground truth 3D models or target view supervision. Regardless, CORN performs well on challenging tasks such as novel view synthesis and single-view 3D reconstruction and achieves performance comparable to state-of-the-art approaches that use direct supervision. For up-to-date information, data, and code, please see our project page: https://nicolaihaeni.github.io/corn/.},
  YEAR = {2020},
  URL = {http://arxiv.org/abs/2007.15627v2},
  FILE = {2007.15627v2.pdf}
 }",https://arxiv.org/pdf/2007.15627.pdf,,,,,,,"Sparse Reconstruction, Generalization, Conditional Neural Field, Lifting 2D CNN Features to 3D, Image-based Rendering, Data-Driven Method",,,,,,,,,,"Nicolai Häni, Selim Engin, Jun-Jee Chao, Volkan Isler",hani2020corn,19,5,"Novel View Synthesis (NVS) is concerned with synthesizing views under camera viewpoint transformations from one or multiple input images. NVS requires explicit reasoning about 3D object structure and unseen parts of the scene to synthesize convincing results. As a result, current approaches typically rely on supervised training with either ground truth 3D models or multiple target images. We propose Continuous Object Representation Networks (CORN), a conditional architecture that encodes an input image's geometry and appearance that map to a 3D consistent scene representation. We can train CORN with only two source images per object by combining our model with a neural renderer. A key feature of CORN is that it requires no ground truth 3D models or target view supervision. Regardless, CORN performs well on challenging tasks such as novel view synthesis and single-view 3D reconstruction and achieves performance comparable to state-of-the-art approaches that use direct supervision. For up-to-date information, data, and code, please see our project page: https://nicolaihaeni.github.io/corn/.",,,
,,,PatchNets: Patch-Based Generalizable Deep Implicit 3D Shape Representations,PatchNets,ECCV 2020,8/4/2020,2020,"@inproceedings{tretschk2020patchnets,
  BOOKTITLE = {Proceedings of the European Conference on Computer Vision (ECCV)},
  ID = {tretschk2020patchnets},
  ENTRYTYPE = {inproceedings},
  AUTHOR = {Edgar Tretschk and Ayush Tewari and Vladislav Golyanik and Michael Zollhofer and Carsten Stoll and Christian Theobalt},
  TITLE = {PatchNets: Patch-Based Generalizable Deep Implicit 3D Shape Representations},
  EPRINT = {2008.01639v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Implicit surface representations, such as signed-distance functions, combined with deep learning have led to impressive models which can represent detailed shapes of objects with arbitrary topology. Since a continuous function is learned, the reconstructions can also be extracted at any arbitrary resolution. However, large datasets such as ShapeNet are required to train such models. In this paper, we present a new mid-level patch-based surface representation. At the level of patches, objects across different categories share similarities, which leads to more generalizable models. We then introduce a novel method to learn this patch-based representation in a canonical space, such that it is as object-agnostic as possible. We show that our representation trained on one category of objects from ShapeNet can also well represent detailed shapes from any other category. In addition, it can be trained using much fewer shapes, compared to existing approaches. We show several applications of our new representation, including shape interpolation and partial point cloud completion. Due to explicit control over positions, orientations and scales of patches, our representation is also more controllable compared to object-level representations, which enables us to deform encoded shapes non-rigidly.},
  YEAR = {2020},
  URL = {http://arxiv.org/abs/2008.01639v2},
  FILE = {2008.01639v2.pdf}
 }",https://arxiv.org/pdf/2008.01639.pdf,http://gvv.mpi-inf.mpg.de/projects/PatchNets/,https://github.com/edgar-tr/patchnets,,"http://gvv.mpi-inf.mpg.de/projects/PatchNets/data/patchnets_short.mp4, http://gvv.mpi-inf.mpg.de/projects/PatchNets/data/patchnets_supplemental.mp4, http://gvv.mpi-inf.mpg.de/projects/PatchNets/data/patchnets_talk.mp4",http://gvv.mpi-inf.mpg.de/projects/PatchNets/data/patchnets_supplemental.pdf,,"Human (Body), Generalization, Conditional Neural Field, Volume partitioning, Data-Driven Method",,SDF,,,,,,,,"Edgar Tretschk, Ayush Tewari, Vladislav Golyanik, Michael Zollhöfer, Carsten Stoll, Christian Theobalt",tretschk2020patchnets,20,16,"Implicit surface representations, such as signed-distance functions, combined with deep learning have led to impressive models which can represent detailed shapes of objects with arbitrary topology. Since a continuous function is learned, the reconstructions can also be extracted at any arbitrary resolution. However, large datasets such as ShapeNet are required to train such models. In this paper, we present a new mid-level patch-based surface representation. At the level of patches, objects across different categories share similarities, which leads to more generalizable models. We then introduce a novel method to learn this patch-based representation in a canonical space, such that it is as object-agnostic as possible. We show that our representation trained on one category of objects from ShapeNet can also well represent detailed shapes from any other category. In addition, it can be trained using much fewer shapes, compared to existing approaches. We show several applications of our new representation, including shape interpolation and partial point cloud completion. Due to explicit control over positions, orientations and scales of patches, our representation is also more controllable compared to object-level representations, which enables us to deform encoded shapes non-rigidly.",,,
,,,Pix2Surf: Learning Parametric 3D Surface Models of Objects from Images,Pix2Surf,ECCV 2020,8/18/2020,2020,"@inproceedings{lei2020pix2surf,
  BOOKTITLE = {Proceedings of the European Conference on Computer Vision (ECCV)},
  ID = {lei2020pix2surf},
  ENTRYTYPE = {inproceedings},
  AUTHOR = {Jiahui Lei and Srinath Sridhar and Paul Guerrero and Minhyuk Sung and Niloy Mitra and Leonidas J. Guibas},
  TITLE = {Pix2Surf: Learning Parametric 3D Surface Models of Objects from Images},
  EPRINT = {2008.07760v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We investigate the problem of learning to generate 3D parametric surface representations for novel object instances, as seen from one or more views. Previous work on learning shape reconstruction from multiple views uses discrete representations such as point clouds or voxels, while continuous surface generation approaches lack multi-view consistency. We address these issues by designing neural networks capable of generating high-quality parametric 3D surfaces which are also consistent between views. Furthermore, the generated 3D surfaces preserve accurate image pixel to 3D surface point correspondences, allowing us to lift texture information to reconstruct shapes with rich geometry and appearance. Our method is supervised and trained on a public dataset of shapes from common object categories. Quantitative results indicate that our method significantly outperforms previous work, while qualitative results demonstrate the high quality of our reconstructions.},
  YEAR = {2020},
  URL = {http://arxiv.org/abs/2008.07760v1},
  FILE = {2008.07760v1.pdf}
 }",https://arxiv.org/pdf/2008.07760.pdf,https://geometry.stanford.edu/projects/pix2surf/,https://github.com/JiahuiLei/Pix2Surf,,https://www.youtube.com/watch?v=jaxB0VSuvms,https://geometry.stanford.edu/projects/pix2surf/pub/pix2surf_supp.pdf,,"Sparse Reconstruction, Generalization, Generative Models, Conditional Neural Field, Lifting 2D CNN Features to 3D",Fourier Feature (NeRF),"Atlas, Explicit",,,,,,,,"Jiahui Lei, Srinath Sridhar, Paul Guerrero, Minhyuk Sung, Niloy Mitra, Leonidas J. Guibas",lei2020pix2surf,21,,"We investigate the problem of learning to generate 3D parametric surface representations for novel object instances, as seen from one or more views. Previous work on learning shape reconstruction from multiple views uses discrete representations such as point clouds or voxels, while continuous surface generation approaches lack multi-view consistency. We address these issues by designing neural networks capable of generating high-quality parametric 3D surfaces which are also consistent between views. Furthermore, the generated 3D surfaces preserve accurate image pixel to 3D surface point correspondences, allowing us to lift texture information to reconstruct shapes with rich geometry and appearance. Our method is supervised and trained on a public dataset of shapes from common object categories. Quantitative results indicate that our method significantly outperforms previous work, while qualitative results demonstrate the high quality of our reconstructions.",No,,
,,,Neural Scene Graphs for Dynamic Scenes,,CVPR 2021 (Oral),11/20/2020,2021,"@inproceedings{ost2021neural,
  BOOKTITLE = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  ID = {ost2021neural},
  ENTRYTYPE = {inproceedings},
  AUTHOR = {Julian Ost and Fahim Mannan and Nils Thuerey and Julian Knodt and Felix Heide},
  TITLE = {Neural Scene Graphs for Dynamic Scenes},
  EPRINT = {2011.10379v3},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Recent implicit neural rendering methods have demonstrated that it is possible to learn accurate view synthesis for complex scenes by predicting their volumetric density and color supervised solely by a set of RGB images. However, existing methods are restricted to learning efficient representations of static scenes that encode all scene objects into a single neural network, and lack the ability to represent dynamic scenes and decompositions into individual scene objects. In this work, we present the first neural rendering method that decomposes dynamic scenes into scene graphs. We propose a learned scene graph representation, which encodes object transformation and radiance, to efficiently render novel arrangements and views of the scene. To this end, we learn implicitly encoded scenes, combined with a jointly learned latent representation to describe objects with a single implicit function. We assess the proposed method on synthetic and real automotive data, validating that our approach learns dynamic scenes -- only by observing a video of this scene -- and allows for rendering novel photo-realistic views of novel scene compositions with unseen sets of objects at unseen poses.},
  YEAR = {2021},
  URL = {http://arxiv.org/abs/2011.10379v3},
  FILE = {2011.10379v3.pdf}
 }",https://arxiv.org/pdf/2011.10379.pdf,https://light.princeton.edu/publication/neural-scene-graphs/,,,https://www.youtube.com/watch?v=ea4Y6P0Hk3o,https://light.cs.princeton.edu/wp-content/uploads/2021/02/NeuralSceneGraphs_Supplement.pdf,,"Dynamic, Segmentation/composition, Beyond Visual Computing, Conditional Neural Field, Object-Centric Hybrid Geometry Parameterization",Fourier Feature (NeRF),,,,,,,,,"Julian Ost, Fahim Mannan, Nils Thuerey, Julian Knodt, Felix Heide",ost2021neural,22,12,"Recent implicit neural rendering methods have demonstrated that it is possible to learn accurate view synthesis for complex scenes by predicting their volumetric density and color supervised solely by a set of RGB images. However, existing methods are restricted to learning efficient representations of static scenes that encode all scene objects into a single neural network, and lack the ability to represent dynamic scenes and decompositions into individual scene objects. In this work, we present the first neural rendering method that decomposes dynamic scenes into scene graphs. We propose a learned scene graph representation, which encodes object transformation and radiance, to efficiently render novel arrangements and views of the scene. To this end, we learn implicitly encoded scenes, combined with a jointly learned latent representation to describe objects with a single implicit function. We assess the proposed method on synthetic and real automotive data, validating that our approach learns dynamic scenes -- only by observing a video of this scene -- and allows for rendering novel photo-realistic views of novel scene compositions with unseen sets of objects at unseen poses.",,,
,,,GIRAFFE: Representing Scenes as Compositional Generative Neural Feature Fields,GIRAFFE,"CVPR 2021 (Oral, Best Paper Award)",11/24/2020,2021,"@inproceedings{niemeyer2021giraffe,
  BOOKTITLE = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  ID = {niemeyer2021giraffe},
  ENTRYTYPE = {inproceedings},
  AUTHOR = {Michael Niemeyer and Andreas Geiger},
  TITLE = {GIRAFFE: Representing Scenes as Compositional Generative Neural Feature Fields},
  EPRINT = {2011.12100v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Deep generative models allow for photorealistic image synthesis at high resolutions. But for many applications, this is not enough: content creation also needs to be controllable. While several recent works investigate how to disentangle underlying factors of variation in the data, most of them operate in 2D and hence ignore that our world is three-dimensional. Further, only few works consider the compositional nature of scenes. Our key hypothesis is that incorporating a compositional 3D scene representation into the generative model leads to more controllable image synthesis. Representing scenes as compositional generative neural feature fields allows us to disentangle one or multiple objects from the background as well as individual objects' shapes and appearances while learning from unstructured and unposed image collections without any additional supervision. Combining this scene representation with a neural rendering pipeline yields a fast and realistic image synthesis model. As evidenced by our experiments, our model is able to disentangle individual objects and allows for translating and rotating them in the scene as well as changing the camera pose.},
  YEAR = {2021},
  URL = {http://arxiv.org/abs/2011.12100v2},
  FILE = {2011.12100v2.pdf}
 }",https://arxiv.org/pdf/2011.12100.pdf,,,,,,,"Segmentation/composition, Generative Models, Conditional Neural Field",Fourier Feature (NeRF),,,,,,,,,"Michael Niemeyer, Andreas Geiger",niemeyer2021giraffe,23,21,"Deep generative models allow for photorealistic image synthesis at high resolutions. But for many applications, this is not enough: content creation also needs to be controllable. While several recent works investigate how to disentangle underlying factors of variation in the data, most of them operate in 2D and hence ignore that our world is three-dimensional. Further, only few works consider the compositional nature of scenes. Our key hypothesis is that incorporating a compositional 3D scene representation into the generative model leads to more controllable image synthesis. Representing scenes as compositional generative neural feature fields allows us to disentangle one or multiple objects from the background as well as individual objects' shapes and appearances while learning from unstructured and unposed image collections without any additional supervision. Combining this scene representation with a neural rendering pipeline yields a fast and realistic image synthesis model. As evidenced by our experiments, our model is able to disentangle individual objects and allows for translating and rotating them in the scene as well as changing the camera pose.",,,
,,,Adversarial Generation of Continuous Images,INR-GAN,CVPR 2021,11/24/2020,2021,"@inproceedings{skorokhodov2021inrgan,
  BOOKTITLE = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  ID = {skorokhodov2021inrgan},
  ENTRYTYPE = {inproceedings},
  AUTHOR = {Ivan Skorokhodov and Savva Ignatyev and Mohamed Elhoseiny},
  TITLE = {Adversarial Generation of Continuous Images},
  EPRINT = {2011.12026v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {In most existing learning systems, images are typically viewed as 2D pixel arrays. However, in another paradigm gaining popularity, a 2D image is represented as an implicit neural representation (INR) - an MLP that predicts an RGB pixel value given its (x,y) coordinate. In this paper, we propose two novel architectural techniques for building INR-based image decoders: factorized multiplicative modulation and multi-scale INRs, and use them to build a state-of-the-art continuous image GAN. Previous attempts to adapt INRs for image generation were limited to MNIST-like datasets and do not scale to complex real-world data. Our proposed INR-GAN architecture improves the performance of continuous image generators by several times, greatly reducing the gap between continuous image GANs and pixel-based ones. Apart from that, we explore several exciting properties of the INR-based decoders, like out-of-the-box superresolution, meaningful image-space interpolation, accelerated inference of low-resolution images, an ability to extrapolate outside of image boundaries, and strong geometric prior. The project page is located at https://universome.github.io/inr-gan.},
  YEAR = {2021},
  URL = {http://arxiv.org/abs/2011.12026v2},
  FILE = {2011.12026v2.pdf}
 }",https://arxiv.org/pdf/2011.12026.pdf,https://universome.github.io/inr-gan,https://github.com/universome/inr-gan,,,,,"Generalization, Image, Generative Models, Conditional Neural Field, Hypernetwork/Meta-learning",,,,,,,,,,"Ivan Skorokhodov, Savva Ignatyev, Mohamed Elhoseiny",skorokhodov2021inrgan,24,7,"In most existing learning systems, images are typically viewed as 2D pixel arrays. However, in another paradigm gaining popularity, a 2D image is represented as an implicit neural representation (INR) - an MLP that predicts an RGB pixel value given its (x,y) coordinate. In this paper, we propose two novel architectural techniques for building INR-based image decoders: factorized multiplicative modulation and multi-scale INRs, and use them to build a state-of-the-art continuous image GAN. Previous attempts to adapt INRs for image generation were limited to MNIST-like datasets and do not scale to complex real-world data. Our proposed INR-GAN architecture improves the performance of continuous image generators by several times, greatly reducing the gap between continuous image GANs and pixel-based ones. Apart from that, we explore several exciting properties of the INR-based decoders, like out-of-the-box superresolution, meaningful image-space interpolation, accelerated inference of low-resolution images, an ability to extrapolate outside of image boundaries, and strong geometric prior. The project page is located at https://universome.github.io/inr-gan.",,,
,,,D-NeRF: Deformable Neural Radiance Fields,"D-NeRF, Nerfies",ICCV 2021,11/25/2020,2021,"@inproceedings{park2021dnerf,
  BOOKTITLE = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
  ID = {park2021dnerf},
  ENTRYTYPE = {inproceedings},
  AUTHOR = {Keunhong Park and Utkarsh Sinha and Jonathan T. Barron and Sofien Bouaziz and Dan B Goldman and Steven M. Seitz and Ricardo Martin-Brualla},
  TITLE = {Nerfies: Deformable Neural Radiance Fields},
  EPRINT = {2011.12948v5},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We present the first method capable of photorealistically reconstructing deformable scenes using photos/videos captured casually from mobile phones. Our approach augments neural radiance fields (NeRF) by optimizing an additional continuous volumetric deformation field that warps each observed point into a canonical 5D NeRF. We observe that these NeRF-like deformation fields are prone to local minima, and propose a coarse-to-fine optimization method for coordinate-based models that allows for more robust optimization. By adapting principles from geometry processing and physical simulation to NeRF-like models, we propose an elastic regularization of the deformation field that further improves robustness. We show that our method can turn casually captured selfie photos/videos into deformable NeRF models that allow for photorealistic renderings of the subject from arbitrary viewpoints, which we dub ""nerfies."" We evaluate our method by collecting time-synchronized data using a rig with two mobile phones, yielding train/validation images of the same pose at different viewpoints. We show that our method faithfully reconstructs non-rigidly deforming scenes and reproduces unseen views with high fidelity.},
  YEAR = {2021},
  URL = {http://arxiv.org/abs/2011.12948v5},
  FILE = {2011.12948v5.pdf}
 }",https://arxiv.org/pdf/2011.12948.pdf,https://nerfies.github.io/,,,https://www.youtube.com/watch?v=MrKrnHhk8IA,,,"Dynamic, Conditional Neural Field, Coarse-to-Fine, Coordinate Re-mapping",,,,,,,,,,"Keunhong Park, Utkarsh Sinha, Jonathan T. Barron, Sofien Bouaziz, Dan B Goldman, Steven M. Seitz, Ricardo Martin-Brualla",park2021dnerf,25,54,"We present the first method capable of photorealistically reconstructing deformable scenes using photos/videos captured casually from mobile phones. Our approach augments neural radiance fields (NeRF) by optimizing an additional continuous volumetric deformation field that warps each observed point into a canonical 5D NeRF. We observe that these NeRF-like deformation fields are prone to local minima, and propose a coarse-to-fine optimization method for coordinate-based models that allows for more robust optimization. By adapting principles from geometry processing and physical simulation to NeRF-like models, we propose an elastic regularization of the deformation field that further improves robustness. We show that our method can turn casually captured selfie photos/videos into deformable NeRF models that allow for photorealistic renderings of the subject from arbitrary viewpoints, which we dub ""nerfies."" We evaluate our method by collecting time-synchronized data using a rig with two mobile phones, yielding train/validation images of the same pose at different viewpoints. We show that our method faithfully reconstructs non-rigidly deforming scenes and reproduces unseen views with high fidelity.",,,
,,,i3DMM: Deep Implicit 3D Morphable Model of Human Heads,i3DMM,CVPR 2021,11/28/2020,2021,"@inproceedings{yenamandra2021i3dmm,
  BOOKTITLE = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  ID = {yenamandra2021i3dmm},
  ENTRYTYPE = {inproceedings},
  AUTHOR = {Tarun Yenamandra and Ayush Tewari and Florian Bernard and Hans-Peter Seidel and Mohamed Elgharib and Daniel Cremers and Christian Theobalt},
  TITLE = {i3DMM: Deep Implicit 3D Morphable Model of Human Heads},
  EPRINT = {2011.14143v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We present the first deep implicit 3D morphable model (i3DMM) of full heads. Unlike earlier morphable face models it not only captures identity-specific geometry, texture, and expressions of the frontal face, but also models the entire head, including hair. We collect a new dataset consisting of 64 people with different expressions and hairstyles to train i3DMM. Our approach has the following favorable properties: (i) It is the first full head morphable model that includes hair. (ii) In contrast to mesh-based models it can be trained on merely rigidly aligned scans, without requiring difficult non-rigid registration. (iii) We design a novel architecture to decouple the shape model into an implicit reference shape and a deformation of this reference shape. With that, dense correspondences between shapes can be learned implicitly. (iv) This architecture allows us to semantically disentangle the geometry and color components, as color is learned in the reference space. Geometry is further disentangled as identity, expressions, and hairstyle, while color is disentangled as identity and hairstyle components. We show the merits of i3DMM using ablation studies, comparisons to state-of-the-art models, and applications such as semantic head editing and texture transfer. We will make our model publicly available.},
  YEAR = {2021},
  URL = {http://arxiv.org/abs/2011.14143v1},
  FILE = {2011.14143v1.pdf}
 }",https://arxiv.org/pdf/2011.14143.pdf,,Coming soon,,https://www.youtube.com/watch?v=4pYzV3ButPY,,,"Dynamic, Human (Head), Editable, Conditional Neural Field, Coordinate Re-mapping",Fourier Feature (NeRF),SDF,,,,,,,,"Tarun Yenamandra, Ayush Tewari, Florian Bernard, Hans-Peter Seidel, Mohamed Elgharib, Daniel Cremers, Christian Theobalt",yenamandra2021i3dmm,26,3,"We present the first deep implicit 3D morphable model (i3DMM) of full heads. Unlike earlier morphable face models it not only captures identity-specific geometry, texture, and expressions of the frontal face, but also models the entire head, including hair. We collect a new dataset consisting of 64 people with different expressions and hairstyles to train i3DMM. Our approach has the following favorable properties: (i) It is the first full head morphable model that includes hair. (ii) In contrast to mesh-based models it can be trained on merely rigidly aligned scans, without requiring difficult non-rigid registration. (iii) We design a novel architecture to decouple the shape model into an implicit reference shape and a deformation of this reference shape. With that, dense correspondences between shapes can be learned implicitly. (iv) This architecture allows us to semantically disentangle the geometry and color components, as color is learned in the reference space. Geometry is further disentangled as identity, expressions, and hairstyle, while color is disentangled as identity and hairstyle components. We show the merits of i3DMM using ablation studies, comparisons to state-of-the-art models, and applications such as semantic head editing and texture transfer. We will make our model publicly available.",,,
,,,Dynamic Neural Radiance Fields for Monocular 4D Facial Avatar Reconstruction,NerFACE,CVPR 2021,12/5/2020,2021,"@inproceedings{gafni2021nerface,
  BOOKTITLE = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  ID = {gafni2021nerface},
  ENTRYTYPE = {inproceedings},
  AUTHOR = {Guy Gafni and Justus Thies and Michael Zollhofer and Matthias Niessner},
  TITLE = {Dynamic Neural Radiance Fields for Monocular 4D Facial Avatar Reconstruction},
  EPRINT = {2012.03065v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We present dynamic neural radiance fields for modeling the appearance and dynamics of a human face. Digitally modeling and reconstructing a talking human is a key building-block for a variety of applications. Especially, for telepresence applications in AR or VR, a faithful reproduction of the appearance including novel viewpoints or head-poses is required. In contrast to state-of-the-art approaches that model the geometry and material properties explicitly, or are purely image-based, we introduce an implicit representation of the head based on scene representation networks. To handle the dynamics of the face, we combine our scene representation network with a low-dimensional morphable model which provides explicit control over pose and expressions. We use volumetric rendering to generate images from this hybrid representation and demonstrate that such a dynamic neural scene representation can be learned from monocular input data only, without the need of a specialized capture setup. In our experiments, we show that this learned volumetric representation allows for photo-realistic image generation that surpasses the quality of state-of-the-art video-based reenactment methods.},
  YEAR = {2021},
  URL = {http://arxiv.org/abs/2012.03065v1},
  FILE = {2012.03065v1.pdf}
 }",https://arxiv.org/pdf/2012.03065.pdf,https://gafniguy.github.io/4D-Facial-Avatars/,https://github.com/gafniguy/4D-Facial-Avatars,Available upon request,"https://www.youtube.com/watch?v=XihxC65tmyA, https://www.youtube.com/watch?v=m7oROLdQnjk",,,"Human (Head), Conditional Neural Field, Data-Driven Method",,Density,,,,,,,,"Guy Gafni, Justus Thies, Michael Zollhöfer, Matthias Nießner",gafni2021nerface,27,18,"We present dynamic neural radiance fields for modeling the appearance and dynamics of a human face. Digitally modeling and reconstructing a talking human is a key building-block for a variety of applications. Especially, for telepresence applications in AR or VR, a faithful reproduction of the appearance including novel viewpoints or head-poses is required. In contrast to state-of-the-art approaches that model the geometry and material properties explicitly, or are purely image-based, we introduce an implicit representation of the head based on scene representation networks. To handle the dynamics of the face, we combine our scene representation network with a low-dimensional morphable model which provides explicit control over pose and expressions. We use volumetric rendering to generate images from this hybrid representation and demonstrate that such a dynamic neural scene representation can be learned from monocular input data only, without the need of a specialized capture setup. In our experiments, we show that this learned volumetric representation allows for photo-realistic image generation that surpasses the quality of state-of-the-art video-based reenactment methods.",,,
,,,Deep Optimized Priors for 3D Shape Modeling and Reconstruction,,CVPR 2021,12/14/2020,2021,"@inproceedings{yang2021deep,
  BOOKTITLE = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  ID = {yang2021deep},
  ENTRYTYPE = {inproceedings},
  AUTHOR = {Mingyue Yang and Yuxin Wen and Weikai Chen and Yongwei Chen and Kui Jia},
  TITLE = {Deep Optimized Priors for 3D Shape Modeling and Reconstruction},
  EPRINT = {2012.07241v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Many learning-based approaches have difficulty scaling to unseen data, as the generality of its learned prior is limited to the scale and variations of the training samples. This holds particularly true with 3D learning tasks, given the sparsity of 3D datasets available. We introduce a new learning framework for 3D modeling and reconstruction that greatly improves the generalization ability of a deep generator. Our approach strives to connect the good ends of both learning-based and optimization-based methods. In particular, unlike the common practice that fixes the pre-trained priors at test time, we propose to further optimize the learned prior and latent code according to the input physical measurements after the training. We show that the proposed strategy effectively breaks the barriers constrained by the pre-trained priors and could lead to high-quality adaptation to unseen data. We realize our framework using the implicit surface representation and validate the efficacy of our approach in a variety of challenging tasks that take highly sparse or collapsed observations as input. Experimental results show that our approach compares favorably with the state-of-the-art methods in terms of both generality and accuracy.},
  YEAR = {2021},
  URL = {http://arxiv.org/abs/2012.07241v1},
  FILE = {2012.07241v1.pdf}
 }",https://arxiv.org/pdf/2012.07241.pdf,,,,,,,"Generalization, Conditional Neural Field, Per-instance fine-tuning, Data-Driven Method",,,,,,,,,,"Mingyue Yang, Yuxin Wen, Weikai Chen, Yongwei Chen, Kui Jia",yang2021deep,28,3,"Many learning-based approaches have difficulty scaling to unseen data, as the generality of its learned prior is limited to the scale and variations of the training samples. This holds particularly true with 3D learning tasks, given the sparsity of 3D datasets available. We introduce a new learning framework for 3D modeling and reconstruction that greatly improves the generalization ability of a deep generator. Our approach strives to connect the good ends of both learning-based and optimization-based methods. In particular, unlike the common practice that fixes the pre-trained priors at test time, we propose to further optimize the learned prior and latent code according to the input physical measurements after the training. We show that the proposed strategy effectively breaks the barriers constrained by the pre-trained priors and could lead to high-quality adaptation to unseen data. We realize our framework using the implicit surface representation and validate the efficacy of our approach in a variety of challenging tasks that take highly sparse or collapsed observations as input. Experimental results show that our approach compares favorably with the state-of-the-art methods in terms of both generality and accuracy.",,,
,,,Learning Continuous Image Representation with Local Implicit Image Function,LIIF,CVPR 2021 Oral,12/16/2020,2021,"@article{chenOralliif,
  ID = {chenOralliif},
  ENTRYTYPE = {article},
  AUTHOR = {Yinbo Chen and Sifei Liu and Xiaolong Wang},
  TITLE = {Learning Continuous Image Representation with Local Implicit Image Function},
  EPRINT = {2012.09161v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {How to represent an image? While the visual world is presented in a continuous manner, machines store and see the images in a discrete way with 2D arrays of pixels. In this paper, we seek to learn a continuous representation for images. Inspired by the recent progress in 3D reconstruction with implicit neural representation, we propose Local Implicit Image Function (LIIF), which takes an image coordinate and the 2D deep features around the coordinate as inputs, predicts the RGB value at a given coordinate as an output. Since the coordinates are continuous, LIIF can be presented in arbitrary resolution. To generate the continuous representation for images, we train an encoder with LIIF representation via a self-supervised task with super-resolution. The learned continuous representation can be presented in arbitrary resolution even extrapolate to x30 higher resolution, where the training tasks are not provided. We further show that LIIF representation builds a bridge between discrete and continuous representation in 2D, it naturally supports the learning tasks with size-varied image ground-truths and significantly outperforms the method with resizing the ground-truths.},
  YEAR = {2020},
  MONTH = {Dec},
  URL = {http://arxiv.org/abs/2012.09161v2},
  FILE = {2012.09161v2.pdf}
 }",https://arxiv.org/pdf/2012.09161.pdf,https://yinboc.github.io/liif/,https://github.com/yinboc/liif,,https://www.youtube.com/watch?v=6f2roieSY_8,,,"Image, Fundamentals, Conditional Neural Field, Hybrid Geometry Parameterization, Data-Driven Method",,,,,,,,,,"Yinbo Chen, Sifei Liu, Xiaolong Wang",chenoralliif,29,10,"How to represent an image? While the visual world is presented in a continuous manner, machines store and see the images in a discrete way with 2D arrays of pixels. In this paper, we seek to learn a continuous representation for images. Inspired by the recent progress in 3D reconstruction with implicit neural representation, we propose Local Implicit Image Function (LIIF), which takes an image coordinate and the 2D deep features around the coordinate as inputs, predicts the RGB value at a given coordinate as an output. Since the coordinates are continuous, LIIF can be presented in arbitrary resolution. To generate the continuous representation for images, we train an encoder with LIIF representation via a self-supervised task with super-resolution. The learned continuous representation can be presented in arbitrary resolution even extrapolate to x30 higher resolution, where the training tasks are not provided. We further show that LIIF representation builds a bridge between discrete and continuous representation in 2D, it naturally supports the learning tasks with size-varied image ground-truths and significantly outperforms the method with resizing the ground-truths.",,,
,,,Non-Rigid Neural Radiance Fields: Reconstruction and Novel View Synthesis of a Dynamic Scene From Monocular Video,NR-NeRF,ICCV 2021,12/22/2020,2021,"@inproceedings{tretschk2021nrnerf,
  BOOKTITLE = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
  ID = {tretschk2021nrnerf},
  ENTRYTYPE = {inproceedings},
  AUTHOR = {Edgar Tretschk and Ayush Tewari and Vladislav Golyanik and Michael Zollhofer and Christoph Lassner and Christian Theobalt},
  TITLE = {Non-Rigid Neural Radiance Fields: Reconstruction and Novel View Synthesis of a Dynamic Scene From Monocular Video},
  EPRINT = {2012.12247v4},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We present Non-Rigid Neural Radiance Fields (NR-NeRF), a reconstruction and novel view synthesis approach for general non-rigid dynamic scenes. Our approach takes RGB images of a dynamic scene as input (e.g., from a monocular video recording), and creates a high-quality space-time geometry and appearance representation. We show that a single handheld consumer-grade camera is sufficient to synthesize sophisticated renderings of a dynamic scene from novel virtual camera views, e.g. a `bullet-time' video effect. NR-NeRF disentangles the dynamic scene into a canonical volume and its deformation. Scene deformation is implemented as ray bending, where straight rays are deformed non-rigidly. We also propose a novel rigidity network to better constrain rigid regions of the scene, leading to more stable results. The ray bending and rigidity network are trained without explicit supervision. Our formulation enables dense correspondence estimation across views and time, and compelling video editing applications such as motion exaggeration. Our code will be open sourced.},
  YEAR = {2021},
  URL = {http://arxiv.org/abs/2012.12247v4},
  FILE = {2012.12247v4.pdf}
 }",https://arxiv.org/pdf/2012.12247.pdf,,,,,,,"Dynamic, Conditional Neural Field, Coordinate Re-mapping",,,,,,,,,,"Edgar Tretschk, Ayush Tewari, Vladislav Golyanik, Michael Zollhöfer, Christoph Lassner, Christian Theobalt",tretschk2021nrnerf,30,18,"We present Non-Rigid Neural Radiance Fields (NR-NeRF), a reconstruction and novel view synthesis approach for general non-rigid dynamic scenes. Our approach takes RGB images of a dynamic scene as input, e.g., from a monocular video recording, and creates a high-quality space-time geometry and appearance representation. In particular, we show that even a single handheld consumer-grade camera is sufficient to synthesize sophisticated renderings of a dynamic scene from novel virtual camera views, for example a `bullet-time' video effect. Our method disentangles the dynamic scene into a canonical volume and its deformation. Scene deformation is implemented as ray bending, where straight rays are deformed non-rigidly to represent scene motion. We also propose a novel rigidity regression network that enables us to better constrain rigid regions of the scene, which leads to more stable results. The ray bending and rigidity network are trained without any explicit supervision. In addition to novel view synthesis, our formulation enables dense correspondence estimation across views and time, as well as compelling video editing applications such as motion exaggeration. We demonstrate the effectiveness of our method using extensive evaluations, including ablation studies and comparisons to the state of the art. We urge the reader to watch the supplemental video for qualitative results. Our code will be open sourced.",,,
,,,"S3: Neural Shape, Skeleton, and Skinning Fields for 3D Human Modeling",S3,CVPR 2021,1/17/2021,2021,"@inproceedings{yang2021s3,
  BOOKTITLE = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  ID = {yang2021s3},
  ENTRYTYPE = {inproceedings},
  AUTHOR = {Ze Yang and Shenlong Wang and Sivabalan Manivasagam and Zeng Huang and Wei-Chiu Ma and Xinchen Yan and Ersin Yumer and Raquel Urtasun},
  TITLE = {S3: Neural Shape, Skeleton, and Skinning Fields for 3D Human Modeling},
  EPRINT = {2101.06571v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Constructing and animating humans is an important component for building virtual worlds in a wide variety of applications such as virtual reality or robotics testing in simulation. As there are exponentially many variations of humans with different shape, pose and clothing, it is critical to develop methods that can automatically reconstruct and animate humans at scale from real world data. Towards this goal, we represent the pedestrian's shape, pose and skinning weights as neural implicit functions that are directly learned from data. This representation enables us to handle a wide variety of different pedestrian shapes and poses without explicitly fitting a human parametric body model, allowing us to handle a wider range of human geometries and topologies. We demonstrate the effectiveness of our approach on various datasets and show that our reconstructions outperform existing state-of-the-art methods. Furthermore, our re-animation experiments show that we can generate 3D human animations at scale from a single RGB image (and/or an optional LiDAR sweep) as input.},
  YEAR = {2021},
  URL = {http://arxiv.org/abs/2101.06571v1},
  FILE = {2101.06571v1.pdf}
 }",https://arxiv.org/pdf/2101.06571.pdf,,,,,,,"Human (Body), Editable, Conditional Neural Field, Voxel Grid, Feature volume",,Occupancy,,,,,,,,"Ze Yang, Shenlong Wang, Sivabalan Manivasagam, Zeng Huang, Wei-Chiu Ma, Xinchen Yan, Ersin Yumer, Raquel Urtasun",yang2021s3,31,,"Constructing and animating humans is an important component for building virtual worlds in a wide variety of applications such as virtual reality or robotics testing in simulation. As there are exponentially many variations of humans with different shape, pose and clothing, it is critical to develop methods that can automatically reconstruct and animate humans at scale from real world data. Towards this goal, we represent the pedestrian's shape, pose and skinning weights as neural implicit functions that are directly learned from data. This representation enables us to handle a wide variety of different pedestrian shapes and poses without explicitly fitting a human parametric body model, allowing us to handle a wider range of human geometries and topologies. We demonstrate the effectiveness of our approach on various datasets and show that our reconstructions outperform existing state-of-the-art methods. Furthermore, our re-animation experiments show that we can generate 3D human animations at scale from a single RGB image (and/or an optional LiDAR sweep) as input.",,No,
,,,Neural Geometric Level of Detail: Real-time Rendering with Implicit 3D Shapes,,CVPR 2021,1/26/2021,2021,"@inproceedings{takikawa2021neural,
  BOOKTITLE = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  ID = {takikawa2021neural},
  ENTRYTYPE = {inproceedings},
  AUTHOR = {Towaki Takikawa and Joey Litalien and Kangxue Yin and Karsten Kreis and Charles Loop and Derek Nowrouzezahrai and Alec Jacobson and Morgan McGuire and Sanja Fidler},
  TITLE = {Neural Geometric Level of Detail: Real-time Rendering with Implicit 3D Shapes},
  EPRINT = {2101.10994v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Neural signed distance functions (SDFs) are emerging as an effective representation for 3D shapes. State-of-the-art methods typically encode the SDF with a large, fixed-size neural network to approximate complex shapes with implicit surfaces. Rendering with these large networks is, however, computationally expensive since it requires many forward passes through the network for every pixel, making these representations impractical for real-time graphics. We introduce an efficient neural representation that, for the first time, enables real-time rendering of high-fidelity neural SDFs, while achieving state-of-the-art geometry reconstruction quality. We represent implicit surfaces using an octree-based feature volume which adaptively fits shapes with multiple discrete levels of detail (LODs), and enables continuous LOD with SDF interpolation. We further develop an efficient algorithm to directly render our novel neural SDF representation in real-time by querying only the necessary LODs with sparse octree traversal. We show that our representation is 2-3 orders of magnitude more efficient in terms of rendering speed compared to previous works. Furthermore, it produces state-of-the-art reconstruction quality for complex shapes under both 3D geometric and 2D image-space metrics.},
  YEAR = {2021},
  URL = {http://arxiv.org/abs/2101.10994v1},
  FILE = {2101.10994v1.pdf}
 }",https://arxiv.org/pdf/2101.10994.pdf,nv-tlabs.github.io/nglod,https://github.com/nv-tlabs/nglod,,https://www.youtube.com/watch?v=Pi7W6XrFtMs,,,"Speed (Training), Speed (Rendering), Generalization, Conditional Neural Field, Coarse-to-Fine, Sampling, Voxel Grid, Hybrid Geometry Parameterization, Volume partitioning",,SDF,,,,,,,,"Towaki Takikawa, Joey Litalien, Kangxue Yin, Karsten Kreis, Charles Loop, Derek Nowrouzezahrai, Alec Jacobson, Morgan McGuire, Sanja Fidler",takikawa2021neural,32,12,"Neural signed distance functions (SDFs) are emerging as an effective representation for 3D shapes. State-of-the-art methods typically encode the SDF with a large, fixed-size neural network to approximate complex shapes with implicit surfaces. Rendering with these large networks is, however, computationally expensive since it requires many forward passes through the network for every pixel, making these representations impractical for real-time graphics. We introduce an efficient neural representation that, for the first time, enables real-time rendering of high-fidelity neural SDFs, while achieving state-of-the-art geometry reconstruction quality. We represent implicit surfaces using an octree-based feature volume which adaptively fits shapes with multiple discrete levels of detail (LODs), and enables continuous LOD with SDF interpolation. We further develop an efficient algorithm to directly render our novel neural SDF representation in real-time by querying only the necessary LODs with sparse octree traversal. We show that our representation is 2-3 orders of magnitude more efficient in terms of rendering speed compared to previous works. Furthermore, it produces state-of-the-art reconstruction quality for complex shapes under both 3D geometric and 2D image-space metrics.",,,
,,,Towards Generalising Neural Implicit Representations,,ARXIV 2021,1/29/2021,2021,"@article{costain2021towards,
  JOURNAL = {arXiv preprint arXiv:2101.12690},
  BOOKTITLE = {ArXiv Pre-print},
  ID = {costain2021towards},
  ENTRYTYPE = {article},
  AUTHOR = {Theo W. Costain and Victor Adrian Prisacariu},
  TITLE = {Towards Generalising Neural Implicit Representations},
  EPRINT = {2101.12690v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Neural implicit representations have shown substantial improvements in efficiently storing 3D data, when compared to conventional formats. However, the focus of existing work has mainly been on storage and subsequent reconstruction. In this work, we show that training neural representations for reconstruction tasks alongside conventional tasks can produce more general encodings that admit equal quality reconstructions to single task training, whilst improving results on conventional tasks when compared to single task encodings. We reformulate the semantic segmentation task, creating a more representative task for implicit representation contexts, and through multi-task experiments on reconstruction, classification, and segmentation, show our approach learns feature rich encodings that admit equal performance for each task.},
  YEAR = {2021},
  URL = {http://arxiv.org/abs/2101.12690v2},
  FILE = {2101.12690v2.pdf}
 }",https://arxiv.org/pdf/2101.12690.pdf,,,,,,,"Generalization, Beyond Visual Computing, Conditional Neural Field",,,,,,,,,,"Theo W. Costain, Victor Adrian Prisacariu",costain2021towards,33,0,"Neural implicit representations have shown substantial improvements in efficiently storing 3D data, when compared to conventional formats. However, the focus of existing work has mainly been on storage and subsequent reconstruction. In this work, we show that training neural representations for reconstruction tasks alongside conventional tasks can produce more general encodings that admit equal quality reconstructions to single task training, whilst improving results on conventional tasks when compared to single task encodings. We reformulate the semantic segmentation task, creating a more representative task for implicit representation contexts, and through multi-task experiments on reconstruction, classification, and segmentation, show our approach learns feature rich encodings that admit equal performance for each task.",,,
,,,Neural 3D Video Synthesis,DyNeRF,ARXIV 2021,3/3/2021,2021,"@article{li2021dynerf,
  JOURNAL = {arXiv preprint arXiv:2103.02597},
  BOOKTITLE = {ArXiv Pre-print},
  ID = {li2021dynerf},
  ENTRYTYPE = {article},
  AUTHOR = {Tianye Li and Mira Slavcheva and Michael Zollhoefer and Simon Green and Christoph Lassner and Changil Kim and Tanner Schmidt and Steven Lovegrove and Michael Goesele and Zhaoyang Lv},
  TITLE = {Neural 3D Video Synthesis},
  EPRINT = {2103.02597v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We propose a novel approach for 3D video synthesis that is able to represent multi-view video recordings of a dynamic real-world scene in a compact, yet expressive representation that enables high-quality view synthesis and motion interpolation. Our approach takes the high quality and compactness of static neural radiance fields in a new direction: to a model-free, dynamic setting. At the core of our approach is a novel time-conditioned neural radiance fields that represents scene dynamics using a set of compact latent codes. To exploit the fact that changes between adjacent frames of a video are typically small and locally consistent, we propose two novel strategies for efficient training of our neural network: 1) An efficient hierarchical training scheme, and 2) an importance sampling strategy that selects the next rays for training based on the temporal variation of the input videos. In combination, these two strategies significantly boost the training speed, lead to fast convergence of the training process, and enable high quality results. Our learned representation is highly compact and able to represent a 10 second 30 FPS multi-view video recording by 18 cameras with a model size of just 28MB. We demonstrate that our method can render high-fidelity wide-angle novel views at over 1K resolution, even for highly complex and dynamic scenes. We perform an extensive qualitative and quantitative evaluation that shows that our approach outperforms the current state of the art. We include additional video and information at: https://neural-3d-video.github.io/},
  YEAR = {2021},
  URL = {http://arxiv.org/abs/2103.02597v1},
  FILE = {2103.02597v1.pdf}
 }",https://arxiv.org/pdf/2103.02597.pdf,https://neural-3d-video.github.io/,,Coming soon,https://neural-3d-video.github.io/resources/video.mp4,,,"Dynamic, Conditional Neural Field",,,,,,,,,,"Tianye Li, Mira Slavcheva, Michael Zollhoefer, Simon Green, Christoph Lassner, Changil Kim, Tanner Schmidt, Steven Lovegrove, Michael Goesele, Zhaoyang Lv",li2021dynerf,34,3,"We propose a novel approach for 3D video synthesis that is able to represent multi-view video recordings of a dynamic real-world scene in a compact, yet expressive representation that enables high-quality view synthesis and motion interpolation. Our approach takes the high quality and compactness of static neural radiance fields in a new direction: to a model-free, dynamic setting. At the core of our approach is a novel time-conditioned neural radiance fields that represents scene dynamics using a set of compact latent codes. To exploit the fact that changes between adjacent frames of a video are typically small and locally consistent, we propose two novel strategies for efficient training of our neural network: 1) An efficient hierarchical training scheme, and 2) an importance sampling strategy that selects the next rays for training based on the temporal variation of the input videos. In combination, these two strategies significantly boost the training speed, lead to fast convergence of the training process, and enable high quality results. Our learned representation is highly compact and able to represent a 10 second 30 FPS multi-view video recording by 18 cameras with a model size of just 28MB. We demonstrate that our method can render high-fidelity wide-angle novel views at over 1K resolution, even for highly complex and dynamic scenes. We perform an extensive qualitative and quantitative evaluation that shows that our approach outperforms the current state of the art. We include additional video and information at: https://neural-3d-video.github.io/",,,
,,,SMPLicit: Topology-aware Generative Model for Clothed People,SMPLicit,CVPR 2021,3/11/2021,2021,"@inproceedings{corona2021smplicit,
  BOOKTITLE = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  ID = {corona2021smplicit},
  ENTRYTYPE = {inproceedings},
  AUTHOR = {Enric Corona and Albert Pumarola and Guillem Alenya and Gerard Pons-Moll and Francesc Moreno-Noguer},
  TITLE = {SMPLicit: Topology-aware Generative Model for Clothed People},
  EPRINT = {2103.06871v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {In this paper we introduce SMPLicit, a novel generative model to jointly represent body pose, shape and clothing geometry. In contrast to existing learning-based approaches that require training specific models for each type of garment, SMPLicit can represent in a unified manner different garment topologies (e.g. from sleeveless tops to hoodies and to open jackets), while controlling other properties like the garment size or tightness/looseness. We show our model to be applicable to a large variety of garments including T-shirts, hoodies, jackets, shorts, pants, skirts, shoes and even hair. The representation flexibility of SMPLicit builds upon an implicit model conditioned with the SMPL human body parameters and a learnable latent space which is semantically interpretable and aligned with the clothing attributes. The proposed model is fully differentiable, allowing for its use into larger end-to-end trainable systems. In the experimental section, we demonstrate SMPLicit can be readily used for fitting 3D scans and for 3D reconstruction in images of dressed people. In both cases we are able to go beyond state of the art, by retrieving complex garment geometries, handling situations with multiple clothing layers and providing a tool for easy outfit editing. To stimulate further research in this direction, we will make our code and model publicly available at http://www.iri.upc.edu/people/ecorona/smplicit/.},
  YEAR = {2021},
  URL = {http://arxiv.org/abs/2103.06871v2},
  FILE = {2103.06871v2.pdf}
 }",https://arxiv.org/pdf/2103.06871.pdf,http://www.iri.upc.edu/people/ecorona/smplicit/,https://github.com/enriccorona/SMPLicit,,,,,"Human (Body), Editable, Generative Models, Conditional Neural Field",,UDF,,,,,,,,"Enric Corona, Albert Pumarola, Guillem Alenyà, Gerard Pons-Moll, Francesc Moreno-Noguer",corona2021smplicit,35,,"In this paper we introduce SMPLicit, a novel generative model to jointly represent body pose, shape and clothing geometry. In contrast to existing learning-based approaches that require training specific models for each type of garment, SMPLicit can represent in a unified manner different garment topologies (e.g. from sleeveless tops to hoodies and to open jackets), while controlling other properties like the garment size or tightness/looseness. We show our model to be applicable to a large variety of garments including T-shirts, hoodies, jackets, shorts, pants, skirts, shoes and even hair. The representation flexibility of SMPLicit builds upon an implicit model conditioned with the SMPL human body parameters and a learnable latent space which is semantically interpretable and aligned with the clothing attributes. The proposed model is fully differentiable, allowing for its use into larger end-to-end trainable systems. In the experimental section, we demonstrate SMPLicit can be readily used for fitting 3D scans and for 3D reconstruction in images of dressed people. In both cases we are able to go beyond state of the art, by retrieving complex garment geometries, handling situations with multiple clothing layers and providing a tool for easy outfit editing. To stimulate further research in this direction, we will make our code and model publicly available at http://www.iri.upc.edu/people/ecorona/smplicit/.",,,
,,,AD-NeRF: Audio Driven Neural Radiance Fields for Talking Head Synthesis,AD-NeRF,ICCV 2021,3/20/2021,2021,"@inproceedings{guo2021adnerf,
  BOOKTITLE = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
  ID = {guo2021adnerf},
  ENTRYTYPE = {inproceedings},
  AUTHOR = {Yudong Guo and Keyu Chen and Sen Liang and Yong-Jin Liu and Hujun Bao and Juyong Zhang},
  TITLE = {AD-NeRF: Audio Driven Neural Radiance Fields for Talking Head Synthesis},
  EPRINT = {2103.11078v3},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Generating high-fidelity talking head video by fitting with the input audio sequence is a challenging problem that receives considerable attentions recently. In this paper, we address this problem with the aid of neural scene representation networks. Our method is completely different from existing methods that rely on intermediate representations like 2D landmarks or 3D face models to bridge the gap between audio input and video output. Specifically, the feature of input audio signal is directly fed into a conditional implicit function to generate a dynamic neural radiance field, from which a high-fidelity talking-head video corresponding to the audio signal is synthesized using volume rendering. Another advantage of our framework is that not only the head (with hair) region is synthesized as previous methods did, but also the upper body is generated via two individual neural radiance fields. Experimental results demonstrate that our novel framework can (1) produce high-fidelity and natural results, and (2) support free adjustment of audio signals, viewing directions, and background images. Code is available at https://github.com/YudongGuo/AD-NeRF.},
  YEAR = {2021},
  URL = {http://arxiv.org/abs/2103.11078v3},
  FILE = {2103.11078v3.pdf}
 }",https://arxiv.org/pdf/2103.11078.pdf,,,,https://www.youtube.com/watch?v=TQO2EBYXLyU,,,"Dynamic, Beyond Visual Computing, Audio, Conditional Neural Field, Volume partitioning",,,,,,,,,,"Yudong Guo, Keyu Chen, Sen Liang, Yongjin Liu, Hujun Bao, Juyong Zhang",guo2021adnerf,36,0,"Generating high-fidelity talking head video by fitting with the input audio sequence is a challenging problem that receives considerable attentions recently. In this paper, we address this problem with the aid of neural scene representation networks. Our method is completely different from existing methods that rely on intermediate representations like 2D landmarks or 3D face models to bridge the gap between audio input and video output. Specifically, the feature of input audio signal is directly fed into a conditional implicit function to generate a dynamic neural radiance field, from which a high-fidelity talking-head video corresponding to the audio signal is synthesized using volume rendering. Another advantage of our framework is that not only the head (with hair) region is synthesized as previous methods did, but also the upper body is generated via two individual neural radiance fields. Experimental results demonstrate that our novel framework can (1) produce high-fidelity and natural results, and (2) support free adjustment of audio signals, viewing directions, and background images. Code is available at https://github.com/YudongGuo/AD-NeRF.",,,
,,,CAMPARI: Camera-Aware Decomposed Generative Neural Radiance Fields,CAMPARI,ARXIV 2021,3/31/2021,2021,"@article{niemeyer2021campari,
  JOURNAL = {arXiv preprint arXiv:2103.17269},
  BOOKTITLE = {ArXiv Pre-print},
  ID = {niemeyer2021campari},
  ENTRYTYPE = {article},
  AUTHOR = {Michael Niemeyer and Andreas Geiger},
  TITLE = {CAMPARI: Camera-Aware Decomposed Generative Neural Radiance Fields},
  EPRINT = {2103.17269v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Tremendous progress in deep generative models has led to photorealistic image synthesis. While achieving compelling results, most approaches operate in the two-dimensional image domain, ignoring the three-dimensional nature of our world. Several recent works therefore propose generative models which are 3D-aware, i.e., scenes are modeled in 3D and then rendered differentiably to the image plane. This leads to impressive 3D consistency, but incorporating such a bias comes at a price: the camera needs to be modeled as well. Current approaches assume fixed intrinsics and a predefined prior over camera pose ranges. As a result, parameter tuning is typically required for real-world data, and results degrade if the data distribution is not matched. Our key hypothesis is that learning a camera generator jointly with the image generator leads to a more principled approach to 3D-aware image synthesis. Further, we propose to decompose the scene into a background and foreground model, leading to more efficient and disentangled scene representations. While training from raw, unposed image collections, we learn a 3D- and camera-aware generative model which faithfully recovers not only the image but also the camera data distribution. At test time, our model generates images with explicit control over the camera as well as the shape and appearance of the scene.},
  YEAR = {2021},
  URL = {http://arxiv.org/abs/2103.17269v1},
  FILE = {2103.17269v1.pdf}
 }",https://arxiv.org/pdf/2103.17269.pdf,,,,,,,"Image, Generative Models, Conditional Neural Field, Lifting 2D CNN Features to 3D, Volume partitioning, Data-Driven Method",,,,,,,,,,"Michael Niemeyer, Andreas Geiger",niemeyer2021campari,37,2,"Tremendous progress in deep generative models has led to photorealistic image synthesis. While achieving compelling results, most approaches operate in the two-dimensional image domain, ignoring the three-dimensional nature of our world. Several recent works therefore propose generative models which are 3D-aware, i.e., scenes are modeled in 3D and then rendered differentiably to the image plane. This leads to impressive 3D consistency, but incorporating such a bias comes at a price: the camera needs to be modeled as well. Current approaches assume fixed intrinsics and a predefined prior over camera pose ranges. As a result, parameter tuning is typically required for real-world data, and results degrade if the data distribution is not matched. Our key hypothesis is that learning a camera generator jointly with the image generator leads to a more principled approach to 3D-aware image synthesis. Further, we propose to decompose the scene into a background and foreground model, leading to more efficient and disentangled scene representations. While training from raw, unposed image collections, we learn a 3D- and camera-aware generative model which faithfully recovers not only the image but also the camera data distribution. At test time, our model generates images with explicit control over the camera as well as the shape and appearance of the scene.",,,
,,,NPMs: Neural Parametric Models for 3D Deformable Shapes,NPMs,ICCV 2021,4/1/2021,2021,"@inproceedings{palafox2021npms,
  BOOKTITLE = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
  ID = {palafox2021npms},
  ENTRYTYPE = {inproceedings},
  AUTHOR = {Pablo Palafox and Aljaz Bozic and Justus Thies and Matthias Niessner and Angela Dai},
  TITLE = {NPMs: Neural Parametric Models for 3D Deformable Shapes},
  EPRINT = {2104.00702v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Parametric 3D models have enabled a wide variety of tasks in computer graphics and vision, such as modeling human bodies, faces, and hands. However, the construction of these parametric models is often tedious, as it requires heavy manual tweaking, and they struggle to represent additional complexity and details such as wrinkles or clothing. To this end, we propose Neural Parametric Models (NPMs), a novel, learned alternative to traditional, parametric 3D models, which does not require hand-crafted, object-specific constraints. In particular, we learn to disentangle 4D dynamics into latent-space representations of shape and pose, leveraging the flexibility of recent developments in learned implicit functions. Crucially, once learned, our neural parametric models of shape and pose enable optimization over the learned spaces to fit to new observations, similar to the fitting of a traditional parametric model, e.g., SMPL. This enables NPMs to achieve a significantly more accurate and detailed representation of observed deformable sequences. We show that NPMs improve notably over both parametric and non-parametric state of the art in reconstruction and tracking of monocular depth sequences of clothed humans and hands. Latent-space interpolation as well as shape/pose transfer experiments further demonstrate the usefulness of NPMs. Code is publicly available at https://pablopalafox.github.io/npms.},
  YEAR = {2021},
  URL = {http://arxiv.org/abs/2104.00702v2},
  FILE = {2104.00702v2.pdf}
 }",https://arxiv.org/pdf/2104.00702.pdf,,,,,,,"Human (Body), Conditional Neural Field, Coordinate Re-mapping",,SDF,,,,,,,,"Pablo Palafox, Aljaž Božič, Justus Thies, Matthias Nießner, Angela Dai",palafox2021npms,38,2,"Parametric 3D models have enabled a wide variety of tasks in computer graphics and vision, such as modeling human bodies, faces, and hands. However, the construction of these parametric models is often tedious, as it requires heavy manual tweaking, and they struggle to represent additional complexity and details such as wrinkles or clothing. To this end, we propose Neural Parametric Models (NPMs), a novel, learned alternative to traditional, parametric 3D models, which does not require hand-crafted, object-specific constraints. In particular, we learn to disentangle 4D dynamics into latent-space representations of shape and pose, leveraging the flexibility of recent developments in learned implicit functions. Crucially, once learned, our neural parametric models of shape and pose enable optimization over the learned spaces to fit to new observations, similar to the fitting of a traditional parametric model, e.g., SMPL. This enables NPMs to achieve a significantly more accurate and detailed representation of observed deformable sequences. We show that NPMs improve notably over both parametric and non-parametric state of the art in reconstruction and tracking of monocular depth sequences of clothed humans and hands. Latent-space interpolation as well as shape/pose transfer experiments further demonstrate the usefulness of NPMs. Code is publicly available at https://pablopalafox.github.io/npms.",,,
,,,NeRF-VAE: A Geometry Aware 3D Scene Generative Model,NeRF-VAE,ARXIV 2021,4/1/2021,2021,"@article{kosiorek2021nerfvae,
  JOURNAL = {arXiv preprint arXiv:2104.00587},
  BOOKTITLE = {ArXiv Pre-print},
  ID = {kosiorek2021nerfvae},
  ENTRYTYPE = {article},
  AUTHOR = {Adam R. Kosiorek and Heiko Strathmann and Daniel Zoran and Pol Moreno and Rosalia Schneider and Sona Mokra and Danilo J. Rezende},
  TITLE = {NeRF-VAE: A Geometry Aware 3D Scene Generative Model},
  EPRINT = {2104.00587v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {stat.ML},
  ABSTRACT = {We propose NeRF-VAE, a 3D scene generative model that incorporates geometric structure via NeRF and differentiable volume rendering. In contrast to NeRF, our model takes into account shared structure across scenes, and is able to infer the structure of a novel scene -- without the need to re-train -- using amortized inference. NeRF-VAE's explicit 3D rendering process further contrasts previous generative models with convolution-based rendering which lacks geometric structure. Our model is a VAE that learns a distribution over radiance fields by conditioning them on a latent scene representation. We show that, once trained, NeRF-VAE is able to infer and render geometrically-consistent scenes from previously unseen 3D environments using very few input images. We further demonstrate that NeRF-VAE generalizes well to out-of-distribution cameras, while convolutional models do not. Finally, we introduce and study an attention-based conditioning mechanism of NeRF-VAE's decoder, which improves model performance.},
  YEAR = {2021},
  URL = {http://arxiv.org/abs/2104.00587v1},
  FILE = {2104.00587v1.pdf}
 }",https://arxiv.org/pdf/2104.00587.pdf,,,,https://www.youtube.com/watch?v=f-T3BLVuXkY,,,"Sparse Reconstruction, Generalization, Image, Generative Models, Conditional Neural Field",,,,,,,,,,"Adam R. Kosiorek, Heiko Strathmann, Daniel Zoran, Pol Moreno, Rosalia Schneider, Soňa Mokrá, Danilo J. Rezende",kosiorek2021nerfvae,39,6,"We propose NeRF-VAE, a 3D scene generative model that incorporates geometric structure via NeRF and differentiable volume rendering. In contrast to NeRF, our model takes into account shared structure across scenes, and is able to infer the structure of a novel scene -- without the need to re-train -- using amortized inference. NeRF-VAE's explicit 3D rendering process further contrasts previous generative models with convolution-based rendering which lacks geometric structure. Our model is a VAE that learns a distribution over radiance fields by conditioning them on a latent scene representation. We show that, once trained, NeRF-VAE is able to infer and render geometrically-consistent scenes from previously unseen 3D environments using very few input images. We further demonstrate that NeRF-VAE generalizes well to out-of-distribution cameras, while convolutional models do not. Finally, we introduce and study an attention-based conditioning mechanism of NeRF-VAE's decoder, which improves model performance.",,,
,,,Unconstrained Scene Generation with Locally Conditioned Radiance Fields,,ICCV 2021,4/1/2021,2021,"@inproceedings{devries2021unconstrained,
  BOOKTITLE = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
  ID = {devries2021unconstrained},
  ENTRYTYPE = {inproceedings},
  AUTHOR = {Terrance DeVries and Miguel Angel Bautista and Nitish Srivastava and Graham W. Taylor and Joshua M. Susskind},
  TITLE = {Unconstrained Scene Generation with Locally Conditioned Radiance Fields},
  EPRINT = {2104.00670v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We tackle the challenge of learning a distribution over complex, realistic, indoor scenes. In this paper, we introduce Generative Scene Networks (GSN), which learns to decompose scenes into a collection of many local radiance fields that can be rendered from a free moving camera. Our model can be used as a prior to generate new scenes, or to complete a scene given only sparse 2D observations. Recent work has shown that generative models of radiance fields can capture properties such as multi-view consistency and view-dependent lighting. However, these models are specialized for constrained viewing of single objects, such as cars or faces. Due to the size and complexity of realistic indoor environments, existing models lack the representational capacity to adequately capture them. Our decomposition scheme scales to larger and more complex scenes while preserving details and diversity, and the learned prior enables high-quality rendering from viewpoints that are significantly different from observed viewpoints. When compared to existing models, GSN produces quantitatively higher-quality scene renderings across several different scene datasets.},
  YEAR = {2021},
  URL = {http://arxiv.org/abs/2104.00670v1},
  FILE = {2104.00670v1.pdf}
 }",https://arxiv.org/pdf/2104.00670.pdf,https://apple.github.io/ml-gsn/,https://github.com/apple/ml-gsn,,,,,"Generative Models, Conditional Neural Field, Feature volume",,,,,,,,,,"Terrance DeVries, Miguel Angel Bautista, Nitish Srivastava, Graham W. Taylor, Joshua M. Susskind",devries2021unconstrained,40,0,"We tackle the challenge of learning a distribution over complex, realistic, indoor scenes. In this paper, we introduce Generative Scene Networks (GSN), which learns to decompose scenes into a collection of many local radiance fields that can be rendered from a free moving camera. Our model can be used as a prior to generate new scenes, or to complete a scene given only sparse 2D observations. Recent work has shown that generative models of radiance fields can capture properties such as multi-view consistency and view-dependent lighting. However, these models are specialized for constrained viewing of single objects, such as cars or faces. Due to the size and complexity of realistic indoor environments, existing models lack the representational capacity to adequately capture them. Our decomposition scheme scales to larger and more complex scenes while preserving details and diversity, and the learned prior enables high-quality rendering from viewpoints that are significantly different from observed viewpoints. When compared to existing models, GSN produces quantitatively higher-quality scene renderings across several different scene datasets.",,,
,,,Decomposing 3D Scenes into Objects via Unsupervised Volume Segmentation,OBSuRF,ARXIV 2021,4/2/2021,2021,"@article{stelzner2021obsurf,
  JOURNAL = {arXiv preprint arXiv:2104.01148},
  BOOKTITLE = {ArXiv Pre-print},
  ID = {stelzner2021obsurf},
  ENTRYTYPE = {article},
  AUTHOR = {Karl Stelzner and Kristian Kersting and Adam R. Kosiorek},
  TITLE = {Decomposing 3D Scenes into Objects via Unsupervised Volume Segmentation},
  EPRINT = {2104.01148v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We present ObSuRF, a method which turns a single image of a scene into a 3D model represented as a set of Neural Radiance Fields (NeRFs), with each NeRF corresponding to a different object. A single forward pass of an encoder network outputs a set of latent vectors describing the objects in the scene. These vectors are used independently to condition a NeRF decoder, defining the geometry and appearance of each object. We make learning more computationally efficient by deriving a novel loss, which allows training NeRFs on RGB-D inputs without explicit ray marching. After confirming that the model performs equal or better than state of the art on three 2D image segmentation benchmarks, we apply it to two multi-object 3D datasets: A multiview version of CLEVR, and a novel dataset in which scenes are populated by ShapeNet models. We find that after training ObSuRF on RGB-D views of training scenes, it is capable of not only recovering the 3D geometry of a scene depicted in a single input image, but also to segment it into objects, despite receiving no supervision in that regard.},
  YEAR = {2021},
  URL = {http://arxiv.org/abs/2104.01148v1},
  FILE = {2104.01148v1.pdf}
 }",https://arxiv.org/pdf/2104.01148.pdf,https://stelzner.github.io/obsurf/,Coming soon,Coming soon,,,,"Segmentation/composition, Conditional Neural Field, Volume partitioning, Object-Centric Hybrid Geometry Parameterization",,,,,,,,,,"Karl Stelzner, Kristian Kersting, Adam R. Kosiorek",stelzner2021obsurf,41,1,"We present ObSuRF, a method which turns a single image of a scene into a 3D model represented as a set of Neural Radiance Fields (NeRFs), with each NeRF corresponding to a different object. A single forward pass of an encoder network outputs a set of latent vectors describing the objects in the scene. These vectors are used independently to condition a NeRF decoder, defining the geometry and appearance of each object. We make learning more computationally efficient by deriving a novel loss, which allows training NeRFs on RGB-D inputs without explicit ray marching. After confirming that the model performs equal or better than state of the art on three 2D image segmentation benchmarks, we apply it to two multi-object 3D datasets: A multiview version of CLEVR, and a novel dataset in which scenes are populated by ShapeNet models. We find that after training ObSuRF on RGB-D views of training scenes, it is capable of not only recovering the 3D geometry of a scene depicted in a single input image, but also to segment it into objects, despite receiving no supervision in that regard.",,,
,,,pi-GAN: Periodic Implicit Generative Adversarial Networks for 3D-Aware Image Synthesis,pi-GAN,CVPR 2021,4/5/2021,2021,"@inproceedings{chan2021pigan,
  BOOKTITLE = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  ID = {chan2021pigan},
  ENTRYTYPE = {inproceedings},
  AUTHOR = {Eric R. Chan and Marco Monteiro and Petr Kellnhofer and Jiajun Wu and Gordon Wetzstein},
  TITLE = {pi-GAN: Periodic Implicit Generative Adversarial Networks for 3D-Aware Image Synthesis},
  EPRINT = {2012.00926v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We have witnessed rapid progress on 3D-aware image synthesis, leveraging recent advances in generative visual models and neural rendering. Existing approaches however fall short in two ways: first, they may lack an underlying 3D representation or rely on view-inconsistent rendering, hence synthesizing images that are not multi-view consistent; second, they often depend upon representation network architectures that are not expressive enough, and their results thus lack in image quality. We propose a novel generative model, named Periodic Implicit Generative Adversarial Networks ($\pi$-GAN or pi-GAN), for high-quality 3D-aware image synthesis. $\pi$-GAN leverages neural representations with periodic activation functions and volumetric rendering to represent scenes as view-consistent 3D representations with fine detail. The proposed approach obtains state-of-the-art results for 3D-aware image synthesis with multiple real and synthetic datasets.},
  YEAR = {2021},
  URL = {http://arxiv.org/abs/2012.00926v2},
  FILE = {2012.00926v2.pdf}
 }",https://arxiv.org/pdf/2012.00926.pdf,https://marcoamonteiro.github.io/pi-GAN-website/,Coming soon,,https://www.youtube.com/watch?v=0HCdof9BGtw,,,"Generalization, Generative Models, Conditional Neural Field",,,,,,,,,,"Eric R. Chan, Marco Monteiro, Petr Kellnhofer, Jiajun Wu, Gordon Wetzstein",chan2021pigan,42,20,"We have witnessed rapid progress on 3D-aware image synthesis, leveraging recent advances in generative visual models and neural rendering. Existing approaches however fall short in two ways: first, they may lack an underlying 3D representation or rely on view-inconsistent rendering, hence synthesizing images that are not multi-view consistent; second, they often depend upon representation network architectures that are not expressive enough, and their results thus lack in image quality. We propose a novel generative model, named Periodic Implicit Generative Adversarial Networks ($\pi$-GAN or pi-GAN), for high-quality 3D-aware image synthesis. $\pi$-GAN leverages neural representations with periodic activation functions and volumetric rendering to represent scenes as view-consistent 3D representations with fine detail. The proposed approach obtains state-of-the-art results for 3D-aware image synthesis with multiple real and synthetic datasets.",,,
,,,Modulated Periodic Activations for Generalizable Local Functional Representations,,ICCV 2021,4/8/2021,2021,"@inproceedings{mehta2021modulated,
  BOOKTITLE = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
  ID = {mehta2021modulated},
  ENTRYTYPE = {inproceedings},
  AUTHOR = {Ishit Mehta and Michael Gharbi and Connelly Barnes and Eli Shechtman and Ravi Ramamoorthi and Manmohan Chandraker},
  TITLE = {Modulated Periodic Activations for Generalizable Local Functional Representations},
  EPRINT = {2104.03960v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Multi-Layer Perceptrons (MLPs) make powerful functional representations for sampling and reconstruction problems involving low-dimensional signals like images,shapes and light fields. Recent works have significantly improved their ability to represent high-frequency content by using periodic activations or positional encodings. This often came at the expense of generalization: modern methods are typically optimized for a single signal. We present a new representation that generalizes to multiple instances and achieves state-of-the-art fidelity. We use a dual-MLP architecture to encode the signals. A synthesis network creates a functional mapping from a low-dimensional input (e.g. pixel-position) to the output domain (e.g. RGB color). A modulation network maps a latent code corresponding to the target signal to parameters that modulate the periodic activations of the synthesis network. We also propose a local-functional representation which enables generalization. The signal's domain is partitioned into a regular grid,with each tile represented by a latent code. At test time, the signal is encoded with high-fidelity by inferring (or directly optimizing) the latent code-book. Our approach produces generalizable functional representations of images, videos and shapes, and achieves higher reconstruction quality than prior works that are optimized for a single signal.},
  YEAR = {2021},
  URL = {http://arxiv.org/abs/2104.03960v1},
  FILE = {2104.03960v1.pdf}
 }",https://arxiv.org/pdf/2104.03960.pdf,,,,,,,"Generalization, Compression, Fundamentals, Conditional Neural Field, Hypernetwork/Meta-learning",,,,,,,,,,"Ishit Mehta, Michaël Gharbi, Connelly Barnes, Eli Shechtman, Ravi Ramamoorthi, Manmohan Chandraker",mehta2021modulated,43,1,"Multi-Layer Perceptrons (MLPs) make powerful functional representations for sampling and reconstruction problems involving low-dimensional signals like images,shapes and light fields. Recent works have significantly improved their ability to represent high-frequency content by using periodic activations or positional encodings. This often came at the expense of generalization: modern methods are typically optimized for a single signal. We present a new representation that generalizes to multiple instances and achieves state-of-the-art fidelity. We use a dual-MLP architecture to encode the signals. A synthesis network creates a functional mapping from a low-dimensional input (e.g. pixel-position) to the output domain (e.g. RGB color). A modulation network maps a latent code corresponding to the target signal to parameters that modulate the periodic activations of the synthesis network. We also propose a local-functional representation which enables generalization. The signal's domain is partitioned into a regular grid,with each tile represented by a latent code. At test time, the signal is encoded with high-fidelity by inferring (or directly optimizing) the latent code-book. Our approach produces generalizable functional representations of images, videos and shapes, and achieves higher reconstruction quality than prior works that are optimized for a single signal.",,,
,,,Neural RGB-D Surface Reconstruction,,ARXIV 2021,4/9/2021,2021,"@article{azinovic2021neural,
  JOURNAL = {arXiv preprint arXiv:2104.04532},
  BOOKTITLE = {ArXiv Pre-print},
  ID = {azinovic2021neural},
  ENTRYTYPE = {article},
  AUTHOR = {Dejan Azinovic and Ricardo Martin-Brualla and Dan B Goldman and Matthias Niessner and Justus Thies},
  TITLE = {Neural RGB-D Surface Reconstruction},
  EPRINT = {2104.04532v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {In this work, we explore how to leverage the success of implicit novel view synthesis methods for surface reconstruction. Methods which learn a neural radiance field have shown amazing image synthesis results, but the underlying geometry representation is only a coarse approximation of the real geometry. We demonstrate how depth measurements can be incorporated into the radiance field formulation to produce more detailed and complete reconstruction results than using methods based on either color or depth data alone. In contrast to a density field as the underlying geometry representation, we propose to learn a deep neural network which stores a truncated signed distance field. Using this representation, we show that one can still leverage differentiable volume rendering to estimate color values of the observed images during training to compute a reconstruction loss. This is beneficial for learning the signed distance field in regions with missing depth measurements. Furthermore, we correct misalignment errors of the camera, improving the overall reconstruction quality. In several experiments, we showcase our method and compare to existing works on classical RGB-D fusion and learned representations.},
  YEAR = {2021},
  URL = {http://arxiv.org/abs/2104.04532v1},
  FILE = {2104.04532v1.pdf}
 }",https://arxiv.org/pdf/2104.04532.pdf,,,,,,,"Camera Parameter Estimation, Conditional Neural Field",,,,,,,,,,"Dejan Azinović, Ricardo Martin-Brualla, Dan B Goldman, Matthias Nießner, Justus Thies",azinovic2021neural,44,1,"In this work, we explore how to leverage the success of implicit novel view synthesis methods for surface reconstruction. Methods which learn a neural radiance field have shown amazing image synthesis results, but the underlying geometry representation is only a coarse approximation of the real geometry. We demonstrate how depth measurements can be incorporated into the radiance field formulation to produce more detailed and complete reconstruction results than using methods based on either color or depth data alone. In contrast to a density field as the underlying geometry representation, we propose to learn a deep neural network which stores a truncated signed distance field. Using this representation, we show that one can still leverage differentiable volume rendering to estimate color values of the observed images during training to compute a reconstruction loss. This is beneficial for learning the signed distance field in regions with missing depth measurements. Furthermore, we correct misalignment errors of the camera, improving the overall reconstruction quality. In several experiments, we showcase our method and compare to existing works on classical RGB-D fusion and learned representations.",,,
,,,StereoPIFu: Depth Aware Clothed Human Digitization via Stereo Vision,StereoPIFu,CVPR 2021,4/12/2021,2021,"@inproceedings{hong2021stereopifu,
  BOOKTITLE = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  ID = {hong2021stereopifu},
  ENTRYTYPE = {inproceedings},
  AUTHOR = {Yang Hong and Juyong Zhang and Boyi Jiang and Yudong Guo and Ligang Liu and Hujun Bao},
  TITLE = {StereoPIFu: Depth Aware Clothed Human Digitization via Stereo Vision},
  EPRINT = {2104.05289v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {In this paper, we propose StereoPIFu, which integrates the geometric constraints of stereo vision with implicit function representation of PIFu, to recover the 3D shape of the clothed human from a pair of low-cost rectified images. First, we introduce the effective voxel-aligned features from a stereo vision-based network to enable depth-aware reconstruction. Moreover, the novel relative z-offset is employed to associate predicted high-fidelity human depth and occupancy inference, which helps restore fine-level surface details. Second, a network structure that fully utilizes the geometry information from the stereo images is designed to improve the human body reconstruction quality. Consequently, our StereoPIFu can naturally infer the human body's spatial location in camera space and maintain the correct relative position of different parts of the human body, which enables our method to capture human performance. Compared with previous works, our StereoPIFu significantly improves the robustness, completeness, and accuracy of the clothed human reconstruction, which is demonstrated by extensive experimental results.},
  YEAR = {2021},
  URL = {http://arxiv.org/abs/2104.05289v2},
  FILE = {2104.05289v2.pdf}
 }",https://arxiv.org/pdf/2104.05289.pdf,https://hy1995.top/StereoPIFuProject/,https://github.com/CrisHY1995/StereoPIFu_Code,,,,,"Human (Body), Conditional Neural Field, Lifting 2D CNN Features to 3D, Feature volume, Data-Driven Method",,Occupancy,,,,,,,,"Yang Hong, Juyong Zhang, Boyi Jiang, Yudong Guo, Ligang Liu, Hujun Bao",hong2021stereopifu,45,,"In this paper, we propose StereoPIFu, which integrates the geometric constraints of stereo vision with implicit function representation of PIFu, to recover the 3D shape of the clothed human from a pair of low-cost rectified images. First, we introduce the effective voxel-aligned features from a stereo vision-based network to enable depth-aware reconstruction. Moreover, the novel relative z-offset is employed to associate predicted high-fidelity human depth and occupancy inference, which helps restore fine-level surface details. Second, a network structure that fully utilizes the geometry information from the stereo images is designed to improve the human body reconstruction quality. Consequently, our StereoPIFu can naturally infer the human body's spatial location in camera space and maintain the correct relative position of different parts of the human body, which enables our method to capture human performance. Compared with previous works, our StereoPIFu significantly improves the robustness, completeness, and accuracy of the clothed human reconstruction, which is demonstrated by extensive experimental results.",,Yes,
,,,LEAP: Learning Articulated Occupancy of People,LEAP,CVPR 2021,4/14/2021,2021,"@inproceedings{mihajlovic2021leap,
  BOOKTITLE = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  ID = {mihajlovic2021leap},
  ENTRYTYPE = {inproceedings},
  AUTHOR = {Marko Mihajlovic and Yan Zhang and Michael J. Black and Siyu Tang},
  TITLE = {LEAP: Learning Articulated Occupancy of People},
  EPRINT = {2104.06849v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Substantial progress has been made on modeling rigid 3D objects using deep implicit representations. Yet, extending these methods to learn neural models of human shape is still in its infancy. Human bodies are complex and the key challenge is to learn a representation that generalizes such that it can express body shape deformations for unseen subjects in unseen, highly-articulated, poses. To address this challenge, we introduce LEAP (LEarning Articulated occupancy of People), a novel neural occupancy representation of the human body. Given a set of bone transformations (i.e. joint locations and rotations) and a query point in space, LEAP first maps the query point to a canonical space via learned linear blend skinning (LBS) functions and then efficiently queries the occupancy value via an occupancy network that models accurate identity- and pose-dependent deformations in the canonical space. Experiments show that our canonicalized occupancy estimation with the learned LBS functions greatly improves the generalization capability of the learned occupancy representation across various human shapes and poses, outperforming existing solutions in all settings.},
  YEAR = {2021},
  URL = {http://arxiv.org/abs/2104.06849v1},
  FILE = {2104.06849v1.pdf}
 }",https://arxiv.org/pdf/2104.06849.pdf,https://neuralbodies.github.io/LEAP/,https://github.com/neuralbodies/leap,,https://www.youtube.com/watch?v=UVB8A_T5e3c,,,"Human (Body), Editable, Conditional Neural Field, Articulated, Coordinate Re-mapping, Data-Driven Method",,Occupancy,,,,,,,,"Marko Mihajlovic, Yan Zhang, Michael J. Black, Siyu Tang",mihajlovic2021leap,46,,"Substantial progress has been made on modeling rigid 3D objects using deep implicit representations. Yet, extending these methods to learn neural models of human shape is still in its infancy. Human bodies are complex and the key challenge is to learn a representation that generalizes such that it can express body shape deformations for unseen subjects in unseen, highly-articulated, poses. To address this challenge, we introduce LEAP (LEarning Articulated occupancy of People), a novel neural occupancy representation of the human body. Given a set of bone transformations (i.e. joint locations and rotations) and a query point in space, LEAP first maps the query point to a canonical space via learned linear blend skinning (LBS) functions and then efficiently queries the occupancy value via an occupancy network that models accurate identity- and pose-dependent deformations in the canonical space. Experiments show that our canonicalized occupancy estimation with the learned LBS functions greatly improves the generalization capability of the learned occupancy representation across various human shapes and poses, outperforming existing solutions in all settings.",,,
,,,A-SDF: Learning Disentangled Signed Distance Functions for Articulated Shape Representation,A-SDF,ARXIV 2021,4/15/2021,2021,"@article{mu2021asdf,
  JOURNAL = {arXiv preprint arXiv:2104.07645},
  BOOKTITLE = {ArXiv Pre-print},
  ID = {mu2021asdf},
  ENTRYTYPE = {article},
  AUTHOR = {Jiteng Mu and Weichao Qiu and Adam Kortylewski and Alan Yuille and Nuno Vasconcelos and Xiaolong Wang},
  TITLE = {A-SDF: Learning Disentangled Signed Distance Functions for Articulated Shape Representation},
  EPRINT = {2104.07645v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Recent work has made significant progress on using implicit functions, as a continuous representation for 3D rigid object shape reconstruction. However, much less effort has been devoted to modeling general articulated objects. Compared to rigid objects, articulated objects have higher degrees of freedom, which makes it hard to generalize to unseen shapes. To deal with the large shape variance, we introduce Articulated Signed Distance Functions (A-SDF) to represent articulated shapes with a disentangled latent space, where we have separate codes for encoding shape and articulation. We assume no prior knowledge on part geometry, articulation status, joint type, joint axis, and joint location. With this disentangled continuous representation, we demonstrate that we can control the articulation input and animate unseen instances with unseen joint angles. Furthermore, we propose a Test-Time Adaptation inference algorithm to adjust our model during inference. We demonstrate our model generalize well to out-of-distribution and unseen data, e.g., partial point clouds and real-world depth images.},
  YEAR = {2021},
  URL = {http://arxiv.org/abs/2104.07645v1},
  FILE = {2104.07645v1.pdf}
 }",https://arxiv.org/pdf/2104.07645.pdf,https://jitengmu.github.io/A-SDF/,Coming soon,,https://www.youtube.com/watch?v=P5WTcaXzC7A,,,"Editable, Conditional Neural Field, Articulated",,,,,,,,,,"Jiteng Mu, Weichao Qiu, Adam Kortylewski, Alan Yuille, Nuno Vasconcelos, Xiaolong Wang",mu2021asdf,47,2,"Recent work has made significant progress on using implicit functions, as a continuous representation for 3D rigid object shape reconstruction. However, much less effort has been devoted to modeling general articulated objects. Compared to rigid objects, articulated objects have higher degrees of freedom, which makes it hard to generalize to unseen shapes. To deal with the large shape variance, we introduce Articulated Signed Distance Functions (A-SDF) to represent articulated shapes with a disentangled latent space, where we have separate codes for encoding shape and articulation. We assume no prior knowledge on part geometry, articulation status, joint type, joint axis, and joint location. With this disentangled continuous representation, we demonstrate that we can control the articulation input and animate unseen instances with unseen joint angles. Furthermore, we propose a Test-Time Adaptation inference algorithm to adjust our model during inference. We demonstrate our model generalize well to out-of-distribution and unseen data, e.g., partial point clouds and real-world depth images.",,,
,,,FiG-NeRF: Figure-Ground Neural Radiance Fields for 3D Object Category Modelling,FiG-NeRF,ARXIV 2021,4/17/2021,2021,"@article{xie2021fignerf,
  JOURNAL = {arXiv preprint arXiv:2104.08418},
  BOOKTITLE = {ArXiv Pre-print},
  ID = {xie2021fignerf},
  ENTRYTYPE = {article},
  AUTHOR = {Christopher Xie and Keunhong Park and Ricardo Martin-Brualla and Matthew Brown},
  TITLE = {FiG-NeRF: Figure-Ground Neural Radiance Fields for 3D Object Category Modelling},
  EPRINT = {2104.08418v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We investigate the use of Neural Radiance Fields (NeRF) to learn high quality 3D object category models from collections of input images. In contrast to previous work, we are able to do this whilst simultaneously separating foreground objects from their varying backgrounds. We achieve this via a 2-component NeRF model, FiG-NeRF, that prefers explanation of the scene as a geometrically constant background and a deformable foreground that represents the object category. We show that this method can learn accurate 3D object category models using only photometric supervision and casually captured images of the objects. Additionally, our 2-part decomposition allows the model to perform accurate and crisp amodal segmentation. We quantitatively evaluate our method with view synthesis and image fidelity metrics, using synthetic, lab-captured, and in-the-wild data. Our results demonstrate convincing 3D object category modelling that exceed the performance of existing methods.},
  YEAR = {2021},
  URL = {http://arxiv.org/abs/2104.08418v1},
  FILE = {2104.08418v1.pdf}
 }",https://arxiv.org/pdf/2104.08418.pdf,https://fig-nerf.github.io/,,,https://www.youtube.com/watch?v=WtZxuv_hkic,,,"Generalization, Segmentation/composition, Fundamentals, Conditional Neural Field",,,,,,,,,,"Christopher Xie, Keunhong Park, Ricardo Martin-Brualla, Matthew Brown",xie2021fignerf,48,1,"We investigate the use of Neural Radiance Fields (NeRF) to learn high quality 3D object category models from collections of input images. In contrast to previous work, we are able to do this whilst simultaneously separating foreground objects from their varying backgrounds. We achieve this via a 2-component NeRF model, FiG-NeRF, that prefers explanation of the scene as a geometrically constant background and a deformable foreground that represents the object category. We show that this method can learn accurate 3D object category models using only photometric supervision and casually captured images of the objects. Additionally, our 2-part decomposition allows the model to perform accurate and crisp amodal segmentation. We quantitatively evaluate our method with view synthesis and image fidelity metrics, using synthetic, lab-captured, and in-the-wild data. Our results demonstrate convincing 3D object category modelling that exceed the performance of existing methods.",,,
,,,acorn: Adaptive Coordinate Networks for Neural Scene Representation,ACORN,SIGGRAPH 2021,5/6/2021,2021,"@article{martel2021acorn,
  PUBLISHER = {Association for Computing Machinery},
  JOURNAL = {ACM Transactions on Graphics (TOG)},
  ID = {martel2021acorn},
  ENTRYTYPE = {article},
  AUTHOR = {Julien N. P. Martel and David B. Lindell and Connor Z. Lin and Eric R. Chan and Marco Monteiro and Gordon Wetzstein},
  TITLE = {ACORN: Adaptive Coordinate Networks for Neural Scene Representation},
  EPRINT = {2105.02788v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Neural representations have emerged as a new paradigm for applications in rendering, imaging, geometric modeling, and simulation. Compared to traditional representations such as meshes, point clouds, or volumes they can be flexibly incorporated into differentiable learning-based pipelines. While recent improvements to neural representations now make it possible to represent signals with fine details at moderate resolutions (e.g., for images and 3D shapes), adequately representing large-scale or complex scenes has proven a challenge. Current neural representations fail to accurately represent images at resolutions greater than a megapixel or 3D scenes with more than a few hundred thousand polygons. Here, we introduce a new hybrid implicit-explicit network architecture and training strategy that adaptively allocates resources during training and inference based on the local complexity of a signal of interest. Our approach uses a multiscale block-coordinate decomposition, similar to a quadtree or octree, that is optimized during training. The network architecture operates in two stages: using the bulk of the network parameters, a coordinate encoder generates a feature grid in a single forward pass. Then, hundreds or thousands of samples within each block can be efficiently evaluated using a lightweight feature decoder. With this hybrid implicit-explicit network architecture, we demonstrate the first experiments that fit gigapixel images to nearly 40 dB peak signal-to-noise ratio. Notably this represents an increase in scale of over 1000x compared to the resolution of previously demonstrated image-fitting experiments. Moreover, our approach is able to represent 3D shapes significantly faster and better than previous techniques; it reduces training times from days to hours or minutes and memory requirements by over an order of magnitude.},
  YEAR = {2021},
  URL = {http://arxiv.org/abs/2105.02788v1},
  FILE = {2105.02788v1.pdf}
 }",https://arxiv.org/pdf/2105.02788.pdf,,,,,,,"Speed (Training), Speed (Rendering), Conditional Neural Field, Coarse-to-Fine, Sampling, Voxel Grid, Feature volume, Hybrid Geometry Parameterization",,Occupancy,,,,,,,,"Julien N. P. Martel, David B. Lindell, Connor Z. Lin, Eric R. Chan, Marco Monteiro, Gordon Wetzstein",martel2021acorn,49,2,"Neural representations have emerged as a new paradigm for applications in rendering, imaging, geometric modeling, and simulation. Compared to traditional representations such as meshes, point clouds, or volumes they can be flexibly incorporated into differentiable learning-based pipelines. While recent improvements to neural representations now make it possible to represent signals with fine details at moderate resolutions (e.g., for images and 3D shapes), adequately representing large-scale or complex scenes has proven a challenge. Current neural representations fail to accurately represent images at resolutions greater than a megapixel or 3D scenes with more than a few hundred thousand polygons. Here, we introduce a new hybrid implicit-explicit network architecture and training strategy that adaptively allocates resources during training and inference based on the local complexity of a signal of interest. Our approach uses a multiscale block-coordinate decomposition, similar to a quadtree or octree, that is optimized during training. The network architecture operates in two stages: using the bulk of the network parameters, a coordinate encoder generates a feature grid in a single forward pass. Then, hundreds or thousands of samples within each block can be efficiently evaluated using a lightweight feature decoder. With this hybrid implicit-explicit network architecture, we demonstrate the first experiments that fit gigapixel images to nearly 40 dB peak signal-to-noise ratio. Notably this represents an increase in scale of over 1000x compared to the resolution of previously demonstrated image-fitting experiments. Moreover, our approach is able to represent 3D shapes significantly faster and better than previous techniques; it reduces training times from days to hours or minutes and memory requirements by over an order of magnitude.",,,
,,,Light Field Networks: Neural Scene Representations with Single-Evaluation Rendering,LFNs,IJCAI 2021,6/4/2021,2021,"@inproceedings{sitzmann2021lfns,
  PUBLISHER = {International Joint Conferences on Artificial Intelligence Organization},
  BOOKTITLE = {Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence (IJCAI)},
  ID = {sitzmann2021lfns},
  ENTRYTYPE = {inproceedings},
  AUTHOR = {Vincent Sitzmann and Semon Rezchikov and William T. Freeman and Joshua B. Tenenbaum and Fredo Durand},
  TITLE = {Light Field Networks: Neural Scene Representations with Single-Evaluation Rendering},
  EPRINT = {2106.02634v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Inferring representations of 3D scenes from 2D observations is a fundamental problem of computer graphics, computer vision, and artificial intelligence. Emerging 3D-structured neural scene representations are a promising approach to 3D scene understanding. In this work, we propose a novel neural scene representation, Light Field Networks or LFNs, which represent both geometry and appearance of the underlying 3D scene in a 360-degree, four-dimensional light field parameterized via a neural implicit representation. Rendering a ray from an LFN requires only a *single* network evaluation, as opposed to hundreds of evaluations per ray for ray-marching or volumetric based renderers in 3D-structured neural scene representations. In the setting of simple scenes, we leverage meta-learning to learn a prior over LFNs that enables multi-view consistent light field reconstruction from as little as a single image observation. This results in dramatic reductions in time and memory complexity, and enables real-time rendering. The cost of storing a 360-degree light field via an LFN is two orders of magnitude lower than conventional methods such as the Lumigraph. Utilizing the analytical differentiability of neural implicit representations and a novel parameterization of light space, we further demonstrate the extraction of sparse depth maps from LFNs.},
  YEAR = {2021},
  URL = {http://arxiv.org/abs/2106.02634v1},
  FILE = {2106.02634v1.pdf}
 }",https://arxiv.org/pdf/2106.02634.pdf,https://vsitzmann.github.io/lfns/,Coming soon,Coming soon,https://www.youtube.com/watch?v=x3sSreTNFw4,,,"Speed (Training), Speed (Rendering), Generalization, Conditional Neural Field, Hypernetwork/Meta-learning, Hybrid Geometry Parameterization, Sampling",,,,,,,,,,"Vincent Sitzmann, Semon Rezchikov, William T. Freeman, Joshua B. Tenenbaum, Fredo Durand",sitzmann2021lfns,50,,"Inferring representations of 3D scenes from 2D observations is a fundamental problem of computer graphics, computer vision, and artificial intelligence. Emerging 3D-structured neural scene representations are a promising approach to 3D scene understanding. In this work, we propose a novel neural scene representation, Light Field Networks or LFNs, which represent both geometry and appearance of the underlying 3D scene in a 360-degree, four-dimensional light field parameterized via a neural implicit representation. Rendering a ray from an LFN requires only a *single* network evaluation, as opposed to hundreds of evaluations per ray for ray-marching or volumetric based renderers in 3D-structured neural scene representations. In the setting of simple scenes, we leverage meta-learning to learn a prior over LFNs that enables multi-view consistent light field reconstruction from as little as a single image observation. This results in dramatic reductions in time and memory complexity, and enables real-time rendering. The cost of storing a 360-degree light field via an LFN is two orders of magnitude lower than conventional methods such as the Lumigraph. Utilizing the analytical differentiability of neural implicit representations and a novel parameterization of light space, we further demonstrate the extraction of sparse depth maps from LFNs.",,,
,,,Unsupervised Video Prediction from a Single Frame by Estimating 3D Dynamic Scene Structure,,ARXIV 2021,6/16/2021,2021,"@article{henderson2021unsupervised,
  JOURNAL = {arXiv preprint arXiv:2106.09051},
  BOOKTITLE = {ArXiv Pre-print},
  ID = {henderson2021unsupervised},
  ENTRYTYPE = {article},
  AUTHOR = {Paul Henderson and Christoph H. Lampert and Bernd Bickel},
  TITLE = {Unsupervised Video Prediction from a Single Frame by Estimating 3D Dynamic Scene Structure},
  EPRINT = {2106.09051v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Our goal in this work is to generate realistic videos given just one initial frame as input. Existing unsupervised approaches to this task do not consider the fact that a video typically shows a 3D environment, and that this should remain coherent from frame to frame even as the camera and objects move. We address this by developing a model that first estimates the latent 3D structure of the scene, including the segmentation of any moving objects. It then predicts future frames by simulating the object and camera dynamics, and rendering the resulting views. Importantly, it is trained end-to-end using only the unsupervised objective of predicting future frames, without any 3D information nor segmentation annotations. Experiments on two challenging datasets of natural videos show that our model can estimate 3D structure and motion segmentation from a single frame, and hence generate plausible and varied predictions.},
  YEAR = {2021},
  URL = {http://arxiv.org/abs/2106.09051v1},
  FILE = {2106.09051v1.pdf}
 }",https://arxiv.org/pdf/2106.09051.pdf,http://pmh47.net/vipl4s/,,,,,,"Dynamic, Beyond Visual Computing, Conditional Neural Field, Volume partitioning",,,,,,,,,,"Paul Henderson, Christoph H. Lampert, Bernd Bickel",henderson2021unsupervised,51,,"Our goal in this work is to generate realistic videos given just one initial frame as input. Existing unsupervised approaches to this task do not consider the fact that a video typically shows a 3D environment, and that this should remain coherent from frame to frame even as the camera and objects move. We address this by developing a model that first estimates the latent 3D structure of the scene, including the segmentation of any moving objects. It then predicts future frames by simulating the object and camera dynamics, and rendering the resulting views. Importantly, it is trained end-to-end using only the unsupervised objective of predicting future frames, without any 3D information nor segmentation annotations. Experiments on two challenging datasets of natural videos show that our model can estimate 3D structure and motion segmentation from a single frame, and hence generate plausible and varied predictions.",,,
,,,NeRF-Tex: Neural Reflectance Field Textures,NeRF-Tex,EGSR 2021,6/22/2021,2021,"@article{baatz2021nerftex,
  JOURNAL = {Computer Graphics Forum},
  ID = {baatz2021nerftex},
  ENTRYTYPE = {article},
  TITLE = {NeRF-Tex: Neural Reflectance Field Textures},
  AUTHOR = {Hendrik Baatz and Jonathan Granskog and Marios Papas and Fabrice Rousselle and Jan Nov{\'a}k},
  YEAR = {2021},
  PUBLISHER = {The Eurographics Association and John Wiley & Sons Ltd.}
 }",https://d1qx31qr3h6wln.cloudfront.net/publications/NeRFTex.pdf,,,,https://d1qx31qr3h6wln.cloudfront.net/publications/NeRFTex_video.mp4,,,"Material/Lighting Estimation, Challenging materials (fur, Conditional Neural Field, Hybrid Geometry Parameterization",,Density,,,,,,,,"Hendrik Baatz, Jonathan Granskog, Marios Papas, Fabrice Rousselle, Jan Nov{\'a}k",baatz2021nerftex,52,,"We investigate the use of neural fields for modeling diverse mesoscale structures, such as fur, fabric, and grass. Instead of using classical graphics primitives to model the structure, we propose to employ a versatile volumetric primitive represented by a neural reflectance field (NeRF-Tex), which jointly models the geometry of the material and its response to lighting. The NeRF-Tex primitive can be instantiated over a base mesh to''texture''it with the desired meso and microscale appearance. We condition the reflectance field on user-defined",,,
,,,HyperNeRF: A Higher-Dimensional Representation for Topologically Varying Neural Radiance Fields,HyperNeRF,ARXIV 2021,6/24/2021,2021,"@article{park2021hypernerf,
  JOURNAL = {arXiv preprint arXiv:2106.13228},
  BOOKTITLE = {ArXiv Pre-print},
  ID = {park2021hypernerf},
  ENTRYTYPE = {article},
  AUTHOR = {Keunhong Park and Utkarsh Sinha and Peter Hedman and Jonathan T. Barron and Sofien Bouaziz and Dan B Goldman and Ricardo Martin-Brualla and Steven M. Seitz},
  TITLE = {HyperNeRF: A Higher-Dimensional Representation for Topologically Varying Neural Radiance Fields},
  EPRINT = {2106.13228v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Neural Radiance Fields (NeRF) are able to reconstruct scenes with unprecedented fidelity, and various recent works have extended NeRF to handle dynamic scenes. A common approach to reconstruct such non-rigid scenes is through the use of a learned deformation field mapping from coordinates in each input image into a canonical template coordinate space. However, these deformation-based approaches struggle to model changes in topology, as topological changes require a discontinuity in the deformation field, but these deformation fields are necessarily continuous. We address this limitation by lifting NeRFs into a higher dimensional space, and by representing the 5D radiance field corresponding to each individual input image as a slice through this ""hyper-space"". Our method is inspired by level set methods, which model the evolution of surfaces as slices through a higher dimensional surface. We evaluate our method on two tasks: (i) interpolating smoothly between ""moments"", i.e., configurations of the scene, seen in the input images while maintaining visual plausibility, and (ii) novel-view synthesis at fixed moments. We show that our method, which we dub HyperNeRF, outperforms existing methods on both tasks. Compared to Nerfies, HyperNeRF reduces average error rates by 4.1% for interpolation and 8.6% for novel-view synthesis, as measured by LPIPS. Additional videos, results, and visualizations are available at https://hypernerf.github.io.},
  YEAR = {2021},
  URL = {http://arxiv.org/abs/2106.13228v2},
  FILE = {2106.13228v2.pdf}
 }",https://arxiv.org/pdf/2106.13228.pdf,,,,,,,"Dynamic, Fundamentals, Conditional Neural Field",,Density,,,,,,,,"Keunhong Park, Utkarsh Sinha, Peter Hedman, Jonathan T. Barron, Sofien Bouaziz, Dan B Goldman, Ricardo Martin-Brualla, Steven M. Seitz",park2021hypernerf,53,,"Neural Radiance Fields (NeRF) are able to reconstruct scenes with unprecedented fidelity, and various recent works have extended NeRF to handle dynamic scenes. A common approach to reconstruct such non-rigid scenes is through the use of a learned deformation field mapping from coordinates in each input image into a canonical template coordinate space. However, these deformation-based approaches struggle to model changes in topology, as topological changes require a discontinuity in the deformation field, but these deformation fields are necessarily continuous. We address this limitation by lifting NeRFs into a higher dimensional space, and by representing the 5D radiance field corresponding to each individual input image as a slice through this ""hyper-space"". Our method is inspired by level set methods, which model the evolution of surfaces as slices through a higher dimensional surface. We evaluate our method on two tasks: (i) interpolating smoothly between ""moments"", i.e., configurations of the scene, seen in the input images while maintaining visual plausibility, and (ii) novel-view synthesis at fixed moments. We show that our method, which we dub HyperNeRF, outperforms existing methods on both tasks by significant margins. Compared to Nerfies, HyperNeRF reduces average error rates by 8.6% for interpolation and 8.8% for novel-view synthesis, as measured by LPIPS.",,,
,,,Unsupervised Discovery of Object Radiance Fields,uORF,ARXIV 2021,7/16/2021,2021,"@article{yu2021uorf,
  JOURNAL = {arXiv preprint arXiv:2107.07905},
  BOOKTITLE = {ArXiv Pre-print},
  ID = {yu2021uorf},
  ENTRYTYPE = {article},
  AUTHOR = {Hong-Xing Yu and Leonidas J. Guibas and Jiajun Wu},
  TITLE = {Unsupervised Discovery of Object Radiance Fields},
  EPRINT = {2107.07905v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We study the problem of inferring an object-centric scene representation from a single image, aiming to derive a representation that explains the image formation process, captures the scene's 3D nature, and is learned without supervision. Most existing methods on scene decomposition lack one or more of these characteristics, due to the fundamental challenge in integrating the complex 3D-to-2D image formation process into powerful inference schemes like deep networks. In this paper, we propose unsupervised discovery of Object Radiance Fields (uORF), integrating recent progresses in neural 3D scene representations and rendering with deep inference networks for unsupervised 3D scene decomposition. Trained on multi-view RGB images without annotations, uORF learns to decompose complex scenes with diverse, textured background from a single image. We show that uORF performs well on unsupervised 3D scene segmentation, novel view synthesis, and scene editing on three datasets.},
  YEAR = {2021},
  URL = {http://arxiv.org/abs/2107.07905v1},
  FILE = {2107.07905v1.pdf}
 }",https://arxiv.org/pdf/2107.07905.pdf,https://kovenyu.com/uorf/,https://github.com/KovenYu/uORF,,https://www.youtube.com/watch?v=6J9OpvT4dCA,https://kovenyu.com/uorf/static/uORF_supp.pdf,,"Generalization, Editable, Segmentation/composition, Conditional Neural Field, Lifting 2D CNN Features to 3D, Volume partitioning, Object-Centric Hybrid Geometry Parameterization, Segmentation, Data-Driven Method",,Density,,,,,,,,"Hong-Xing Yu, Leonidas J. Guibas, Jiajun Wu",yu2021uorf,54,,"We study the problem of inferring an object-centric scene representation from a single image, aiming to derive a representation that explains the image formation process, captures the scene's 3D nature, and is learned without supervision. Most existing methods on scene decomposition lack one or more of these characteristics, due to the fundamental challenge in integrating the complex 3D-to-2D image formation process into powerful inference schemes like deep networks. In this paper, we propose unsupervised discovery of Object Radiance Fields (uORF), integrating recent progresses in neural 3D scene representations and rendering with deep inference networks for unsupervised 3D scene decomposition. Trained on multi-view RGB images without annotations, uORF learns to decompose complex scenes with diverse, textured background from a single image. We show that uORF performs well on unsupervised 3D scene segmentation, novel view synthesis, and scene editing on three datasets.",,,
,,,H3D-Net: Few-Shot High-Fidelity 3D Head Reconstruction,H3D-Net,ARXIV 2021,7/26/2021,2021,"@article{ramon2021h3dnet,
  JOURNAL = {arXiv preprint arXiv:2107.12512},
  BOOKTITLE = {ArXiv Pre-print},
  ID = {ramon2021h3dnet},
  ENTRYTYPE = {article},
  AUTHOR = {Eduard Ramon and Gil Triginer and Janna Escur and Albert Pumarola and Jaime Garcia and Xavier Giro-i-Nieto and Francesc Moreno-Noguer},
  TITLE = {H3D-Net: Few-Shot High-Fidelity 3D Head Reconstruction},
  EPRINT = {2107.12512v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Recent learning approaches that implicitly represent surface geometry using coordinate-based neural representations have shown impressive results in the problem of multi-view 3D reconstruction. The effectiveness of these techniques is, however, subject to the availability of a large number (several tens) of input views of the scene, and computationally demanding optimizations. In this paper, we tackle these limitations for the specific problem of few-shot full 3D head reconstruction, by endowing coordinate-based representations with a probabilistic shape prior that enables faster convergence and better generalization when using few input images (down to three). First, we learn a shape model of 3D heads from thousands of incomplete raw scans using implicit representations. At test time, we jointly overfit two coordinate-based neural networks to the scene, one modeling the geometry and another estimating the surface radiance, using implicit differentiable rendering. We devise a two-stage optimization strategy in which the learned prior is used to initialize and constrain the geometry during an initial optimization phase. Then, the prior is unfrozen and fine-tuned to the scene. By doing this, we achieve high-fidelity head reconstructions, including hair and shoulders, and with a high level of detail that consistently outperforms both state-of-the-art 3D Morphable Models methods in the few-shot scenario, and non-parametric methods when large sets of views are available.},
  YEAR = {2021},
  URL = {http://arxiv.org/abs/2107.12512v1},
  FILE = {2107.12512v1.pdf}
 }",https://arxiv.org/pdf/2107.12512.pdf,https://crisalixsa.github.io/h3d-net/,https://github.com/CrisalixSA/h3ds,https://github.com/CrisalixSA/h3ds,,,,"Human (Head), Sparse Reconstruction, Conditional Neural Field, Per-instance fine-tuning, Data-Driven Method",,SDF,,,,,,,,"Eduard Ramon, Gil Triginer, Janna Escur, Albert Pumarola, Jaime Garcia, Xavier Giro-i-Nieto, Francesc Moreno-Noguer",ramon2021h3dnet,55,,"Recent learning approaches that implicitly represent surface geometry using coordinate-based neural representations have shown impressive results in the problem of multi-view 3D reconstruction. The effectiveness of these techniques is, however, subject to the availability of a large number (several tens) of input views of the scene, and computationally demanding optimizations. In this paper, we tackle these limitations for the specific problem of few-shot full 3D head reconstruction, by endowing coordinate-based representations with a probabilistic shape prior that enables faster convergence and better generalization when using few input images (down to three). First, we learn a shape model of 3D heads from thousands of incomplete raw scans using implicit representations. At test time, we jointly overfit two coordinate-based neural networks to the scene, one modeling the geometry and another estimating the surface radiance, using implicit differentiable rendering. We devise a two-stage optimization strategy in which the learned prior is used to initialize and constrain the geometry during an initial optimization phase. Then, the prior is unfrozen and fine-tuned to the scene. By doing this, we achieve high-fidelity head reconstructions, including hair and shoulders, and with a high level of detail that consistently outperforms both state-of-the-art 3D Morphable Models methods in the few-shot scenario, and non-parametric methods when large sets of views are available.",,,
,,,FLAME-in-NeRF: Neural control of Radiance Fields for Free View Face Animation,FLAME-in-NeRF,ARXIV 2021,8/10/2021,2021,"@article{athar2021flameinnerf,
  JOURNAL = {arXiv preprint arXiv:2108.04913},
  BOOKTITLE = {ArXiv Pre-print},
  ID = {athar2021flameinnerf},
  ENTRYTYPE = {article},
  AUTHOR = {ShahRukh Athar and Zhixin Shu and Dimitris Samaras},
  TITLE = {FLAME-in-NeRF : Neural control of Radiance Fields for Free View Face Animation},
  EPRINT = {2108.04913v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {This paper presents a neural rendering method for controllable portrait video synthesis. Recent advances in volumetric neural rendering, such as neural radiance fields (NeRF), has enabled the photorealistic novel view synthesis of static scenes with impressive results. However, modeling dynamic and controllable objects as part of a scene with such scene representations is still challenging. In this work, we design a system that enables both novel view synthesis for portrait video, including the human subject and the scene background, and explicit control of the facial expressions through a low-dimensional expression representation. We leverage the expression space of a 3D morphable face model (3DMM) to represent the distribution of human facial expressions, and use it to condition the NeRF volumetric function. Furthermore, we impose a spatial prior brought by 3DMM fitting to guide the network to learn disentangled control for scene appearance and facial actions. We demonstrate the effectiveness of our method on free view synthesis of portrait videos with expression controls. To train a scene, our method only requires a short video of a subject captured by a mobile device.},
  YEAR = {2021},
  URL = {http://arxiv.org/abs/2108.04913v1},
  FILE = {2108.04913v1.pdf}
 }",https://arxiv.org/pdf/2108.04913.pdf,,,,,,,"Dynamic, Human (Head), Generalization, Editable, Conditional Neural Field, Coarse-to-Fine, Coordinate Re-mapping, Data-Driven Method",,Density,,,,,,,,"ShahRukh Athar, Zhixin Shu, Dimitris Samaras",athar2021flameinnerf,56,,"This paper presents a neural rendering method for controllable portrait video synthesis. Recent advances in volumetric neural rendering, such as neural radiance fields (NeRF), has enabled the photorealistic novel view synthesis of static scenes with impressive results. However, modeling dynamic and controllable objects as part of a scene with such scene representations is still challenging. In this work, we design a system that enables both novel view synthesis for portrait video, including the human subject and the scene background, and explicit control of the facial expressions through a low-dimensional expression representation. We leverage the expression space of a 3D morphable face model (3DMM) to represent the distribution of human facial expressions, and use it to condition the NeRF volumetric function. Furthermore, we impose a spatial prior brought by 3DMM fitting to guide the network to learn disentangled control for scene appearance and facial actions. We demonstrate the effectiveness of our method on free view synthesis of portrait videos with expression controls. To train a scene, our method only requires a short video of a subject captured by a mobile device.",,,
,,,Continual Neural Mapping: Learning An Implicit Scene Representation from Sequential Observations,,ICCV 2021,8/12/2021,2021,"@inproceedings{yan2021continual,
  BOOKTITLE = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
  ID = {yan2021continual},
  ENTRYTYPE = {inproceedings},
  AUTHOR = {Zike Yan and Yuxin Tian and Xuesong Shi and Ping Guo and Peng Wang and Hongbin Zha},
  TITLE = {Continual Neural Mapping: Learning An Implicit Scene Representation from Sequential Observations},
  EPRINT = {2108.05851v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Recent advances have enabled a single neural network to serve as an implicit scene representation, establishing the mapping function between spatial coordinates and scene properties. In this paper, we make a further step towards continual learning of the implicit scene representation directly from sequential observations, namely Continual Neural Mapping. The proposed problem setting bridges the gap between batch-trained implicit neural representations and commonly used streaming data in robotics and vision communities. We introduce an experience replay approach to tackle an exemplary task of continual neural mapping: approximating a continuous signed distance function (SDF) from sequential depth images as a scene geometry representation. We show for the first time that a single network can represent scene geometry over time continually without catastrophic forgetting, while achieving promising trade-offs between accuracy and efficiency.},
  YEAR = {2021},
  URL = {http://arxiv.org/abs/2108.05851v1},
  FILE = {2108.05851v1.pdf}
 }",https://arxiv.org/pdf/2108.05851.pdf,,,,https://zikeyan.github.io/videos/iccv2021.mp4,,,"Robotics, Multi-task/Continual/Transfer learning, Conditional Neural Field, Volume partitioning",,SDF,,,,,,,,"Zike Yan, Yuxin Tian, Xuesong Shi, Ping Guo, Peng Wang, Hongbin Zha",yan2021continual,57,,"Recent advances have enabled a single neural network to serve as an implicit scene representation, establishing the mapping function between spatial coordinates and scene properties. In this paper, we make a further step towards continual learning of the implicit scene representation directly from sequential observations, namely Continual Neural Mapping. The proposed problem setting bridges the gap between batch-trained implicit neural representations and commonly used streaming data in robotics and vision communities. We introduce an experience replay approach to tackle an exemplary task of continual neural mapping: approximating a continuous signed distance function (SDF) from sequential depth images as a scene geometry representation. We show for the first time that a single network can represent scene geometry over time continually without catastrophic forgetting, while achieving promising trade-offs between accuracy and efficiency.",,,
,,,ARCH++: Animation-Ready Clothed Human Reconstruction Revisited,ARCH++,ICCV 2021,8/17/2021,2021,"@inproceedings{he2021arch++,
  BOOKTITLE = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
  ID = {he2021arch++},
  ENTRYTYPE = {inproceedings},
  AUTHOR = {Tong He and Yuanlu Xu and Shunsuke Saito and Stefano Soatto and Tony Tung},
  TITLE = {ARCH++: Animation-Ready Clothed Human Reconstruction Revisited},
  EPRINT = {2108.07845v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We present ARCH++, an image-based method to reconstruct 3D avatars with arbitrary clothing styles. Our reconstructed avatars are animation-ready and highly realistic, in both the visible regions from input views and the unseen regions. While prior work shows great promise of reconstructing animatable clothed humans with various topologies, we observe that there exist fundamental limitations resulting in sub-optimal reconstruction quality. In this paper, we revisit the major steps of image-based avatar reconstruction and address the limitations with ARCH++. First, we introduce an end-to-end point based geometry encoder to better describe the semantics of the underlying 3D human body, in replacement of previous hand-crafted features. Second, in order to address the occupancy ambiguity caused by topological changes of clothed humans in the canonical pose, we propose a co-supervising framework with cross-space consistency to jointly estimate the occupancy in both the posed and canonical spaces. Last, we use image-to-image translation networks to further refine detailed geometry and texture on the reconstructed surface, which improves the fidelity and consistency across arbitrary viewpoints. In the experiments, we demonstrate improvements over the state of the art on both public benchmarks and user studies in reconstruction quality and realism.},
  YEAR = {2021},
  URL = {http://arxiv.org/abs/2108.07845v1},
  FILE = {2108.07845v1.pdf}
 }",https://arxiv.org/pdf/2108.07845.pdf,,,,,,,"Human (Body), Sparse Reconstruction, Editable, Conditional Neural Field, Lifting 2D CNN Features to 3D, Voxel Grid, Feature volume, Data-Driven Method",,,,,,,,,,"Tong He, Yuanlu Xu, Shunsuke Saito, Stefano Soatto, Tony Tung",he2021arch++,58,,"We present ARCH++, an image-based method to reconstruct 3D avatars with arbitrary clothing styles. Our reconstructed avatars are animation-ready and highly realistic, in both the visible regions from input views and the unseen regions. While prior work shows great promise of reconstructing animatable clothed humans with various topologies, we observe that there exist fundamental limitations resulting in sub-optimal reconstruction quality. In this paper, we revisit the major steps of image-based avatar reconstruction and address the limitations with ARCH++. First, we introduce an end-to-end point based geometry encoder to better describe the semantics of the underlying 3D human body, in replacement of previous hand-crafted features. Second, in order to address the occupancy ambiguity caused by topological changes of clothed humans in the canonical pose, we propose a co-supervising framework with cross-space consistency to jointly estimate the occupancy in both the posed and canonical spaces. Last, we use image-to-image translation networks to further refine detailed geometry and texture on the reconstructed surface, which improves the fidelity and consistency across arbitrary viewpoints. In the experiments, we demonstrate improvements over the state of the art on both public benchmarks and user studies in reconstruction quality and realism.",,,
,,,imGHUM: Implicit Generative Models of 3D Human Shape and Articulated Pose,imGHUM,CVPR 2021,8/24/2021,2021,"@inproceedings{alldieck2021imghum,
  BOOKTITLE = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  ID = {alldieck2021imghum},
  ENTRYTYPE = {inproceedings},
  AUTHOR = {Thiemo Alldieck and Hongyi Xu and Cristian Sminchisescu},
  TITLE = {imGHUM: Implicit Generative Models of 3D Human Shape and Articulated Pose},
  EPRINT = {2108.10842v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We present imGHUM, the first holistic generative model of 3D human shape and articulated pose, represented as a signed distance function. In contrast to prior work, we model the full human body implicitly as a function zero-level-set and without the use of an explicit template mesh. We propose a novel network architecture and a learning paradigm, which make it possible to learn a detailed implicit generative model of human pose, shape, and semantics, on par with state-of-the-art mesh-based models. Our model features desired detail for human models, such as articulated pose including hand motion and facial expressions, a broad spectrum of shape variations, and can be queried at arbitrary resolutions and spatial locations. Additionally, our model has attached spatial semantics making it straightforward to establish correspondences between different shape instances, thus enabling applications that are difficult to tackle using classical implicit representations. In extensive experiments, we demonstrate the model accuracy and its applicability to current research problems.},
  YEAR = {2021},
  URL = {http://arxiv.org/abs/2108.10842v1},
  FILE = {2108.10842v1.pdf}
 }",https://arxiv.org/pdf/2108.10842.pdf,https://research.google/pubs/pub50642/,,,,,,"Human (Body), Conditional Neural Field, Volume partitioning",,SDF,,,,,,,,"Thiemo Alldieck, Hongyi Xu, Cristian Sminchisescu",alldieck2021imghum,59,,"We present imGHUM, the first holistic generative model of 3D human shape and articulated pose, represented as a signed distance function. In contrast to prior work, we model the full human body implicitly as a function zero-level-set and without the use of an explicit template mesh. We propose a novel network architecture and a learning paradigm, which make it possible to learn a detailed implicit generative model of human pose, shape, and semantics, on par with state-of-the-art mesh-based models. Our model features desired detail for human models, such as articulated pose including hand motion and facial expressions, a broad spectrum of shape variations, and can be queried at arbitrary resolutions and spatial locations. Additionally, our model has attached spatial semantics making it straightforward to establish correspondences between different shape instances, thus enabling applications that are difficult to tackle using classical implicit representations. In extensive experiments, we demonstrate the model accuracy and its applicability to current research problems.",,,
,,,Common Objects in 3D: Large-Scale Learning and Evaluation of Real-life 3D Category Reconstruction,CO3D,CVPR 2021,9/1/2021,2021,"@inproceedings{reizenstein2021co3d,
  BOOKTITLE = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  ID = {reizenstein2021co3d},
  ENTRYTYPE = {inproceedings},
  AUTHOR = {Jeremy Reizenstein and Roman Shapovalov and Philipp Henzler and Luca Sbordone and Patrick Labatut and David Novotny},
  TITLE = {Common Objects in 3D: Large-Scale Learning and Evaluation of Real-life 3D Category Reconstruction},
  EPRINT = {2109.00512v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Traditional approaches for learning 3D object categories have been predominantly trained and evaluated on synthetic datasets due to the unavailability of real 3D-annotated category-centric data. Our main goal is to facilitate advances in this field by collecting real-world data in a magnitude similar to the existing synthetic counterparts. The principal contribution of this work is thus a large-scale dataset, called Common Objects in 3D, with real multi-view images of object categories annotated with camera poses and ground truth 3D point clouds. The dataset contains a total of 1.5 million frames from nearly 19,000 videos capturing objects from 50 MS-COCO categories and, as such, it is significantly larger than alternatives both in terms of the number of categories and objects. We exploit this new dataset to conduct one of the first large-scale ""in-the-wild"" evaluations of several new-view-synthesis and category-centric 3D reconstruction methods. Finally, we contribute NerFormer - a novel neural rendering method that leverages the powerful Transformer to reconstruct an object given a small number of its views. The CO3D dataset is available at https://github.com/facebookresearch/co3d .},
  YEAR = {2021},
  NOTE = {International Conference on Computer Vision, 2021},
  URL = {http://arxiv.org/abs/2109.00512v1},
  FILE = {2109.00512v1.pdf}
 }",https://arxiv.org/pdf/2109.00512.pdf,,https://github.com/facebookresearch/co3d,https://ai.facebook.com/datasets/co3d-downloads/,https://www.youtube.com/watch?v=hMx9nzG50xQ,,,"Sparse Reconstruction, Generalization, Conditional Neural Field, Lifting 2D CNN Features to 3D, Transformer",,Density,,,,,,,,"Jeremy Reizenstein, Roman Shapovalov, Philipp Henzler, Luca Sbordone, Patrick Labatut, David Novotny",reizenstein2021co3d,60,,"Traditional approaches for learning 3D object categories have been predominantly trained and evaluated on synthetic datasets due to the unavailability of real 3D-annotated category-centric data. Our main goal is to facilitate advances in this field by collecting real-world data in a magnitude similar to the existing synthetic counterparts. The principal contribution of this work is thus a large-scale dataset, called Common Objects in 3D, with real multi-view images of object categories annotated with camera poses and ground truth 3D point clouds. The dataset contains a total of 1.5 million frames from nearly 19,000 videos capturing objects from 50 MS-COCO categories and, as such, it is significantly larger than alternatives both in terms of the number of categories and objects. We exploit this new dataset to conduct one of the first large-scale ""in-the-wild"" evaluations of several new-view-synthesis and category-centric 3D reconstruction methods. Finally, we contribute NerFormer - a novel neural rendering method that leverages the powerful Transformer to reconstruct an object given a small number of its views. The CO3D dataset is available at https://github.com/facebookresearch/co3d .",,,
,,,CodeNeRF: Disentangled Neural Radiance Fields for Object Categories,CodeNeRF,ICCV 2021,9/3/2021,2021,"@inproceedings{jang2021codenerf,
  BOOKTITLE = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
  ID = {jang2021codenerf},
  ENTRYTYPE = {inproceedings},
  AUTHOR = {Wonbong Jang and Lourdes Agapito},
  TITLE = {CodeNeRF: Disentangled Neural Radiance Fields for Object Categories},
  EPRINT = {2109.01750v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.GR},
  ABSTRACT = {CodeNeRF is an implicit 3D neural representation that learns the variation of object shapes and textures across a category and can be trained, from a set of posed images, to synthesize novel views of unseen objects. Unlike the original NeRF, which is scene specific, CodeNeRF learns to disentangle shape and texture by learning separate embeddings. At test time, given a single unposed image of an unseen object, CodeNeRF jointly estimates camera viewpoint, and shape and appearance codes via optimization. Unseen objects can be reconstructed from a single image, and then rendered from new viewpoints or their shape and texture edited by varying the latent codes. We conduct experiments on the SRN benchmark, which show that CodeNeRF generalises well to unseen objects and achieves on-par performance with methods that require known camera pose at test time. Our results on real-world images demonstrate that CodeNeRF can bridge the sim-to-real gap. Project page: \url{https://github.com/wayne1123/code-nerf}},
  YEAR = {2021},
  URL = {http://arxiv.org/abs/2109.01750v1},
  FILE = {2109.01750v1.pdf}
 }",https://arxiv.org/pdf/2109.01750.pdf,https://sites.google.com/view/wbjang/home/codenerf,https://github.com/wayne1123/code-nerf,,https://user-images.githubusercontent.com/32883157/130004248-0ff74d4e-993e-43f2-91ee-bd25776e65bc.mp4,,,"Generalization, Editable, Conditional Neural Field, Per-instance fine-tuning, Data-Driven Method",,Density,,,,,,,,"Wonbong Jang, Lourdes Agapito",jang2021codenerf,61,,"CodeNeRF is an implicit 3D neural representation that learns the variation of object shapes and textures across a category and can be trained, from a set of posed images, to synthesize novel views of unseen objects. Unlike the original NeRF, which is scene specific, CodeNeRF learns to disentangle shape and texture by learning separate embeddings. At test time, given a single unposed image of an unseen object, CodeNeRF jointly estimates camera viewpoint, and shape and appearance codes via optimization. Unseen objects can be reconstructed from a single image, and then rendered from new viewpoints or their shape and texture edited by varying the latent codes. We conduct experiments on the SRN benchmark, which show that CodeNeRF generalises well to unseen objects and achieves on-par performance with methods that require known camera pose at test time. Our results on real-world images demonstrate that CodeNeRF can bridge the sim-to-real gap. Project page: \url{https://github.com/wayne1123/code-nerf}",,,
,,,Learning Object-Compositional Neural Radiance Field for Editable Scene Rendering,,ICCV 2021,9/4/2021,2021,"@inproceedings{yang2021learning,
  BOOKTITLE = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
  ID = {yang2021learning},
  ENTRYTYPE = {inproceedings},
  AUTHOR = {Bangbang Yang and Yinda Zhang and Yinghao Xu and Yijin Li and Han Zhou and Hujun Bao and Guofeng Zhang and Zhaopeng Cui},
  TITLE = {Learning Object-Compositional Neural Radiance Field for Editable Scene Rendering},
  EPRINT = {2109.01847v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Implicit neural rendering techniques have shown promising results for novel view synthesis. However, existing methods usually encode the entire scene as a whole, which is generally not aware of the object identity and limits the ability to the high-level editing tasks such as moving or adding furniture. In this paper, we present a novel neural scene rendering system, which learns an object-compositional neural radiance field and produces realistic rendering with editing capability for a clustered and real-world scene. Specifically, we design a novel two-pathway architecture, in which the scene branch encodes the scene geometry and appearance, and the object branch encodes each standalone object conditioned on learnable object activation codes. To survive the training in heavily cluttered scenes, we propose a scene-guided training strategy to solve the 3D space ambiguity in the occluded regions and learn sharp boundaries for each object. Extensive experiments demonstrate that our system not only achieves competitive performance for static scene novel-view synthesis, but also produces realistic rendering for object-level editing.},
  YEAR = {2021},
  URL = {http://arxiv.org/abs/2109.01847v1},
  FILE = {2109.01847v1.pdf}
 }",https://arxiv.org/pdf/2109.01847.pdf,https://zju3dv.github.io/object_nerf/,https://github.com/zju3dv/object_nerf,,https://www.youtube.com/watch?v=VTEROu-Yz04,http://www.cad.zju.edu.cn/home/gfzhang/papers/object_nerf/object_nerf_supp.pdf,,"Editable, Conditional Neural Field, Voxel Grid, Volume partitioning, Object-Centric Hybrid Geometry Parameterization, Segmentation",,Density,,,,,,,,"Bangbang Yang, Yinda Zhang, Yinghao Xu, Yijin Li, Han Zhou, Hujun Bao, Guofeng Zhang, Zhaopeng Cui",yang2021learning,62,,"Implicit neural rendering techniques have shown promising results for novel view synthesis. However, existing methods usually encode the entire scene as a whole, which is generally not aware of the object identity and limits the ability to the high-level editing tasks such as moving or adding furniture. In this paper, we present a novel neural scene rendering system, which learns an object-compositional neural radiance field and produces realistic rendering with editing capability for a clustered and real-world scene. Specifically, we design a novel two-pathway architecture, in which the scene branch encodes the scene geometry and appearance, and the object branch encodes each standalone object conditioned on learnable object activation codes. To survive the training in heavily cluttered scenes, we propose a scene-guided training strategy to solve the 3D space ambiguity in the occluded regions and learn sharp boundaries for each object. Extensive experiments demonstrate that our system not only achieves competitive performance for static scene novel-view synthesis, but also produces realistic rendering for object-level editing.",,,
,,,Implicit Neural Distance Representation for Unsupervised and Supervised Classification of Complex Anatomies,,MICCAI 2021,9/21/2021,2021,"@inproceedings{juhl2021implicit,
  ID = {juhl2021implicit},
  ENTRYTYPE = {inproceedings},
  ABSTRACT = {The task of 3D shape classification is closely related to finding a good representation of the shapes. In this study, we focus on surface representations of complex anatomies and on how such representations can be utilized for super-and unsupervised classification. We present a novel Implicit Neural Distance Representation based on unsigned distance fields (UDFs). The UDFs can be embedded into a low-dimensional latent space, which is optimized using only the shape itself. We demonstrate that this self-optimized latent space},
  AUTHOR = {Kristine Aavild Juhl and Xabier Morales and Ole de Backer and Oscar Camara and Rasmus Reinhold Paulsen},
  BOOKTITLE = {International Conference on Medical Image Computing and Computer-Assisted Intervention},
  ORGANIZATION = {Springer},
  PAGES = {405--415},
  PUB_YEAR = {2021},
  TITLE = {Implicit Neural Distance Representation for Unsupervised and Supervised Classification of Complex Anatomies},
  VENUE = {... Conference on Medical ...}
 }",https://link.springer.com/content/pdf/10.1007%2F978-3-030-87196-3.pdf,,https://github.com/kristineaajuhl/Implicit-Neural-Distance-Representation-of-Complex-Anatomie,,,,,"Human (Body), Human (Head), Beyond Visual Computing, Physics-Informed Neural Networks/Science & Engineering, Classification, Conditional Neural Field",,UDF,,,,,,,,"Kristine Aavild Juhl, Xabier Morales, Oscar Camara, Ole de Backer and Rasmus Reinhold Paulsen",juhl2021implicit,63,,"The task of 3D shape classification is closely related to finding a good representation of the shapes. In this study, we focus on surface representations of complex anatomies and on how such representations can be utilized for super-and unsupervised classification. We present a novel Implicit Neural Distance Representation based on unsigned distance fields (UDFs). The UDFs can be embedded into a low-dimensional latent space, which is optimized using only the shape itself. We demonstrate that this self-optimized latent space",No,,
,,,A Skeleton-Driven Neural Occupancy Representation for Articulated Hands,HALO,ARXIV 2021,9/23/2021,2021,"@article{karunratanakul2021halo,
  JOURNAL = {arXiv preprint arXiv:2109.11399},
  BOOKTITLE = {ArXiv Pre-print},
  ID = {karunratanakul2021halo},
  ENTRYTYPE = {article},
  AUTHOR = {Korrawe Karunratanakul and Adrian Spurr and Zicong Fan and Otmar Hilliges and Siyu Tang},
  TITLE = {A Skeleton-Driven Neural Occupancy Representation for Articulated Hands},
  EPRINT = {2109.11399v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We present Hand ArticuLated Occupancy (HALO), a novel representation of articulated hands that bridges the advantages of 3D keypoints and neural implicit surfaces and can be used in end-to-end trainable architectures. Unlike existing statistical parametric hand models (e.g.~MANO), HALO directly leverages 3D joint skeleton as input and produces a neural occupancy volume representing the posed hand surface. The key benefits of HALO are (1) it is driven by 3D key points, which have benefits in terms of accuracy and are easier to learn for neural networks than the latent hand-model parameters; (2) it provides a differentiable volumetric occupancy representation of the posed hand; (3) it can be trained end-to-end, allowing the formulation of losses on the hand surface that benefit the learning of 3D keypoints. We demonstrate the applicability of HALO to the task of conditional generation of hands that grasp 3D objects. The differentiable nature of HALO is shown to improve the quality of the synthesized hands both in terms of physical plausibility and user preference.},
  YEAR = {2021},
  URL = {http://arxiv.org/abs/2109.11399v1},
  FILE = {2109.11399v1.pdf}
 }",https://arxiv.org/pdf/2109.11399.pdf,,,,,,,"Human hand, Conditional Neural Field, Articulated, Coordinate Re-mapping",,Occupancy,,,,,,,,"Korrawe Karunratanakul, Adrian Spurr, Zicong Fan, Otmar Hilliges, Siyu Tang",karunratanakul2021halo,64,,"We present Hand ArticuLated Occupancy (HALO), a novel representation of articulated hands that bridges the advantages of 3D keypoints and neural implicit surfaces and can be used in end-to-end trainable architectures. Unlike existing statistical parametric hand models (e.g.~MANO), HALO directly leverages 3D joint skeleton as input and produces a neural occupancy volume representing the posed hand surface. The key benefits of HALO are (1) it is driven by 3D key points, which have benefits in terms of accuracy and are easier to learn for neural networks than the latent hand-model parameters; (2) it provides a differentiable volumetric occupancy representation of the posed hand; (3) it can be trained end-to-end, allowing the formulation of losses on the hand surface that benefit the learning of 3D keypoints. We demonstrate the applicability of HALO to the task of conditional generation of hands that grasp 3D objects. The differentiable nature of HALO is shown to improve the quality of the synthesized hands both in terms of physical plausibility and user preference.",,,
,,,Neural Knitworks: Patched Neural Implicit Representation Networks,Neural Knitworks,ARXIV 2021,,2021,,https://arxiv.org/pdf/2109.14406.pdf,,,,,,,"Image, Generative Models, Conditional Neural Field",,,,,,,,,,,,,,,No,Yes,
,,,Pixel Codec Avatars,PiCA,CVPR 2021 (Oral),,2021,,https://arxiv.org/pdf/2104.04638.pdf,,,,,,,"Human (Head), Generalization, Image, Conditional Neural Field, Hybrid Geometry Parameterization, Data-Driven Method",,,,,,,,,,,,,,,,,
,,,DISN: Deep Implicit Surface Network for High-quality Single-view 3D Reconstruction,DISN,NeurIPS 2019,,2019,,https://arxiv.org/pdf/1905.10711.pdf,,https://github.com/laughtervv/DISN,,,,,"Conditional Neural Field, Lifting 2D CNN Features to 3D",,SDF,,,,,,,,,,,,,,,