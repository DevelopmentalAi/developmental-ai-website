Timestamp,Title,Nickname,Date,PDF,Project Webpage,Code Release,Talk/Video,Supplement (pdf),Supplement (video),Paper summary figure #,Citation,Task,Techniques,Frequency Encoding,Geometry proxy,Generalization,Training time (hr),Rendering time (FPS),Dataset(s) used,# of views,Lighting,Inputs,Venue,Data Release,Geometry only,Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z),Authors,Bibtex Name,UID,Abstract,Citation Count,"Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)",Coordinates all at once,,,
8/29/2021 20:09:57,Approximating Reflectance Functions using Neural Networks,,6/29/1998,https://link.springer.com/chapter/10.1007/978-3-7091-6453-2_3,,,,,,,"@inproceedings{gargan1998approximating,
  TITLE = {Approximating reflectance functions using neural networks},
  AUTHOR = {Gargan, David and Neelamkavil, Francis},
  BOOKTITLE = {Eurographics Workshop on Rendering Techniques},
  PAGES = {23--34},
  YEAR = {1998},
  ORGANIZATION = {Springer}
 }",Material/lighting estimation,,None,,,,,,,,,,,,,"David Gargan, Francis Neelamkavil",gargan1998approximating,00000176,"We present a new representation for the storage and reconstruction of arbitrary reflectance functions. This non-linear representation, based on a neural network model, accurately captures the spectral and spatial variation of these functions. It is both computationally efficient and concise, yet expressive. We reconstruct the subtle reflection characteristics of an analytic reflection model as well as measured and simulated reflection data",11,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnuf1YMAGP9Cmsm8aMpGiVTOx68DFhN77pwCt1WXI7CuM84pJpN6_MY2Ibdmt_ezOQrw
6/29/2021 15:19:16,3D Object Reconstruction and Representation Using Neural Networks,,6/15/2004,http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.101.6810&rep=rep1&type=pdf,,,,,,,"@book{lim20043d,
  TITLE = {3D Object Reconstruction and Representation Using Neural Networks},
  AUTHOR = {Lim, Wen Peng and Shamsuddin, Siti Mariyam},
  YEAR = {2004},
  PUBLISHER = {Universiti Teknologi Malaysia}
 }",Fundamentals,,None,Other,Per-scene,,,,,,,,,Yes,,"Lim Wen Peng, Siti Mariyam Shamsuddin",lim20043d,00000000,"3D object reconstruction is frequent used in various fields such as product design, engineering, medical and artistic applications. Numerous reconstruction techniques and software were introduced and developed. However, the purpose of this paper is to fully integrate an adaptive artificial neural network (ANN) based method in reconstructing and representing 3D objects. This study explores the ability of neural networks in learning through experience when reconstructing an object by estimating it’s z-coordinate. Neural networks’ capability in representing most classes of 3D objects used in computer graphics is also proven. Simple affined transformation is applied on different objects using this approach and compared with the real objects. The results show that neural network is a promising approach for reconstruction and representation of 3D objects.",23,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnudnSrtQeTInFMc9JVQbhnPHwLuoOgHm3rPC2rryGdkCj1qQ7_Tcpcn6GFMD3S5bFDw
7/19/2021 22:02:18,FoldingNet: Point Cloud Auto-encoder via Deep Grid Deformation,FoldingNet,12/19/2017,https://arxiv.org/pdf/1712.07262.pdf,https://ai4ce.github.io/publication/yang-2018-foldingnet/,https://www.merl.com/research/license#FoldingNet,https://www.youtube.com/embed/WrEKJeK-Wow?rel=0&start=4130&end=4365,,,,"@article{yang2017foldingnet,
  AUTHOR = {Yaoqing Yang and Chen Feng and Yiru Shen and Dong Tian},
  TITLE = {FoldingNet: Point Cloud Auto-encoder via Deep Grid Deformation},
  EPRINT = {1712.07262v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Recent deep networks that directly handle points in a point set, e.g.,PointNet, have been state-of-the-art for supervised learning tasks on pointclouds such as classification and segmentation. In this work, a novelend-to-end deep auto-encoder is proposed to address unsupervised learningchallenges on point clouds. On the encoder side, a graph-based enhancement isenforced to promote local structures on top of PointNet. Then, a novelfolding-based decoder deforms a canonical 2D grid onto the underlying 3D objectsurface of a point cloud, achieving low reconstruction errors even for objectswith delicate structures. The proposed decoder only uses about 7% parameters ofa decoder with fully-connected neural networks, yet leads to a morediscriminative representation that achieves higher linear SVM classificationaccuracy than the benchmark. In addition, the proposed decoder structure isshown, in theory, to be a generic architecture that is able to reconstruct anarbitrary point cloud from a 2D grid. Our code is available athttp://www.merl.com/research/license#FoldingNet},
  YEAR = {2017},
  MONTH = {Dec},
  URL = {http://arxiv.org/abs/1712.07262v2},
  FILE = {1712.07262v2.pdf}
 }",Surface reconstruction,,None,Atlas,Per-scene,,,,,,,CVPR 2018,,Yes,Direct,"Yaoqing Yang, Chen Feng, Yiru Shen, Dong Tian",yang2017foldingnet,00000001,"Recent deep networks that directly handle points in a point set, e.g., PointNet, have been state-of-the-art for supervised learning tasks on point clouds such as classification and segmentation. In this work, a novel end-to-end deep auto-encoder is proposed to address unsupervised learning challenges on point clouds. On the encoder side, a graph-based enhancement is enforced to promote local structures on top of PointNet. Then, a novel folding-based decoder deforms a canonical 2D grid onto the underlying 3D object surface of a point cloud, achieving low reconstruction errors even for objects with delicate structures. The proposed decoder only uses about 7% parameters of a decoder with fully-connected neural networks, yet leads to a more discriminative representation that achieves higher linear SVM classification accuracy than the benchmark. In addition, the proposed decoder structure is shown, in theory, to be a generic architecture that is able to reconstruct an arbitrary point cloud from a 2D grid. Our code is available at http://www.merl.com/research/license#FoldingNet",433,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnuek-Fyp-QpxmpzXbybZvUEJ3m6_4VXUfE-ROdsgIVvjwtAE6yHl1ZxVyZNJ5JMQdOQ
5/23/2021 19:23:20,AtlasNet: A Papier-Mâché Approach to Learning 3D Surface Generation,AtlasNet,2/15/2018,https://arxiv.org/pdf/1802.05384.pdf,http://imagine.enpc.fr/~groueixt/atlasnet/,https://github.com/ThibaultGROUEIX/AtlasNet,"http://imagine.enpc.fr/~groueixt/atlasnet/atlasnet_slides_spotlight_CVPR.pptx, http://imagine.enpc.fr/~groueixt/atlasnet/atlasnet_poster.pdf",,,,"@article{groueix2018atlasnet,
  AUTHOR = {Thibault Groueix and Matthew Fisher and Vladimir G. Kim and Bryan C. Russell and Mathieu Aubry},
  TITLE = {AtlasNet: A Papier-Mache Approach to Learning 3D Surface Generation},
  EPRINT = {1802.05384v3},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We introduce a method for learning to generate the surface of 3D shapes. Ourapproach represents a 3D shape as a collection of parametric surface elementsand, in contrast to methods generating voxel grids or point clouds, naturallyinfers a surface representation of the shape. Beyond its novelty, our new shapegeneration framework, AtlasNet, comes with significant advantages, such asimproved precision and generalization capabilities, and the possibility togenerate a shape of arbitrary resolution without memory issues. We demonstratethese benefits and compare to strong baselines on the ShapeNet benchmark fortwo applications: (i) auto-encoding shapes, and (ii) single-view reconstructionfrom a still image. We also provide results showing its potential for otherapplications, such as morphing, parametrization, super-resolution, matching,and co-segmentation.},
  YEAR = {2018},
  MONTH = {Feb},
  URL = {http://arxiv.org/abs/1802.05384v3},
  FILE = {1802.05384v3.pdf}
 }",,"Conditional neural field, Sampling, Data-driven component (pre-trained, cross-scene)",,,,,,,,,,,,Yes,,"Thibault Groueix, Matthew Fisher, Vladimir G. Kim, Bryan C. Russell, Mathieu Aubry",groueix2018atlasnet,00000002,"We introduce a method for learning to generate the surface of 3D shapes. Our approach represents a 3D shape as a collection of parametric surface elements and, in contrast to methods generating voxel grids or point clouds, naturally infers a surface representation of the shape. Beyond its novelty, our new shape generation framework, AtlasNet, comes with significant advantages, such as improved precision and generalization capabilities, and the possibility to generate a shape of arbitrary resolution without memory issues. We demonstrate these benefits and compare to strong baselines on the ShapeNet benchmark for two applications: (i) auto-encoding shapes, and (ii) single-view reconstruction from a still image. We also provide results showing its potential for other applications, such as morphing, parametrization, super-resolution, matching, and co-segmentation.",6,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnueiuL51nb8dOvM7QtFI9CQ5z5pYBoBLBJPY1t3Zu0wfM_CfuefApLYoHk8G4OIOyd0
9/17/2021 22:42:54,Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations,,6/13/2018,https://www.sciencedirect.com/science/article/pii/S0021999118307125,https://maziarraissi.github.io/PINNs/,https://github.com/maziarraissi/PINNs,,,,,"@article{raissi2019physics,
  ABSTRACT = {We introduce physics-informed neural networks-neural networks that are trained to solve supervised learning tasks while respecting any given laws of physics described by general nonlinear partial differential equations. In this work, we present our developments in the context of solving two main classes of problems: data-driven solution and data-driven discovery of partial differential equations. Depending on the nature and arrangement of the available data, we devise two distinct types of algorithms, namely continuous time and},
  AUTHOR = {Raissi, Maziar and Perdikaris, Paris and Karniadakis, George E},
  JOURNAL = {Journal of Computational Physics},
  PAGES = {686--707},
  PUB_YEAR = {2019},
  PUBLISHER = {Elsevier},
  TITLE = {Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations},
  VENUE = {Journal of Computational Physics},
  VOLUME = {378}
 }","Beyond graphics, Fundamentals, Science and engineering, PDE",,,,,,,,,,,,,,,"Maziar Raissi, Paris Perdikaris, George E Karniadakis",raissi2019physics,00000213,"We introduce physics-informed neural networks–neural networks that are trained to solve supervised learning tasks while respecting any given laws of physics described by general nonlinear partial differential equations. In this work, we present our developments in the context of solving two main classes of problems: data-driven solution and data-driven discovery of partial differential equations. Depending on the nature and arrangement of the available data, we devise two distinct types of algorithms, namely continuous time and",,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnudUCBrQjg0VOBD1ZBTShMPRRN57oj8R4w__1WJMHq-4ZKHI28uhdmcwpWD2HIAyDKA
6/29/2021 15:56:46,Deep Geometric Prior for Surface Reconstruction,,11/27/2018,https://arxiv.org/pdf/1811.10943.pdf,,https://github.com/fwilliams/deep-geometric-prior,,,,,"@article{williams2018deep,
  AUTHOR = {Francis Williams and Teseo Schneider and Claudio Silva and Denis Zorin and Joan Bruna and Daniele Panozzo},
  TITLE = {Deep Geometric Prior for Surface Reconstruction},
  EPRINT = {1811.10943v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {The reconstruction of a discrete surface from a point cloud is a fundamentalgeometry processing problem that has been studied for decades, with manymethods developed. We propose the use of a deep neural network as a geometricprior for surface reconstruction. Specifically, we overfit a neural networkrepresenting a local chart parameterization to part of an input point cloudusing the Wasserstein distance as a measure of approximation. By jointlyfitting many such networks to overlapping parts of the point cloud, whileenforcing a consistency condition, we compute a manifold atlas. By samplingthis atlas, we can produce a dense reconstruction of the surface approximatingthe input cloud. The entire procedure does not require any training data orexplicit regularization, yet, we show that it is able to perform remarkablywell: not introducing typical overfitting artifacts, and approximating sharpfeatures closely at the same time. We experimentally show that this geometricprior produces good results for both man-made objects containing sharp featuresand smoother organic objects, as well as noisy inputs. We compare our methodwith a number of well-known reconstruction methods on a standard surfacereconstruction benchmark.},
  YEAR = {2018},
  MONTH = {Nov},
  URL = {http://arxiv.org/abs/1811.10943v2},
  FILE = {1811.10943v2.pdf}
 }",,,None,Atlas,Per-scene,,,,,,,,,Yes,,"Francis Williams, Teseo Schneider, Claudio Silva, Denis Zorin, Joan Bruna, Daniele Panozzo",williams2018deep,00000003,"The reconstruction of a discrete surface from a point cloud is a fundamental geometry processing problem that has been studied for decades, with many methods developed. We propose the use of a deep neural network as a geometric prior for surface reconstruction. Specifically, we overfit a neural network representing a local chart parameterization to part of an input point cloud using the Wasserstein distance as a measure of approximation. By jointly fitting many such networks to overlapping parts of the point cloud, while enforcing a consistency condition, we compute a manifold atlas. By sampling this atlas, we can produce a dense reconstruction of the surface approximating the input cloud. The entire procedure does not require any training data or explicit regularization, yet, we show that it is able to perform remarkably well: not introducing typical overfitting artifacts, and approximating sharp features closely at the same time. We experimentally show that this geometric prior produces good results for both man-made objects containing sharp features and smoother organic objects, as well as noisy inputs. We compare our method with a number of well-known reconstruction methods on a standard surface reconstruction benchmark.",64,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnufMO8wx0iBcNCelxUupJeAX8Vzy4HUWdSjUP0PePvb7lh81v7Vx_EqHgPj3ay3zfi4
6/29/2021 15:38:57,Learning Implicit Fields for Generative Shape Modeling,IM-NET,12/6/2018,https://arxiv.org/pdf/1812.02822.pdf,https://www.sfu.ca/~zhiqinc/imgan/Readme.html,https://github.com/czq142857/implicit-decoder,,,,,"@article{chen2018imnet,
  AUTHOR = {Zhiqin Chen and Hao Zhang},
  TITLE = {Learning Implicit Fields for Generative Shape Modeling},
  EPRINT = {1812.02822v5},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.GR},
  ABSTRACT = {We advocate the use of implicit fields for learning generative models ofshapes and introduce an implicit field decoder, called IM-NET, for shapegeneration, aimed at improving the visual quality of the generated shapes. Animplicit field assigns a value to each point in 3D space, so that a shape canbe extracted as an iso-surface. IM-NET is trained to perform this assignment bymeans of a binary classifier. Specifically, it takes a point coordinate, alongwith a feature vector encoding a shape, and outputs a value which indicateswhether the point is outside the shape or not. By replacing conventionaldecoders by our implicit decoder for representation learning (via IM-AE) andshape generation (via IM-GAN), we demonstrate superior results for tasks suchas generative shape modeling, interpolation, and single-view 3D reconstruction,particularly in terms of visual quality. Code and supplementary material areavailable at https://github.com/czq142857/implicit-decoder.},
  YEAR = {2018},
  MONTH = {Dec},
  URL = {http://arxiv.org/abs/1812.02822v5},
  FILE = {1812.02822v5.pdf}
 }",Generalization,"Generative/adversarial formulation, Conditional neural field, Data-driven component (pre-trained, cross-scene)",None,Occupancy,Category-level,,,,,,,CVPR 2019,,Yes,,"Zhiqin Chen, Hao Zhang",chen2018imnet,00000004,"We advocate the use of implicit fields for learning generative models of shapes and introduce an implicit field decoder, called IM-NET, for shape generation, aimed at improving the visual quality of the generated shapes. An implicit field assigns a value to each point in 3D space, so that a shape can be extracted as an iso-surface. IM-NET is trained to perform this assignment by means of a binary classifier. Specifically, it takes a point coordinate, along with a feature vector encoding a shape, and outputs a value which indicates whether the point is outside the shape or not. By replacing conventional decoders by our implicit decoder for representation learning (via IM-AE) and shape generation (via IM-GAN), we demonstrate superior results for tasks such as generative shape modeling, interpolation, and single-view 3D reconstruction, particularly in terms of visual quality. Code and supplementary material are available at https://github.com/czq142857/implicit-decoder.",324,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnucCJ2CBz62kpppZ18WGIe-PKNL0VGowvckqJAEYGlVGln-46_mRtMAd61SRpYZGkdo
5/23/2021 19:22:38,Occupancy Networks: Learning 3D Reconstruction in Function Space,Occupancy Networks,12/10/2018,https://arxiv.org/pdf/1812.03828.pdf,https://avg.is.tuebingen.mpg.de/publications/occupancy-networks,https://github.com/autonomousvision/occupancy_networks,https://www.youtube.com/watch?v=w1Qo3bOiPaE,,,,"@article{mescheder2018occupancy networks,
  AUTHOR = {Lars Mescheder and Michael Oechsle and Michael Niemeyer and Sebastian Nowozin and Andreas Geiger},
  TITLE = {Occupancy Networks: Learning 3D Reconstruction in Function Space},
  EPRINT = {1812.03828v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {With the advent of deep neural networks, learning-based approaches for 3Dreconstruction have gained popularity. However, unlike for images, in 3D thereis no canonical representation which is both computationally and memoryefficient yet allows for representing high-resolution geometry of arbitrarytopology. Many of the state-of-the-art learning-based 3D reconstructionapproaches can hence only represent very coarse 3D geometry or are limited to arestricted domain. In this paper, we propose Occupancy Networks, a newrepresentation for learning-based 3D reconstruction methods. Occupancy networksimplicitly represent the 3D surface as the continuous decision boundary of adeep neural network classifier. In contrast to existing approaches, ourrepresentation encodes a description of the 3D output at infinite resolutionwithout excessive memory footprint. We validate that our representation canefficiently encode 3D structure and can be inferred from various kinds ofinput. Our experiments demonstrate competitive results, both qualitatively andquantitatively, for the challenging tasks of 3D reconstruction from singleimages, noisy point clouds and coarse discrete voxel grids. We believe thatoccupancy networks will become a useful tool in a wide variety oflearning-based 3D tasks.},
  YEAR = {2018},
  MONTH = {Dec},
  URL = {http://arxiv.org/abs/1812.03828v2},
  FILE = {1812.03828v2.pdf}
 }",Generalization,"Conditional neural field, Sampling",,Occupancy,,,,,,,,CVPR 2019,,Yes,,"Lars Mescheder, Michael Oechsle, Michael Niemeyer, Sebastian Nowozin, Andreas Geiger",mescheder2018occupancy,00000005,"With the advent of deep neural networks, learning-based approaches for 3D reconstruction have gained popularity. However, unlike for images, in 3D there is no canonical representation which is both computationally and memory efficient yet allows for representing high-resolution geometry of arbitrary topology. Many of the state-of-the-art learning-based 3D reconstruction approaches can hence only represent very coarse 3D geometry or are limited to a restricted domain. In this paper, we propose Occupancy Networks, a new representation for learning-based 3D reconstruction methods. Occupancy networks implicitly represent the 3D surface as the continuous decision boundary of a deep neural network classifier. In contrast to existing approaches, our representation encodes a description of the 3D output at infinite resolution without excessive memory footprint. We validate that our representation can efficiently encode 3D structure and can be inferred from various kinds of input. Our experiments demonstrate competitive results, both qualitatively and quantitatively, for the challenging tasks of 3D reconstruction from single images, noisy point clouds and coarse discrete voxel grids. We believe that occupancy networks will become a useful tool in a wide variety of learning-based 3D tasks.",540,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnudfTlW6ReA6w6Q9rC38zylpXEvcdvJgheHeBPcMtCZAa-wddBVC1C71lBZSNAzXmrM
5/23/2021 19:20:13,DeepSDF: Learning Continuous Signed Distance Functions for Shape Representation,DeepSDF,1/16/2019,https://arxiv.org/pdf/1901.05103.pdf,,https://github.com/facebookresearch/DeepSDF,,,,,"@article{park2019deepsdf,
  AUTHOR = {Jeong Joon Park and Peter Florence and Julian Straub and Richard Newcombe and Steven Lovegrove},
  TITLE = {DeepSDF: Learning Continuous Signed Distance Functions for ShapeRepresentation},
  EPRINT = {1901.05103v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Computer graphics, 3D computer vision and robotics communities have producedmultiple approaches to representing 3D geometry for rendering andreconstruction. These provide trade-offs across fidelity, efficiency andcompression capabilities. In this work, we introduce DeepSDF, a learnedcontinuous Signed Distance Function (SDF) representation of a class of shapesthat enables high quality shape representation, interpolation and completionfrom partial and noisy 3D input data. DeepSDF, like its classical counterpart,represents a shape's surface by a continuous volumetric field: the magnitude ofa point in the field represents the distance to the surface boundary and thesign indicates whether the region is inside (-) or outside (+) of the shape,hence our representation implicitly encodes a shape's boundary as thezero-level-set of the learned function while explicitly representing theclassification of space as being part of the shapes interior or not. Whileclassical SDF's both in analytical or discretized voxel form typicallyrepresent the surface of a single shape, DeepSDF can represent an entire classof shapes. Furthermore, we show state-of-the-art performance for learned 3Dshape representation and completion while reducing the model size by an orderof magnitude compared with previous work.},
  YEAR = {2019},
  MONTH = {Jan},
  URL = {http://arxiv.org/abs/1901.05103v1},
  FILE = {1901.05103v1.pdf}
 }",Generalization,"Generative/adversarial formulation, Conditional neural field",,SDF,,,,,,,,,,Yes,,"Jeong Joon Park, Peter Florence, Julian Straub, Richard Newcombe, Steven Lovegrove",park2019deepsdf,00000006,"Computer graphics, 3D computer vision and robotics communities have produced multiple approaches to representing 3D geometry for rendering and reconstruction. These provide trade-offs across fidelity, efficiency and compression capabilities. In this work, we introduce DeepSDF, a learned continuous Signed Distance Function (SDF) representation of a class of shapes that enables high quality shape representation, interpolation and completion from partial and noisy 3D input data. DeepSDF, like its classical counterpart, represents a shape's surface by a continuous volumetric field: the magnitude of a point in the field represents the distance to the surface boundary and the sign indicates whether the region is inside (-) or outside (+) of the shape, hence our representation implicitly encodes a shape's boundary as the zero-level-set of the learned function while explicitly representing the classification of space as being part of the shapes interior or not. While classical SDF's both in analytical or discretized voxel form typically represent the surface of a single shape, DeepSDF can represent an entire class of shapes. Furthermore, we show state-of-the-art performance for learned 3D shape representation and completion while reducing the model size by an order of magnitude compared with previous work.",593,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnucQsvhhQQJI8wojBk0UOw9WwZdQB6q3wR3pgISbiYGRVyzSUaNEqcRkm422DKqbjIA
6/29/2021 16:44:22,PIFu: Pixel-Aligned Implicit Function for High-Resolution Clothed Human Digitization,PIFu,5/13/2019,https://arxiv.org/pdf/1905.05172.pdf,https://shunsukesaito.github.io/PIFu/,https://github.com/shunsukesaito/PIFu,https://www.youtube.com/watch?v=S1FpjwKqtPs,,,,"@article{saito2019pifu,
  AUTHOR = {Shunsuke Saito and Zeng Huang and Ryota Natsume and Shigeo Morishima and Angjoo Kanazawa and Hao Li},
  TITLE = {PIFu: Pixel-Aligned Implicit Function for High-Resolution Clothed HumanDigitization},
  EPRINT = {1905.05172v3},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We introduce Pixel-aligned Implicit Function (PIFu), a highly effectiveimplicit representation that locally aligns pixels of 2D images with the globalcontext of their corresponding 3D object. Using PIFu, we propose an end-to-enddeep learning method for digitizing highly detailed clothed humans that caninfer both 3D surface and texture from a single image, and optionally, multipleinput images. Highly intricate shapes, such as hairstyles, clothing, as well astheir variations and deformations can be digitized in a unified way. Comparedto existing representations used for 3D deep learning, PIFu can producehigh-resolution surfaces including largely unseen regions such as the back of aperson. In particular, it is memory efficient unlike the voxel representation,can handle arbitrary topology, and the resulting surface is spatially alignedwith the input image. Furthermore, while previous techniques are designed toprocess either a single image or multiple views, PIFu extends naturally toarbitrary number of views. We demonstrate high-resolution and robustreconstructions on real world images from the DeepFashion dataset, whichcontains a variety of challenging clothing types. Our method achievesstate-of-the-art performance on a public benchmark and outperforms the priorwork for clothed human digitization from a single image.},
  YEAR = {2019},
  MONTH = {May},
  NOTE = {The IEEE International Conference on Computer Vision (ICCV), 2019,
  PP.2304-2314},
  URL = {http://arxiv.org/abs/1905.05172v3},
  FILE = {1905.05172v3.pdf}
 }","Human body, Few-shot reconstruction, Generalization","Lifting 2D features to 3D, Image-based rendering, Data-driven component (pre-trained, cross-scene)",None,Occupancy,Category-level,,,,,,,CVPR,,No,,"Shunsuke Saito, Zeng Huang, Ryota Natsume, Shigeo Morishima, Angjoo Kanazawa, Hao Li",saito2019pifu,00000007,"We introduce Pixel-aligned Implicit Function (PIFu), a highly effective implicit representation that locally aligns pixels of 2D images with the global context of their corresponding 3D object. Using PIFu, we propose an end-to-end deep learning method for digitizing highly detailed clothed humans that can infer both 3D surface and texture from a single image, and optionally, multiple input images. Highly intricate shapes, such as hairstyles, clothing, as well as their variations and deformations can be digitized in a unified way. Compared to existing representations used for 3D deep learning, PIFu can produce high-resolution surfaces including largely unseen regions such as the back of a person. In particular, it is memory efficient unlike the voxel representation, can handle arbitrary topology, and the resulting surface is spatially aligned with the input image. Furthermore, while previous techniques are designed to process either a single image or multiple views, PIFu extends naturally to arbitrary number of views. We demonstrate high-resolution and robust reconstructions on real world images from the DeepFashion dataset, which contains a variety of challenging clothing types. Our method achieves state-of-the-art performance on a public benchmark and outperforms the prior work for clothed human digitization from a single image.",295,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnuczDvzOo6jijfwpqZsDKHYM4_gVORdLznomp7IN1xtFDnBhVE2uRNDdzRs1Ix4EndI
8/29/2021 21:45:27,Texture Fields: Learning Texture Representations in Function Space,Texture Fields,5/17/2019,https://arxiv.org/pdf/1905.07259.pdf,https://autonomousvision.github.io/texture-fields/,https://github.com/autonomousvision/texture_fields,https://www.youtube.com/watch?v=pbfeE0qmD2E,http://www.cvlibs.net/publications/Oechsle2019ICCV_supplementary.pdf,,,"@article{oechsle2019texturefields,
  AUTHOR = {Michael Oechsle and Lars Mescheder and Michael Niemeyer and Thilo Strauss and Andreas Geiger},
  TITLE = {Texture Fields: Learning Texture Representations in Function Space},
  EPRINT = {1905.07259v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {In recent years, substantial progress has been achieved in learning-basedreconstruction of 3D objects. At the same time, generative models were proposedthat can generate highly realistic images. However, despite this success inthese closely related tasks, texture reconstruction of 3D objects has receivedlittle attention from the research community and state-of-the-art methods areeither limited to comparably low resolution or constrained experimental setups.A major reason for these limitations is that common representations of textureare inefficient or hard to interface for modern deep learning techniques. Inthis paper, we propose Texture Fields, a novel texture representation which isbased on regressing a continuous 3D function parameterized with a neuralnetwork. Our approach circumvents limiting factors like shape discretizationand parameterization, as the proposed texture representation is independent ofthe shape representation of the 3D object. We show that Texture Fields are ableto represent high frequency texture and naturally blend with modern deeplearning techniques. Experimentally, we find that Texture Fields comparefavorably to state-of-the-art methods for conditional texture reconstruction of3D objects and enable learning of probabilistic generative models for texturingunseen 3D models. We believe that Texture Fields will become an importantbuilding block for the next generation of generative 3D models.},
  YEAR = {2019},
  MONTH = {May},
  URL = {http://arxiv.org/abs/1905.07259v1},
  FILE = {1905.07259v1.pdf}
 }",Generalization,"Generative/adversarial formulation, Conditional neural field, Data-driven component (pre-trained, cross-scene)",None,,,,,,,,,ICCV 2019,,,Direct,"Michael Oechsle, Lars Mescheder, Michael Niemeyer, Thilo Strauss, Andreas Geiger",oechsle2019texturefields,00000186,"In recent years, substantial progress has been achieved in learning-based reconstruction of 3D objects. At the same time, generative models were proposed that can generate highly realistic images. However, despite this success in these closely related tasks, texture reconstruction of 3D objects has received little attention from the research community and state-of-the-art methods are either limited to comparably low resolution or constrained experimental setups. A major reason for these limitations is that common representations of texture are inefficient or hard to interface for modern deep learning techniques. In this paper, we propose Texture Fields, a novel texture representation which is based on regressing a continuous 3D function parameterized with a neural network. Our approach circumvents limiting factors like shape discretization and parameterization, as the proposed texture representation is independent of the shape representation of the 3D object. We show that Texture Fields are able to represent high frequency texture and naturally blend with modern deep learning techniques. Experimentally, we find that Texture Fields compare favorably to state-of-the-art methods for conditional texture reconstruction of 3D objects and enable learning of probabilistic generative models for texturing unseen 3D models. We believe that Texture Fields will become an important building block for the next generation of generative 3D models.",77,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnucXdMLOIdNu7tSreaD7IfcKXpYg9JOMk_XSeRkeXsGh2SEXjGv-1oSuNKi82-sgqA4
8/29/2021 17:38:51,Controlling Neural Level Sets,,5/28/2019,https://arxiv.org/pdf/1905.11911.pdf,https://github.com/matanatz/ControllingNeuralLevelsets,,,,,,"@article{atzmon2019controlling,
  AUTHOR = {Matan Atzmon and Niv Haim and Lior Yariv and Ofer Israelov and Haggai Maron and Yaron Lipman},
  TITLE = {Controlling Neural Level Sets},
  EPRINT = {1905.11911v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.LG},
  ABSTRACT = {The level sets of neural networks represent fundamental properties such asdecision boundaries of classifiers and are used to model non-linear manifolddata such as curves and surfaces. Thus, methods for controlling the neurallevel sets could find many applications in machine learning.In this paper we present a simple and scalable approach to directly controllevel sets of a deep neural network. Our method consists of two parts: (i)sampling of the neural level sets, and (ii) relating the samples' positions tothe network parameters. The latter is achieved by a sample network that isconstructed by adding a single fixed linear layer to the original network. Inturn, the sample network can be used to incorporate the level set samples intoa loss function of interest.We have tested our method on three different learning tasks: improvinggeneralization to unseen data, training networks robust to adversarial attacks,and curve and surface reconstruction from point clouds. For surfacereconstruction, we produce high fidelity surfaces directly from raw 3D pointclouds. When training small to medium networks to be robust to adversarialattacks we obtain robust accuracy comparable to state-of-the-art methods.},
  YEAR = {2019},
  MONTH = {May},
  URL = {http://arxiv.org/abs/1905.11911v2},
  FILE = {1905.11911v2.pdf}
 }","Generalization, Fundamentals","Conditional neural field, Sampling, Data-driven component (pre-trained, cross-scene)",,,,,,,,,,NeurIPS 2019,http://faust.is.tue.mpg.de/,Yes,Direct,"Matan Atzmon, Niv Haim, Lior Yariv, Ofer Israelov, Haggai Maron, Yaron Lipman",atzmon2019controlling,00000173,"The level sets of neural networks represent fundamental properties such as decision boundaries of classifiers and are used to model non-linear manifold data such as curves and surfaces. Thus, methods for controlling the neural level sets could find many applications in machine learning. In this paper we present a simple and scalable approach to directly control level sets of a deep neural network. Our method consists of two parts: (i) sampling of the neural level sets, and (ii) relating the samples' positions to the network parameters. The latter is achieved by a sample network that is constructed by adding a single fixed linear layer to the original network. In turn, the sample network can be used to incorporate the level set samples into a loss function of interest. We have tested our method on three different learning tasks: improving generalization to unseen data, training networks robust to adversarial attacks, and curve and surface reconstruction from point clouds. For surface reconstruction, we produce high fidelity surfaces directly from raw 3D point clouds. When training small to medium networks to be robust to adversarial attacks we obtain robust accuracy comparable to state-of-the-art methods.",30,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnufhBXeC1lFFWdAEu25o_CWpKVV1rQ4t3HZyqh7DQn_2okZHTrtNFaT0Ps-mrG3Pdkg
5/23/2021 19:15:08,Scene Representation Networks: Continuous 3D-Structure-Aware Neural Scene Representations,SRN,6/4/2019,https://arxiv.org/pdf/1906.01618.pdf,https://vsitzmann.github.io/srns/,https://github.com/vsitzmann/scene-representation-networks,"https://www.youtube.com/watch?v=6vMEBWD8O20, https://slideslive.com/38922305/scene-representation-networks-continuous-3dstructureaware-neural-scene-representations",,,,"@article{sitzmann2019srn,
  AUTHOR = {Vincent Sitzmann and Michael Zollhofer and Gordon Wetzstein},
  TITLE = {Scene Representation Networks: Continuous 3D-Structure-Aware NeuralScene Representations},
  EPRINT = {1906.01618v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Unsupervised learning with generative models has the potential of discoveringrich representations of 3D scenes. While geometric deep learning has explored3D-structure-aware representations of scene geometry, these models typicallyrequire explicit 3D supervision. Emerging neural scene representations can betrained only with posed 2D images, but existing methods ignore thethree-dimensional structure of scenes. We propose Scene Representation Networks(SRNs), a continuous, 3D-structure-aware scene representation that encodes bothgeometry and appearance. SRNs represent scenes as continuous functions that mapworld coordinates to a feature representation of local scene properties. Byformulating the image formation as a differentiable ray-marching algorithm,SRNs can be trained end-to-end from only 2D images and their camera poses,without access to depth or shape. This formulation naturally generalizes acrossscenes, learning powerful geometry and appearance priors in the process. Wedemonstrate the potential of SRNs by evaluating them for novel view synthesis,few-shot reconstruction, joint shape and appearance interpolation, andunsupervised discovery of a non-rigid face model.},
  YEAR = {2019},
  MONTH = {Jun},
  URL = {http://arxiv.org/abs/1906.01618v2},
  FILE = {1906.01618v2.pdf}
 }",Generalization,"Conditional neural field, Hypernetwork",,,,,,,,,,CVPR 2020,https://drive.google.com/drive/folders/1OkYgeRcIcLOFu1ft5mRODWNQaPJ0ps90,No,,"Vincent Sitzmann, Michael Zollhöfer, Gordon Wetzstein",sitzmann2019srn,00000008,"Unsupervised learning with generative models has the potential of discovering rich representations of 3D scenes. While geometric deep learning has explored 3D-structure-aware representations of scene geometry, these models typically require explicit 3D supervision. Emerging neural scene representations can be trained only with posed 2D images, but existing methods ignore the three-dimensional structure of scenes. We propose Scene Representation Networks (SRNs), a continuous, 3D-structure-aware scene representation that encodes both geometry and appearance. SRNs represent scenes as continuous functions that map world coordinates to a feature representation of local scene properties. By formulating the image formation as a differentiable ray-marching algorithm, SRNs can be trained end-to-end from only 2D images and their camera poses, without access to depth or shape. This formulation naturally generalizes across scenes, learning powerful geometry and appearance priors in the process. We demonstrate the potential of SRNs by evaluating them for novel view synthesis, few-shot reconstruction, joint shape and appearance interpolation, and unsupervised discovery of a non-rigid face model.",262,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnud0ylozZ3FiZsRhcRQIYWGg0I-KmEgL0HEBLWcVP-2kwLSWsybQ9EtFp52q-3y4JIQ
5/23/2021 19:19:44,Neural Volumes: Learning Dynamic Renderable Volumes from Images,NV,6/18/2019,https://arxiv.org/pdf/1906.07751.pdf,https://stephenlombardi.github.io/projects/neuralvolumes/,https://github.com/facebookresearch/neuralvolumes,"https://youtu.be/JlyGNvbGKB8?t=5347, https://crossminds.ai/video/neural-volumes-learning-dynamic-renderable-volumes-from-images-606f94d175292b321dd0906f/",,,,"@article{lombardi2019nv,
  AUTHOR = {Stephen Lombardi and Tomas Simon and Jason Saragih and Gabriel Schwartz and Andreas Lehrmann and Yaser Sheikh},
  TITLE = {Neural Volumes: Learning Dynamic Renderable Volumes from Images},
  EPRINT = {1906.07751v1},
  DOI = {10.1145/3306346.3323020},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.GR},
  ABSTRACT = {Modeling and rendering of dynamic scenes is challenging, as natural scenesoften contain complex phenomena such as thin structures, evolving topology,translucency, scattering, occlusion, and biological motion. Mesh-basedreconstruction and tracking often fail in these cases, and other approaches(e.g., light field video) typically rely on constrained viewing conditions,which limit interactivity. We circumvent these difficulties by presenting alearning-based approach to representing dynamic objects inspired by theintegral projection model used in tomographic imaging. The approach issupervised directly from 2D images in a multi-view capture setting and does notrequire explicit reconstruction or tracking of the object. Our method has twoprimary components: an encoder-decoder network that transforms input imagesinto a 3D volume representation, and a differentiable ray-marching operationthat enables end-to-end training. By virtue of its 3D representation, ourconstruction extrapolates better to novel viewpoints compared to screen-spacerendering techniques. The encoder-decoder architecture learns a latentrepresentation of a dynamic scene that enables us to produce novel contentsequences not seen during training. To overcome memory limitations ofvoxel-based representations, we learn a dynamic irregular grid structureimplemented with a warp field during ray-marching. This structure greatlyimproves the apparent resolution and reduces grid-like artifacts and jaggedmotion. Finally, we demonstrate how to incorporate surface-basedrepresentations into our volumetric-learning framework for applications wherethe highest resolution is required, using facial performance capture as a casein point.},
  YEAR = {2019},
  MONTH = {Jun},
  NOTE = {ACM Transactions on Graphics (SIGGRAPH 2019) 38, 4, Article 65},
  URL = {http://arxiv.org/abs/1906.07751v1},
  FILE = {1906.07751v1.pdf}
 }",Dynamic,"Conditional neural field, Lifting 2D features to 3D, Representation, Warping field/Flow field",,,,,,,,,,SIGGRAPH 2019,,No,,"Stephen Lombardi, Tomas Simon, Jason Saragih, Gabriel Schwartz, Andreas Lehrmann, Yaser Sheikh",lombardi2019nv,00000009,"Modeling and rendering of dynamic scenes is challenging, as natural scenes often contain complex phenomena such as thin structures, evolving topology, translucency, scattering, occlusion, and biological motion. Mesh-based reconstruction and tracking often fail in these cases, and other approaches (e.g., light field video) typically rely on constrained viewing conditions, which limit interactivity. We circumvent these difficulties by presenting a learning-based approach to representing dynamic objects inspired by the integral projection model used in tomographic imaging. The approach is supervised directly from 2D images in a multi-view capture setting and does not require explicit reconstruction or tracking of the object. Our method has two primary components: an encoder-decoder network that transforms input images into a 3D volume representation, and a differentiable ray-marching operation that enables end-to-end training. By virtue of its 3D representation, our construction extrapolates better to novel viewpoints compared to screen-space rendering techniques. The encoder-decoder architecture learns a latent representation of a dynamic scene that enables us to produce novel content sequences not seen during training. To overcome memory limitations of voxel-based representations, we learn a dynamic irregular grid structure implemented with a warp field during ray-marching. This structure greatly improves the apparent resolution and reduces grid-like artifacts and jagged motion. Finally, we demonstrate how to incorporate surface-based representations into our volumetric-learning framework for applications where the highest resolution is required, using facial performance capture as a case in point.",161,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnudSV31PVn2TvxMvkS_mQG4JEOpa4HEGaodRqwTpo21CBLHQhKtIyikp_QRBIYOZvMo
7/19/2021 21:58:28,Learning elementary structures for 3D shape generation and matching,,8/13/2019,https://arxiv.org/pdf/1908.04725.pdf,,,,,,,"@article{deprelle2019learning,
  AUTHOR = {Theo Deprelle and Thibault Groueix and Matthew Fisher and Vladimir G. Kim and Bryan C. Russell and Mathieu Aubry},
  TITLE = {Learning elementary structures for 3D shape generation and matching},
  EPRINT = {1908.04725v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We propose to represent shapes as the deformation and combination oflearnable elementary 3D structures, which are primitives resulting fromtraining over a collection of shape. We demonstrate that the learned elementary3D structures lead to clear improvements in 3D shape generation and matching.More precisely, we present two complementary approaches for learning elementarystructures: (i) patch deformation learning and (ii) point translation learning.Both approaches can be extended to abstract structures of higher dimensions forimproved results. We evaluate our method on two tasks: reconstructing ShapeNetobjects and estimating dense correspondences between human scans (FAUST interchallenge). We show 16% improvement over surface deformation approaches forshape reconstruction and outperform FAUST inter challenge state of the art by6%.},
  YEAR = {2019},
  MONTH = {Aug},
  URL = {http://arxiv.org/abs/1908.04725v2},
  FILE = {1908.04725v2.pdf}
 }",Surface reconstruction,Conditional neural field,None,Atlas,Per-scene,,,,,,,,,Yes,Direct,"Theo Deprelle, Thibault Groueix, Matthew Fisher, Vladimir G. Kim, Bryan C. Russell, Mathieu Aubry",deprelle2019learning,00000010,"We propose to represent shapes as the deformation and combination of learnable elementary 3D structures, which are primitives resulting from training over a collection of shape. We demonstrate that the learned elementary 3D structures lead to clear improvements in 3D shape generation and matching. More precisely, we present two complementary approaches for learning elementary structures: (i) patch deformation learning and (ii) point translation learning. Both approaches can be extended to abstract structures of higher dimensions for improved results. We evaluate our method on two tasks: reconstructing ShapeNet objects and estimating dense correspondences between human scans (FAUST inter challenge). We show 16% improvement over surface deformation approaches for shape reconstruction and outperform FAUST inter challenge state of the art by 6%.",61,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnucWz0JbhTmqnEnC07hZk6dfXbutol328ey3mcCNjzye-IafqvSveTTQ6e2LgkCFGGg
8/29/2021 21:39:35,Occupancy Flow: 4D Reconstruction by Learning Particle Dynamics,Occupancy Flow,10/1/2019,https://openaccess.thecvf.com/content_ICCV_2019/papers/Niemeyer_Occupancy_Flow_4D_Reconstruction_by_Learning_Particle_Dynamics_ICCV_2019_paper.pdf,https://avg.is.tuebingen.mpg.de/publications/niemeyer2019iccv,https://github.com/autonomousvision/occupancy_flow,https://www.youtube.com/watch?v=c0yOugTgrWc,http://www.cvlibs.net/publications/Niemeyer2019ICCV_supplementary.pdf,,,"@inproceedings{niemeyer2019occupancy,
  TITLE = {Occupancy flow: 4d reconstruction by learning particle dynamics},
  AUTHOR = {Niemeyer, Michael and Mescheder, Lars and Oechsle, Michael and Geiger, Andreas},
  BOOKTITLE = {Proceedings of the IEEE/CVF International Conference on Computer Vision},
  PAGES = {5379--5389},
  YEAR = {2019}
 }","Dynamic, Human body",Warping field/Flow field,None,Occupancy,,,,,,,,ICCV 2019,,Yes,Direct,"Michael Niemeyer, Lars Mescheder, Michael Oechsle, Andreas Geiger",niemeyer2019occupancy,00000185,"Deep learning based 3D reconstruction techniques have recently achieved impressive results. However, while stateof-the-art methods are able to output complex 3D geometry, it is not clear how to extend these results to time-varying topologies. Approaches treating each time step individually lack continuity and exhibit slow inference, while traditional 4D reconstruction methods often utilize a template model or discretize the 4D space at fixed resolution. In this work, we present Occupancy Flow, a novel spatio-temporal representation of time-varying 3D geometry with implicit correspondences. Towards this goal, we learn a temporally and spatially continuous vector field which assigns a motion vector to every point in space and time. In order to perform dense 4D reconstruction from images or sparse point clouds, we combine our method with a continuous 3D representation. Implicitly, our model yields correspondences over time, thus enabling fast inference while providing a sound physical description of the temporal dynamics. We show that our method can be used for interpolation and reconstruction tasks, and demonstrate the accuracy of the learned correspondences. We believe that Occupancy Flow is a promising new 4D representation which will be useful for a variety of spatio-temporal reconstruction tasks.",65,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnucLoBtsV4XpWH8QTfo_tqTthd-FXM7fmOIlKawfZ8Kq3BBdUyuXDu4oPh8tD8K_r0s
7/19/2021 21:31:35,Learning to Infer Implicit Surfaces without 3D Supervision,,11/2/2019,https://arxiv.org/pdf/1911.00767.pdf,,,,,,,"@article{liu2019learning,
  AUTHOR = {Shichen Liu and Shunsuke Saito and Weikai Chen and Hao Li},
  TITLE = {Learning to Infer Implicit Surfaces without 3D Supervision},
  EPRINT = {1911.00767v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Recent advances in 3D deep learning have shown that it is possible to trainhighly effective deep models for 3D shape generation, directly from 2D images.This is particularly interesting since the availability of 3D models is stilllimited compared to the massive amount of accessible 2D images, which isinvaluable for training. The representation of 3D surfaces itself is a keyfactor for the quality and resolution of the 3D output. While explicitrepresentations, such as point clouds and voxels, can span a wide range ofshape variations, their resolutions are often limited. Mesh-basedrepresentations are more efficient but are limited by their ability to handlevarying topologies. Implicit surfaces, however, can robustly handle complexshapes, topologies, and also provide flexible resolution control. We addressthe fundamental problem of learning implicit surfaces for shape inferencewithout the need of 3D supervision. Despite their advantages, it remainsnontrivial to (1) formulate a differentiable connection between implicitsurfaces and their 2D renderings, which is needed for image-based supervision;and (2) ensure precise geometric properties and control, such as localsmoothness. In particular, sampling implicit surfaces densely is also known tobe a computationally demanding and very slow operation. To this end, we proposea novel ray-based field probing technique for efficient image-to-fieldsupervision, as well as a general geometric regularizer for implicit surfaces,which provides natural shape priors in unconstrained regions. We demonstratethe effectiveness of our framework on the task of single-view image-based 3Dshape digitization and show how we outperform state-of-the-art techniques bothquantitatively and qualitatively.},
  YEAR = {2019},
  MONTH = {Nov},
  URL = {http://arxiv.org/abs/1911.00767v1},
  FILE = {1911.00767v1.pdf}
 }",Fundamentals,"Representation, Sampling",None,Occupancy,,,,,,,,,,Yes,Direct,"Shichen Liu, Shunsuke Saito, Weikai Chen, Hao Li",liu2019learning,00000011,"Recent advances in 3D deep learning have shown that it is possible to train highly effective deep models for 3D shape generation, directly from 2D images. This is particularly interesting since the availability of 3D models is still limited compared to the massive amount of accessible 2D images, which is invaluable for training. The representation of 3D surfaces itself is a key factor for the quality and resolution of the 3D output. While explicit representations, such as point clouds and voxels, can span a wide range of shape variations, their resolutions are often limited. Mesh-based representations are more efficient but are limited by their ability to handle varying topologies. Implicit surfaces, however, can robustly handle complex shapes, topologies, and also provide flexible resolution control. We address the fundamental problem of learning implicit surfaces for shape inference without the need of 3D supervision. Despite their advantages, it remains nontrivial to (1) formulate a differentiable connection between implicit surfaces and their 2D renderings, which is needed for image-based supervision; and (2) ensure precise geometric properties and control, such as local smoothness. In particular, sampling implicit surfaces densely is also known to be a computationally demanding and very slow operation. To this end, we propose a novel ray-based field probing technique for efficient image-to-field supervision, as well as a general geometric regularizer for implicit surfaces, which provides natural shape priors in unconstrained regions. We demonstrate the effectiveness of our framework on the task of single-view image-based 3D shape digitization and show how we outperform state-of-the-art techniques both quantitatively and qualitatively.",69,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnud-Oh-fyxVSOD7ivE12s_etP1s-4t3G4Bzfqta6cXWVObfDnsCwnMC1Q3zXgMwgKdg
6/29/2021 16:19:49,SAL: Sign Agnostic Learning of Shapes from Raw Data,SAL,11/23/2019,https://arxiv.org/pdf/1911.10414.pdf,,https://github.com/matanatz/SAL,,,,,"@article{atzmon2019sal,
  AUTHOR = {Matan Atzmon and Yaron Lipman},
  TITLE = {SAL: Sign Agnostic Learning of Shapes from Raw Data},
  EPRINT = {1911.10414v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Recently, neural networks have been used as implicit representations forsurface reconstruction, modelling, learning, and generation. So far, trainingneural networks to be implicit representations of surfaces required trainingdata sampled from a ground-truth signed implicit functions such as signeddistance or occupancy functions, which are notoriously hard to compute.In this paper we introduce Sign Agnostic Learning (SAL), a deep learningapproach for learning implicit shape representations directly from raw,unsigned geometric data, such as point clouds and triangle soups.We have tested SAL on the challenging problem of surface reconstruction froman un-oriented point cloud, as well as end-to-end human shape space learningdirectly from raw scans dataset, and achieved state of the art reconstructionscompared to current approaches. We believe SAL opens the door to many geometricdeep learning applications with real-world data, alleviating the usualpainstaking, often manual pre-process.},
  YEAR = {2019},
  MONTH = {Nov},
  URL = {http://arxiv.org/abs/1911.10414v2},
  FILE = {1911.10414v2.pdf}
 }",,,,SDF,Per-scene,,,,,,,CVPR 2020,,Yes,,"Matan Atzmon, Yaron Lipman",atzmon2019sal,00000012,"Recently, neural networks have been used as implicit representations for surface reconstruction, modelling, learning, and generation. So far, training neural networks to be implicit representations of surfaces required training data sampled from a ground-truth signed implicit functions such as signed distance or occupancy functions, which are notoriously hard to compute. In this paper we introduce Sign Agnostic Learning (SAL), a deep learning approach for learning implicit shape representations directly from raw, unsigned geometric data, such as point clouds and triangle soups. We have tested SAL on the challenging problem of surface reconstruction from an un-oriented point cloud, as well as end-to-end human shape space learning directly from raw scans dataset, and achieved state of the art reconstructions compared to current approaches. We believe SAL opens the door to many geometric deep learning applications with real-world data, alleviating the usual painstaking, often manual pre-process.",68,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnufuPXmJWqL2T5e91KneRVXLaE6EgNa744eFUeYRdvS8OFRbF5kjJYxLUcgrGIVKJv0
8/29/2021 20:20:23,DIST: Rendering Deep Implicit Signed Distance Function with Differentiable Sphere Tracing,DIST,11/29/2019,https://arxiv.org/pdf/1911.13225.pdf,http://b1ueber2y.me/projects/DIST-Renderer/,https://github.com/B1ueber2y/DIST-Renderer,https://www.youtube.com/watch?v=KjfNS1mnqoM,http://b1ueber2y.me/projects/DIST-Renderer/dist-supp.pdf,,,"@article{liu2019dist,
  AUTHOR = {Shaohui Liu and Yinda Zhang and Songyou Peng and Boxin Shi and Marc Pollefeys and Zhaopeng Cui},
  TITLE = {DIST: Rendering Deep Implicit Signed Distance Function withDifferentiable Sphere Tracing},
  EPRINT = {1911.13225v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We propose a differentiable sphere tracing algorithm to bridge the gapbetween inverse graphics methods and the recently proposed deep learning basedimplicit signed distance function. Due to the nature of the implicit function,the rendering process requires tremendous function queries, which isparticularly problematic when the function is represented as a neural network.We optimize both the forward and backward passes of our rendering layer to makeit run efficiently with affordable memory consumption on a commodity graphicscard. Our rendering method is fully differentiable such that losses can bedirectly computed on the rendered 2D observations, and the gradients can bepropagated backwards to optimize the 3D geometry. We show that our renderingmethod can effectively reconstruct accurate 3D shapes from various inputs, suchas sparse depth and multi-view images, through inverse optimization. With thegeometry based reasoning, our 3D shape prediction methods show excellentgeneralization capability and robustness against various noises.},
  YEAR = {2019},
  MONTH = {Nov},
  URL = {http://arxiv.org/abs/1911.13225v2},
  FILE = {1911.13225v2.pdf}
 }",Fundamentals,"Data-driven component (pre-trained, cross-scene)",None,SDF,Category-level,,,,,,,CVPR 2020,,Yes,Direct,"Shaohui Liu, Yinda Zhang, Songyou Peng, Boxin Shi, Marc Pollefeys, Zhaopeng Cui",liu2019dist,00000177,"We propose a differentiable sphere tracing algorithm to bridge the gap between inverse graphics methods and the recently proposed deep learning based implicit signed distance function. Due to the nature of the implicit function, the rendering process requires tremendous function queries, which is particularly problematic when the function is represented as a neural network. We optimize both the forward and backward passes of our rendering layer to make it run efficiently with affordable memory consumption on a commodity graphics card. Our rendering method is fully differentiable such that losses can be directly computed on the rendered 2D observations, and the gradients can be propagated backwards to optimize the 3D geometry. We show that our rendering method can effectively reconstruct accurate 3D shapes from various inputs, such as sparse depth and multi-view images, through inverse optimization. With the geometry based reasoning, our 3D shape prediction methods show excellent generalization capability and robustness against various noises.",68,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnucSv_N_1mcoMDYwuoGvgJZ-coaXPLwWIPJ97ZfIa73DvfmfZlitc_9OiJMGtx3LSJ8
9/17/2021 11:55:52,NASA: Neural Articulated Shape Approximation,NASA,12/6/2019,https://arxiv.org/pdf/1912.03207.pdf,,,,,,,"@article{deng2019nasa,
  AUTHOR = {Boyang Deng and JP Lewis and Timothy Jeruzalski and Gerard Pons-Moll and Geoffrey Hinton and Mohammad Norouzi and Andrea Tagliasacchi},
  TITLE = {NASA: Neural Articulated Shape Approximation},
  EPRINT = {1912.03207v4},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Efficient representation of articulated objects such as human bodies is animportant problem in computer vision and graphics. To efficiently simulatedeformation, existing approaches represent 3D objects using polygonal meshesand deform them using skinning techniques. This paper introduces neuralarticulated shape approximation (NASA), an alternative framework that enablesefficient representation of articulated deformable objects using neuralindicator functions that are conditioned on pose. Occupancy testing using NASAis straightforward, circumventing the complexity of meshes and the issue ofwater-tightness. We demonstrate the effectiveness of NASA for 3D trackingapplications, and discuss other potential extensions.},
  YEAR = {2019},
  MONTH = {Dec},
  URL = {http://arxiv.org/abs/1912.03207v4},
  FILE = {1912.03207v4.pdf}
 }",Human body,Articulated,,Occupancy,Category-level,,,,,,,ECCV 2020,,Yes,Direct,"Boyang Deng, JP Lewis, Timothy Jeruzalski, Gerard Pons-Moll, Geoffrey Hinton, Mohammad Norouzi, Andrea Tagliasacchi",deng2019nasa,00000199,"Efficient representation of articulated objects such as human bodies is an important problem in computer vision and graphics. To efficiently simulate deformation, existing approaches represent 3D objects using polygonal meshes and deform them using skinning techniques. This paper introduces neural articulated shape approximation (NASA), an alternative framework that enables efficient representation of articulated deformable objects using neural indicator functions that are conditioned on pose. Occupancy testing using NASA is straightforward, circumventing the complexity of meshes and the issue of water-tightness. We demonstrate the effectiveness of NASA for 3D tracking applications, and discuss other potential extensions.",,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnucxBBsUM7zxUOUHkZfLi4G7EGr12VqOFqm2_uTwGYxYQav94kO8GbhHDfgDpPi1xJE
6/29/2021 16:55:27,Local Deep Implicit Functions for 3D Shape,LDIF,12/12/2019,https://arxiv.org/pdf/1912.06126.pdf,https://ldif.cs.princeton.edu/,https://github.com/google/ldif,https://www.youtube.com/watch?v=3RAITzNWVJs,,,,"@article{genova2019ldif,
  AUTHOR = {Kyle Genova and Forrester Cole and Avneesh Sud and Aaron Sarna and Thomas Funkhouser},
  TITLE = {Local Deep Implicit Functions for 3D Shape},
  EPRINT = {1912.06126v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {The goal of this project is to learn a 3D shape representation that enablesaccurate surface reconstruction, compact storage, efficient computation,consistency for similar shapes, generalization across diverse shape categories,and inference from depth camera observations. Towards this end, we introduceLocal Deep Implicit Functions (LDIF), a 3D shape representation that decomposesspace into a structured set of learned implicit functions. We provide networksthat infer the space decomposition and local deep implicit functions from a 3Dmesh or posed depth image. During experiments, we find that it provides 10.3points higher surface reconstruction accuracy (F-Score) than thestate-of-the-art (OccNet), while requiring fewer than 1 percent of the networkparameters. Experiments on posed depth image completion and generalization tounseen classes show 15.8 and 17.8 point improvements over the state-of-the-art,while producing a structured 3D representation for each input with consistencyacross diverse shape collections.},
  YEAR = {2019},
  MONTH = {Dec},
  URL = {http://arxiv.org/abs/1912.06126v2},
  FILE = {1912.06126v2.pdf}
 }",Human body,"Conditional neural field, Representation, Data-driven component (pre-trained, cross-scene)",None,Occupancy,Category-level,,,,,,,CVPR 2020,,Yes,,"Kyle Genova, Forrester Cole, Avneesh Sud, Aaron Sarna, Thomas Funkhouser",genova2019ldif,00000013,"The goal of this project is to learn a 3D shape representation that enables accurate surface reconstruction, compact storage, efficient computation, consistency for similar shapes, generalization across diverse shape categories, and inference from depth camera observations. Towards this end, we introduce Local Deep Implicit Functions (LDIF), a 3D shape representation that decomposes space into a structured set of learned implicit functions. We provide networks that infer the space decomposition and local deep implicit functions from a 3D mesh or posed depth image. During experiments, we find that it provides 10.3 points higher surface reconstruction accuracy (F-Score) than the state-of-the-art (OccNet), while requiring fewer than 1 percent of the network parameters. Experiments on posed depth image completion and generalization to unseen classes show 15.8 and 17.8 point improvements over the state-of-the-art, while producing a structured 3D representation for each input with consistency across diverse shape collections.",70,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnud0NOtW3Us7A67Nz7L1V_gDGI8rCiFPD9tFO9fIct-W2f3R8N0u35RxdhggiZ6vTC4
7/7/2021 17:49:56,Differentiable Volumetric Rendering: Learning Implicit 3D Representations without 3D Supervision,DVR,12/16/2019,https://arxiv.org/pdf/1912.07372.pdf,https://www.youtube.com/watch?v=U_jIN3qWVEw,https://github.com/autonomousvision/differentiable_volumetric_rendering,https://www.youtube.com/watch?v=U_jIN3qWVEw,http://www.cvlibs.net/publications/Niemeyer2020CVPR_supplementary.pdf,https://www.youtube.com/watch?v=lcub1KH-mmk,,"@article{niemeyer2019dvr,
  AUTHOR = {Michael Niemeyer and Lars Mescheder and Michael Oechsle and Andreas Geiger},
  TITLE = {Differentiable Volumetric Rendering: Learning Implicit 3DRepresentations without 3D Supervision},
  EPRINT = {1912.07372v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Learning-based 3D reconstruction methods have shown impressive results.However, most methods require 3D supervision which is often hard to obtain forreal-world datasets. Recently, several works have proposed differentiablerendering techniques to train reconstruction models from RGB images.Unfortunately, these approaches are currently restricted to voxel- andmesh-based representations, suffering from discretization or low resolution. Inthis work, we propose a differentiable rendering formulation for implicit shapeand texture representations. Implicit representations have recently gainedpopularity as they represent shape and texture continuously. Our key insight isthat depth gradients can be derived analytically using the concept of implicitdifferentiation. This allows us to learn implicit shape and texturerepresentations directly from RGB images. We experimentally show that oursingle-view reconstructions rival those learned with full 3D supervision.Moreover, we find that our method can be used for multi-view 3D reconstruction,directly resulting in watertight meshes.},
  YEAR = {2019},
  MONTH = {Dec},
  URL = {http://arxiv.org/abs/1912.07372v2},
  FILE = {1912.07372v2.pdf}
 }",,Conditional neural field,,Occupancy,,,,,,,,,,No,,"Michael Niemeyer, Lars Mescheder, Michael Oechsle, Andreas Geiger",niemeyer2019dvr,00000014,"Learning-based 3D reconstruction methods have shown impressive results. However, most methods require 3D supervision which is often hard to obtain for real-world datasets. Recently, several works have proposed differentiable rendering techniques to train reconstruction models from RGB images. Unfortunately, these approaches are currently restricted to voxel- and mesh-based representations, suffering from discretization or low resolution. In this work, we propose a differentiable rendering formulation for implicit shape and texture representations. Implicit representations have recently gained popularity as they represent shape and texture continuously. Our key insight is that depth gradients can be derived analytically using the concept of implicit differentiation. This allows us to learn implicit shape and texture representations directly from RGB images. We experimentally show that our single-view reconstructions rival those learned with full 3D supervision. Moreover, we find that our method can be used for multi-view 3D reconstruction, directly resulting in watertight meshes.",142,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnueaYgL3YRw3Cfa4MN2bTCh6_mHXH5Mt9hAYUZkmYozqr36xsY1sNkUKCtl0tPReSuY
6/29/2021 16:32:13,Implicit Geometric Regularization for Learning Shapes,IGR,2/24/2020,https://arxiv.org/pdf/2002.10099.pdf,,https://github.com/amosgropp/IGR,https://www.youtube.com/watch?v=6cOvBGBQF9g,,,,"@article{gropp2020igr,
  AUTHOR = {Amos Gropp and Lior Yariv and Niv Haim and Matan Atzmon and Yaron Lipman},
  TITLE = {Implicit Geometric Regularization for Learning Shapes},
  EPRINT = {2002.10099v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.LG},
  ABSTRACT = {Representing shapes as level sets of neural networks has been recently provedto be useful for different shape analysis and reconstruction tasks. So far,such representations were computed using either: (i) pre-computed implicitshape representations; or (ii) loss functions explicitly defined over theneural level sets. In this paper we offer a new paradigm for computing highfidelity implicit neural representations directly from raw data (i.e., pointclouds, with or without normal information). We observe that a rather simpleloss function, encouraging the neural network to vanish on the input pointcloud and to have a unit norm gradient, possesses an implicit geometricregularization property that favors smooth and natural zero level set surfaces,avoiding bad zero-loss solutions. We provide a theoretical analysis of thisproperty for the linear case, and show that, in practice, our method leads tostate of the art implicit neural representations with higher level-of-detailsand fidelity compared to previous methods.},
  YEAR = {2020},
  MONTH = {Feb},
  URL = {http://arxiv.org/abs/2002.10099v2},
  FILE = {2002.10099v2.pdf}
 }","Human body, Generalization, Fundamentals",,None,SDF,,,,,,,,CVPR 2020,,Yes,,"Amos Gropp, Lior Yariv, Niv Haim, Matan Atzmon, Yaron Lipman",gropp2020igr,00000015,"Representing shapes as level sets of neural networks has been recently proved to be useful for different shape analysis and reconstruction tasks. So far, such representations were computed using either: (i) pre-computed implicit shape representations; or (ii) loss functions explicitly defined over the neural level sets. In this paper we offer a new paradigm for computing high fidelity implicit neural representations directly from raw data (i.e., point clouds, with or without normal information). We observe that a rather simple loss function, encouraging the neural network to vanish on the input point cloud and to have a unit norm gradient, possesses an implicit geometric regularization property that favors smooth and natural zero level set surfaces, avoiding bad zero-loss solutions. We provide a theoretical analysis of this property for the linear case, and show that, in practice, our method leads to state of the art implicit neural representations with higher level-of-details and fidelity compared to previous methods.",65,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnucK-qvSkUpnWszOx8LNQ2oWAVukBIPL2UDQi3XsH1MjyKbSt0zpsVnxhSXzbt3B95g
7/19/2021 21:08:05,Implicit Functions in Feature Space for 3D Shape Reconstruction and Completion,IF-Net,3/3/2020,https://arxiv.org/pdf/2003.01456.pdf,https://virtualhumans.mpi-inf.mpg.de/ifnets/,https://github.com/jchibane/if-net,https://www.youtube.com/watch?v=cko07jINRZg,http://virtualhumans.mpi-inf.mpg.de/papers/chibane20ifnet/chibane20ifnet_supp.pdf,,,"@article{chibane2020ifnets,
  AUTHOR = {Julian Chibane and Thiemo Alldieck and Gerard Pons-Moll},
  TITLE = {Implicit Functions in Feature Space for 3D Shape Reconstruction andCompletion},
  EPRINT = {2003.01456v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {While many works focus on 3D reconstruction from images, in this paper, wefocus on 3D shape reconstruction and completion from a variety of 3D inputs,which are deficient in some respect: low and high resolution voxels, sparse anddense point clouds, complete or incomplete. Processing of such 3D inputs is anincreasingly important problem as they are the output of 3D scanners, which arebecoming more accessible, and are the intermediate output of 3D computer visionalgorithms. Recently, learned implicit functions have shown great promise asthey produce continuous reconstructions. However, we identified two limitationsin reconstruction from 3D inputs: 1) details present in the input data are notretained, and 2) poor reconstruction of articulated humans. To solve this, wepropose Implicit Feature Networks (IF-Nets), which deliver continuous outputs,can handle multiple topologies, and complete shapes for missing or sparse inputdata retaining the nice properties of recent learned implicit functions, butcritically they can also retain detail when it is present in the input data,and can reconstruct articulated humans. Our work differs from prior work in twocrucial aspects. First, instead of using a single vector to encode a 3D shape,we extract a learnable 3-dimensional multi-scale tensor of deep features, whichis aligned with the original Euclidean space embedding the shape. Second,instead of classifying x-y-z point coordinates directly, we classify deepfeatures extracted from the tensor at a continuous query point. We show thatthis forces our model to make decisions based on global and local shapestructure, as opposed to point coordinates, which are arbitrary under Euclideantransformations. Experiments demonstrate that IF-Nets clearly outperform priorwork in 3D object reconstruction in ShapeNet, and obtain significantly moreaccurate 3D human reconstructions.},
  YEAR = {2020},
  MONTH = {Mar},
  NOTE = {{IEEE} Conference on Computer Vision and Pattern Recognition
  (CVPR)2020},
  URL = {http://arxiv.org/abs/2003.01456v2},
  FILE = {2003.01456v2.pdf}
 }",Generalization,"Feature volume, Data-driven component (pre-trained, cross-scene)",None,Occupancy,,,,,,,,CVPR 2020,,Yes,,"Julian Chibane, Thiemo Alldieck, Gerard Pons-Moll",chibane2020ifnets,00000016,"While many works focus on 3D reconstruction from images, in this paper, we focus on 3D shape reconstruction and completion from a variety of 3D inputs, which are deficient in some respect: low and high resolution voxels, sparse and dense point clouds, complete or incomplete. Processing of such 3D inputs is an increasingly important problem as they are the output of 3D scanners, which are becoming more accessible, and are the intermediate output of 3D computer vision algorithms. Recently, learned implicit functions have shown great promise as they produce continuous reconstructions. However, we identified two limitations in reconstruction from 3D inputs: 1) details present in the input data are not retained, and 2) poor reconstruction of articulated humans. To solve this, we propose Implicit Feature Networks (IF-Nets), which deliver continuous outputs, can handle multiple topologies, and complete shapes for missing or sparse input data retaining the nice properties of recent learned implicit functions, but critically they can also retain detail when it is present in the input data, and can reconstruct articulated humans. Our work differs from prior work in two crucial aspects. First, instead of using a single vector to encode a 3D shape, we extract a learnable 3-dimensional multi-scale tensor of deep features, which is aligned with the original Euclidean space embedding the shape. Second, instead of classifying x-y-z point coordinates directly, we classify deep features extracted from the tensor at a continuous query point. We show that this forces our model to make decisions based on global and local shape structure, as opposed to point coordinates, which are arbitrary under Euclidean transformations. Experiments demonstrate that IF-Nets clearly outperform prior work in 3D object reconstruction in ShapeNet, and obtain significantly more accurate 3D human reconstructions.",86,Yes,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnuf_ZfZLbi1YiQ1xg5UdIFo3Ss6BmrIuaSBjkiz9dJIlGfkw7nO1drbjs35YZ1HJKVo
5/23/2021 19:21:26,Convolutional Occupancy Networks,,3/10/2020,https://arxiv.org/pdf/2003.04618.pdf,https://pengsongyou.github.io/conv_onet,https://github.com/autonomousvision/convolutional_occupancy_networks,"https://www.youtube.com/watch?v=k0monzIcjUo, https://www.youtube.com/watch?v=EmauovgrDSM",http://www.cvlibs.net/publications/Peng2020ECCV_supplementary.pdf,,,"@article{peng2020convolutional,
  AUTHOR = {Songyou Peng and Michael Niemeyer and Lars Mescheder and Marc Pollefeys and Andreas Geiger},
  TITLE = {Convolutional Occupancy Networks},
  EPRINT = {2003.04618v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Recently, implicit neural representations have gained popularity forlearning-based 3D reconstruction. While demonstrating promising results, mostimplicit approaches are limited to comparably simple geometry of single objectsand do not scale to more complicated or large-scale scenes. The key limitingfactor of implicit methods is their simple fully-connected network architecturewhich does not allow for integrating local information in the observations orincorporating inductive biases such as translational equivariance. In thispaper, we propose Convolutional Occupancy Networks, a more flexible implicitrepresentation for detailed reconstruction of objects and 3D scenes. Bycombining convolutional encoders with implicit occupancy decoders, our modelincorporates inductive biases, enabling structured reasoning in 3D space. Weinvestigate the effectiveness of the proposed representation by reconstructingcomplex geometry from noisy point clouds and low-resolution voxelrepresentations. We empirically find that our method enables the fine-grainedimplicit 3D reconstruction of single objects, scales to large indoor scenes,and generalizes well from synthetic to real data.},
  YEAR = {2020},
  MONTH = {Mar},
  URL = {http://arxiv.org/abs/2003.04618v2},
  FILE = {2003.04618v2.pdf}
 }",,Feature volume,,Occupancy,,,,,,,,ECCV 2020,,Yes,,"Songyou Peng, Michael Niemeyer, Lars Mescheder, Marc Pollefeys, Andreas Geiger",peng2020convolutional,00000017,"Recently, implicit neural representations have gained popularity for learning-based 3D reconstruction. While demonstrating promising results, most implicit approaches are limited to comparably simple geometry of single objects and do not scale to more complicated or large-scale scenes. The key limiting factor of implicit methods is their simple fully-connected network architecture which does not allow for integrating local information in the observations or incorporating inductive biases such as translational equivariance. In this paper, we propose Convolutional Occupancy Networks, a more flexible implicit representation for detailed reconstruction of objects and 3D scenes. By combining convolutional encoders with implicit occupancy decoders, our model incorporates inductive biases, enabling structured reasoning in 3D space. We investigate the effectiveness of the proposed representation by reconstructing complex geometry from noisy point clouds and low-resolution voxel representations. We empirically find that our method enables the fine-grained implicit 3D reconstruction of single objects, scales to large indoor scenes, and generalizes well from synthetic to real data.",99,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnudXHuBDr4Q8p_15m5RK3TztFHuKAcvQUzQbPetpBJZfk9Vu9KHDJ_DJD7ECAj47iRM
5/23/2021 19:13:45,NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis,NeRF,3/19/2020,https://arxiv.org/pdf/2003.08934.pdf,https://www.matthewtancik.com/nerf,https://github.com/bmild/nerf,https://www.youtube.com/watch?v=JuH79E8rdKc,,,,"@article{mildenhall2020nerf,
  AUTHOR = {Ben Mildenhall and Pratul P. Srinivasan and Matthew Tancik and Jonathan T. Barron and Ravi Ramamoorthi and Ren Ng},
  TITLE = {NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis},
  EPRINT = {2003.08934v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We present a method that achieves state-of-the-art results for synthesizingnovel views of complex scenes by optimizing an underlying continuous volumetricscene function using a sparse set of input views. Our algorithm represents ascene using a fully-connected (non-convolutional) deep network, whose input isa single continuous 5D coordinate (spatial location $(x,y,z)$ and viewingdirection $(\theta, \phi)$) and whose output is the volume density andview-dependent emitted radiance at that spatial location. We synthesize viewsby querying 5D coordinates along camera rays and use classic volume renderingtechniques to project the output colors and densities into an image. Becausevolume rendering is naturally differentiable, the only input required tooptimize our representation is a set of images with known camera poses. Wedescribe how to effectively optimize neural radiance fields to renderphotorealistic novel views of scenes with complicated geometry and appearance,and demonstrate results that outperform prior work on neural rendering and viewsynthesis. View synthesis results are best viewed as videos, so we urge readersto view our supplementary video for convincing comparisons.},
  YEAR = {2020},
  MONTH = {Mar},
  URL = {http://arxiv.org/abs/2003.08934v2},
  FILE = {2003.08934v2.pdf}
 }",,Sampling,,,,,,,,,,ECCV 2020,https://drive.google.com/drive/folders/128yBriW1IG_3NJ5Rp7APSTZsJqdJdfc1,No,,"Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, Ren Ng",mildenhall2020nerf,00000018,"We present a method that achieves state-of-the-art results for synthesizing novel views of complex scenes by optimizing an underlying continuous volumetric scene function using a sparse set of input views. Our algorithm represents a scene using a fully-connected (non-convolutional) deep network, whose input is a single continuous 5D coordinate (spatial location $(x,y,z)$ and viewing direction $(\theta, \phi)$) and whose output is the volume density and view-dependent emitted radiance at that spatial location. We synthesize views by querying 5D coordinates along camera rays and use classic volume rendering techniques to project the output colors and densities into an image. Because volume rendering is naturally differentiable, the only input required to optimize our representation is a set of images with known camera poses. We describe how to effectively optimize neural radiance fields to render photorealistic novel views of scenes with complicated geometry and appearance, and demonstrate results that outperform prior work on neural rendering and view synthesis. View synthesis results are best viewed as videos, so we urge readers to view our supplementary video for convincing comparisons.",366,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnucKxcWv5YPEuxLZqt2UJ82MumR-k667T611Wse7OMzzd73j4GkWQTdYAbMc4EB9lHw
5/23/2021 19:12:39,Multiview Neural Surface Reconstruction by Disentangling Geometry and Appearance,IDR,3/22/2020,https://arxiv.org/pdf/2003.09852.pdf,,https://github.com/lioryariv/idr,,,,,"@article{yariv2020idr,
  AUTHOR = {Lior Yariv and Yoni Kasten and Dror Moran and Meirav Galun and Matan Atzmon and Ronen Basri and Yaron Lipman},
  TITLE = {Multiview Neural Surface Reconstruction by Disentangling Geometry andAppearance},
  EPRINT = {2003.09852v3},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {In this work we address the challenging problem of multiview 3D surfacereconstruction. We introduce a neural network architecture that simultaneouslylearns the unknown geometry, camera parameters, and a neural renderer thatapproximates the light reflected from the surface towards the camera. Thegeometry is represented as a zero level-set of a neural network, while theneural renderer, derived from the rendering equation, is capable of(implicitly) modeling a wide set of lighting conditions and materials. Wetrained our network on real world 2D images of objects with different materialproperties, lighting conditions, and noisy camera initializations from the DTUMVS dataset. We found our model to produce state of the art 3D surfacereconstructions with high fidelity, resolution and detail.},
  YEAR = {2020},
  MONTH = {Mar},
  URL = {http://arxiv.org/abs/2003.09852v3},
  FILE = {2003.09852v3.pdf}
 }",Material/lighting estimation,,,SDF,,,,,,,,,,No,,"Lior Yariv, Yoni Kasten, Dror Moran, Meirav Galun, Matan Atzmon, Ronen Basri, Yaron Lipman",yariv2020idr,00000019,"In this work we address the challenging problem of multiview 3D surface reconstruction. We introduce a neural network architecture that simultaneously learns the unknown geometry, camera parameters, and a neural renderer that approximates the light reflected from the surface towards the camera. The geometry is represented as a zero level-set of a neural network, while the neural renderer, derived from the rendering equation, is capable of (implicitly) modeling a wide set of lighting conditions and materials. We trained our network on real world 2D images of objects with different material properties, lighting conditions, and noisy camera initializations from the DTU MVS dataset. We found our model to produce state of the art 3D surface reconstructions with high fidelity, resolution and detail.",52,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnufODFVKNyzjLhfblkhTNH0QkXBma935Iecu45JjCKNfBPMoIpSbezUV9eVH6uytPfs
6/29/2021 16:58:44,Deep Local Shapes: Learning Local SDF Priors for Detailed 3D Reconstruction,DeepLS,3/24/2020,https://arxiv.org/pdf/2003.10983.pdf,,,,,,,"@article{chabra2020deepls,
  AUTHOR = {Rohan Chabra and Jan Eric Lenssen and Eddy Ilg and Tanner Schmidt and Julian Straub and Steven Lovegrove and Richard Newcombe},
  TITLE = {Deep Local Shapes: Learning Local SDF Priors for Detailed 3DReconstruction},
  EPRINT = {2003.10983v3},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Efficiently reconstructing complex and intricate surfaces at scale is along-standing goal in machine perception. To address this problem we introduceDeep Local Shapes (DeepLS), a deep shape representation that enables encodingand reconstruction of high-quality 3D shapes without prohibitive memoryrequirements. DeepLS replaces the dense volumetric signed distance function(SDF) representation used in traditional surface reconstruction systems with aset of locally learned continuous SDFs defined by a neural network, inspired byrecent work such as DeepSDF. Unlike DeepSDF, which represents an object-levelSDF with a neural network and a single latent code, we store a grid ofindependent latent codes, each responsible for storing information aboutsurfaces in a small local neighborhood. This decomposition of scenes into localshapes simplifies the prior distribution that the network must learn, and alsoenables efficient inference. We demonstrate the effectiveness andgeneralization power of DeepLS by showing object shape encoding andreconstructions of full scenes, where DeepLS delivers high compression,accuracy, and local shape completion.},
  YEAR = {2020},
  MONTH = {Mar},
  URL = {http://arxiv.org/abs/2003.10983v3},
  FILE = {2003.10983v3.pdf}
 }",Generalization,Voxelization,None,SDF,Category-level,,,,,,,ECCV 2020,,Yes,,"Rohan Chabra, Jan Eric Lenssen, Eddy Ilg, Tanner Schmidt, Julian Straub, Steven Lovegrove, Richard Newcombe",chabra2020deepls,00000020,"Efficiently reconstructing complex and intricate surfaces at scale is a long-standing goal in machine perception. To address this problem we introduce Deep Local Shapes (DeepLS), a deep shape representation that enables encoding and reconstruction of high-quality 3D shapes without prohibitive memory requirements. DeepLS replaces the dense volumetric signed distance function (SDF) representation used in traditional surface reconstruction systems with a set of locally learned continuous SDFs defined by a neural network, inspired by recent work such as DeepSDF. Unlike DeepSDF, which represents an object-level SDF with a neural network and a single latent code, we store a grid of independent latent codes, each responsible for storing information about surfaces in a small local neighborhood. This decomposition of scenes into local shapes simplifies the prior distribution that the network must learn, and also enables efficient inference. We demonstrate the effectiveness and generalization power of DeepLS by showing object shape encoding and reconstructions of full scenes, where DeepLS delivers high compression, accuracy, and local shape completion.",61,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnueuo3x0dYSpqD8MtAaQsUB25TUCG_zMh6a-1FZGNKC7CyzBXQIDz9RsQa5F70LU8_E
9/18/2021 9:39:17,EikoNet: Solving the Eikonal equation with Deep Neural Networks,EikoNet,3/25/2020,https://arxiv.org/pdf/2004.00361.pdf,,https://github.com/Ulvetanna/EikoNet,,,,,"@article{smith2020eikonet,
  AUTHOR = {Jonathan D. Smith and Kamyar Azizzadenesheli and Zachary E. Ross},
  TITLE = {EikoNet: Solving the Eikonal equation with Deep Neural Networks},
  EPRINT = {2004.00361v3},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {physics.comp-ph},
  ABSTRACT = {The recent deep learning revolution has created an enormous opportunity foraccelerating compute capabilities in the context of physics-based simulations.Here, we propose EikoNet, a deep learning approach to solving the Eikonalequation, which characterizes the first-arrival-time field in heterogeneous 3Dvelocity structures. Our grid-free approach allows for rapid determination ofthe travel time between any two points within a continuous 3D domain. Thesetravel time solutions are allowed to violate the differential equation - whichcasts the problem as one of optimization - with the goal of finding networkparameters that minimize the degree to which the equation is violated. In doingso, the method exploits the differentiability of neural networks to calculatethe spatial gradients analytically, meaning the network can be trained on itsown without ever needing solutions from a finite difference algorithm. EikoNetis rigorously tested on several velocity models and sampling methods todemonstrate robustness and versatility. Training and inference are highlyparallelized, making the approach well-suited for GPUs. EikoNet has low memoryoverhead, and further avoids the need for travel-time lookup tables. Thedeveloped approach has important applications to earthquake hypocenterinversion, ray multi-pathing, and tomographic modeling, as well as to otherfields beyond seismology where ray tracing is essential.},
  YEAR = {2020},
  MONTH = {Mar},
  URL = {http://arxiv.org/abs/2004.00361v3},
  FILE = {2004.00361v3.pdf}
 }","Beyond graphics, Science and engineering, PDE",,,,,,,,,,,,,,Direct,"Jonathan D. Smith, Kamyar Azizzadenesheli, Zachary E. Ross",smith2020eikonet,00000219,"The recent deep learning revolution has created an enormous opportunity for accelerating compute capabilities in the context of physics-based simulations. Here, we propose EikoNet, a deep learning approach to solving the Eikonal equation, which characterizes the first-arrival-time field in heterogeneous 3D velocity structures. Our grid-free approach allows for rapid determination of the travel time between any two points within a continuous 3D domain. These travel time solutions are allowed to violate the differential equation - which casts the problem as one of optimization - with the goal of finding network parameters that minimize the degree to which the equation is violated. In doing so, the method exploits the differentiability of neural networks to calculate the spatial gradients analytically, meaning the network can be trained on its own without ever needing solutions from a finite difference algorithm. EikoNet is rigorously tested on several velocity models and sampling methods to demonstrate robustness and versatility. Training and inference are highly parallelized, making the approach well-suited for GPUs. EikoNet has low memory overhead, and further avoids the need for travel-time lookup tables. The developed approach has important applications to earthquake hypocenter inversion, ray multi-pathing, and tomographic modeling, as well as to other fields beyond seismology where ray tracing is essential.",,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnucBTg5g2tBHzeBTOM1r25YI9zt5UCACnZGBmKft9X2YGCDlLM3QuyHzDs2cM7722RM
5/23/2021 19:16:06,Semantic Implicit Neural Scene Representations With Semi-Supervised Training,,3/28/2020,https://arxiv.org/pdf/2003.12673.pdf,http://www.computationalimaging.org/publications/semantic-srn/,,https://www.youtube.com/watch?v=iVubC_ymE5w,,,,"@article{kohli2020semantic,
  AUTHOR = {Amit Kohli and Vincent Sitzmann and Gordon Wetzstein},
  TITLE = {Semantic Implicit Neural Scene Representations With Semi-SupervisedTraining},
  EPRINT = {2003.12673v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {The recent success of implicit neural scene representations has presented aviable new method for how we capture and store 3D scenes. Unlike conventional3D representations, such as point clouds, which explicitly store sceneproperties in discrete, localized units, these implicit representations encodea scene in the weights of a neural network which can be queried at anycoordinate to produce these same scene properties. Thus far, implicitrepresentations have primarily been optimized to estimate only the appearanceand/or 3D geometry information in a scene. We take the next step anddemonstrate that an existing implicit representation (SRNs) is actuallymulti-modal; it can be further leveraged to perform per-point semanticsegmentation while retaining its ability to represent appearance and geometry.To achieve this multi-modal behavior, we utilize a semi-supervised learningstrategy atop the existing pre-trained scene representation. Our method issimple, general, and only requires a few tens of labeled 2D segmentation masksin order to achieve dense 3D semantic segmentation. We explore two novelapplications for this semantically aware implicit neural scene representation:3D novel view and semantic label synthesis given only a single input RGB imageor 2D label mask, as well as 3D interpolation of appearance and semantics.},
  YEAR = {2020},
  MONTH = {Mar},
  URL = {http://arxiv.org/abs/2003.12673v2},
  FILE = {2003.12673v2.pdf}
 }",,"Generative/adversarial formulation, Conditional neural field",,,,,,,,,,3DV 2020,,No,,"Amit Kohli, Vincent Sitzmann, Gordon Wetzstein",kohli2020semantic,00000021,"The recent success of implicit neural scene representations has presented a viable new method for how we capture and store 3D scenes. Unlike conventional 3D representations, such as point clouds, which explicitly store scene properties in discrete, localized units, these implicit representations encode a scene in the weights of a neural network which can be queried at any coordinate to produce these same scene properties. Thus far, implicit representations have primarily been optimized to estimate only the appearance and/or 3D geometry information in a scene. We take the next step and demonstrate that an existing implicit representation (SRNs) is actually multi-modal; it can be further leveraged to perform per-point semantic segmentation while retaining its ability to represent appearance and geometry. To achieve this multi-modal behavior, we utilize a semi-supervised learning strategy atop the existing pre-trained scene representation. Our method is simple, general, and only requires a few tens of labeled 2D segmentation masks in order to achieve dense 3D semantic segmentation. We explore two novel applications for this semantically aware implicit neural scene representation: 3D novel view and semantic label synthesis given only a single input RGB image or 2D label mask, as well as 3D interpolation of appearance and semantics.",7,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnuc4oAW62td1yAqjr5q9X7YITQVMi3qSgHOproN38kbQPeOrAUaT4FKi_Fpg_lfIfho
6/29/2021 16:42:37,PIFuHD: Multi-Level Pixel-Aligned Implicit Function for High-Resolution 3D Human Digitization,PIFuHD,4/1/2020,https://arxiv.org/pdf/2004.00452.pdf,https://shunsukesaito.github.io/PIFuHD/,https://github.com/facebookresearch/pifuhd,"https://www.youtube.com/watch?v=uEDqCxvF5yc, https://www.youtube.com/watch?v=-1XYTmm8HhE",,,,"@article{saito2020pifuhd,
  AUTHOR = {Shunsuke Saito and Tomas Simon and Jason Saragih and Hanbyul Joo},
  TITLE = {PIFuHD: Multi-Level Pixel-Aligned Implicit Function for High-Resolution3D Human Digitization},
  EPRINT = {2004.00452v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Recent advances in image-based 3D human shape estimation have been driven bythe significant improvement in representation power afforded by deep neuralnetworks. Although current approaches have demonstrated the potential in realworld settings, they still fail to produce reconstructions with the level ofdetail often present in the input images. We argue that this limitation stemsprimarily form two conflicting requirements; accurate predictions require largecontext, but precise predictions require high resolution. Due to memorylimitations in current hardware, previous approaches tend to take lowresolution images as input to cover large spatial context, and produce lessprecise (or low resolution) 3D estimates as a result. We address thislimitation by formulating a multi-level architecture that is end-to-endtrainable. A coarse level observes the whole image at lower resolution andfocuses on holistic reasoning. This provides context to an fine level whichestimates highly detailed geometry by observing higher-resolution images. Wedemonstrate that our approach significantly outperforms existingstate-of-the-art techniques on single image human shape reconstruction by fullyleveraging 1k-resolution input images.},
  YEAR = {2020},
  MONTH = {Apr},
  NOTE = {The IEEE Conference on Computer Vision and Pattern Recognition
  (CVPR),2020},
  URL = {http://arxiv.org/abs/2004.00452v1},
  FILE = {2004.00452v1.pdf}
 }","Human body, Few-shot reconstruction, Generalization","Lifting 2D features to 3D, Image-based rendering, Coarse-to-fine, Data-driven component (pre-trained, cross-scene)",None,Occupancy,Category-level,,,,,,,3DV,,No,,"Shunsuke Saito, Tomas Simon, Jason Saragih, Hanbyul Joo",saito2020pifuhd,00000022,"Recent advances in image-based 3D human shape estimation have been driven by the significant improvement in representation power afforded by deep neural networks. Although current approaches have demonstrated the potential in real world settings, they still fail to produce reconstructions with the level of detail often present in the input images. We argue that this limitation stems primarily form two conflicting requirements; accurate predictions require large context, but precise predictions require high resolution. Due to memory limitations in current hardware, previous approaches tend to take low resolution images as input to cover large spatial context, and produce less precise (or low resolution) 3D estimates as a result. We address this limitation by formulating a multi-level architecture that is end-to-end trainable. A coarse level observes the whole image at lower resolution and focuses on holistic reasoning. This provides context to an fine level which estimates highly detailed geometry by observing higher-resolution images. We demonstrate that our approach significantly outperforms existing state-of-the-art techniques on single image human shape reconstruction by fully leveraging 1k-resolution input images.",115,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnufsdL1aTwRvEhjdWWzIHXTDlfJn2_3Ou5qfqS6keEs3uMKWFAT_B-A1cDUtxHwie_s
8/29/2021 16:20:20,DualSDF: Semantic Shape Manipulation using a Two-Level Representation,DualSDF,4/6/2020,https://arxiv.org/pdf/2004.02869.pdf,https://www.cs.cornell.edu/~hadarelor/dualsdf/,https://github.com/zekunhao1995/DualSDF,https://www.youtube.com/watch?v=pAszEMLd5Xk,,https://www.youtube.com/watch?v=u40ZwDINz0A,,"@article{hao2020dualsdf,
  AUTHOR = {Zekun Hao and Hadar Averbuch-Elor and Noah Snavely and Serge Belongie},
  TITLE = {DualSDF: Semantic Shape Manipulation using a Two-Level Representation},
  EPRINT = {2004.02869v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We are seeing a Cambrian explosion of 3D shape representations for use inmachine learning. Some representations seek high expressive power in capturinghigh-resolution detail. Other approaches seek to represent shapes ascompositions of simple parts, which are intuitive for people to understand andeasy to edit and manipulate. However, it is difficult to achieve both fidelityand interpretability in the same representation. We propose DualSDF, arepresentation expressing shapes at two levels of granularity, one capturingfine details and the other representing an abstracted proxy shape using simpleand semantically consistent shape primitives. To achieve a tight couplingbetween the two representations, we use a variational objective over a sharedlatent space. Our two-level model gives rise to a new shape manipulationtechnique in which a user can interactively manipulate the coarse proxy shapeand see the changes instantly mirrored in the high-resolution shape. Moreover,our model actively augments and guides the manipulation towards producingsemantically meaningful shapes, making complex manipulations possible withminimal user input.},
  YEAR = {2020},
  MONTH = {Apr},
  URL = {http://arxiv.org/abs/2004.02869v1},
  FILE = {2004.02869v1.pdf}
 }",Editable,"Conditional neural field, Representation, Data-driven component (pre-trained, cross-scene)",,SDF,Category-level,,,,,,,CVPR 2020,,Yes,Direct,"Zekun Hao, Hadar Averbuch-Elor, Noah Snavely, Serge Belongie",hao2020dualsdf,00000163,"We are seeing a Cambrian explosion of 3D shape representations for use in machine learning. Some representations seek high expressive power in capturing high-resolution detail. Other approaches seek to represent shapes as compositions of simple parts, which are intuitive for people to understand and easy to edit and manipulate. However, it is difficult to achieve both fidelity and interpretability in the same representation. We propose DualSDF, a representation expressing shapes at two levels of granularity, one capturing fine details and the other representing an abstracted proxy shape using simple and semantically consistent shape primitives. To achieve a tight coupling between the two representations, we use a variational objective over a shared latent space. Our two-level model gives rise to a new shape manipulation technique in which a user can interactively manipulate the coarse proxy shape and see the changes instantly mirrored in the high-resolution shape. Moreover, our model actively augments and guides the manipulation towards producing semantically meaningful shapes, making complex manipulations possible with minimal user input.",21,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnue-HObu5J7VlGN99qsg14uLDkQG_UMtC_ZweSQOtxABI8ilDdlcWizYVSJk3Ni2a5M
9/17/2021 13:51:22,Robust 3D Self-portraits in Seconds,PIFusion,4/6/2020,https://arxiv.org/pdf/2004.02460.pdf,http://www.liuyebin.com/portrait/portrait.html,,https://www.youtube.com/watch?v=tayZT0exfVA,,http://www.liuyebin.com/portrait/assets/portrait.mp4,,"@article{li2020pifusion,
  AUTHOR = {Zhe Li and Tao Yu and Chuanyu Pan and Zerong Zheng and Yebin Liu},
  TITLE = {Robust 3D Self-portraits in Seconds},
  EPRINT = {2004.02460v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {In this paper, we propose an efficient method for robust 3D self-portraitsusing a single RGBD camera. Benefiting from the proposed PIFusion andlightweight bundle adjustment algorithm, our method can generate detailed 3Dself-portraits in seconds and shows the ability to handle subjects wearingextremely loose clothes. To achieve highly efficient and robust reconstruction,we propose PIFusion, which combines learning-based 3D recovery with volumetricnon-rigid fusion to generate accurate sparse partial scans of the subject.Moreover, a non-rigid volumetric deformation method is proposed to continuouslyrefine the learned shape prior. Finally, a lightweight bundle adjustmentalgorithm is proposed to guarantee that all the partial scans can not only""loop"" with each other but also remain consistent with the selected live keyobservations. The results and experiments show that the proposed methodachieves more robust and efficient 3D self-portraits compared withstate-of-the-art methods.},
  YEAR = {2020},
  MONTH = {Apr},
  URL = {http://arxiv.org/abs/2004.02460v1},
  FILE = {2004.02460v1.pdf}
 }",Human body,"Lifting 2D features to 3D, Feature volume",,,Category-level,,,,,,,CVPR 2020,,No,,"Zhe Li, Tao Yu, Chuanyu Pan, Zerong Zheng, Yebin Liu",li2020pifusion,00000203,"In this paper, we propose an efficient method for robust 3D self-portraits using a single RGBD camera. Benefiting from the proposed PIFusion and lightweight bundle adjustment algorithm, our method can generate detailed 3D self-portraits in seconds and shows the ability to handle subjects wearing extremely loose clothes. To achieve highly efficient and robust reconstruction, we propose PIFusion, which combines learning-based 3D recovery with volumetric non-rigid fusion to generate accurate sparse partial scans of the subject. Moreover, a non-rigid volumetric deformation method is proposed to continuously refine the learned shape prior. Finally, a lightweight bundle adjustment algorithm is proposed to guarantee that all the partial scans can not only ""loop"" with each other but also remain consistent with the selected live key observations. The results and experiments show that the proposed method achieves more robust and efficient 3D self-portraits compared with state-of-the-art methods.",,Yes,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnueII465_YWFznXlunBgk3ynB25lDaAhpFgEmUC_QRxcR9_Mk7GrxnlcP6j88g_QO8A
9/17/2021 14:27:00,ARCH: Animatable Reconstruction of Clothed Humans,ARCH,4/8/2020,https://arxiv.org/pdf/2004.04572.pdf,https://vgl.ict.usc.edu/Research/ARCH/,,https://www.youtube.com/watch?v=DG3QNMcmTvo,,,,"@article{huang2020arch,
  AUTHOR = {Zeng Huang and Yuanlu Xu and Christoph Lassner and Hao Li and Tony Tung},
  TITLE = {ARCH: Animatable Reconstruction of Clothed Humans},
  EPRINT = {2004.04572v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.GR},
  ABSTRACT = {In this paper, we propose ARCH (Animatable Reconstruction of Clothed Humans),a novel end-to-end framework for accurate reconstruction of animation-ready 3Dclothed humans from a monocular image. Existing approaches to digitize 3Dhumans struggle to handle pose variations and recover details. Also, they donot produce models that are animation ready. In contrast, ARCH is a learnedpose-aware model that produces detailed 3D rigged full-body human avatars froma single unconstrained RGB image. A Semantic Space and a Semantic DeformationField are created using a parametric 3D body estimator. They allow thetransformation of 2D/3D clothed humans into a canonical space, reducingambiguities in geometry caused by pose variations and occlusions in trainingdata. Detailed surface geometry and appearance are learned using an implicitfunction representation with spatial local features. Furthermore, we proposeadditional per-pixel supervision on the 3D reconstruction using opacity-awaredifferentiable rendering. Our experiments indicate that ARCH increases thefidelity of the reconstructed humans. We obtain more than 50% lowerreconstruction errors for standard metrics compared to state-of-the-art methodson public datasets. We also show numerous qualitative examples of animated,high-quality reconstructed avatars unseen in the literature so far.},
  YEAR = {2020},
  MONTH = {Apr},
  URL = {http://arxiv.org/abs/2004.04572v2},
  FILE = {2004.04572v2.pdf}
 }",Human body,"Conditional neural field, Lifting 2D features to 3D, Voxelization, Feature volume, Warping field/Flow field, Data-driven component (pre-trained, cross-scene)",None,Occupancy,Category-level,,,,,,,CVPR 2020,,No,Direct,"Zeng Huang, Yuanlu Xu, Christoph Lassner, Hao Li, Tony Tung",huang2020arch,00000208,"In this paper, we propose ARCH (Animatable Reconstruction of Clothed Humans), a novel end-to-end framework for accurate reconstruction of animation-ready 3D clothed humans from a monocular image. Existing approaches to digitize 3D humans struggle to handle pose variations and recover details. Also, they do not produce models that are animation ready. In contrast, ARCH is a learned pose-aware model that produces detailed 3D rigged full-body human avatars from a single unconstrained RGB image. A Semantic Space and a Semantic Deformation Field are created using a parametric 3D body estimator. They allow the transformation of 2D/3D clothed humans into a canonical space, reducing ambiguities in geometry caused by pose variations and occlusions in training data. Detailed surface geometry and appearance are learned using an implicit function representation with spatial local features. Furthermore, we propose additional per-pixel supervision on the 3D reconstruction using opacity-aware differentiable rendering. Our experiments indicate that ARCH increases the fidelity of the reconstructed humans. We obtain more than 50% lower reconstruction errors for standard metrics compared to state-of-the-art methods on public datasets. We also show numerous qualitative examples of animated, high-quality reconstructed avatars unseen in the literature so far.",,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnucXLVLDkbvnTkIi4-2ibJ3hstWdwvJFYTorAlfJd_69xrTw1OXy4sFPNHFTMV14Yqs
8/29/2021 20:52:57,MeshfreeFlowNet: A Physics-Constrained Deep Continuous Space-Time Super-Resolution Framework,MeshfreeFlowNet,5/1/2020,https://arxiv.org/pdf/2005.01463.pdf,http://www.maxjiang.ml/proj/meshfreeflownet,https://github.com/maxjiang93/space_time_pde,"https://www.youtube.com/watch?v=mjqwPch9gDo, https://www.youtube.com/watch?v=anZ_gLrvnYs&t=538s",,,,"@article{jiang2020meshfreeflownet,
  AUTHOR = {Chiyu Max Jiang and Soheil Esmaeilzadeh and Kamyar Azizzadenesheli and Karthik Kashinath and Mustafa Mustafa and Hamdi A. Tchelepi and Philip Marcus and Prabhat and Anima Anandkumar},
  TITLE = {MeshfreeFlowNet: A Physics-Constrained Deep Continuous Space-TimeSuper-Resolution Framework},
  EPRINT = {2005.01463v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.LG},
  ABSTRACT = {We propose MeshfreeFlowNet, a novel deep learning-based super-resolutionframework to generate continuous (grid-free) spatio-temporal solutions from thelow-resolution inputs. While being computationally efficient, MeshfreeFlowNetaccurately recovers the fine-scale quantities of interest. MeshfreeFlowNetallows for: (i) the output to be sampled at all spatio-temporal resolutions,(ii) a set of Partial Differential Equation (PDE) constraints to be imposed,and (iii) training on fixed-size inputs on arbitrarily sized spatio-temporaldomains owing to its fully convolutional encoder. We empirically study theperformance of MeshfreeFlowNet on the task of super-resolution of turbulentflows in the Rayleigh-Benard convection problem. Across a diverse set ofevaluation metrics, we show that MeshfreeFlowNet significantly outperformsexisting baselines. Furthermore, we provide a large scale implementation ofMeshfreeFlowNet and show that it efficiently scales across large clusters,achieving 96.80% scaling efficiency on up to 128 GPUs and a training time ofless than 4 minutes.},
  YEAR = {2020},
  MONTH = {May},
  URL = {http://arxiv.org/abs/2005.01463v2},
  FILE = {2005.01463v2.pdf}
 }","Beyond graphics, Science and engineering",PDE,,,,,,,,,,"SC20: International Conference for High Performance Computing, Networking, Storage and Analysis",,,,"Chiyu Max Jiang, Soheil Esmaeilzadeh, Kamyar Azizzadenesheli, Karthik Kashinath, Mustafa Mustafa, Hamdi A. Tchelepi, Philip Marcus, Prabhat, Anima Anandkumar",jiang2020meshfreeflownet,00000182,"We propose MeshfreeFlowNet, a novel deep learning-based super-resolution framework to generate continuous (grid-free) spatio-temporal solutions from the low-resolution inputs. While being computationally efficient, MeshfreeFlowNet accurately recovers the fine-scale quantities of interest. MeshfreeFlowNet allows for: (i) the output to be sampled at all spatio-temporal resolutions, (ii) a set of Partial Differential Equation (PDE) constraints to be imposed, and (iii) training on fixed-size inputs on arbitrarily sized spatio-temporal domains owing to its fully convolutional encoder. We empirically study the performance of MeshfreeFlowNet on the task of super-resolution of turbulent flows in the Rayleigh-Benard convection problem. Across a diverse set of evaluation metrics, we show that MeshfreeFlowNet significantly outperforms existing baselines. Furthermore, we provide a large scale implementation of MeshfreeFlowNet and show that it efficiently scales across large clusters, achieving 96.80% scaling efficiency on up to 128 GPUs and a training time of less than 4 minutes.",11,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnucgMNXLa8_plRY-082Ge4OOBbA0HY5tnFMXhtYCvClgxojkgIypyQhgys8L7ivXfHQ
9/17/2021 14:03:22,Geo-PIFu: Geometry and Pixel Aligned Implicit Functions for Single-view Human Reconstruction,Geo-PIFu,6/15/2020,https://arxiv.org/pdf/2006.08072.pdf,,https://github.com/simpleig/Geo-PIFu,,,,,"@article{he2020geopifu,
  AUTHOR = {Tong He and John Collomosse and Hailin Jin and Stefano Soatto},
  TITLE = {Geo-PIFu: Geometry and Pixel Aligned Implicit Functions for Single-viewHuman Reconstruction},
  EPRINT = {2006.08072v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We propose Geo-PIFu, a method to recover a 3D mesh from a monocular colorimage of a clothed person. Our method is based on a deep implicitfunction-based representation to learn latent voxel features using astructure-aware 3D U-Net, to constrain the model in two ways: first, to resolvefeature ambiguities in query point encoding, second, to serve as a coarse humanshape proxy to regularize the high-resolution mesh and encourage global shaperegularity. We show that, by both encoding query points and constraining globalshape using latent voxel features, the reconstruction we obtain for clothedhuman meshes exhibits less shape distortion and improved surface detailscompared to competing methods. We evaluate Geo-PIFu on a recent human meshpublic dataset that is $10 \times$ larger than the private commercial datasetused in PIFu and previous derivative work. On average, we exceed the state ofthe art by $42.7\%$ reduction in Chamfer and Point-to-Surface Distances, and$19.4\%$ reduction in normal estimation errors.},
  YEAR = {2020},
  MONTH = {Jun},
  URL = {http://arxiv.org/abs/2006.08072v2},
  FILE = {2006.08072v2.pdf}
 }",Human body,"Conditional neural field, Lifting 2D features to 3D, Voxelization, Feature volume",,Occupancy,Category-level,,,,,,,NeurIPS 2020,,Yes,,"Tong He, John Collomosse, Hailin Jin, Stefano Soatto",he2020geopifu,00000205,"We propose Geo-PIFu, a method to recover a 3D mesh from a monocular color image of a clothed person. Our method is based on a deep implicit function-based representation to learn latent voxel features using a structure-aware 3D U-Net, to constrain the model in two ways: first, to resolve feature ambiguities in query point encoding, second, to serve as a coarse human shape proxy to regularize the high-resolution mesh and encourage global shape regularity. We show that, by both encoding query points and constraining global shape using latent voxel features, the reconstruction we obtain for clothed human meshes exhibits less shape distortion and improved surface details compared to competing methods. We evaluate Geo-PIFu on a recent human mesh public dataset that is $10 \times$ larger than the private commercial dataset used in PIFu and previous derivative work. On average, we exceed the state of the art by $42.7\%$ reduction in Chamfer and Point-to-Surface Distances, and $19.4\%$ reduction in normal estimation errors.",,Yes,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnudUOifJELEJTZsyXt9MOLfEej-C6vpG0K6CoXDZJeg-nsJARGtR-2XzXC6ZuYbYEcM
6/23/2021 13:23:51,Implicit Neural Representations with Periodic Activation Functions,SIREN,6/17/2020,https://arxiv.org/pdf/2006.09661.pdf,https://vsitzmann.github.io/siren/,https://github.com/vsitzmann/siren,,,,,"@article{sitzmann2020siren,
  AUTHOR = {Vincent Sitzmann and Julien N. P. Martel and Alexander W. Bergman and David B. Lindell and Gordon Wetzstein},
  TITLE = {Implicit Neural Representations with Periodic Activation Functions},
  EPRINT = {2006.09661v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Implicitly defined, continuous, differentiable signal representationsparameterized by neural networks have emerged as a powerful paradigm, offeringmany possible benefits over conventional representations. However, currentnetwork architectures for such implicit neural representations are incapable ofmodeling signals with fine detail, and fail to represent a signal's spatial andtemporal derivatives, despite the fact that these are essential to manyphysical signals defined implicitly as the solution to partial differentialequations. We propose to leverage periodic activation functions for implicitneural representations and demonstrate that these networks, dubbed sinusoidalrepresentation networks or Sirens, are ideally suited for representing complexnatural signals and their derivatives. We analyze Siren activation statisticsto propose a principled initialization scheme and demonstrate therepresentation of images, wavefields, video, sound, and their derivatives.Further, we show how Sirens can be leveraged to solve challenging boundaryvalue problems, such as particular Eikonal equations (yielding signed distancefunctions), the Poisson equation, and the Helmholtz and wave equations. Lastly,we combine Sirens with hypernetworks to learn priors over the space of Sirenfunctions.},
  YEAR = {2020},
  MONTH = {Jun},
  URL = {http://arxiv.org/abs/2006.09661v1},
  FILE = {2006.09661v1.pdf}
 }","Generalization, Beyond graphics, Fundamentals","Hypernetwork, PDE",SIREN,,,,,,,,,3DV,https://drive.google.com/drive/u/1/folders/1_iq__37-hw7FJOEUK1tX7mdp8SKB368K,,,"Vincent Sitzmann, Julien N. P. Martel, Alexander W. Bergman, David B. Lindell, Gordon Wetzstein",sitzmann2020siren,00000023,"Implicitly defined, continuous, differentiable signal representations parameterized by neural networks have emerged as a powerful paradigm, offering many possible benefits over conventional representations. However, current network architectures for such implicit neural representations are incapable of modeling signals with fine detail, and fail to represent a signal's spatial and temporal derivatives, despite the fact that these are essential to many physical signals defined implicitly as the solution to partial differential equations. We propose to leverage periodic activation functions for implicit neural representations and demonstrate that these networks, dubbed sinusoidal representation networks or Sirens, are ideally suited for representing complex natural signals and their derivatives. We analyze Siren activation statistics to propose a principled initialization scheme and demonstrate the representation of images, wavefields, video, sound, and their derivatives. Further, we show how Sirens can be leveraged to solve challenging boundary value problems, such as particular Eikonal equations (yielding signed distance functions), the Poisson equation, and the Helmholtz and wave equations. Lastly, we combine Sirens with hypernetworks to learn priors over the space of Siren functions.",185,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnuchy2Ku09XRMFAZaI4OkCuQG_njfVCkZw_RifvIWqzEav4GGxG5bFeciybd7ml-riQ
7/19/2021 17:57:41,MetaSDF: Meta-learning Signed Distance Functions,MetaSDF,6/17/2020,https://arxiv.org/pdf/2006.09662.pdf,https://vsitzmann.github.io/metasdf/,https://github.com/vsitzmann/metasdf,,,,,"@article{sitzmann2020metasdf,
  AUTHOR = {Vincent Sitzmann and Eric R. Chan and Richard Tucker and Noah Snavely and Gordon Wetzstein},
  TITLE = {MetaSDF: Meta-learning Signed Distance Functions},
  EPRINT = {2006.09662v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Neural implicit shape representations are an emerging paradigm that offersmany potential benefits over conventional discrete representations, includingmemory efficiency at a high spatial resolution. Generalizing across shapes withsuch neural implicit representations amounts to learning priors over therespective function space and enables geometry reconstruction from partial ornoisy observations. Existing generalization methods rely on conditioning aneural network on a low-dimensional latent code that is either regressed by anencoder or jointly optimized in the auto-decoder framework. Here, we formalizelearning of a shape space as a meta-learning problem and leveragegradient-based meta-learning algorithms to solve this task. We demonstrate thatthis approach performs on par with auto-decoder based approaches while being anorder of magnitude faster at test-time inference. We further demonstrate thatthe proposed gradient-based method outperforms encoder-decoder based methodsthat leverage pooling-based set encoders.},
  YEAR = {2020},
  MONTH = {Jun},
  URL = {http://arxiv.org/abs/2006.09662v1},
  FILE = {2006.09662v1.pdf}
 }",Generalization,"Hypernetwork, Data-driven component (pre-trained, cross-scene)",,,,,,,,,,3DV,,,,"Vincent Sitzmann, Eric R. Chan, Richard Tucker, Noah Snavely, Gordon Wetzstein",sitzmann2020metasdf,00000024,"Neural implicit shape representations are an emerging paradigm that offers many potential benefits over conventional discrete representations, including memory efficiency at a high spatial resolution. Generalizing across shapes with such neural implicit representations amounts to learning priors over the respective function space and enables geometry reconstruction from partial or noisy observations. Existing generalization methods rely on conditioning a neural network on a low-dimensional latent code that is either regressed by an encoder or jointly optimized in the auto-decoder framework. Here, we formalize learning of a shape space as a meta-learning problem and leverage gradient-based meta-learning algorithms to solve this task. We demonstrate that this approach performs on par with auto-decoder based approaches while being an order of magnitude faster at test-time inference. We further demonstrate that the proposed gradient-based method outperforms encoder-decoder based methods that leverage pooling-based set encoders.",25,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnudSE7uYyxUnuqCU7qaGrjjLno8KNztX7xgGiJ1y9HCo9ie52MGhpEf1TwqayYYOBOU
6/23/2021 13:23:16,Fourier Features Let Networks Learn High Frequency Functions in Low Dimensional Domains,FFN,6/18/2020,https://arxiv.org/pdf/2006.10739.pdf,https://bmild.github.io/fourfeat/,https://github.com/tancik/fourier-feature-networks,"https://www.youtube.com/watch?v=iKyIJ_EtSkw, https://www.youtube.com/watch?v=h0SXP6lJxak",,,,"@article{tancik2020ffn,
  AUTHOR = {Matthew Tancik and Pratul P. Srinivasan and Ben Mildenhall and Sara Fridovich-Keil and Nithin Raghavan and Utkarsh Singhal and Ravi Ramamoorthi and Jonathan T. Barron and Ren Ng},
  TITLE = {Fourier Features Let Networks Learn High Frequency Functions in LowDimensional Domains},
  EPRINT = {2006.10739v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We show that passing input points through a simple Fourier feature mappingenables a multilayer perceptron (MLP) to learn high-frequency functions inlow-dimensional problem domains. These results shed light on recent advances incomputer vision and graphics that achieve state-of-the-art results by usingMLPs to represent complex 3D objects and scenes. Using tools from the neuraltangent kernel (NTK) literature, we show that a standard MLP fails to learnhigh frequencies both in theory and in practice. To overcome this spectralbias, we use a Fourier feature mapping to transform the effective NTK into astationary kernel with a tunable bandwidth. We suggest an approach forselecting problem-specific Fourier features that greatly improves theperformance of MLPs for low-dimensional regression tasks relevant to thecomputer vision and graphics communities.},
  YEAR = {2020},
  MONTH = {Jun},
  URL = {http://arxiv.org/abs/2006.10739v1},
  FILE = {2006.10739v1.pdf}
 }",Fundamentals,,NeRF,,,,,,,,,3DV,,,,"Matthew Tancik, Pratul P. Srinivasan, Ben Mildenhall, Sara Fridovich-Keil, Nithin Raghavan, Utkarsh Singhal, Ravi Ramamoorthi, Jonathan T. Barron, Ren Ng",tancik2020ffn,00000025,"We show that passing input points through a simple Fourier feature mapping enables a multilayer perceptron (MLP) to learn high-frequency functions in low-dimensional problem domains. These results shed light on recent advances in computer vision and graphics that achieve state-of-the-art results by using MLPs to represent complex 3D objects and scenes. Using tools from the neural tangent kernel (NTK) literature, we show that a standard MLP fails to learn high frequencies both in theory and in practice. To overcome this spectral bias, we use a Fourier feature mapping to transform the effective NTK into a stationary kernel with a tunable bandwidth. We suggest an approach for selecting problem-specific Fourier features that greatly improves the performance of MLPs for low-dimensional regression tasks relevant to the computer vision and graphics communities.",135,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnufZqolm0dE5lZ7ju0TWx9uVGSHBZ9xcthSFoWlJXTW_yOA7EtKm596UAhW28Sf52Us
5/23/2021 19:11:04,Deep Reflectance Volumes,,6/20/2020,https://arxiv.org/pdf/2007.09892.pdf,,,https://drive.google.com/file/d/1JEbeIrIttznaowJJcBGZD56KR22j635q/view,https://drive.google.com/file/d/12IAg73kWtGtvKp2RNeaJ8WrmlsTehV4U/view,,,"@article{bi2020deep,
  AUTHOR = {Sai Bi and Zexiang Xu and Kalyan Sunkavalli and Milos Hasan and Yannick Hold-Geoffroy and David Kriegman and Ravi Ramamoorthi},
  TITLE = {Deep Reflectance Volumes: Relightable Reconstructions from Multi-ViewPhotometric Images},
  EPRINT = {2007.09892v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We present a deep learning approach to reconstruct scene appearance fromunstructured images captured under collocated point lighting. At the heart ofDeep Reflectance Volumes is a novel volumetric scene representation consistingof opacity, surface normal and reflectance voxel grids. We present a novelphysically-based differentiable volume ray marching framework to render thesescene volumes under arbitrary viewpoint and lighting. This allows us tooptimize the scene volumes to minimize the error between their rendered imagesand the captured images. Our method is able to reconstruct real scenes withchallenging non-Lambertian reflectance and complex geometry with occlusions andshadowing. Moreover, it accurately generalizes to novel viewpoints andlighting, including non-collocated lighting, rendering photorealistic imagesthat are significantly better than state-of-the-art mesh-based methods. We alsoshow that our learned reflectance volumes are editable, allowing for modifyingthe materials of the captured scenes.},
  YEAR = {2020},
  MONTH = {Jul},
  URL = {http://arxiv.org/abs/2007.09892v1},
  FILE = {2007.09892v1.pdf}
 }",Material/lighting estimation,,,,,,,,,,,ECCV 2020,,,,"Sai Bi, Zexiang Xu, Kalyan Sunkavalli, Miloš Hašan, Yannick Hold-Geoffroy, David Kriegman, Ravi Ramamoorthi",bi2020deep,00000026,"We present a deep learning approach to reconstruct scene appearance from unstructured images captured under collocated point lighting. At the heart of Deep Reflectance Volumes is a novel volumetric scene representation consisting of opacity, surface normal and reflectance voxel grids. We present a novel physically-based differentiable volume ray marching framework to render these scene volumes under arbitrary viewpoint and lighting. This allows us to optimize the scene volumes to minimize the error between their rendered images and the captured images. Our method is able to reconstruct real scenes with challenging non-Lambertian reflectance and complex geometry with occlusions and shadowing. Moreover, it accurately generalizes to novel viewpoints and lighting, including non-collocated lighting, rendering photorealistic images that are significantly better than state-of-the-art mesh-based methods. We also show that our learned reflectance volumes are editable, allowing for modifying the materials of the captured scenes.",18,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnudq_D_2uOWJeFi3fSOv_tV6wJafd0uqUd2oZqqpY9Kk4xDRpqRe9tTUDTLaKBabduY
6/29/2021 16:22:47,Neural Splines: Fitting 3D Surfaces with Infinitely-Wide Neural Networks,Neural Splines,6/24/2020,https://arxiv.org/pdf/2006.13782.pdf,,https://github.com/fwilliams/neural-splines,,,,,"@article{williams2020neural splines,
  AUTHOR = {Francis Williams and Matthew Trager and Joan Bruna and Denis Zorin},
  TITLE = {Neural Splines: Fitting 3D Surfaces with Infinitely-Wide Neural Networks},
  EPRINT = {2006.13782v3},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We present Neural Splines, a technique for 3D surface reconstruction that isbased on random feature kernels arising from infinitely-wide shallow ReLUnetworks. Our method achieves state-of-the-art results, outperforming recentneural network-based techniques and widely used Poisson Surface Reconstruction(which, as we demonstrate, can also be viewed as a type of kernel method).Because our approach is based on a simple kernel formulation, it is easy toanalyze and can be accelerated by general techniques designed for kernel-basedlearning. We provide explicit analytical expressions for our kernel and arguethat our formulation can be seen as a generalization of cubic splineinterpolation to higher dimensions. In particular, the RKHS norm associatedwith Neural Splines biases toward smooth interpolants.},
  YEAR = {2020},
  MONTH = {Jun},
  URL = {http://arxiv.org/abs/2006.13782v3},
  FILE = {2006.13782v3.pdf}
 }",Fundamentals,,None,SDF,Per-scene,,,,,,,,,Yes,,"Francis Williams, Matthew Trager, Joan Bruna, Denis Zorin",williams2020neural,00000027,"We present Neural Splines, a technique for 3D surface reconstruction that is based on random feature kernels arising from infinitely-wide shallow ReLU networks. Our method achieves state-of-the-art results, outperforming recent neural network-based techniques and widely used Poisson Surface Reconstruction (which, as we demonstrate, can also be viewed as a type of kernel method). Because our approach is based on a simple kernel formulation, it is easy to analyze and can be accelerated by general techniques designed for kernel-based learning. We provide explicit analytical expressions for our kernel and argue that our formulation can be seen as a generalization of cubic spline interpolation to higher dimensions. In particular, the RKHS norm associated with Neural Splines biases toward smooth interpolants.",2,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnueUgFLaCKrHEZwzadFVEpe1VV1Fp0Cb6EvUy1S1NQpJPWH9j0LBGZlQ-5RqNFaqIN4
5/23/2021 19:12:08,GRAF: Generative Radiance Fields for 3D-Aware Image Synthesis,GRAF,7/5/2020,https://arxiv.org/pdf/2007.02442.pdf,,https://github.com/autonomousvision/graf,https://www.youtube.com/watch?v=akQf7WaCOHo,,,,"@article{schwarz2020graf,
  AUTHOR = {Katja Schwarz and Yiyi Liao and Michael Niemeyer and Andreas Geiger},
  TITLE = {GRAF: Generative Radiance Fields for 3D-Aware Image Synthesis},
  EPRINT = {2007.02442v4},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {While 2D generative adversarial networks have enabled high-resolution imagesynthesis, they largely lack an understanding of the 3D world and the imageformation process. Thus, they do not provide precise control over cameraviewpoint or object pose. To address this problem, several recent approachesleverage intermediate voxel-based representations in combination withdifferentiable rendering. However, existing methods either produce low imageresolution or fall short in disentangling camera and scene properties, e.g.,the object identity may vary with the viewpoint. In this paper, we propose agenerative model for radiance fields which have recently proven successful fornovel view synthesis of a single scene. In contrast to voxel-basedrepresentations, radiance fields are not confined to a coarse discretization ofthe 3D space, yet allow for disentangling camera and scene properties whiledegrading gracefully in the presence of reconstruction ambiguity. Byintroducing a multi-scale patch-based discriminator, we demonstrate synthesisof high-resolution images while training our model from unposed 2D imagesalone. We systematically analyze our approach on several challenging syntheticand real-world datasets. Our experiments reveal that radiance fields are apowerful representation for generative image synthesis, leading to 3Dconsistent models that render with high fidelity.},
  YEAR = {2020},
  MONTH = {Jul},
  NOTE = {Advances in Neural Information Processing Systems, NeurIPS 2020},
  URL = {http://arxiv.org/abs/2007.02442v4},
  FILE = {2007.02442v4.pdf}
 }",Generalization,"Generative/adversarial formulation, Conditional neural field, Lifting 2D features to 3D, Data-driven component (pre-trained, cross-scene)",,,,,,,,,,,,,,"Katja Schwarz, Yiyi Liao, Michael Niemeyer, Andreas Geiger",schwarz2020graf,00000028,"While 2D generative adversarial networks have enabled high-resolution image synthesis, they largely lack an understanding of the 3D world and the image formation process. Thus, they do not provide precise control over camera viewpoint or object pose. To address this problem, several recent approaches leverage intermediate voxel-based representations in combination with differentiable rendering. However, existing methods either produce low image resolution or fall short in disentangling camera and scene properties, e.g., the object identity may vary with the viewpoint. In this paper, we propose a generative model for radiance fields which have recently proven successful for novel view synthesis of a single scene. In contrast to voxel-based representations, radiance fields are not confined to a coarse discretization of the 3D space, yet allow for disentangling camera and scene properties while degrading gracefully in the presence of reconstruction ambiguity. By introducing a multi-scale patch-based discriminator, we demonstrate synthesis of high-resolution images while training our model from unposed 2D images alone. We systematically analyze our approach on several challenging synthetic and real-world datasets. Our experiments reveal that radiance fields are a powerful representation for generative image synthesis, leading to 3D consistent models that render with high fidelity.",61,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnud6yJHfbEWHbmVUAkwoT-pQkO8O0Mog5CgI1e6fSFZ6AT-zgHlplVObL6CAYl51aAg
5/23/2021 19:11:37,Neural Sparse Voxel Fields,NSVF,7/22/2020,https://arxiv.org/pdf/2007.11571.pdf,https://lingjie0206.github.io/papers/NSVF/,https://github.com/facebookresearch/NSVF,https://www.youtube.com/watch?v=RFqPwH7QFEI,,,,"@article{liu2020nsvf,
  AUTHOR = {Lingjie Liu and Jiatao Gu and Kyaw Zaw Lin and Tat-Seng Chua and Christian Theobalt},
  TITLE = {Neural Sparse Voxel Fields},
  EPRINT = {2007.11571v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Photo-realistic free-viewpoint rendering of real-world scenes using classicalcomputer graphics techniques is challenging, because it requires the difficultstep of capturing detailed appearance and geometry models. Recent studies havedemonstrated promising results by learning scene representations thatimplicitly encode both geometry and appearance without 3D supervision. However,existing approaches in practice often show blurry renderings caused by thelimited network capacity or the difficulty in finding accurate intersections ofcamera rays with the scene geometry. Synthesizing high-resolution imagery fromthese representations often requires time-consuming optical ray marching. Inthis work, we introduce Neural Sparse Voxel Fields (NSVF), a new neural scenerepresentation for fast and high-quality free-viewpoint rendering. NSVF definesa set of voxel-bounded implicit fields organized in a sparse voxel octree tomodel local properties in each cell. We progressively learn the underlyingvoxel structures with a differentiable ray-marching operation from only a setof posed RGB images. With the sparse voxel octree structure, rendering novelviews can be accelerated by skipping the voxels containing no relevant scenecontent. Our method is typically over 10 times faster than the state-of-the-art(namely, NeRF(Mildenhall et al., 2020)) at inference time while achievinghigher quality results. Furthermore, by utilizing an explicit sparse voxelrepresentation, our method can easily be applied to scene editing and scenecomposition. We also demonstrate several challenging tasks, includingmulti-scene learning, free-viewpoint rendering of a moving human, andlarge-scale scene rendering. Code and data are available at our website:https://github.com/facebookresearch/NSVF.},
  YEAR = {2020},
  MONTH = {Jul},
  URL = {http://arxiv.org/abs/2007.11571v2},
  FILE = {2007.11571v2.pdf}
 }","Performance (training), Performance (rendering)","Coarse-to-fine, Sampling, Voxelization, Feature volume, Representation",,,,,,,,,,ECCV 2020,,,,"Lingjie Liu, Jiatao Gu, Kyaw Zaw Lin, Tat-Seng Chua, Christian Theobalt",liu2020nsvf,00000029,"Photo-realistic free-viewpoint rendering of real-world scenes using classical computer graphics techniques is challenging, because it requires the difficult step of capturing detailed appearance and geometry models. Recent studies have demonstrated promising results by learning scene representations that implicitly encode both geometry and appearance without 3D supervision. However, existing approaches in practice often show blurry renderings caused by the limited network capacity or the difficulty in finding accurate intersections of camera rays with the scene geometry. Synthesizing high-resolution imagery from these representations often requires time-consuming optical ray marching. In this work, we introduce Neural Sparse Voxel Fields (NSVF), a new neural scene representation for fast and high-quality free-viewpoint rendering. NSVF defines a set of voxel-bounded implicit fields organized in a sparse voxel octree to model local properties in each cell. We progressively learn the underlying voxel structures with a differentiable ray-marching operation from only a set of posed RGB images. With the sparse voxel octree structure, rendering novel views can be accelerated by skipping the voxels containing no relevant scene content. Our method is typically over 10 times faster than the state-of-the-art (namely, NeRF(Mildenhall et al., 2020)) at inference time while achieving higher quality results. Furthermore, by utilizing an explicit sparse voxel representation, our method can easily be applied to scene editing and scene composition. We also demonstrate several challenging tasks, including multi-scene learning, free-viewpoint rendering of a moving human, and large-scale scene rendering. Code and data are available at our website: https://github.com/facebookresearch/NSVF.",90,Yes,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnudKuKRt2X526ZlgyJ0Yi_pH7BxGziEZYfP1f65YWnTcRpmddePjVCgC3PhXgEdOHeo
9/17/2021 13:11:20,Combining Implicit Function Learning and Parametric Models for 3D Human Reconstruction,IP-Net,7/22/2020,https://arxiv.org/pdf/2007.11432.pdf,https://virtualhumans.mpi-inf.mpg.de/ipnet/,https://github.com/bharat-b7/IPNet,https://virtualhumans.mpi-inf.mpg.de/ipnet/ECCV_short.mp4,,,,"@article{bhatnagar2020ipnet,
  AUTHOR = {Bharat Lal Bhatnagar and Cristian Sminchisescu and Christian Theobalt and Gerard Pons-Moll},
  TITLE = {Combining Implicit Function Learning and Parametric Models for 3D HumanReconstruction},
  EPRINT = {2007.11432v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Implicit functions represented as deep learning approximations are powerfulfor reconstructing 3D surfaces. However, they can only produce static surfacesthat are not controllable, which provides limited ability to modify theresulting model by editing its pose or shape parameters. Nevertheless, suchfeatures are essential in building flexible models for both computer graphicsand computer vision. In this work, we present methodology that combinesdetail-rich implicit functions and parametric representations in order toreconstruct 3D models of people that remain controllable and accurate even inthe presence of clothing. Given sparse 3D point clouds sampled on the surfaceof a dressed person, we use an Implicit Part Network (IP-Net)to jointly predictthe outer 3D surface of the dressed person, the and inner body surface, and thesemantic correspondences to a parametric body model. We subsequently usecorrespondences to fit the body model to our inner surface and then non-rigidlydeform it (under a parametric body + displacement model) to the outer surfacein order to capture garment, face and hair detail. In quantitative andqualitative experiments with both full body data and hand scans we show thatthe proposed methodology generalizes, and is effective even given incompletepoint clouds collected from single-view depth images. Our models and code canbe downloaded from http://virtualhumans.mpi-inf.mpg.de/ipnet.},
  YEAR = {2020},
  MONTH = {Jul},
  URL = {http://arxiv.org/abs/2007.11432v1},
  FILE = {2007.11432v1.pdf}
 }",Human body,"Voxelization, Feature volume, Representation",,Occupancy,,,,,,,,ECCV 2020 Oral,,Yes,Direct,"Bharat Lal Bhatnagar, Cristian Sminchisescu, Christian Theobalt, Gerard Pons-Moll",bhatnagar2020ipnet,00000201,"Implicit functions represented as deep learning approximations are powerful for reconstructing 3D surfaces. However, they can only produce static surfaces that are not controllable, which provides limited ability to modify the resulting model by editing its pose or shape parameters. Nevertheless, such features are essential in building flexible models for both computer graphics and computer vision. In this work, we present methodology that combines detail-rich implicit functions and parametric representations in order to reconstruct 3D models of people that remain controllable and accurate even in the presence of clothing. Given sparse 3D point clouds sampled on the surface of a dressed person, we use an Implicit Part Network (IP-Net)to jointly predict the outer 3D surface of the dressed person, the and inner body surface, and the semantic correspondences to a parametric body model. We subsequently use correspondences to fit the body model to our inner surface and then non-rigidly deform it (under a parametric body + displacement model) to the outer surface in order to capture garment, face and hair detail. In quantitative and qualitative experiments with both full body data and hand scans we show that the proposed methodology generalizes, and is effective even given incomplete point clouds collected from single-view depth images. Our models and code can be downloaded from http://virtualhumans.mpi-inf.mpg.de/ipnet.",,Yes,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnucvPhphJ-gdaWaQHOTB7tsu16sT0TJjKz8I2y5ETXAAasFdbtCSJRLHu5FjPUC9CNw
8/29/2021 16:29:22,Ladybird: Quasi-Monte Carlo Sampling for Deep Implicit Field Based 3D Reconstruction with Symmetry,Ladybird,7/27/2020,https://arxiv.org/pdf/2007.13393.pdf,,https://github.com/FuxiCV/Ladybird,,,,,"@article{xu2020ladybird,
  AUTHOR = {Yifan Xu and Tianqi Fan and Yi Yuan and Gurprit Singh},
  TITLE = {Ladybird: Quasi-Monte Carlo Sampling for Deep Implicit Field Based 3DReconstruction with Symmetry},
  EPRINT = {2007.13393v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Deep implicit field regression methods are effective for 3D reconstructionfrom single-view images. However, the impact of different sampling patterns onthe reconstruction quality is not well-understood. In this work, we first studythe effect of point set discrepancy on the network training. Based on FarthestPoint Sampling algorithm, we propose a sampling scheme that theoreticallyencourages better generalization performance, and results in fast convergencefor SGD-based optimization algorithms. Secondly, based on the reflectivesymmetry of an object, we propose a feature fusion method that alleviatesissues due to self-occlusions which makes it difficult to utilize local imagefeatures. Our proposed system Ladybird is able to create high quality 3D objectreconstructions from a single input image. We evaluate Ladybird on a largescale 3D dataset (ShapeNet) demonstrating highly competitive results in termsof Chamfer distance, Earth Mover's distance and Intersection Over Union (IoU).},
  YEAR = {2020},
  MONTH = {Jul},
  URL = {http://arxiv.org/abs/2007.13393v1},
  FILE = {2007.13393v1.pdf}
 }","Few-shot reconstruction, Fundamentals","Sampling, Symmetry",,,,,,,,,,ECCV 2020 Oral,,,,"Yifan Xu, Tianqi Fan, Yi Yuan, Gurprit Singh",xu2020ladybird,00000165,"Deep implicit field regression methods are effective for 3D reconstruction from single-view images. However, the impact of different sampling patterns on the reconstruction quality is not well-understood. In this work, we first study the effect of point set discrepancy on the network training. Based on Farthest Point Sampling algorithm, we propose a sampling scheme that theoretically encourages better generalization performance, and results in fast convergence for SGD-based optimization algorithms. Secondly, based on the reflective symmetry of an object, we propose a feature fusion method that alleviates issues due to self-occlusions which makes it difficult to utilize local image features. Our proposed system Ladybird is able to create high quality 3D object reconstructions from a single input image. We evaluate Ladybird on a large scale 3D dataset (ShapeNet) demonstrating highly competitive results in terms of Chamfer distance, Earth Mover's distance and Intersection Over Union (IoU).",6,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnufK_x2Z_Xd0qqU17yj-9ilOiq2eYj9AnRYbTNg1xMJO3ObAzrZkK0L_V2v1voq0m6E
9/17/2021 14:11:39,Monocular Real-Time Volumetric Performance Capture,Monoport,7/28/2020,https://arxiv.org/pdf/2007.13988.pdf,https://project-splinter.github.io/monoport/,https://github.com/Project-Splinter/MonoPort,https://github.com/Project-Splinter/MonoPort,,,,"@article{li2020monoport,
  AUTHOR = {Ruilong Li and Yuliang Xiu and Shunsuke Saito and Zeng Huang and Kyle Olszewski and Hao Li},
  TITLE = {Monocular Real-Time Volumetric Performance Capture},
  EPRINT = {2007.13988v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We present the first approach to volumetric performance capture andnovel-view rendering at real-time speed from monocular video, eliminating theneed for expensive multi-view systems or cumbersome pre-acquisition of apersonalized template model. Our system reconstructs a fully textured 3D humanfrom each frame by leveraging Pixel-Aligned Implicit Function (PIFu). WhilePIFu achieves high-resolution reconstruction in a memory-efficient manner, itscomputationally expensive inference prevents us from deploying such a systemfor real-time applications. To this end, we propose a novel hierarchicalsurface localization algorithm and a direct rendering method without explicitlyextracting surface meshes. By culling unnecessary regions for evaluation in acoarse-to-fine manner, we successfully accelerate the reconstruction by twoorders of magnitude from the baseline without compromising the quality.Furthermore, we introduce an Online Hard Example Mining (OHEM) technique thateffectively suppresses failure modes due to the rare occurrence of challengingexamples. We adaptively update the sampling probability of the training databased on the current reconstruction accuracy, which effectively alleviatesreconstruction artifacts. Our experiments and evaluations demonstrate therobustness of our system to various challenging angles, illuminations, poses,and clothing styles. We also show that our approach compares favorably with thestate-of-the-art monocular performance capture. Our proposed approach removesthe need for multi-view studio settings and enables a consumer-accessiblesolution for volumetric capture.},
  YEAR = {2020},
  MONTH = {Jul},
  URL = {http://arxiv.org/abs/2007.13988v1},
  FILE = {2007.13988v1.pdf}
 }",Human body,Lifting 2D features to 3D,,,Category-level,,,,,,,ECCV 2020,,Yes,,"Ruilong Li, Yuliang Xiu, Shunsuke Saito, Zeng Huang, Kyle Olszewski, Hao Li",li2020monoport,00000206,"We present the first approach to volumetric performance capture and novel-view rendering at real-time speed from monocular video, eliminating the need for expensive multi-view systems or cumbersome pre-acquisition of a personalized template model. Our system reconstructs a fully textured 3D human from each frame by leveraging Pixel-Aligned Implicit Function (PIFu). While PIFu achieves high-resolution reconstruction in a memory-efficient manner, its computationally expensive inference prevents us from deploying such a system for real-time applications. To this end, we propose a novel hierarchical surface localization algorithm and a direct rendering method without explicitly extracting surface meshes. By culling unnecessary regions for evaluation in a coarse-to-fine manner, we successfully accelerate the reconstruction by two orders of magnitude from the baseline without compromising the quality. Furthermore, we introduce an Online Hard Example Mining (OHEM) technique that effectively suppresses failure modes due to the rare occurrence of challenging examples. We adaptively update the sampling probability of the training data based on the current reconstruction accuracy, which effectively alleviates reconstruction artifacts. Our experiments and evaluations demonstrate the robustness of our system to various challenging angles, illuminations, poses, and clothing styles. We also show that our approach compares favorably with the state-of-the-art monocular performance capture. Our proposed approach removes the need for multi-view studio settings and enables a consumer-accessible solution for volumetric capture.",,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnufQ-gEi5aEMeh13NEXaszFR27WELg2bhtg_cqLGrjJDFpf_RAIGOrFllkf0yTz9XTg
7/19/2021 21:28:27,Continuous Object Representation Networks: Novel View Synthesis without Target View Supervision,CORN,7/30/2020,https://arxiv.org/pdf/2007.15627.pdf,,,,,,,"@article{hani2020corn,
  AUTHOR = {Nicolai Hani and Selim Engin and Jun-Jee Chao and Volkan Isler},
  TITLE = {Continuous Object Representation Networks: Novel View Synthesis withoutTarget View Supervision},
  EPRINT = {2007.15627v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Novel View Synthesis (NVS) is concerned with synthesizing views under cameraviewpoint transformations from one or multiple input images. NVS requiresexplicit reasoning about 3D object structure and unseen parts of the scene tosynthesize convincing results. As a result, current approaches typically relyon supervised training with either ground truth 3D models or multiple targetimages. We propose Continuous Object Representation Networks (CORN), aconditional architecture that encodes an input image's geometry and appearancethat map to a 3D consistent scene representation. We can train CORN with onlytwo source images per object by combining our model with a neural renderer. Akey feature of CORN is that it requires no ground truth 3D models or targetview supervision. Regardless, CORN performs well on challenging tasks such asnovel view synthesis and single-view 3D reconstruction and achieves performancecomparable to state-of-the-art approaches that use direct supervision. Forup-to-date information, data, and code, please see our project page:https://nicolaihaeni.github.io/corn/.},
  YEAR = {2020},
  MONTH = {Jul},
  URL = {http://arxiv.org/abs/2007.15627v2},
  FILE = {2007.15627v2.pdf}
 }","Few-shot reconstruction, Generalization","Conditional neural field, Lifting 2D features to 3D, Image-based rendering, Data-driven component (pre-trained, cross-scene)",,,,,,,,,,NeurIPS 2020,,,,"Nicolai Häni, Selim Engin, Jun-Jee Chao, Volkan Isler",hani2020corn,00000030,"Novel View Synthesis (NVS) is concerned with synthesizing views under camera viewpoint transformations from one or multiple input images. NVS requires explicit reasoning about 3D object structure and unseen parts of the scene to synthesize convincing results. As a result, current approaches typically rely on supervised training with either ground truth 3D models or multiple target images. We propose Continuous Object Representation Networks (CORN), a conditional architecture that encodes an input image's geometry and appearance that map to a 3D consistent scene representation. We can train CORN with only two source images per object by combining our model with a neural renderer. A key feature of CORN is that it requires no ground truth 3D models or target view supervision. Regardless, CORN performs well on challenging tasks such as novel view synthesis and single-view 3D reconstruction and achieves performance comparable to state-of-the-art approaches that use direct supervision. For up-to-date information, data, and code, please see our project page: https://nicolaihaeni.github.io/corn/.",5,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnuftJ9c3qSAjCWifaKzzfZXaq86dYBG3gy9jo51d55nlWDvs-UOKGVR-dZHCRwPuHbw
8/29/2021 21:13:48,PatchNets: Patch-Based Generalizable Deep Implicit 3D Shape Representations,PatchNets,8/4/2020,https://arxiv.org/pdf/2008.01639.pdf,http://gvv.mpi-inf.mpg.de/projects/PatchNets/,https://github.com/edgar-tr/patchnets,"http://gvv.mpi-inf.mpg.de/projects/PatchNets/data/patchnets_short.mp4, http://gvv.mpi-inf.mpg.de/projects/PatchNets/data/patchnets_supplemental.mp4, http://gvv.mpi-inf.mpg.de/projects/PatchNets/data/patchnets_talk.mp4",http://gvv.mpi-inf.mpg.de/projects/PatchNets/data/patchnets_supplemental.pdf,,,"@article{tretschk2020patchnets,
  AUTHOR = {Edgar Tretschk and Ayush Tewari and Vladislav Golyanik and Michael Zollhofer and Carsten Stoll and Christian Theobalt},
  TITLE = {PatchNets: Patch-Based Generalizable Deep Implicit 3D ShapeRepresentations},
  EPRINT = {2008.01639v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Implicit surface representations, such as signed-distance functions, combinedwith deep learning have led to impressive models which can represent detailedshapes of objects with arbitrary topology. Since a continuous function islearned, the reconstructions can also be extracted at any arbitrary resolution.However, large datasets such as ShapeNet are required to train such models. Inthis paper, we present a new mid-level patch-based surface representation. Atthe level of patches, objects across different categories share similarities,which leads to more generalizable models. We then introduce a novel method tolearn this patch-based representation in a canonical space, such that it is asobject-agnostic as possible. We show that our representation trained on onecategory of objects from ShapeNet can also well represent detailed shapes fromany other category. In addition, it can be trained using much fewer shapes,compared to existing approaches. We show several applications of our newrepresentation, including shape interpolation and partial point cloudcompletion. Due to explicit control over positions, orientations and scales ofpatches, our representation is also more controllable compared to object-levelrepresentations, which enables us to deform encoded shapes non-rigidly.},
  YEAR = {2020},
  MONTH = {Aug},
  URL = {http://arxiv.org/abs/2008.01639v2},
  FILE = {2008.01639v2.pdf}
 }","Human body, Generalization","Conditional neural field, Volume partitioning, Data-driven component (pre-trained, cross-scene)",,SDF,Universal,,,,,,,ECCV 2020,,Yes,Direct,"Edgar Tretschk, Ayush Tewari, Vladislav Golyanik, Michael Zollhöfer, Carsten Stoll, Christian Theobalt",tretschk2020patchnets,00000184,"Implicit surface representations, such as signed-distance functions, combined with deep learning have led to impressive models which can represent detailed shapes of objects with arbitrary topology. Since a continuous function is learned, the reconstructions can also be extracted at any arbitrary resolution. However, large datasets such as ShapeNet are required to train such models. In this paper, we present a new mid-level patch-based surface representation. At the level of patches, objects across different categories share similarities, which leads to more generalizable models. We then introduce a novel method to learn this patch-based representation in a canonical space, such that it is as object-agnostic as possible. We show that our representation trained on one category of objects from ShapeNet can also well represent detailed shapes from any other category. In addition, it can be trained using much fewer shapes, compared to existing approaches. We show several applications of our new representation, including shape interpolation and partial point cloud completion. Due to explicit control over positions, orientations and scales of patches, our representation is also more controllable compared to object-level representations, which enables us to deform encoded shapes non-rigidly.",16,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnufOhNCcU_6_tYUecQo7kJc6KXhF7IhIvsUy2NRVH4wWhjTVpZwo2t34enCiJC93gDw
5/23/2021 19:07:32,NeRF in the Wild: Neural Radiance Fields for Unconstrained Photo Collections,NeRF-W,8/5/2020,https://arxiv.org/pdf/2008.02268.pdf,https://nerf-w.github.io/,,,,,,"@article{martin-brualla2020nerfw,
  AUTHOR = {Ricardo Martin-Brualla and Noha Radwan and Mehdi S. M. Sajjadi and Jonathan T. Barron and Alexey Dosovitskiy and Daniel Duckworth},
  TITLE = {NeRF in the Wild: Neural Radiance Fields for Unconstrained PhotoCollections},
  EPRINT = {2008.02268v3},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We present a learning-based method for synthesizing novel views of complexscenes using only unstructured collections of in-the-wild photographs. We buildon Neural Radiance Fields (NeRF), which uses the weights of a multilayerperceptron to model the density and color of a scene as a function of 3Dcoordinates. While NeRF works well on images of static subjects captured undercontrolled settings, it is incapable of modeling many ubiquitous, real-worldphenomena in uncontrolled images, such as variable illumination or transientoccluders. We introduce a series of extensions to NeRF to address these issues,thereby enabling accurate reconstructions from unstructured image collectionstaken from the internet. We apply our system, dubbed NeRF-W, to internet photocollections of famous landmarks, and demonstrate temporally consistent novelview renderings that are significantly closer to photorealism than the priorstate of the art.},
  YEAR = {2020},
  MONTH = {Aug},
  URL = {http://arxiv.org/abs/2008.02268v3},
  FILE = {2008.02268v3.pdf}
 }","Dynamic, Segmentation/composition, Material/lighting estimation",Volume partitioning,,,,,,,,,,NeurIPS,,,,"Ricardo Martin-Brualla, Noha Radwan, Mehdi S. M. Sajjadi, Jonathan T. Barron, Alexey Dosovitskiy, Daniel Duckworth",martin-brualla2020nerfw,00000031,"We present a learning-based method for synthesizing novel views of complex scenes using only unstructured collections of in-the-wild photographs. We build on Neural Radiance Fields (NeRF), which uses the weights of a multilayer perceptron to model the density and color of a scene as a function of 3D coordinates. While NeRF works well on images of static subjects captured under controlled settings, it is incapable of modeling many ubiquitous, real-world phenomena in uncontrolled images, such as variable illumination or transient occluders. We introduce a series of extensions to NeRF to address these issues, thereby enabling accurate reconstructions from unstructured image collections taken from the internet. We apply our system, dubbed NeRF-W, to internet photo collections of famous landmarks, and demonstrate temporally consistent novel view renderings that are significantly closer to photorealism than the prior state of the art.",78,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnudJbhp-mitDgz-cxQCv7HRgbz58nsTTyYkD-QsDKH9KPAkxLJK5tIlPfnNSqGKn8cg
5/23/2021 19:10:06,Neural Reflectance Fields for Appearance Acquisition,,8/9/2020,https://arxiv.org/pdf/2008.03824.pdf,,,https://www.youtube.com/watch?v=tQZk5OoFgsc,,,,"@article{bi2020neural,
  AUTHOR = {Sai Bi and Zexiang Xu and Pratul Srinivasan and Ben Mildenhall and Kalyan Sunkavalli and Milos Hasan and Yannick Hold-Geoffroy and David Kriegman and Ravi Ramamoorthi},
  TITLE = {Neural Reflectance Fields for Appearance Acquisition},
  EPRINT = {2008.03824v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We present Neural Reflectance Fields, a novel deep scene representation thatencodes volume density, normal and reflectance properties at any 3D point in ascene using a fully-connected neural network. We combine this representationwith a physically-based differentiable ray marching framework that can renderimages from a neural reflectance field under any viewpoint and light. Wedemonstrate that neural reflectance fields can be estimated from imagescaptured with a simple collocated camera-light setup, and accurately model theappearance of real-world scenes with complex geometry and reflectance. Onceestimated, they can be used to render photo-realistic images under novelviewpoint and (non-collocated) lighting conditions and accurately reproducechallenging effects like specularities, shadows and occlusions. This allows usto perform high-quality view synthesis and relighting that is significantlybetter than previous methods. We also demonstrate that we can compose theestimated neural reflectance field of a real scene with traditional scenemodels and render them using standard Monte Carlo rendering engines. Our workthus enables a complete pipeline from high-quality and practical appearanceacquisition to 3D scene composition and rendering.},
  YEAR = {2020},
  MONTH = {Aug},
  URL = {http://arxiv.org/abs/2008.03824v2},
  FILE = {2008.03824v2.pdf}
 }",Material/lighting estimation,,,,,,,,,,,,,,,"Sai Bi, Zexiang Xu, Pratul Srinivasan, Ben Mildenhall, Kalyan Sunkavalli, Miloš Hašan, Yannick Hold-Geoffroy, David Kriegman, Ravi Ramamoorthi",bi2020neural,00000032,"We present Neural Reflectance Fields, a novel deep scene representation that encodes volume density, normal and reflectance properties at any 3D point in a scene using a fully-connected neural network. We combine this representation with a physically-based differentiable ray marching framework that can render images from a neural reflectance field under any viewpoint and light. We demonstrate that neural reflectance fields can be estimated from images captured with a simple collocated camera-light setup, and accurately model the appearance of real-world scenes with complex geometry and reflectance. Once estimated, they can be used to render photo-realistic images under novel viewpoint and (non-collocated) lighting conditions and accurately reproduce challenging effects like specularities, shadows and occlusions. This allows us to perform high-quality view synthesis and relighting that is significantly better than previous methods. We also demonstrate that we can compose the estimated neural reflectance field of a real scene with traditional scene models and render them using standard Monte Carlo rendering engines. Our work thus enables a complete pipeline from high-quality and practical appearance acquisition to 3D scene composition and rendering.",21,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnufVMe_pxDvMSWMA0vTHYGNLHJPzFsgZg3saq62pxKFePzAFXydwpW9JDVDeeFkDMAY
9/17/2021 11:38:32,Grasping Field: Learning Implicit Representations for Human Grasps,Grasping Field,8/10/2020,https://arxiv.org/pdf/2008.04451.pdf,https://mano.is.tue.mpg.de/,https://github.com/korrawe/grasping_field,https://www.youtube.com/watch?v=_1o21xc3TD0,,,,"@article{karunratanakul2020grasping field,
  AUTHOR = {Korrawe Karunratanakul and Jinlong Yang and Yan Zhang and Michael Black and Krikamol Muandet and Siyu Tang},
  TITLE = {Grasping Field: Learning Implicit Representations for Human Grasps},
  EPRINT = {2008.04451v3},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Robotic grasping of house-hold objects has made remarkable progress in recentyears. Yet, human grasps are still difficult to synthesize realistically. Thereare several key reasons: (1) the human hand has many degrees of freedom (morethan robotic manipulators); (2) the synthesized hand should conform to thesurface of the object; and (3) it should interact with the object in asemantically and physically plausible manner. To make progress in thisdirection, we draw inspiration from the recent progress on learning-basedimplicit representations for 3D object reconstruction. Specifically, we proposean expressive representation for human grasp modelling that is efficient andeasy to integrate with deep neural networks. Our insight is that every point ina three-dimensional space can be characterized by the signed distances to thesurface of the hand and the object, respectively. Consequently, the hand, theobject, and the contact area can be represented by implicit surfaces in acommon space, in which the proximity between the hand and the object can bemodelled explicitly. We name this 3D to 2D mapping as Grasping Field,parameterize it with a deep neural network, and learn it from data. Wedemonstrate that the proposed grasping field is an effective and expressiverepresentation for human grasp generation. Specifically, our generative modelis able to synthesize high-quality human grasps, given only on a 3D objectpoint cloud. The extensive experiments demonstrate that our generative modelcompares favorably with a strong baseline and approaches the level of naturalhuman grasps. Our method improves the physical plausibility of the hand-objectcontact reconstruction and achieves comparable performance for 3D handreconstruction compared to state-of-the-art methods.},
  YEAR = {2020},
  MONTH = {Aug},
  URL = {http://arxiv.org/abs/2008.04451v3},
  FILE = {2008.04451v3.pdf}
 }","Beyond graphics, Robotics",,,SDF,Category-level,,,,,,,,,Yes,"Direct, Indirect","Korrawe Karunratanakul, Jinlong Yang, Yan Zhang, Michael Black, Krikamol Muandet, Siyu Tang",karunratanakul2020grasping field,00000195,"Robotic grasping of house-hold objects has made remarkable progress in recent years. Yet, human grasps are still difficult to synthesize realistically. There are several key reasons: (1) the human hand has many degrees of freedom (more than robotic manipulators); (2) the synthesized hand should conform to the surface of the object; and (3) it should interact with the object in a semantically and physically plausible manner. To make progress in this direction, we draw inspiration from the recent progress on learning-based implicit representations for 3D object reconstruction. Specifically, we propose an expressive representation for human grasp modelling that is efficient and easy to integrate with deep neural networks. Our insight is that every point in a three-dimensional space can be characterized by the signed distances to the surface of the hand and the object, respectively. Consequently, the hand, the object, and the contact area can be represented by implicit surfaces in a common space, in which the proximity between the hand and the object can be modelled explicitly. We name this 3D to 2D mapping as Grasping Field, parameterize it with a deep neural network, and learn it from data. We demonstrate that the proposed grasping field is an effective and expressive representation for human grasp generation. Specifically, our generative model is able to synthesize high-quality human grasps, given only on a 3D object point cloud. The extensive experiments demonstrate that our generative model compares favorably with a strong baseline and approaches the level of natural human grasps. Our method improves the physical plausibility of the hand-object contact reconstruction and achieves comparable performance for 3D hand reconstruction compared to state-of-the-art methods.",,,No,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnufJYxl7u98LVNosbGp7GORnkfzV5dnmTWYC-PWmnsSX1E2BN34qB2SEJQ7fTqviN6M
9/18/2021 10:22:36,Pix2Surf: Learning Parametric 3D Surface Models of Objects from Images,Pix2Surf,8/18/2020,https://arxiv.org/pdf/2008.07760.pdf,https://geometry.stanford.edu/projects/pix2surf/,https://github.com/JiahuiLei/Pix2Surf,https://www.youtube.com/watch?v=jaxB0VSuvms,https://geometry.stanford.edu/projects/pix2surf/pub/pix2surf_supp.pdf,,,"@article{lei2020pix2surf,
  AUTHOR = {Jiahui Lei and Srinath Sridhar and Paul Guerrero and Minhyuk Sung and Niloy Mitra and Leonidas J. Guibas},
  TITLE = {Pix2Surf: Learning Parametric 3D Surface Models of Objects from Images},
  EPRINT = {2008.07760v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We investigate the problem of learning to generate 3D parametric surfacerepresentations for novel object instances, as seen from one or more views.Previous work on learning shape reconstruction from multiple views usesdiscrete representations such as point clouds or voxels, while continuoussurface generation approaches lack multi-view consistency. We address theseissues by designing neural networks capable of generating high-qualityparametric 3D surfaces which are also consistent between views. Furthermore,the generated 3D surfaces preserve accurate image pixel to 3D surface pointcorrespondences, allowing us to lift texture information to reconstruct shapeswith rich geometry and appearance. Our method is supervised and trained on apublic dataset of shapes from common object categories. Quantitative resultsindicate that our method significantly outperforms previous work, whilequalitative results demonstrate the high quality of our reconstructions.},
  YEAR = {2020},
  MONTH = {Aug},
  URL = {http://arxiv.org/abs/2008.07760v1},
  FILE = {2008.07760v1.pdf}
 }","Few-shot reconstruction, Generalization","Generative/adversarial formulation, Conditional neural field, Lifting 2D features to 3D",None,"Atlas, Explicit",Category-level,,,,,,,ECCV 2020,,Yes,Direct,"Jiahui Lei, Srinath Sridhar, Paul Guerrero, Minhyuk Sung, Niloy Mitra, Leonidas J. Guibas",lei2020pix2surf,00000224,"We investigate the problem of learning to generate 3D parametric surface representations for novel object instances, as seen from one or more views. Previous work on learning shape reconstruction from multiple views uses discrete representations such as point clouds or voxels, while continuous surface generation approaches lack multi-view consistency. We address these issues by designing neural networks capable of generating high-quality parametric 3D surfaces which are also consistent between views. Furthermore, the generated 3D surfaces preserve accurate image pixel to 3D surface point correspondences, allowing us to lift texture information to reconstruct shapes with rich geometry and appearance. Our method is supervised and trained on a public dataset of shapes from common object categories. Quantitative results indicate that our method significantly outperforms previous work, while qualitative results demonstrate the high quality of our reconstructions.",,,No,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnueyhdc2bh2JyPf2GXuqOx7mfx7eFhehYVfrxhOzrME4tSznktL0FcXXRDvL8QZydXU
6/29/2021 15:53:44,On the Effectiveness of Weight-Encoded Neural Implicit 3D Shapes,,9/17/2020,https://arxiv.org/pdf/2009.09808.pdf,,https://github.com/u2ni/ICML2021,,,,,"@article{davies2020on,
  AUTHOR = {Thomas Davies and Derek Nowrouzezahrai and Alec Jacobson},
  TITLE = {On the Effectiveness of Weight-Encoded Neural Implicit 3D Shapes},
  EPRINT = {2009.09808v3},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.GR},
  ABSTRACT = {A neural implicit outputs a number indicating whether the given query pointin space is inside, outside, or on a surface. Many prior works have focused on_latent-encoded_ neural implicits, where a latent vector encoding of a specificshape is also fed as input. While affording latent-space interpolation, thiscomes at the cost of reconstruction accuracy for any _single_ shape. Training aspecific network for each 3D shape, a _weight-encoded_ neural implicit mayforgo the latent vector and focus reconstruction accuracy on the details of asingle shape. While previously considered as an intermediary representation for3D scanning tasks or as a toy-problem leading up to latent-encoding tasks,weight-encoded neural implicits have not yet been taken seriously as a 3D shaperepresentation. In this paper, we establish that weight-encoded neuralimplicits meet the criteria of a first-class 3D shape representation. Weintroduce a suite of technical contributions to improve reconstructionaccuracy, convergence, and robustness when learning the signed distance fieldinduced by a polygonal mesh -- the _de facto_ standard representation. Viewedas a lossy compression, our conversion outperforms standard techniques fromgeometry processing. Compared to previous latent- and weight-encoded neuralimplicits we demonstrate superior robustness, scalability, and performance.},
  YEAR = {2020},
  MONTH = {Sep},
  URL = {http://arxiv.org/abs/2009.09808v3},
  FILE = {2009.09808v3.pdf}
 }","Performance (training), Performance (rendering), Compression, Fundamentals",Sampling,,,,,,,,,,,https://ten-thousand-models.appspot.com/,Yes,,"Thomas Davies, Derek Nowrouzezahrai, Alec Jacobson",davies2020on,00000033,"A neural implicit outputs a number indicating whether the given query point in space is inside, outside, or on a surface. Many prior works have focused on _latent-encoded_ neural implicits, where a latent vector encoding of a specific shape is also fed as input. While affording latent-space interpolation, this comes at the cost of reconstruction accuracy for any _single_ shape. Training a specific network for each 3D shape, a _weight-encoded_ neural implicit may forgo the latent vector and focus reconstruction accuracy on the details of a single shape. While previously considered as an intermediary representation for 3D scanning tasks or as a toy-problem leading up to latent-encoding tasks, weight-encoded neural implicits have not yet been taken seriously as a 3D shape representation. In this paper, we establish that weight-encoded neural implicits meet the criteria of a first-class 3D shape representation. We introduce a suite of technical contributions to improve reconstruction accuracy, convergence, and robustness when learning the signed distance field induced by a polygonal mesh -- the _de facto_ standard representation. Viewed as a lossy compression, our conversion outperforms standard techniques from geometry processing. Compared to previous latent- and weight-encoded neural implicits we demonstrate superior robustness, scalability, and performance.",5,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnuc9DlJPzGFkuLraXBw0XEwdEX4RvwduOeIOK1B_Wd_AKGKiqXB_9weIClm5hg4udto
7/19/2021 21:09:05,Implicit Feature Networks for Texture Completion from Partial 3D Data,IF-Net-Texture,9/20/2020,https://arxiv.org/pdf/2009.09458.pdf,https://virtualhumans.mpi-inf.mpg.de/ifnets/,https://github.com/jchibane/if-net_texture,,,,,"@article{chibane2020implicit,
  AUTHOR = {Julian Chibane and Gerard Pons-Moll},
  TITLE = {Implicit Feature Networks for Texture Completion from Partial 3D Data},
  EPRINT = {2009.09458v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Prior work to infer 3D texture use either texture atlases, which requireuv-mappings and hence have discontinuities, or colored voxels, which are memoryinefficient and limited in resolution. Recent work, predicts RGB color at everyXYZ coordinate forming a texture field, but focus on completing texture given asingle 2D image. Instead, we focus on 3D texture and geometry completion frompartial and incomplete 3D scans. IF-Nets have recently achievedstate-of-the-art results on 3D geometry completion using a multi-scale deepfeature encoding, but the outputs lack texture. In this work, we generalizeIF-Nets to texture completion from partial textured scans of humans andarbitrary objects. Our key insight is that 3D texture completion benefits fromincorporating local and global deep features extracted from both the 3D partialtexture and completed geometry. Specifically, given the partial 3D texture andthe 3D geometry completed with IF-Nets, our model successfully in-paints themissing texture parts in consistence with the completed geometry. Our model wonthe SHARP ECCV'20 challenge, achieving highest performance on all challenges.},
  YEAR = {2020},
  MONTH = {Sep},
  NOTE = {SHARP Workshop, European Conference on Computer Vision (ECCV),
  2020},
  URL = {http://arxiv.org/abs/2009.09458v1},
  FILE = {2009.09458v1.pdf}
 }",,"Feature volume, Data-driven component (pre-trained, cross-scene)",None,Occupancy,,,,,,,,ECCV 2020,,No,,"Julian Chibane, Gerard Pons-Moll",chibane2020implicit,00000034,"Prior work to infer 3D texture use either texture atlases, which require uv-mappings and hence have discontinuities, or colored voxels, which are memory inefficient and limited in resolution. Recent work, predicts RGB color at every XYZ coordinate forming a texture field, but focus on completing texture given a single 2D image. Instead, we focus on 3D texture and geometry completion from partial and incomplete 3D scans. IF-Nets have recently achieved state-of-the-art results on 3D geometry completion using a multi-scale deep feature encoding, but the outputs lack texture. In this work, we generalize IF-Nets to texture completion from partial textured scans of humans and arbitrary objects. Our key insight is that 3D texture completion benefits from incorporating local and global deep features extracted from both the 3D partial texture and completed geometry. Specifically, given the partial 3D texture and the 3D geometry completed with IF-Nets, our model successfully in-paints the missing texture parts in consistence with the completed geometry. Our model won the SHARP ECCV'20 challenge, achieving highest performance on all challenges.",6,Yes,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnuf64KwMbm20qLtd5hKjlFfm7gu_XjAyq29NohRI3t_uDpGdqPoxIf-SxIlWSeERpDU
9/1/2021 14:36:37,"ContactNets: Learning Discontinuous Contact Dynamics with Smooth, Implicit Representations",ContactNets,9/23/2020,https://arxiv.org/pdf/2009.11193.pdf,,https://github.com/DAIRLab/contact-nets,https://www.youtube.com/watch?v=I6p8JrIp1Es,,,,"@article{li2021nemi,
  AUTHOR = {Jiaxin Li and Zijian Feng and Qi She and Henghui Ding and Changhu Wang and Gim Hee Lee},
  TITLE = {MINE: Towards Continuous Depth MPI with NeRF for Novel View Synthesis},
  EPRINT = {2103.14910v3},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {In this paper, we propose MINE to perform novel view synthesis and depthestimation via dense 3D reconstruction from a single image. Our approach is acontinuous depth generalization of the Multiplane Images (MPI) by introducingthe NEural radiance fields (NeRF). Given a single image as input, MINE predictsa 4-channel image (RGB and volume density) at arbitrary depth values to jointlyreconstruct the camera frustum and fill in occluded contents. The reconstructedand inpainted frustum can then be easily rendered into novel RGB or depth viewsusing differentiable rendering. Extensive experiments on RealEstate10K, KITTIand Flowers Light Fields show that our MINE outperforms state-of-the-art by alarge margin in novel view synthesis. We also achieve competitive results indepth estimation on iBims-1 and NYU-v2 without annotated depth supervision. Oursource code is available at https://github.com/vincentfung13/MINE},
  YEAR = {2021},
  MONTH = {Mar},
  URL = {http://arxiv.org/abs/2103.14910v3},
  FILE = {2103.14910v3.pdf}
 }","Beyond graphics, Science and engineering, Robotics","Representation, Data-driven component (pre-trained, cross-scene)",,,,,,,,,,CoRL 2020,,,Direct,"Jiaxin Li, Zijian Feng, Qi She, Henghui Ding, Changhu Wang, Gim Hee Lee",li2021nemi,00000087,"In this paper, we propose MINE to perform novel view synthesis and depth estimation via dense 3D reconstruction from a single image. Our approach is a continuous depth generalization of the Multiplane Images (MPI) by introducing the NEural radiance fields (NeRF). Given a single image as input, MINE predicts a 4-channel image (RGB and volume density) at arbitrary depth values to jointly reconstruct the camera frustum and fill in occluded contents. The reconstructed and inpainted frustum can then be easily rendered into novel RGB or depth views using differentiable rendering. Extensive experiments on RealEstate10K, KITTI and Flowers Light Fields show that our MINE outperforms state-of-the-art by a large margin in novel view synthesis. We also achieve competitive results in depth estimation on iBims-1 and NYU-v2 without annotated depth supervision. Our source code is available at https://github.com/vincentfung13/MINE",0,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnufAgQ3J210fFgaT8bsS7yDLaiFrCrCOK9QphFOoc0GyayyAPKTWcTW_1xULBFtQkBU
8/29/2021 21:03:41,"X-Fields: Implicit Neural View-, Light- and Time-Image Interpolation",X-Fields,10/1/2020,https://arxiv.org/pdf/2010.00450.pdf,https://xfields.mpi-inf.mpg.de/,https://github.com/m-bemana/xfields,https://www.youtube.com/watch?v=0tsw7yJGfFI,,,,"@article{bemana2020xfields,
  AUTHOR = {Mojtaba Bemana and Karol Myszkowski and Hans-Peter Seidel and Tobias Ritschel},
  TITLE = {X-Fields: Implicit Neural View-, Light- and Time-Image Interpolation},
  EPRINT = {2010.00450v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We suggest to represent an X-Field -a set of 2D images taken across differentview, time or illumination conditions, i.e., video, light field, reflectancefields or combinations thereof-by learning a neural network (NN) to map theirview, time or light coordinates to 2D images. Executing this NN at newcoordinates results in joint view, time or light interpolation. The key idea tomake this workable is a NN that already knows the ""basic tricks"" of graphics(lighting, 3D projection, occlusion) in a hard-coded and differentiable form.The NN represents the input to that rendering as an implicit map, that for anyview, time, or light coordinate and for any pixel can quantify how it will moveif view, time or light coordinates change (Jacobian of pixel position withrespect to view, time, illumination, etc.). Our X-Field representation istrained for one scene within minutes, leading to a compact set of trainableparameters and hence real-time navigation in view, time and illumination.},
  YEAR = {2020},
  MONTH = {Oct},
  URL = {http://arxiv.org/abs/2010.00450v1},
  FILE = {2010.00450v1.pdf}
 }","Dynamic, Image, Editable, Material/lighting estimation",Warping field/Flow field,,,,,,,,,,SIGGRAPH Asia 2020,https://xfields.mpi-inf.mpg.de/dataset/view_light_time.zip,,"Direct, Indirect","Mojtaba Bemana, Karol Myszkowski, Hans-Peter Seidel, Tobias Ritschel",bemana2020xfields,00000183,"We suggest to represent an X-Field -a set of 2D images taken across different view, time or illumination conditions, i.e., video, light field, reflectance fields or combinations thereof-by learning a neural network (NN) to map their view, time or light coordinates to 2D images. Executing this NN at new coordinates results in joint view, time or light interpolation. The key idea to make this workable is a NN that already knows the ""basic tricks"" of graphics (lighting, 3D projection, occlusion) in a hard-coded and differentiable form. The NN represents the input to that rendering as an implicit map, that for any view, time, or light coordinate and for any pixel can quantify how it will move if view, time or light coordinates change (Jacobian of pixel position with respect to view, time, illumination, etc.). Our X-Field representation is trained for one scene within minutes, leading to a compact set of trainable parameters and hence real-time navigation in view, time and illumination.",17,,Yes,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnuccPU3g-5A5dCmF3yCNLlEchZVbyGZBBA6rYu9cgxDtzrztMAOO3aYJyayFdS3lXA8
7/20/2021 10:58:11,General Radiance Field,GRF,10/9/2020,https://arxiv.org/pdf/2010.04595.pdf,,https://github.com/alextrevithick/GRF,,,https://drive.google.com/file/d/1H2FNeAsKoQqCsO0n7PiA1HcT1ingnwJd/view,,"@article{trevithick2020grf,
  AUTHOR = {Alex Trevithick and Bo Yang},
  TITLE = {GRF: Learning a General Radiance Field for 3D Representation andRendering},
  EPRINT = {2010.04595v3},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We present a simple yet powerful neural network that implicitly representsand renders 3D objects and scenes only from 2D observations. The network models3D geometries as a general radiance field, which takes a set of 2D images withcamera poses and intrinsics as input, constructs an internal representation foreach point of the 3D space, and then renders the corresponding appearance andgeometry of that point viewed from an arbitrary position. The key to ourapproach is to learn local features for each pixel in 2D images and to thenproject these features to 3D points, thus yielding general and rich pointrepresentations. We additionally integrate an attention mechanism to aggregatepixel features from multiple 2D views, such that visual occlusions areimplicitly taken into account. Extensive experiments demonstrate that ourmethod can generate high-quality and realistic novel views for novel objects,unseen categories and challenging real-world scenes.},
  YEAR = {2020},
  MONTH = {Oct},
  URL = {http://arxiv.org/abs/2010.04595v3},
  FILE = {2010.04595v3.pdf}
 }","Performance (training), Generalization","Lifting 2D features to 3D, Image-based rendering",,,,,,,,,,ICCV 2020,,,,"Alex Trevithick, Bo Yang",trevithick2020grf,00000035,"We present a simple yet powerful neural network that implicitly represents and renders 3D objects and scenes only from 2D observations. The network models 3D geometries as a general radiance field, which takes a set of 2D images with camera poses and intrinsics as input, constructs an internal representation for each point of the 3D space, and then renders the corresponding appearance and geometry of that point viewed from an arbitrary position. The key to our approach is to learn local features for each pixel in 2D images and to then project these features to 3D points, thus yielding general and rich point representations. We additionally integrate an attention mechanism to aggregate pixel features from multiple 2D views, such that visual occlusions are implicitly taken into account. Extensive experiments demonstrate that our method can generate high-quality and realistic novel views for novel objects, unseen categories and challenging real-world scenes.",25,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnucOa_bFYMQTE_EOvrVDpHH3FKntR_ptD7laR8Px-6vUwC04waQ2h92JoFyymeuEqIE
5/23/2021 19:04:29,NeRF++: Analyzing and Improving Neural Radiance Fields,NeRF++,10/15/2020,https://arxiv.org/pdf/2010.07492.pdf,https://github.com/Kai-46/nerfplusplus,,https://www.youtube.com/watch?v=Rd0nBO6--bM&feature=youtu.be&t=1992,,,,"@article{zhang2020nerf++,
  AUTHOR = {Kai Zhang and Gernot Riegler and Noah Snavely and Vladlen Koltun},
  TITLE = {NeRF++: Analyzing and Improving Neural Radiance Fields},
  EPRINT = {2010.07492v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Neural Radiance Fields (NeRF) achieve impressive view synthesis results for avariety of capture settings, including 360 capture of bounded scenes andforward-facing capture of bounded and unbounded scenes. NeRF fits multi-layerperceptrons (MLPs) representing view-invariant opacity and view-dependent colorvolumes to a set of training images, and samples novel views based on volumerendering techniques. In this technical report, we first remark on radiancefields and their potential ambiguities, namely the shape-radiance ambiguity,and analyze NeRF's success in avoiding such ambiguities. Second, we address aparametrization issue involved in applying NeRF to 360 captures of objectswithin large-scale, unbounded 3D scenes. Our method improves view synthesisfidelity in this challenging scenario. Code is available athttps://github.com/Kai-46/nerfplusplus.},
  YEAR = {2020},
  MONTH = {Oct},
  URL = {http://arxiv.org/abs/2010.07492v2},
  FILE = {2010.07492v2.pdf}
 }",Fundamentals,"Sampling, Volume partitioning",,,,,,,,,,ICCV,,,,"Kai Zhang, Gernot Riegler, Noah Snavely, Vladlen Koltun",zhang2020nerf++,00000036,"Neural Radiance Fields (NeRF) achieve impressive view synthesis results for a variety of capture settings, including 360 capture of bounded scenes and forward-facing capture of bounded and unbounded scenes. NeRF fits multi-layer perceptrons (MLPs) representing view-invariant opacity and view-dependent color volumes to a set of training images, and samples novel views based on volume rendering techniques. In this technical report, we first remark on radiance fields and their potential ambiguities, namely the shape-radiance ambiguity, and analyze NeRF's success in avoiding such ambiguities. Second, we address a parametrization issue involved in applying NeRF to 360 captures of objects within large-scale, unbounded 3D scenes. Our method improves view synthesis fidelity in this challenging scenario. Code is available at https://github.com/Kai-46/nerfplusplus.",48,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnueRPBNGU8eOxAjdcB5s9K0xGntKzK2_iLVjuKyLpYOYSzmcSYpdJG7N3S4WgsgPO4o
7/19/2021 21:13:04,SDF-SRN: Learning Signed Distance 3D Object Reconstruction from Static Images,SDF-SRN,10/20/2020,https://arxiv.org/pdf/2010.10505.pdf,https://chenhsuanlin.bitbucket.io/signed-distance-SRN/,https://github.com/chenhsuanlin/signed-distance-SRN,"https://chenhsuanlin.bitbucket.io/signed-distance-SRN/video.mp4, https://chenhsuanlin.bitbucket.io/signed-distance-SRN/shorttalk.mp4",https://chenhsuanlin.bitbucket.io/signed-distance-SRN/supplementary.pdf,,,"@article{lin2020sdfsrn,
  AUTHOR = {Chen-Hsuan Lin and Chaoyang Wang and Simon Lucey},
  TITLE = {SDF-SRN: Learning Signed Distance 3D Object Reconstruction from StaticImages},
  EPRINT = {2010.10505v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Dense 3D object reconstruction from a single image has recently witnessedremarkable advances, but supervising neural networks with ground-truth 3Dshapes is impractical due to the laborious process of creating pairedimage-shape datasets. Recent efforts have turned to learning 3D reconstructionwithout 3D supervision from RGB images with annotated 2D silhouettes,dramatically reducing the cost and effort of annotation. These techniques,however, remain impractical as they still require multi-view annotations of thesame object instance during training. As a result, most experimental efforts todate have been limited to synthetic datasets. In this paper, we address thisissue and propose SDF-SRN, an approach that requires only a single view ofobjects at training time, offering greater utility for real-world scenarios.SDF-SRN learns implicit 3D shape representations to handle arbitrary shapetopologies that may exist in the datasets. To this end, we derive a noveldifferentiable rendering formulation for learning signed distance functions(SDF) from 2D silhouettes. Our method outperforms the state of the art underchallenging single-view supervision settings on both synthetic and real-worlddatasets.},
  YEAR = {2020},
  MONTH = {Oct},
  URL = {http://arxiv.org/abs/2010.10505v1},
  FILE = {2010.10505v1.pdf}
 }",Generalization,"Hypernetwork, Data-driven component (pre-trained, cross-scene)",,,,,,,,,,NeurIPS 2020,,Yes,,"Chen-Hsuan Lin, Chaoyang Wang, Simon Lucey",lin2020sdfsrn,00000037,"Dense 3D object reconstruction from a single image has recently witnessed remarkable advances, but supervising neural networks with ground-truth 3D shapes is impractical due to the laborious process of creating paired image-shape datasets. Recent efforts have turned to learning 3D reconstruction without 3D supervision from RGB images with annotated 2D silhouettes, dramatically reducing the cost and effort of annotation. These techniques, however, remain impractical as they still require multi-view annotations of the same object instance during training. As a result, most experimental efforts to date have been limited to synthetic datasets. In this paper, we address this issue and propose SDF-SRN, an approach that requires only a single view of objects at training time, offering greater utility for real-world scenarios. SDF-SRN learns implicit 3D shape representations to handle arbitrary shape topologies that may exist in the datasets. To this end, we derive a novel differentiable rendering formulation for learning signed distance functions (SDF) from 2D silhouettes. Our method outperforms the state of the art under challenging single-view supervision settings on both synthetic and real-world datasets.",7,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnucIwSHgYKgEpr6gj0-2S1hvpldW_7eRVisXXF8etsuv4Pp_-o_-ytyz-YOX_LEHfZM
9/17/2021 13:19:41,"LoopReg: Self-supervised Learning of Implicit Surface Correspondences, Pose and Shape for 3D Human Mesh Registration",LoopReg,10/23/2020,https://arxiv.org/pdf/2010.12447.pdf,,https://github.com/bharat-b7/LoopReg,https://www.youtube.com/watch?v=fIhm_tWG_X8,,,,"@article{bhatnagar2020loopreg,
  AUTHOR = {Bharat Lal Bhatnagar and Cristian Sminchisescu and Christian Theobalt and Gerard Pons-Moll},
  TITLE = {LoopReg: Self-supervised Learning of Implicit Surface Correspondences,Pose and Shape for 3D Human Mesh Registration},
  EPRINT = {2010.12447v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We address the problem of fitting 3D human models to 3D scans of dressedhumans. Classical methods optimize both the data-to-model correspondences andthe human model parameters (pose and shape), but are reliable only wheninitialized close to the solution. Some methods initialize the optimizationbased on fully supervised correspondence predictors, which is notdifferentiable end-to-end, and can only process a single scan at a time. Ourmain contribution is LoopReg, an end-to-end learning framework to register acorpus of scans to a common 3D human model. The key idea is to create aself-supervised loop. A backward map, parameterized by a Neural Network,predicts the correspondence from every scan point to the surface of the humanmodel. A forward map, parameterized by a human model, transforms thecorresponding points back to the scan based on the model parameters (pose andshape), thus closing the loop. Formulating this closed loop is notstraightforward because it is not trivial to force the output of the NN to beon the surface of the human model - outside this surface the human model is noteven defined. To this end, we propose two key innovations. First, we define thecanonical surface implicitly as the zero level set of a distance field in R3,which in contrast to morecommon UV parameterizations, does not require cuttingthe surface, does not have discontinuities, and does not induce distortion.Second, we diffuse the human model to the 3D domain R3. This allows to map theNN predictions forward,even when they slightly deviate from the zero level set.Results demonstrate that we can train LoopRegmainly self-supervised - followinga supervised warm-start, the model becomes increasingly more accurate asadditional unlabelled raw scans are processed. Our code and pre-trained modelscan be downloaded for research.},
  YEAR = {2020},
  MONTH = {Oct},
  NOTE = {NeurIPS 2020},
  URL = {http://arxiv.org/abs/2010.12447v1},
  FILE = {2010.12447v1.pdf}
 }",Human body,Warping field/Flow field,,SDF,Category-level,,,,,,,NeurIPS 2020,,Yes,Direct,"Bharat Lal Bhatnagar, Cristian Sminchisescu, Christian Theobalt, Gerard Pons-Moll",bhatnagar2020loopreg,00000202,"We address the problem of fitting 3D human models to 3D scans of dressed humans. Classical methods optimize both the data-to-model correspondences and the human model parameters (pose and shape), but are reliable only when initialized close to the solution. Some methods initialize the optimization based on fully supervised correspondence predictors, which is not differentiable end-to-end, and can only process a single scan at a time. Our main contribution is LoopReg, an end-to-end learning framework to register a corpus of scans to a common 3D human model. The key idea is to create a self-supervised loop. A backward map, parameterized by a Neural Network, predicts the correspondence from every scan point to the surface of the human model. A forward map, parameterized by a human model, transforms the corresponding points back to the scan based on the model parameters (pose and shape), thus closing the loop. Formulating this closed loop is not straightforward because it is not trivial to force the output of the NN to be on the surface of the human model - outside this surface the human model is not even defined. To this end, we propose two key innovations. First, we define the canonical surface implicitly as the zero level set of a distance field in R3, which in contrast to morecommon UV parameterizations, does not require cutting the surface, does not have discontinuities, and does not induce distortion. Second, we diffuse the human model to the 3D domain R3. This allows to map the NN predictions forward,even when they slightly deviate from the zero level set. Results demonstrate that we can train LoopRegmainly self-supervised - following a supervised warm-start, the model becomes increasingly more accurate as additional unlabelled raw scans are processed. Our code and pre-trained models can be downloaded for research.",,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnufVrGAQg1RxGyxzHnga5jbNq4Sj_lodxkJiDDRjSvHd7wePUtD0L5pyZiqmG2fHmqo
6/29/2021 15:14:51,Neural Unsigned Distance Fields for Implicit Function Learning,NDF,10/26/2020,https://arxiv.org/pdf/2010.13938.pdf,http://virtualhumans.mpi-inf.mpg.de/ndf/,https://github.com/jchibane/ndf/,https://www.youtube.com/watch?v=_xsLdVzX8DY,http://virtualhumans.mpi-inf.mpg.de/papers/chibane2020ndf/chibane2020ndf-supp.pdf,,,"@article{chibane2020ndf,
  AUTHOR = {Julian Chibane and Aymen Mir and Gerard Pons-Moll},
  TITLE = {Neural Unsigned Distance Fields for Implicit Function Learning},
  EPRINT = {2010.13938v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {In this work we target a learnable output representation that allowscontinuous, high resolution outputs of arbitrary shape. Recent works represent3D surfaces implicitly with a Neural Network, thereby breaking previousbarriers in resolution, and ability to represent diverse topologies. However,neural implicit representations are limited to closed surfaces, which dividethe space into inside and outside. Many real world objects such as walls of ascene scanned by a sensor, clothing, or a car with inner structures are notclosed. This constitutes a significant barrier, in terms of data pre-processing(objects need to be artificially closed creating artifacts), and the ability tooutput open surfaces. In this work, we propose Neural Distance Fields (NDF), aneural network based model which predicts the unsigned distance field forarbitrary 3D shapes given sparse point clouds. NDF represent surfaces at highresolutions as prior implicit models, but do not require closed surface data,and significantly broaden the class of representable shapes in the output. NDFallow to extract the surface as very dense point clouds and as meshes. We alsoshow that NDF allow for surface normal calculation and can be rendered using aslight modification of sphere tracing. We find NDF can be used for multi-targetregression (multiple outputs for one input) with techniques that have beenexclusively used for rendering in graphics. Experiments on ShapeNet show thatNDF, while simple, is the state-of-the art, and allows to reconstruct shapeswith inner structures, such as the chairs inside a bus. Notably, we show thatNDF are not restricted to 3D shapes, and can approximate more general opensurfaces such as curves, manifolds, and functions. Code is available forresearch at https://virtualhumans.mpi-inf.mpg.de/ndf/.},
  YEAR = {2020},
  MONTH = {Oct},
  NOTE = {Neural Information Processing Systems (NeurIPS) 2020},
  URL = {http://arxiv.org/abs/2010.13938v1},
  FILE = {2010.13938v1.pdf}
 }",Generalization,"Data-driven component (pre-trained, cross-scene)",,Unsigned Distance Function (UDF),Category-level,,,,,,,NeurIPS 2020,,Yes,,"Julian Chibane, Aymen Mir, Gerard Pons-Moll",chibane2020ndf,00000038,"In this work we target a learnable output representation that allows continuous, high resolution outputs of arbitrary shape. Recent works represent 3D surfaces implicitly with a Neural Network, thereby breaking previous barriers in resolution, and ability to represent diverse topologies. However, neural implicit representations are limited to closed surfaces, which divide the space into inside and outside. Many real world objects such as walls of a scene scanned by a sensor, clothing, or a car with inner structures are not closed. This constitutes a significant barrier, in terms of data pre-processing (objects need to be artificially closed creating artifacts), and the ability to output open surfaces. In this work, we propose Neural Distance Fields (NDF), a neural network based model which predicts the unsigned distance field for arbitrary 3D shapes given sparse point clouds. NDF represent surfaces at high resolutions as prior implicit models, but do not require closed surface data, and significantly broaden the class of representable shapes in the output. NDF allow to extract the surface as very dense point clouds and as meshes. We also show that NDF allow for surface normal calculation and can be rendered using a slight modification of sphere tracing. We find NDF can be used for multi-target regression (multiple outputs for one input) with techniques that have been exclusively used for rendering in graphics. Experiments on ShapeNet show that NDF, while simple, is the state-of-the art, and allows to reconstruct shapes with inner structures, such as the chairs inside a bus. Notably, we show that NDF are not restricted to 3D shapes, and can approximate more general open surfaces such as curves, manifolds, and functions. Code is available for research at https://virtualhumans.mpi-inf.mpg.de/ndf/.",17,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnucGgxZRd_xuEyVO3AkPDoIVCq_sdhhnkuHyiqozLmwJGCBg6I28noFTff07alAiF98
9/17/2021 11:30:00,Neural Light Field 3D Printing,,11/1/2020,https://dl.acm.org/doi/pdf/10.1145/3414685.3417879,https://quan-zheng.github.io/publication/neuralLF3Dprinting20/,Coming soon,,https://quan-zheng.github.io/publication/NeuralLightField3DPrinting-supp.pdf,,,"@article{zheng2020neural,
  ABSTRACT = {Modern 3D printers are capable of printing large-size light-field displays at high-resolutions. However, optimizing such displays in full 3D volume for a given light-field imagery is still a challenging task. Existing light field displays optimize over relatively small resolutions using},
  AUTHOR = {Zheng, Quan and Babaei, Vahid and Wetzstein, Gordon and Seidel, Hans-Peter and Zwicker, Matthias and Singh, Gurprit},
  JOURNAL = {ACM Transactions on Graphics (TOG)},
  NUMBER = {6},
  PAGES = {1--12},
  PUB_YEAR = {2020},
  PUBLISHER = {ACM New York, NY, USA},
  TITLE = {Neural light field 3D printing},
  VENUE = {ACM Transactions on ...},
  VOLUME = {39}
 }","Beyond graphics, Alternative imaging, Science and engineering",,NeRF,,,,,,,,,,https://drive.google.com/uc?id=1EGwwQsAlw4C1IS6jgK_4P9uP0Rf4ICug&export=download,,Direct,"Quan Zheng, Vahid Babaei, Gordon Wetzstein, Hans-Peter Seidel, Matthias Zwicker, Gurprit Singh",zheng2020neural,00000193,"Modern 3D printers are capable of printing large-size light-field displays at high-resolutions. However, optimizing such displays in full 3D volume for a given light-field imagery is still a challenging task. Existing light field displays optimize over relatively small resolutions using",,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnufwmW8XMLbhSzPw_3RyvwVEKgnUV6qSz3zhBiM5SIBd60ZkRzzT7J82ycJwGY5H4b0
6/21/2021 16:40:25,Neural Scene Graphs for Dynamic Scenes,,11/20/2020,https://arxiv.org/pdf/2011.10379.pdf,https://light.princeton.edu/publication/neural-scene-graphs/,,https://www.youtube.com/watch?v=ea4Y6P0Hk3o,https://light.cs.princeton.edu/wp-content/uploads/2021/02/NeuralSceneGraphs_Supplement.pdf,,,"@article{ost2020neural,
  AUTHOR = {Julian Ost and Fahim Mannan and Nils Thuerey and Julian Knodt and Felix Heide},
  TITLE = {Neural Scene Graphs for Dynamic Scenes},
  EPRINT = {2011.10379v3},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Recent implicit neural rendering methods have demonstrated that it ispossible to learn accurate view synthesis for complex scenes by predictingtheir volumetric density and color supervised solely by a set of RGB images.However, existing methods are restricted to learning efficient representationsof static scenes that encode all scene objects into a single neural network,and lack the ability to represent dynamic scenes and decompositions intoindividual scene objects. In this work, we present the first neural renderingmethod that decomposes dynamic scenes into scene graphs. We propose a learnedscene graph representation, which encodes object transformation and radiance,to efficiently render novel arrangements and views of the scene. To this end,we learn implicitly encoded scenes, combined with a jointly learned latentrepresentation to describe objects with a single implicit function. We assessthe proposed method on synthetic and real automotive data, validating that ourapproach learns dynamic scenes -- only by observing a video of this scene --and allows for rendering novel photo-realistic views of novel scenecompositions with unseen sets of objects at unseen poses.},
  YEAR = {2020},
  MONTH = {Nov},
  URL = {http://arxiv.org/abs/2011.10379v3},
  FILE = {2011.10379v3.pdf}
 }","Dynamic, Segmentation/composition, Beyond graphics","Conditional neural field, Object-centric representation",,,,,,,,,,NeurIPS,,,,"Julian Ost, Fahim Mannan, Nils Thuerey, Julian Knodt, Felix Heide",ost2020neural,00000039,"Recent implicit neural rendering methods have demonstrated that it is possible to learn accurate view synthesis for complex scenes by predicting their volumetric density and color supervised solely by a set of RGB images. However, existing methods are restricted to learning efficient representations of static scenes that encode all scene objects into a single neural network, and lack the ability to represent dynamic scenes and decompositions into individual scene objects. In this work, we present the first neural rendering method that decomposes dynamic scenes into scene graphs. We propose a learned scene graph representation, which encodes object transformation and radiance, to efficiently render novel arrangements and views of the scene. To this end, we learn implicitly encoded scenes, combined with a jointly learned latent representation to describe objects with a single implicit function. We assess the proposed method on synthetic and real automotive data, validating that our approach learns dynamic scenes -- only by observing a video of this scene -- and allows for rendering novel photo-realistic views of novel scene compositions with unseen sets of objects at unseen poses.",12,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnueDY6CdA1obXGcDsBFUpFnvkNSn2p5iSxY-l-aLPcKDgYGRMw0km20_bxneOI_dx6M
5/23/2021 19:03:39,GIRAFFE: Representing Scenes as Compositional Generative Neural Feature Fields,GIRAFFE,11/24/2020,https://arxiv.org/pdf/2011.12100.pdf,,,,,,,"@article{niemeyer2020giraffe,
  AUTHOR = {Michael Niemeyer and Andreas Geiger},
  TITLE = {GIRAFFE: Representing Scenes as Compositional Generative Neural FeatureFields},
  EPRINT = {2011.12100v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Deep generative models allow for photorealistic image synthesis at highresolutions. But for many applications, this is not enough: content creationalso needs to be controllable. While several recent works investigate how todisentangle underlying factors of variation in the data, most of them operatein 2D and hence ignore that our world is three-dimensional. Further, only fewworks consider the compositional nature of scenes. Our key hypothesis is thatincorporating a compositional 3D scene representation into the generative modelleads to more controllable image synthesis. Representing scenes ascompositional generative neural feature fields allows us to disentangle one ormultiple objects from the background as well as individual objects' shapes andappearances while learning from unstructured and unposed image collectionswithout any additional supervision. Combining this scene representation with aneural rendering pipeline yields a fast and realistic image synthesis model. Asevidenced by our experiments, our model is able to disentangle individualobjects and allows for translating and rotating them in the scene as well aschanging the camera pose.},
  YEAR = {2020},
  MONTH = {Nov},
  URL = {http://arxiv.org/abs/2011.12100v2},
  FILE = {2011.12100v2.pdf}
 }",Segmentation/composition,"Generative/adversarial formulation, Conditional neural field",,,,,,,,,,CVPR 2020,,,,"Michael Niemeyer, Andreas Geiger",niemeyer2020giraffe,00000040,"Deep generative models allow for photorealistic image synthesis at high resolutions. But for many applications, this is not enough: content creation also needs to be controllable. While several recent works investigate how to disentangle underlying factors of variation in the data, most of them operate in 2D and hence ignore that our world is three-dimensional. Further, only few works consider the compositional nature of scenes. Our key hypothesis is that incorporating a compositional 3D scene representation into the generative model leads to more controllable image synthesis. Representing scenes as compositional generative neural feature fields allows us to disentangle one or multiple objects from the background as well as individual objects' shapes and appearances while learning from unstructured and unposed image collections without any additional supervision. Combining this scene representation with a neural rendering pipeline yields a fast and realistic image synthesis model. As evidenced by our experiments, our model is able to disentangle individual objects and allows for translating and rotating them in the scene as well as changing the camera pose.",21,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnueLIEBO3FB_EEaJMtTpVBpDxhilnA8FYjDmF_0sna1bMhb_4HoqnWTikgNd_O9UeXw
8/29/2021 20:43:12,Adversarial Generation of Continuous Images,INR-GAN,11/24/2020,https://arxiv.org/pdf/2011.12026.pdf,https://universome.github.io/inr-gan,https://github.com/universome/inr-gan,,,,,"@article{skorokhodov2020inrgan,
  AUTHOR = {Ivan Skorokhodov and Savva Ignatyev and Mohamed Elhoseiny},
  TITLE = {Adversarial Generation of Continuous Images},
  EPRINT = {2011.12026v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {In most existing learning systems, images are typically viewed as 2D pixelarrays. However, in another paradigm gaining popularity, a 2D image isrepresented as an implicit neural representation (INR) - an MLP that predictsan RGB pixel value given its (x,y) coordinate. In this paper, we propose twonovel architectural techniques for building INR-based image decoders:factorized multiplicative modulation and multi-scale INRs, and use them tobuild a state-of-the-art continuous image GAN. Previous attempts to adapt INRsfor image generation were limited to MNIST-like datasets and do not scale tocomplex real-world data. Our proposed INR-GAN architecture improves theperformance of continuous image generators by several times, greatly reducingthe gap between continuous image GANs and pixel-based ones. Apart from that, weexplore several exciting properties of the INR-based decoders, likeout-of-the-box superresolution, meaningful image-space interpolation,accelerated inference of low-resolution images, an ability to extrapolateoutside of image boundaries, and strong geometric prior. The project page islocated at https://universome.github.io/inr-gan.},
  YEAR = {2020},
  MONTH = {Nov},
  URL = {http://arxiv.org/abs/2011.12026v2},
  FILE = {2011.12026v2.pdf}
 }","Generalization, Image","Generative/adversarial formulation, Conditional neural field, Hypernetwork",,,Category-level,,,,,,,CVPR 2021,,,Direct,"Ivan Skorokhodov, Savva Ignatyev, Mohamed Elhoseiny",skorokhodov2020inrgan,00000180,"In most existing learning systems, images are typically viewed as 2D pixel arrays. However, in another paradigm gaining popularity, a 2D image is represented as an implicit neural representation (INR) - an MLP that predicts an RGB pixel value given its (x,y) coordinate. In this paper, we propose two novel architectural techniques for building INR-based image decoders: factorized multiplicative modulation and multi-scale INRs, and use them to build a state-of-the-art continuous image GAN. Previous attempts to adapt INRs for image generation were limited to MNIST-like datasets and do not scale to complex real-world data. Our proposed INR-GAN architecture improves the performance of continuous image generators by several times, greatly reducing the gap between continuous image GANs and pixel-based ones. Apart from that, we explore several exciting properties of the INR-based decoders, like out-of-the-box superresolution, meaningful image-space interpolation, accelerated inference of low-resolution images, an ability to extrapolate outside of image boundaries, and strong geometric prior. The project page is located at https://universome.github.io/inr-gan.",7,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnuf2_8WKa5v6-WXchVkn0Zj0mRAQTae6nc2psudf6j6EquIh89vcF5ejQmS3RImNG-k
5/23/2021 19:01:49,D-NeRF: Deformable Neural Radiance Fields,"D-NeRF, Nerfies",11/25/2020,https://arxiv.org/pdf/2011.12948.pdf,https://nerfies.github.io/,,https://www.youtube.com/watch?v=MrKrnHhk8IA,,,,"@article{park2020dnerf,
  AUTHOR = {Keunhong Park and Utkarsh Sinha and Jonathan T. Barron and Sofien Bouaziz and Dan B Goldman and Steven M. Seitz and Ricardo Martin-Brualla},
  TITLE = {Nerfies: Deformable Neural Radiance Fields},
  EPRINT = {2011.12948v4},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We present the first method capable of photorealistically reconstructingdeformable scenes using photos/videos captured casually from mobile phones. Ourapproach augments neural radiance fields (NeRF) by optimizing an additionalcontinuous volumetric deformation field that warps each observed point into acanonical 5D NeRF. We observe that these NeRF-like deformation fields are proneto local minima, and propose a coarse-to-fine optimization method forcoordinate-based models that allows for more robust optimization. By adaptingprinciples from geometry processing and physical simulation to NeRF-likemodels, we propose an elastic regularization of the deformation field thatfurther improves robustness. We show that our method can turn casually capturedselfie photos/videos into deformable NeRF models that allow for photorealisticrenderings of the subject from arbitrary viewpoints, which we dub ""nerfies."" Weevaluate our method by collecting time-synchronized data using a rig with twomobile phones, yielding train/validation images of the same pose at differentviewpoints. We show that our method faithfully reconstructs non-rigidlydeforming scenes and reproduces unseen views with high fidelity.},
  YEAR = {2020},
  MONTH = {Nov},
  URL = {http://arxiv.org/abs/2011.12948v4},
  FILE = {2011.12948v4.pdf}
 }",Dynamic,"Conditional neural field, Coarse-to-fine, Warping field/Flow field",,,,,,,,,,CVPR,,,,"Keunhong Park, Utkarsh Sinha, Jonathan T. Barron, Sofien Bouaziz, Dan B Goldman, Steven M. Seitz, Ricardo Martin-Brualla",park2020dnerf,00000041,"We present the first method capable of photorealistically reconstructing deformable scenes using photos/videos captured casually from mobile phones. Our approach augments neural radiance fields (NeRF) by optimizing an additional continuous volumetric deformation field that warps each observed point into a canonical 5D NeRF. We observe that these NeRF-like deformation fields are prone to local minima, and propose a coarse-to-fine optimization method for coordinate-based models that allows for more robust optimization. By adapting principles from geometry processing and physical simulation to NeRF-like models, we propose an elastic regularization of the deformation field that further improves robustness. We show that our method can turn casually captured selfie photos/videos into deformable NeRF models that allow for photorealistic renderings of the subject from arbitrary viewpoints, which we dub ""nerfies."" We evaluate our method by collecting time-synchronized data using a rig with two mobile phones, yielding train/validation images of the same pose at different viewpoints. We show that our method faithfully reconstructs non-rigidly deforming scenes and reproduces unseen views with high fidelity.",54,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnueO-6r__SJZlq6de28PIi-QXdZ_pGQjs2KrXCswr04O3TB7NJXPXEIBWE8oMy7SyA4
5/23/2021 19:01:57,DeRF: Decomposed Radiance Fields,DeRF,11/25/2020,https://arxiv.org/pdf/2011.12490.pdf,,,,,,,"@article{rebain2020derf,
  AUTHOR = {Daniel Rebain and Wei Jiang and Soroosh Yazdani and Ke Li and Kwang Moo Yi and Andrea Tagliasacchi},
  TITLE = {DeRF: Decomposed Radiance Fields},
  EPRINT = {2011.12490v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {With the advent of Neural Radiance Fields (NeRF), neural networks can nowrender novel views of a 3D scene with quality that fools the human eye. Yet,generating these images is very computationally intensive, limiting theirapplicability in practical scenarios. In this paper, we propose a techniquebased on spatial decomposition capable of mitigating this issue. Our keyobservation is that there are diminishing returns in employing larger (deeperand/or wider) networks. Hence, we propose to spatially decompose a scene anddedicate smaller networks for each decomposed part. When working together,these networks can render the whole scene. This allows us near-constantinference time regardless of the number of decomposed parts. Moreover, we showthat a Voronoi spatial decomposition is preferable for this purpose, as it isprovably compatible with the Painter's Algorithm for efficient and GPU-friendlyrendering. Our experiments show that for real-world scenes, our method providesup to 3x more efficient inference than NeRF (with the same rendering quality),or an improvement of up to 1.0~dB in PSNR (for the same inference cost).},
  YEAR = {2020},
  MONTH = {Nov},
  URL = {http://arxiv.org/abs/2011.12490v1},
  FILE = {2011.12490v1.pdf}
 }","Performance (training), Performance (rendering)",Volume partitioning,,,,,,,,,,,,,,"Daniel Rebain, Wei Jiang, Soroosh Yazdani, Ke Li, Kwang Moo Yi, Andrea Tagliasacchi",rebain2020derf,00000042,"With the advent of Neural Radiance Fields (NeRF), neural networks can now render novel views of a 3D scene with quality that fools the human eye. Yet, generating these images is very computationally intensive, limiting their applicability in practical scenarios. In this paper, we propose a technique based on spatial decomposition capable of mitigating this issue. Our key observation is that there are diminishing returns in employing larger (deeper and/or wider) networks. Hence, we propose to spatially decompose a scene and dedicate smaller networks for each decomposed part. When working together, these networks can render the whole scene. This allows us near-constant inference time regardless of the number of decomposed parts. Moreover, we show that a Voronoi spatial decomposition is preferable for this purpose, as it is provably compatible with the Painter's Algorithm for efficient and GPU-friendly rendering. Our experiments show that for real-world scenes, our method provides up to 3x more efficient inference than NeRF (with the same rendering quality), or an improvement of up to 1.0~dB in PSNR (for the same inference cost).",17,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnuc3o4WN1ezG9eCuYjPCyGWLQPweh423-2doTR3rfpw4S8Zsh7lK9y1HlEj2qjfhyEE
5/23/2021 19:02:40,Space-time Neural Irradiance Fields for Free-Viewpoint Video,,11/25/2020,https://arxiv.org/pdf/2011.12950.pdf,https://video-nerf.github.io/,,https://www.youtube.com/watch?v=2tN8ghNu2sI,,,,"@article{xian2020spacetime,
  AUTHOR = {Wenqi Xian and Jia-Bin Huang and Johannes Kopf and Changil Kim},
  TITLE = {Space-time Neural Irradiance Fields for Free-Viewpoint Video},
  EPRINT = {2011.12950v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We present a method that learns a spatiotemporal neural irradiance field fordynamic scenes from a single video. Our learned representation enablesfree-viewpoint rendering of the input video. Our method builds upon recentadvances in implicit representations. Learning a spatiotemporal irradiancefield from a single video poses significant challenges because the videocontains only one observation of the scene at any point in time. The 3Dgeometry of a scene can be legitimately represented in numerous ways sincevarying geometry (motion) can be explained with varying appearance and viceversa. We address this ambiguity by constraining the time-varying geometry ofour dynamic scene representation using the scene depth estimated from videodepth estimation methods, aggregating contents from individual frames into asingle global representation. We provide an extensive quantitative evaluationand demonstrate compelling free-viewpoint rendering results.},
  YEAR = {2020},
  MONTH = {Nov},
  URL = {http://arxiv.org/abs/2011.12950v2},
  FILE = {2011.12950v2.pdf}
 }",Dynamic,,,,,,,,,,,CVPR,,,,"Wenqi Xian, Jia-Bin Huang, Johannes Kopf, Changil Kim",xian2020spacetime,00000043,"We present a method that learns a spatiotemporal neural irradiance field for dynamic scenes from a single video. Our learned representation enables free-viewpoint rendering of the input video. Our method builds upon recent advances in implicit representations. Learning a spatiotemporal irradiance field from a single video poses significant challenges because the video contains only one observation of the scene at any point in time. The 3D geometry of a scene can be legitimately represented in numerous ways since varying geometry (motion) can be explained with varying appearance and vice versa. We address this ambiguity by constraining the time-varying geometry of our dynamic scene representation using the scene depth estimated from video depth estimation methods, aggregating contents from individual frames into a single global representation. We provide an extensive quantitative evaluation and demonstrate compelling free-viewpoint rendering results.",28,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnudmMg_ZIx_zjsnRPyrMPTzRgLTyz7O4bT04jFdpoxu0b-bXbm4sl8sW39187Oyk5A8
5/23/2021 19:03:19,Neural Scene Flow Fields for Space-Time View Synthesis of Dynamic Scenes,NSFF,11/26/2020,https://arxiv.org/pdf/2011.13084.pdf,http://www.cs.cornell.edu/~zl548/NSFF/,https://github.com/zhengqili/Neural-Scene-Flow-Fields,,https://www.cs.cornell.edu/~zl548/NSFF/NSFF_supp.pdf,,,"@article{li2020nsff,
  AUTHOR = {Zhengqi Li and Simon Niklaus and Noah Snavely and Oliver Wang},
  TITLE = {Neural Scene Flow Fields for Space-Time View Synthesis of Dynamic Scenes},
  EPRINT = {2011.13084v3},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We present a method to perform novel view and time synthesis of dynamicscenes, requiring only a monocular video with known camera poses as input. Todo this, we introduce Neural Scene Flow Fields, a new representation thatmodels the dynamic scene as a time-variant continuous function of appearance,geometry, and 3D scene motion. Our representation is optimized through a neuralnetwork to fit the observed input views. We show that our representation can beused for complex dynamic scenes, including thin structures, view-dependenteffects, and natural degrees of motion. We conduct a number of experiments thatdemonstrate our approach significantly outperforms recent monocular viewsynthesis methods, and show qualitative results of space-time view synthesis ona variety of real-world videos.},
  YEAR = {2020},
  MONTH = {Nov},
  URL = {http://arxiv.org/abs/2011.13084v3},
  FILE = {2011.13084v3.pdf}
 }","Dynamic, Segmentation/composition",Warping field/Flow field,,,,,,,,,,CVPR 2020,,,,"Zhengqi Li, Simon Niklaus, Noah Snavely, Oliver Wang",li2020nsff,00000044,"We present a method to perform novel view and time synthesis of dynamic scenes, requiring only a monocular video with known camera poses as input. To do this, we introduce Neural Scene Flow Fields, a new representation that models the dynamic scene as a time-variant continuous function of appearance, geometry, and 3D scene motion. Our representation is optimized through a neural network to fit the observed input views. We show that our representation can be used for complex dynamic scenes, including thin structures, view-dependent effects, and natural degrees of motion. We conduct a number of experiments that demonstrate our approach significantly outperforms recent monocular view synthesis methods, and show qualitative results of space-time view synthesis on a variety of real-world videos.",36,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnufqLswBohA95lyfpF4fsbT6UR-RDvHjxDiaWwlnv27_kE9OmY5GFnDysubK2CBZllA
5/23/2021 18:16:12,D-NeRF: Neural Radiance Fields for Dynamic Scenes,D-NeRF,11/27/2020,https://arxiv.org/pdf/2011.13961.pdf,https://www.albertpumarola.com/research/D-NeRF/index.html,https://github.com/albertpumarola/D-NeRF,https://www.youtube.com/watch?v=lSgzmgi2JPw,,,,"@article{pumarola2020dnerf,
  AUTHOR = {Albert Pumarola and Enric Corona and Gerard Pons-Moll and Francesc Moreno-Noguer},
  TITLE = {D-NeRF: Neural Radiance Fields for Dynamic Scenes},
  EPRINT = {2011.13961v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Neural rendering techniques combining machine learning with geometricreasoning have arisen as one of the most promising approaches for synthesizingnovel views of a scene from a sparse set of images. Among these, stands out theNeural radiance fields (NeRF), which trains a deep network to map 5D inputcoordinates (representing spatial location and viewing direction) into a volumedensity and view-dependent emitted radiance. However, despite achieving anunprecedented level of photorealism on the generated images, NeRF is onlyapplicable to static scenes, where the same spatial location can be queriedfrom different images. In this paper we introduce D-NeRF, a method that extendsneural radiance fields to a dynamic domain, allowing to reconstruct and rendernovel images of objects under rigid and non-rigid motions from a \emph{single}camera moving around the scene. For this purpose we consider time as anadditional input to the system, and split the learning process in two mainstages: one that encodes the scene into a canonical space and another that mapsthis canonical representation into the deformed scene at a particular time.Both mappings are simultaneously learned using fully-connected networks. Oncethe networks are trained, D-NeRF can render novel images, controlling both thecamera view and the time variable, and thus, the object movement. Wedemonstrate the effectiveness of our approach on scenes with objects underrigid, articulated and non-rigid motions. Code, model weights and the dynamicscenes dataset will be released.},
  YEAR = {2020},
  MONTH = {Nov},
  URL = {http://arxiv.org/abs/2011.13961v1},
  FILE = {2011.13961v1.pdf}
 }",Dynamic,Warping field/Flow field,,,,,,,,,,,https://www.dropbox.com/s/0bf6fl0ye2vz3vr/data.zip?dl=0,,,"Albert Pumarola, Enric Corona, Gerard Pons-Moll, Francesc Moreno-Noguer",pumarola2020dnerf,00000045,"Neural rendering techniques combining machine learning with geometric reasoning have arisen as one of the most promising approaches for synthesizing novel views of a scene from a sparse set of images. Among these, stands out the Neural radiance fields (NeRF), which trains a deep network to map 5D input coordinates (representing spatial location and viewing direction) into a volume density and view-dependent emitted radiance. However, despite achieving an unprecedented level of photorealism on the generated images, NeRF is only applicable to static scenes, where the same spatial location can be queried from different images. In this paper we introduce D-NeRF, a method that extends neural radiance fields to a dynamic domain, allowing to reconstruct and render novel images of objects under rigid and non-rigid motions from a \emph{single} camera moving around the scene. For this purpose we consider time as an additional input to the system, and split the learning process in two main stages: one that encodes the scene into a canonical space and another that maps this canonical representation into the deformed scene at a particular time. Both mappings are simultaneously learned using fully-connected networks. Once the networks are trained, D-NeRF can render novel images, controlling both the camera view and the time variable, and thus, the object movement. We demonstrate the effectiveness of our approach on scenes with objects under rigid, articulated and non-rigid motions. Code, model weights and the dynamic scenes dataset will be released.",41,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnufYI3q44iMdRLOHegTcugFMZqjR11Kom80IGPMi2QchtidKU3aQDVh1Q72yYO2MqBc
8/29/2021 20:48:22,Image Generators with Conditionally-Independent Pixel Synthesis,CIPS,11/27/2020,https://arxiv.org/pdf/2011.13775.pdf,,https://github.com/saic-mdal/CIPS,,,,,"@article{anokhin2020cips,
  AUTHOR = {Ivan Anokhin and Kirill Demochkin and Taras Khakhulin and Gleb Sterkin and Victor Lempitsky and Denis Korzhenkov},
  TITLE = {Image Generators with Conditionally-Independent Pixel Synthesis},
  EPRINT = {2011.13775v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Existing image generator networks rely heavily on spatial convolutions and,optionally, self-attention blocks in order to gradually synthesize images in acoarse-to-fine manner. Here, we present a new architecture for imagegenerators, where the color value at each pixel is computed independently giventhe value of a random latent vector and the coordinate of that pixel. Nospatial convolutions or similar operations that propagate information acrosspixels are involved during the synthesis. We analyze the modeling capabilitiesof such generators when trained in an adversarial fashion, and observe the newgenerators to achieve similar generation quality to state-of-the-artconvolutional generators. We also investigate several interesting propertiesunique to the new architecture.},
  YEAR = {2020},
  MONTH = {Nov},
  URL = {http://arxiv.org/abs/2011.13775v1},
  FILE = {2011.13775v1.pdf}
 }",Image,,,,,,,,,,,CVPR 2021,,,,"Ivan Anokhin, Kirill Demochkin, Taras Khakhulin, Gleb Sterkin, Victor Lempitsky, Denis Korzhenkov",anokhin2020cips,00000181,"Existing image generator networks rely heavily on spatial convolutions and, optionally, self-attention blocks in order to gradually synthesize images in a coarse-to-fine manner. Here, we present a new architecture for image generators, where the color value at each pixel is computed independently given the value of a random latent vector and the coordinate of that pixel. No spatial convolutions or similar operations that propagate information across pixels are involved during the synthesis. We analyze the modeling capabilities of such generators when trained in an adversarial fashion, and observe the new generators to achieve similar generation quality to state-of-the-art convolutional generators. We also investigate several interesting properties unique to the new architecture.",11,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnudFI6_Lm4DjzDGJotDsCvFC_BSWZ0a9MTimrNRNckUQ6YhC4OVKnU3KdTS34OXLnTM
7/19/2021 21:22:17,i3DMM: Deep Implicit 3D Morphable Model of Human Heads,i3DMM,11/28/2020,https://arxiv.org/pdf/2011.14143.pdf,,Coming soon,https://www.youtube.com/watch?v=4pYzV3ButPY,,,,"@article{yenamandra2020i3dmm,
  AUTHOR = {Tarun Yenamandra and Ayush Tewari and Florian Bernard and Hans-Peter Seidel and Mohamed Elgharib and Daniel Cremers and Christian Theobalt},
  TITLE = {i3DMM: Deep Implicit 3D Morphable Model of Human Heads},
  EPRINT = {2011.14143v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We present the first deep implicit 3D morphable model (i3DMM) of full heads.Unlike earlier morphable face models it not only captures identity-specificgeometry, texture, and expressions of the frontal face, but also models theentire head, including hair. We collect a new dataset consisting of 64 peoplewith different expressions and hairstyles to train i3DMM. Our approach has thefollowing favorable properties: (i) It is the first full head morphable modelthat includes hair. (ii) In contrast to mesh-based models it can be trained onmerely rigidly aligned scans, without requiring difficult non-rigidregistration. (iii) We design a novel architecture to decouple the shape modelinto an implicit reference shape and a deformation of this reference shape.With that, dense correspondences between shapes can be learned implicitly. (iv)This architecture allows us to semantically disentangle the geometry and colorcomponents, as color is learned in the reference space. Geometry is furtherdisentangled as identity, expressions, and hairstyle, while color isdisentangled as identity and hairstyle components. We show the merits of i3DMMusing ablation studies, comparisons to state-of-the-art models, andapplications such as semantic head editing and texture transfer. We will makeour model publicly available.},
  YEAR = {2020},
  MONTH = {Nov},
  URL = {http://arxiv.org/abs/2011.14143v1},
  FILE = {2011.14143v1.pdf}
 }","Dynamic, Human head, Editable","Conditional neural field, Warping field/Flow field",,SDF,,,,,,,,CVPR,,,,"Tarun Yenamandra, Ayush Tewari, Florian Bernard, Hans-Peter Seidel, Mohamed Elgharib, Daniel Cremers, Christian Theobalt",yenamandra2020i3dmm,00000046,"We present the first deep implicit 3D morphable model (i3DMM) of full heads. Unlike earlier morphable face models it not only captures identity-specific geometry, texture, and expressions of the frontal face, but also models the entire head, including hair. We collect a new dataset consisting of 64 people with different expressions and hairstyles to train i3DMM. Our approach has the following favorable properties: (i) It is the first full head morphable model that includes hair. (ii) In contrast to mesh-based models it can be trained on merely rigidly aligned scans, without requiring difficult non-rigid registration. (iii) We design a novel architecture to decouple the shape model into an implicit reference shape and a deformation of this reference shape. With that, dense correspondences between shapes can be learned implicitly. (iv) This architecture allows us to semantically disentangle the geometry and color components, as color is learned in the reference space. Geometry is further disentangled as identity, expressions, and hairstyle, while color is disentangled as identity and hairstyle components. We show the merits of i3DMM using ablation studies, comparisons to state-of-the-art models, and applications such as semantic head editing and texture transfer. We will make our model publicly available.",3,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnue_Ty5nq9CoDkFmExpeuQlv_RUbnaZlIUEl-CuvEtHHR-QLB-OYCHYOiFMFjxEbwrA
7/19/2021 22:05:43,Neural Deformation Graphs for Globally-consistent Non-rigid Reconstruction,Neural Deformation Graphs,12/2/2020,https://arxiv.org/pdf/2012.01451.pdf,,,,,,,"@article{bozic2020neural,
  AUTHOR = {Aljaz Bozic and Pablo Palafox and Michael Zollhofer and Justus Thies and Angela Dai and Matthias Niessner},
  TITLE = {Neural Deformation Graphs for Globally-consistent Non-rigidReconstruction},
  EPRINT = {2012.01451v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We introduce Neural Deformation Graphs for globally-consistent deformationtracking and 3D reconstruction of non-rigid objects. Specifically, weimplicitly model a deformation graph via a deep neural network. This neuraldeformation graph does not rely on any object-specific structure and, thus, canbe applied to general non-rigid deformation tracking. Our method globallyoptimizes this neural graph on a given sequence of depth camera observations ofa non-rigidly moving object. Based on explicit viewpoint consistency as well asinter-frame graph and surface consistency constraints, the underlying networkis trained in a self-supervised fashion. We additionally optimize for thegeometry of the object with an implicit deformable multi-MLP shaperepresentation. Our approach does not assume sequential input data, thusenabling robust tracking of fast motions or even temporally disconnectedrecordings. Our experiments demonstrate that our Neural Deformation Graphsoutperform state-of-the-art non-rigid reconstruction approaches bothqualitatively and quantitatively, with 64% improved reconstruction and 62%improved deformation tracking performance.},
  YEAR = {2020},
  MONTH = {Dec},
  URL = {http://arxiv.org/abs/2012.01451v1},
  FILE = {2012.01451v1.pdf}
 }","Dynamic, Human body, Beyond graphics","Volume partitioning, Warping field/Flow field",,,,,,,,,,CVPR,,,,"Aljaž Božič, Pablo Palafox, Michael Zollhöfer, Justus Thies, Angela Dai, Matthias Nießner",bozic2020neural,00000047,"We introduce Neural Deformation Graphs for globally-consistent deformation tracking and 3D reconstruction of non-rigid objects. Specifically, we implicitly model a deformation graph via a deep neural network. This neural deformation graph does not rely on any object-specific structure and, thus, can be applied to general non-rigid deformation tracking. Our method globally optimizes this neural graph on a given sequence of depth camera observations of a non-rigidly moving object. Based on explicit viewpoint consistency as well as inter-frame graph and surface consistency constraints, the underlying network is trained in a self-supervised fashion. We additionally optimize for the geometry of the object with an implicit deformable multi-MLP shape representation. Our approach does not assume sequential input data, thus enabling robust tracking of fast motions or even temporally disconnected recordings. Our experiments demonstrate that our Neural Deformation Graphs outperform state-of-the-art non-rigid reconstruction approaches both qualitatively and quantitatively, with 64% improved reconstruction and 62% improved deformation tracking performance.",1,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnufiPk4A-CGDfPmHevzPgzhLtLiMRC4g1XokZcNMIl7z6n_-Omv1Wy6LlRreDEkALtk
5/23/2021 18:50:19,pixelNeRF: Neural Radiance Fields from One or Few Images,pixelNeRF,12/3/2020,https://arxiv.org/pdf/2012.02190.pdf,https://alexyu.net/pixelnerf/,https://github.com/sxyu/pixel-nerf,https://www.youtube.com/watch?v=voebZx7f32g,,,,"@article{yu2020pixelnerf,
  AUTHOR = {Alex Yu and Vickie Ye and Matthew Tancik and Angjoo Kanazawa},
  TITLE = {pixelNeRF: Neural Radiance Fields from One or Few Images},
  EPRINT = {2012.02190v3},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We propose pixelNeRF, a learning framework that predicts a continuous neuralscene representation conditioned on one or few input images. The existingapproach for constructing neural radiance fields involves optimizing therepresentation to every scene independently, requiring many calibrated viewsand significant compute time. We take a step towards resolving theseshortcomings by introducing an architecture that conditions a NeRF on imageinputs in a fully convolutional manner. This allows the network to be trainedacross multiple scenes to learn a scene prior, enabling it to perform novelview synthesis in a feed-forward manner from a sparse set of views (as few asone). Leveraging the volume rendering approach of NeRF, our model can betrained directly from images with no explicit 3D supervision. We conductextensive experiments on ShapeNet benchmarks for single image novel viewsynthesis tasks with held-out objects as well as entire unseen categories. Wefurther demonstrate the flexibility of pixelNeRF by demonstrating it onmulti-object ShapeNet scenes and real scenes from the DTU dataset. In allcases, pixelNeRF outperforms current state-of-the-art baselines for novel viewsynthesis and single image 3D reconstruction. For the video and code, pleasevisit the project website: https://alexyu.net/pixelnerf},
  YEAR = {2020},
  MONTH = {Dec},
  URL = {http://arxiv.org/abs/2012.02190v3},
  FILE = {2012.02190v3.pdf}
 }","Few-shot reconstruction, Generalization","Lifting 2D features to 3D, Data-driven component (pre-trained, cross-scene)",,,,,,,,,,CVPR 2020,https://drive.google.com/drive/folders/1PsT3uKwqHHD2bEEHkIXB99AlIjtmrEiR,,,"Alex Yu, Vickie Ye, Matthew Tancik, Angjoo Kanazawa",yu2020pixelnerf,00000048,"We propose pixelNeRF, a learning framework that predicts a continuous neural scene representation conditioned on one or few input images. The existing approach for constructing neural radiance fields involves optimizing the representation to every scene independently, requiring many calibrated views and significant compute time. We take a step towards resolving these shortcomings by introducing an architecture that conditions a NeRF on image inputs in a fully convolutional manner. This allows the network to be trained across multiple scenes to learn a scene prior, enabling it to perform novel view synthesis in a feed-forward manner from a sparse set of views (as few as one). Leveraging the volume rendering approach of NeRF, our model can be trained directly from images with no explicit 3D supervision. We conduct extensive experiments on ShapeNet benchmarks for single image novel view synthesis tasks with held-out objects as well as entire unseen categories. We further demonstrate the flexibility of pixelNeRF by demonstrating it on multi-object ShapeNet scenes and real scenes from the DTU dataset. In all cases, pixelNeRF outperforms current state-of-the-art baselines for novel view synthesis and single image 3D reconstruction. For the video and code, please visit the project website: https://alexyu.net/pixelnerf",37,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnudFp3lsu5-HcXDDnsV-5NmuOTQhfUSFraiTfKXctt_UiFb06pZKOk2rypLo1Cwkle0
5/23/2021 18:53:10,Learned Initializations for Optimizing Coordinate-Based Neural Representations,,12/3/2020,https://arxiv.org/pdf/2012.02189.pdf,https://www.matthewtancik.com/learnit,,,,,,"@article{tancik2020learned,
  AUTHOR = {Matthew Tancik and Ben Mildenhall and Terrance Wang and Divi Schmidt and Pratul P. Srinivasan and Jonathan T. Barron and Ren Ng},
  TITLE = {Learned Initializations for Optimizing Coordinate-Based NeuralRepresentations},
  EPRINT = {2012.02189v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Coordinate-based neural representations have shown significant promise as analternative to discrete, array-based representations for complex lowdimensional signals. However, optimizing a coordinate-based network fromrandomly initialized weights for each new signal is inefficient. We proposeapplying standard meta-learning algorithms to learn the initial weightparameters for these fully-connected networks based on the underlying class ofsignals being represented (e.g., images of faces or 3D models of chairs).Despite requiring only a minor change in implementation, using these learnedinitial weights enables faster convergence during optimization and can serve asa strong prior over the signal class being modeled, resulting in bettergeneralization when only partial observations of a given signal are available.We explore these benefits across a variety of tasks, including representing 2Dimages, reconstructing CT scans, and recovering 3D shapes and scenes from 2Dimage observations.},
  YEAR = {2020},
  MONTH = {Dec},
  URL = {http://arxiv.org/abs/2012.02189v2},
  FILE = {2012.02189v2.pdf}
 }","Generalization, Fundamentals","Hypernetwork, Data-driven component (pre-trained, cross-scene)",,,,,,,,,,CVPR,,,,"Matthew Tancik, Ben Mildenhall, Terrance Wang, Divi Schmidt, Pratul P. Srinivasan, Jonathan T. Barron, Ren Ng",tancik2020learned,00000049,"Coordinate-based neural representations have shown significant promise as an alternative to discrete, array-based representations for complex low dimensional signals. However, optimizing a coordinate-based network from randomly initialized weights for each new signal is inefficient. We propose applying standard meta-learning algorithms to learn the initial weight parameters for these fully-connected networks based on the underlying class of signals being represented (e.g., images of faces or 3D models of chairs). Despite requiring only a minor change in implementation, using these learned initial weights enables faster convergence during optimization and can serve as a strong prior over the signal class being modeled, resulting in better generalization when only partial observations of a given signal are available. We explore these benefits across a variety of tasks, including representing 2D images, reconstructing CT scans, and recovering 3D shapes and scenes from 2D image observations.",12,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnuesHR6EXqA8uk5Az6mNuTEOpm0NaEuNgSA1qXfgY-Nw5Lds9es7jAF12K2xOvXIAVo
5/23/2021 18:59:09,AutoInt: Automatic Integration for Fast Neural Volume Rendering,AutoInt,12/3/2020,https://arxiv.org/pdf/2012.01714.pdf,http://www.computationalimaging.org/publications/automatic-integration/,,,,,,"@article{lindell2020autoint,
  AUTHOR = {David B. Lindell and Julien N. P. Martel and Gordon Wetzstein},
  TITLE = {AutoInt: Automatic Integration for Fast Neural Volume Rendering},
  EPRINT = {2012.01714v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Numerical integration is a foundational technique in scientific computing andis at the core of many computer vision applications. Among these applications,neural volume rendering has recently been proposed as a new paradigm for viewsynthesis, achieving photorealistic image quality. However, a fundamentalobstacle to making these methods practical is the extreme computational andmemory requirements caused by the required volume integrations along therendered rays during training and inference. Millions of rays, each requiringhundreds of forward passes through a neural network are needed to approximatethose integrations with Monte Carlo sampling. Here, we propose automaticintegration, a new framework for learning efficient, closed-form solutions tointegrals using coordinate-based neural networks. For training, we instantiatethe computational graph corresponding to the derivative of the network. Thegraph is fitted to the signal to integrate. After optimization, we reassemblethe graph to obtain a network that represents the antiderivative. By thefundamental theorem of calculus, this enables the calculation of any definiteintegral in two evaluations of the network. Applying this approach to neuralrendering, we improve a tradeoff between rendering speed and image quality:improving render times by greater than 10 times with a tradeoff of slightlyreduced image quality.},
  YEAR = {2020},
  MONTH = {Dec},
  URL = {http://arxiv.org/abs/2012.01714v2},
  FILE = {2012.01714v2.pdf}
 }","Performance (rendering), Beyond graphics, Fundamentals",Sampling,,,,,,,,,,,,,,"David B. Lindell, Julien N. P. Martel, Gordon Wetzstein",lindell2020autoint,00000050,"Numerical integration is a foundational technique in scientific computing and is at the core of many computer vision applications. Among these applications, neural volume rendering has recently been proposed as a new paradigm for view synthesis, achieving photorealistic image quality. However, a fundamental obstacle to making these methods practical is the extreme computational and memory requirements caused by the required volume integrations along the rendered rays during training and inference. Millions of rays, each requiring hundreds of forward passes through a neural network are needed to approximate those integrations with Monte Carlo sampling. Here, we propose automatic integration, a new framework for learning efficient, closed-form solutions to integrals using coordinate-based neural networks. For training, we instantiate the computational graph corresponding to the derivative of the network. The graph is fitted to the signal to integrate. After optimization, we reassemble the graph to obtain a network that represents the antiderivative. By the fundamental theorem of calculus, this enables the calculation of any definite integral in two evaluations of the network. Applying this approach to neural rendering, we improve a tradeoff between rendering speed and image quality: improving render times by greater than 10 times with a tradeoff of slightly reduced image quality.",16,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnue3brdCUEavbCF7LfKefxnq5cjyBfOw-Y-g24gKZn56xCeNwXyuERyWHZ18s38Efp8
7/19/2021 21:30:04,Dynamic Neural Radiance Fields for Monocular 4D Facial Avatar Reconstruction,NerFACE,12/5/2020,https://arxiv.org/pdf/2012.03065.pdf,https://gafniguy.github.io/4D-Facial-Avatars/,https://github.com/gafniguy/4D-Facial-Avatars,"https://www.youtube.com/watch?v=XihxC65tmyA, https://www.youtube.com/watch?v=m7oROLdQnjk",,,,"@article{gafni2020nerface,
  AUTHOR = {Guy Gafni and Justus Thies and Michael Zollhofer and Matthias Niessner},
  TITLE = {Dynamic Neural Radiance Fields for Monocular 4D Facial AvatarReconstruction},
  EPRINT = {2012.03065v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We present dynamic neural radiance fields for modeling the appearance anddynamics of a human face. Digitally modeling and reconstructing a talking humanis a key building-block for a variety of applications. Especially, fortelepresence applications in AR or VR, a faithful reproduction of theappearance including novel viewpoints or head-poses is required. In contrast tostate-of-the-art approaches that model the geometry and material propertiesexplicitly, or are purely image-based, we introduce an implicit representationof the head based on scene representation networks. To handle the dynamics ofthe face, we combine our scene representation network with a low-dimensionalmorphable model which provides explicit control over pose and expressions. Weuse volumetric rendering to generate images from this hybrid representation anddemonstrate that such a dynamic neural scene representation can be learned frommonocular input data only, without the need of a specialized capture setup. Inour experiments, we show that this learned volumetric representation allows forphoto-realistic image generation that surpasses the quality of state-of-the-artvideo-based reenactment methods.},
  YEAR = {2020},
  MONTH = {Dec},
  URL = {http://arxiv.org/abs/2012.03065v1},
  FILE = {2012.03065v1.pdf}
 }",Human head,"Conditional neural field, Data-driven component (pre-trained, cross-scene)",NeRF,Density,,,,,,,,CVPR,Available upon request,No,Direct,"Guy Gafni, Justus Thies, Michael Zollhöfer, Matthias Nießner",gafni2020nerface,00000051,"We present dynamic neural radiance fields for modeling the appearance and dynamics of a human face. Digitally modeling and reconstructing a talking human is a key building-block for a variety of applications. Especially, for telepresence applications in AR or VR, a faithful reproduction of the appearance including novel viewpoints or head-poses is required. In contrast to state-of-the-art approaches that model the geometry and material properties explicitly, or are purely image-based, we introduce an implicit representation of the head based on scene representation networks. To handle the dynamics of the face, we combine our scene representation network with a low-dimensional morphable model which provides explicit control over pose and expressions. We use volumetric rendering to generate images from this hybrid representation and demonstrate that such a dynamic neural scene representation can be learned from monocular input data only, without the need of a specialized capture setup. In our experiments, we show that this learned volumetric representation allows for photo-realistic image generation that surpasses the quality of state-of-the-art video-based reenactment methods.",18,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnuc2LC9Tf_qnqaRxI3pKVk1xqj1mYmIkJMSk1cJwPY0pBzs_reCa_A_chUg6jf_IbXk
8/29/2021 20:39:19,Spatially-Adaptive Pixelwise Networks for Fast Image Translation,ASAPNet,12/5/2020,https://arxiv.org/pdf/2012.02992.pdf,https://tamarott.github.io/ASAPNet_web/,https://github.com/tamarott/ASAPNet,https://www.youtube.com/watch?v=6-OfZ32CoBE,,,,"@article{shaham2020asapnet,
  AUTHOR = {Tamar Rott Shaham and Michael Gharbi and Richard Zhang and Eli Shechtman and Tomer Michaeli},
  TITLE = {Spatially-Adaptive Pixelwise Networks for Fast Image Translation},
  EPRINT = {2012.02992v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We introduce a new generator architecture, aimed at fast and efficienthigh-resolution image-to-image translation. We design the generator to be anextremely lightweight function of the full-resolution image. In fact, we usepixel-wise networks; that is, each pixel is processed independently of others,through a composition of simple affine transformations and nonlinearities. Wetake three important steps to equip such a seemingly simple function withadequate expressivity. First, the parameters of the pixel-wise networks arespatially varying so they can represent a broader function class than simple1x1 convolutions. Second, these parameters are predicted by a fastconvolutional network that processes an aggressively low-resolutionrepresentation of the input; Third, we augment the input image with asinusoidal encoding of spatial coordinates, which provides an effectiveinductive bias for generating realistic novel high-frequency image content. Asa result, our model is up to 18x faster than state-of-the-art baselines. Weachieve this speedup while generating comparable visual quality acrossdifferent image resolutions and translation domains.},
  YEAR = {2020},
  MONTH = {Dec},
  URL = {http://arxiv.org/abs/2012.02992v1},
  FILE = {2012.02992v1.pdf}
 }","Performance (rendering), Image","Data-driven component (pre-trained, cross-scene)",,,,,,,,,,CVPR 2021,,,"Direct, Indirect","Tamar Rott Shaham, Michael Gharbi, Richard Zhang, Eli Shechtman, Tomer Michaeli",shaham2020asapnet,00000179,"We introduce a new generator architecture, aimed at fast and efficient high-resolution image-to-image translation. We design the generator to be an extremely lightweight function of the full-resolution image. In fact, we use pixel-wise networks; that is, each pixel is processed independently of others, through a composition of simple affine transformations and nonlinearities. We take three important steps to equip such a seemingly simple function with adequate expressivity. First, the parameters of the pixel-wise networks are spatially varying so they can represent a broader function class than simple 1x1 convolutions. Second, these parameters are predicted by a fast convolutional network that processes an aggressively low-resolution representation of the input; Third, we augment the input image with a sinusoidal encoding of spatial coordinates, which provides an effective inductive bias for generating realistic novel high-frequency image content. As a result, our model is up to 18x faster than state-of-the-art baselines. We achieve this speedup while generating comparable visual quality across different image resolutions and translation domains.",2,,Yes,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnue9mtVcJz4QYSzYvoF_7hxN8bSV8HrGhO0R8cMKm6DuL_0XttuQVDsln7S3RpkcUU8
5/23/2021 19:07:06,NeRV: Neural Reflectance and Visibility Fields for Relighting and View Synthesis,NeRV,12/7/2020,https://arxiv.org/pdf/2012.03927.pdf,https://people.eecs.berkeley.edu/~pratul/nerv/?s=09,Coming soon,https://www.youtube.com/watch?v=4XyDdvhhjVo,,,,"@article{srinivasan2020nerv,
  AUTHOR = {Pratul P. Srinivasan and Boyang Deng and Xiuming Zhang and Matthew Tancik and Ben Mildenhall and Jonathan T. Barron},
  TITLE = {NeRV: Neural Reflectance and Visibility Fields for Relighting and ViewSynthesis},
  EPRINT = {2012.03927v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We present a method that takes as input a set of images of a sceneilluminated by unconstrained known lighting, and produces as output a 3Drepresentation that can be rendered from novel viewpoints under arbitrarylighting conditions. Our method represents the scene as a continuous volumetricfunction parameterized as MLPs whose inputs are a 3D location and whose outputsare the following scene properties at that input location: volume density,surface normal, material parameters, distance to the first surface intersectionin any direction, and visibility of the external environment in any direction.Together, these allow us to render novel views of the object under arbitrarylighting, including indirect illumination effects. The predicted visibility andsurface intersection fields are critical to our model's ability to simulatedirect and indirect illumination during training, because the brute-forcetechniques used by prior work are intractable for lighting conditions outsideof controlled setups with a single light. Our method outperforms alternativeapproaches for recovering relightable 3D scene representations, and performswell in complex lighting settings that have posed a significant challenge toprior work.},
  YEAR = {2020},
  MONTH = {Dec},
  URL = {http://arxiv.org/abs/2012.03927v1},
  FILE = {2012.03927v1.pdf}
 }",Material/lighting estimation,,,,,,,,,,,CVPR,,,,"Pratul P. Srinivasan, Boyang Deng, Xiuming Zhang, Matthew Tancik, Ben Mildenhall, Jonathan T. Barron",srinivasan2020nerv,00000052,"We present a method that takes as input a set of images of a scene illuminated by unconstrained known lighting, and produces as output a 3D representation that can be rendered from novel viewpoints under arbitrary lighting conditions. Our method represents the scene as a continuous volumetric function parameterized as MLPs whose inputs are a 3D location and whose outputs are the following scene properties at that input location: volume density, surface normal, material parameters, distance to the first surface intersection in any direction, and visibility of the external environment in any direction. Together, these allow us to render novel views of the object under arbitrary lighting, including indirect illumination effects. The predicted visibility and surface intersection fields are critical to our model's ability to simulate direct and indirect illumination during training, because the brute-force techniques used by prior work are intractable for lighting conditions outside of controlled setups with a single light. Our method outperforms alternative approaches for recovering relightable 3D scene representations, and performs well in complex lighting settings that have posed a significant challenge to prior work.",21,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnufoqNiJL5AJ5sD0BJPXxu1sBvFYbUC3W5CQfC5CwAUxIdTT7qyCc9-5ETEiWmRCyyk
5/23/2021 19:01:05,NeRD: Neural Reflectance Decomposition from Image Collections,NeRD,12/7/2020,https://arxiv.org/pdf/2012.03918.pdf,https://markboss.me/publication/2021-nerd/?s=09,https://github.com/cgtuebingen/NeRD-Neural-Reflectance-Decomposition,https://www.youtube.com/watch?v=JL-qMTXw9VU,,,,"@article{boss2020nerd,
  AUTHOR = {Mark Boss and Raphael Braun and Varun Jampani and Jonathan T. Barron and Ce Liu and Hendrik P. A. Lensch},
  TITLE = {NeRD: Neural Reflectance Decomposition from Image Collections},
  EPRINT = {2012.03918v3},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Decomposing a scene into its shape, reflectance, and illumination is achallenging but essential problem in computer vision and graphics. This problemis inherently more challenging when the illumination is not a single lightsource under laboratory conditions but is instead an unconstrainedenvironmental illumination. Though recent work has shown that implicitrepresentations can be used to model the radiance field of an object, thesetechniques only enable view synthesis and not relighting. Additionally,evaluating these radiance fields is resource and time-intensive. By decomposinga scene into explicit representations, any rendering framework can be leveragedto generate novel views under any illumination in real-time. NeRD is a methodthat achieves this decomposition by introducing physically-based rendering toneural radiance fields. Even challenging non-Lambertian reflectances, complexgeometry, and unknown illumination can be decomposed into high-quality models.The datasets and code is available on the project page:https://markboss.me/publication/2021-nerd/},
  YEAR = {2020},
  MONTH = {Dec},
  URL = {http://arxiv.org/abs/2012.03918v3},
  FILE = {2012.03918v3.pdf}
 }",Material/lighting estimation,Sampling,,,,,,,,,,ICCV 2020,https://github.com/cgtuebingen/NeRD-Neural-Reflectance-Decomposition/blob/master/download_datasets.py,,,"Mark Boss, Raphael Braun, Varun Jampani, Jonathan T. Barron, Ce Liu, Hendrik P. A. Lensch",boss2020nerd,00000053,"Decomposing a scene into its shape, reflectance, and illumination is a challenging but essential problem in computer vision and graphics. This problem is inherently more challenging when the illumination is not a single light source under laboratory conditions but is instead an unconstrained environmental illumination. Though recent work has shown that implicit representations can be used to model the radiance field of an object, these techniques only enable view synthesis and not relighting. Additionally, evaluating these radiance fields is resource and time-intensive. By decomposing a scene into explicit representations, any rendering framework can be leveraged to generate novel views under any illumination in real-time. NeRD is a method that achieves this decomposition by introducing physically-based rendering to neural radiance fields. Even challenging non-Lambertian reflectances, complex geometry, and unknown illumination can be decomposed into high-quality models. The datasets and code is available on the project page: https://markboss.me/publication/2021-nerd/",3,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnuePlOSX_I1aYBlHKcMFZZyIjMxXxMin1bKenitrc5xeANX8ipLrCixWwbVb1r69aro
5/23/2021 18:55:43,iNeRF: Inverting Neural Radiance Fields for Pose Estimation,iNeRF,12/10/2020,https://arxiv.org/pdf/2012.05877.pdf,https://yenchenlin.me/inerf/,,,,,,"@article{yen-chen2020inerf,
  AUTHOR = {Lin Yen-Chen and Pete Florence and Jonathan T. Barron and Alberto Rodriguez and Phillip Isola and Tsung-Yi Lin},
  TITLE = {INeRF: Inverting Neural Radiance Fields for Pose Estimation},
  EPRINT = {2012.05877v3},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We present iNeRF, a framework that performs mesh-free pose estimation by""inverting"" a Neural RadianceField (NeRF). NeRFs have been shown to beremarkably effective for the task of view synthesis - synthesizingphotorealistic novel views of real-world scenes or objects. In this work, weinvestigate whether we can apply analysis-by-synthesis via NeRF for mesh-free,RGB-only 6DoF pose estimation - given an image, find the translation androtation of a camera relative to a 3D object or scene. Our method assumes thatno object mesh models are available during either training or test time.Starting from an initial pose estimate, we use gradient descent to minimize theresidual between pixels rendered from a NeRF and pixels in an observed image.In our experiments, we first study 1) how to sample rays during pose refinementfor iNeRF to collect informative gradients and 2) how different batch sizes ofrays affect iNeRF on a synthetic dataset. We then show that for complexreal-world scenes from the LLFF dataset, iNeRF can improve NeRF by estimatingthe camera poses of novel images and using these images as additional trainingdata for NeRF. Finally, we show iNeRF can perform category-level object poseestimation, including object instances not seen during training, with RGBimages by inverting a NeRF model inferred from a single view.},
  YEAR = {2020},
  MONTH = {Dec},
  URL = {http://arxiv.org/abs/2012.05877v3},
  FILE = {2012.05877v3.pdf}
 }",Camera parameter estimation,Sampling,,,,,,,,,,IROS 2020,,,,"Lin Yen-Chen, Pete Florence, Jonathan T. Barron, Alberto Rodriguez, Phillip Isola, Tsung-Yi Lin",yen-chen2020inerf,00000054,"We present iNeRF, a framework that performs mesh-free pose estimation by ""inverting"" a Neural RadianceField (NeRF). NeRFs have been shown to be remarkably effective for the task of view synthesis - synthesizing photorealistic novel views of real-world scenes or objects. In this work, we investigate whether we can apply analysis-by-synthesis via NeRF for mesh-free, RGB-only 6DoF pose estimation - given an image, find the translation and rotation of a camera relative to a 3D object or scene. Our method assumes that no object mesh models are available during either training or test time. Starting from an initial pose estimate, we use gradient descent to minimize the residual between pixels rendered from a NeRF and pixels in an observed image. In our experiments, we first study 1) how to sample rays during pose refinement for iNeRF to collect informative gradients and 2) how different batch sizes of rays affect iNeRF on a synthetic dataset. We then show that for complex real-world scenes from the LLFF dataset, iNeRF can improve NeRF by estimating the camera poses of novel images and using these images as additional training data for NeRF. Finally, we show iNeRF can perform category-level object pose estimation, including object instances not seen during training, with RGB images by inverting a NeRF model inferred from a single view.",9,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnufbKmsXpug0aE3wyjj5ehz5dWE_8FTn5FNo87VfE1hsruAjJeXo9z7NLOsTubjcORw
7/19/2021 21:25:34,Portrait Neural Radiance Fields from a Single Image,PortraitNeRF,12/10/2020,https://arxiv.org/pdf/2012.05903.pdf,https://portrait-nerf.github.io/,Coming soon,,,,,"@article{gao2020portraitnerf,
  AUTHOR = {Chen Gao and Yichang Shih and Wei-Sheng Lai and Chia-Kai Liang and Jia-Bin Huang},
  TITLE = {Portrait Neural Radiance Fields from a Single Image},
  EPRINT = {2012.05903v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We present a method for estimating Neural Radiance Fields (NeRF) from asingle headshot portrait. While NeRF has demonstrated high-quality viewsynthesis, it requires multiple images of static scenes and thus impracticalfor casual captures and moving subjects. In this work, we propose to pretrainthe weights of a multilayer perceptron (MLP), which implicitly models thevolumetric density and colors, with a meta-learning framework using a lightstage portrait dataset. To improve the generalization to unseen faces, we trainthe MLP in the canonical coordinate space approximated by 3D face morphablemodels. We quantitatively evaluate the method using controlled captures anddemonstrate the generalization to real portrait images, showing favorableresults against state-of-the-arts.},
  YEAR = {2020},
  MONTH = {Dec},
  URL = {http://arxiv.org/abs/2012.05903v2},
  FILE = {2012.05903v2.pdf}
 }","Human head, Few-shot reconstruction, Generalization","Per-instance fine-tuning, Data-driven component (pre-trained, cross-scene)",NeRF,Density,Category-level,,,,,,,IROS,https://drive.google.com/drive/folders/1wbXc8rMHjRKj6cynKQePnEXMvyPb2HAn,No,Direct,"Chen Gao, Yichang Shih, Wei-Sheng Lai, Chia-Kai Liang, Jia-Bin Huang",gao2020portraitnerf,00000055,"We present a method for estimating Neural Radiance Fields (NeRF) from a single headshot portrait. While NeRF has demonstrated high-quality view synthesis, it requires multiple images of static scenes and thus impractical for casual captures and moving subjects. In this work, we propose to pretrain the weights of a multilayer perceptron (MLP), which implicitly models the volumetric density and colors, with a meta-learning framework using a light stage portrait dataset. To improve the generalization to unseen faces, we train the MLP in the canonical coordinate space approximated by 3D face morphable models. We quantitatively evaluate the method using controlled captures and demonstrate the generalization to real portrait images, showing favorable results against state-of-the-arts.",6,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnudfZHqfEaKeWUZAoouLvxQRNvH_v5QXbR-rnaNqQMePCNv0zIHG4W-DBM_wKxQvdUQ
5/23/2021 18:59:28,Iso-Points: Optimizing Neural Implicit Surfaces with Hybrid Representations,Iso-Points,12/11/2020,https://arxiv.org/pdf/2012.06434.pdf,,,,,,,"@article{yifan2020isopoints,
  AUTHOR = {Wang Yifan and Shihao Wu and Cengiz Oztireli and Olga Sorkine-Hornung},
  TITLE = {Iso-Points: Optimizing Neural Implicit Surfaces with HybridRepresentations},
  EPRINT = {2012.06434v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Neural implicit functions have emerged as a powerful representation forsurfaces in 3D. Such a function can encode a high quality surface withintricate details into the parameters of a deep neural network. However,optimizing for the parameters for accurate and robust reconstructions remains achallenge, especially when the input data is noisy or incomplete. In this work,we develop a hybrid neural surface representation that allows us to imposegeometry-aware sampling and regularization, which significantly improves thefidelity of reconstructions. We propose to use \emph{iso-points} as an explicitrepresentation for a neural implicit function. These points are computed andupdated on-the-fly during training to capture important geometric features andimpose geometric constraints on the optimization. We demonstrate that ourmethod can be adopted to improve state-of-the-art techniques for reconstructingneural implicit surfaces from multi-view images or point clouds. Quantitativeand qualitative evaluations show that, compared with existing sampling andoptimization methods, our approach allows faster convergence, bettergeneralization, and accurate recovery of details and topology.},
  YEAR = {2020},
  MONTH = {Dec},
  URL = {http://arxiv.org/abs/2012.06434v2},
  FILE = {2012.06434v2.pdf}
 }",,"Sampling, Representation",,,,,,,,,,CVPR 2020,,,,"Wang Yifan, Shihao Wu, Cengiz Oztireli, Olga Sorkine-Hornung",yifan2020isopoints,00000056,"Neural implicit functions have emerged as a powerful representation for surfaces in 3D. Such a function can encode a high quality surface with intricate details into the parameters of a deep neural network. However, optimizing for the parameters for accurate and robust reconstructions remains a challenge, especially when the input data is noisy or incomplete. In this work, we develop a hybrid neural surface representation that allows us to impose geometry-aware sampling and regularization, which significantly improves the fidelity of reconstructions. We propose to use \emph{iso-points} as an explicit representation for a neural implicit function. These points are computed and updated on-the-fly during training to capture important geometric features and impose geometric constraints on the optimization. We demonstrate that our method can be adopted to improve state-of-the-art techniques for reconstructing neural implicit surfaces from multi-view images or point clouds. Quantitative and qualitative evaluations show that, compared with existing sampling and optimization methods, our approach allows faster convergence, better generalization, and accurate recovery of details and topology.",1,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnudxRsGezq5JndkCRGjsCgcvxlEDPWKGnvM45q0rvU8oBgAjF-anRu7Y6n45LI6lxcA
7/19/2021 21:48:01,Deep Optimized Priors for 3D Shape Modeling and Reconstruction,,12/14/2020,https://arxiv.org/pdf/2012.07241.pdf,,,,,,,"@article{yang2020deep,
  AUTHOR = {Mingyue Yang and Yuxin Wen and Weikai Chen and Yongwei Chen and Kui Jia},
  TITLE = {Deep Optimized Priors for 3D Shape Modeling and Reconstruction},
  EPRINT = {2012.07241v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Many learning-based approaches have difficulty scaling to unseen data, as thegenerality of its learned prior is limited to the scale and variations of thetraining samples. This holds particularly true with 3D learning tasks, giventhe sparsity of 3D datasets available. We introduce a new learning frameworkfor 3D modeling and reconstruction that greatly improves the generalizationability of a deep generator. Our approach strives to connect the good ends ofboth learning-based and optimization-based methods. In particular, unlike thecommon practice that fixes the pre-trained priors at test time, we propose tofurther optimize the learned prior and latent code according to the inputphysical measurements after the training. We show that the proposed strategyeffectively breaks the barriers constrained by the pre-trained priors and couldlead to high-quality adaptation to unseen data. We realize our framework usingthe implicit surface representation and validate the efficacy of our approachin a variety of challenging tasks that take highly sparse or collapsedobservations as input. Experimental results show that our approach comparesfavorably with the state-of-the-art methods in terms of both generality andaccuracy.},
  YEAR = {2020},
  MONTH = {Dec},
  URL = {http://arxiv.org/abs/2012.07241v1},
  FILE = {2012.07241v1.pdf}
 }",Generalization,"Conditional neural field, Per-instance fine-tuning, Data-driven component (pre-trained, cross-scene)",,,,,,,,,,,,,,"Mingyue Yang, Yuxin Wen, Weikai Chen, Yongwei Chen, Kui Jia",yang2020deep,00000057,"Many learning-based approaches have difficulty scaling to unseen data, as the generality of its learned prior is limited to the scale and variations of the training samples. This holds particularly true with 3D learning tasks, given the sparsity of 3D datasets available. We introduce a new learning framework for 3D modeling and reconstruction that greatly improves the generalization ability of a deep generator. Our approach strives to connect the good ends of both learning-based and optimization-based methods. In particular, unlike the common practice that fixes the pre-trained priors at test time, we propose to further optimize the learned prior and latent code according to the input physical measurements after the training. We show that the proposed strategy effectively breaks the barriers constrained by the pre-trained priors and could lead to high-quality adaptation to unseen data. We realize our framework using the implicit surface representation and validate the efficacy of our approach in a variety of challenging tasks that take highly sparse or collapsed observations as input. Experimental results show that our approach compares favorably with the state-of-the-art methods in terms of both generality and accuracy.",3,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnueKUmtABjJmC8fsGXg18r_i_nLmXLQKRZ1uPWl8McYLh4fLCsPBmtUwoIQX-KTLBdc
5/23/2021 18:37:55,Object-Centric Neural Scene Rendering,OSFs,12/15/2020,https://arxiv.org/pdf/2012.08503.pdf,,,,,,,"@article{guo2020osfs,
  AUTHOR = {Michelle Guo and Alireza Fathi and Jiajun Wu and Thomas Funkhouser},
  TITLE = {Object-Centric Neural Scene Rendering},
  EPRINT = {2012.08503v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We present a method for composing photorealistic scenes from captured imagesof objects. Our work builds upon neural radiance fields (NeRFs), whichimplicitly model the volumetric density and directionally-emitted radiance of ascene. While NeRFs synthesize realistic pictures, they only model static scenesand are closely tied to specific imaging conditions. This property makes NeRFshard to generalize to new scenarios, including new lighting or new arrangementsof objects. Instead of learning a scene radiance field as a NeRF does, wepropose to learn object-centric neural scattering functions (OSFs), arepresentation that models per-object light transport implicitly using alighting- and view-dependent neural network. This enables rendering scenes evenwhen objects or lights move, without retraining. Combined with a volumetricpath tracing procedure, our framework is capable of rendering both intra- andinter-object light transport effects including occlusions, specularities,shadows, and indirect illumination. We evaluate our approach on scenecomposition and show that it generalizes to novel illumination conditions,producing photorealistic, physically accurate renderings of multi-objectscenes.},
  YEAR = {2020},
  MONTH = {Dec},
  URL = {http://arxiv.org/abs/2012.08503v1},
  FILE = {2012.08503v1.pdf}
 }","Editable, Segmentation/composition, Material/lighting estimation",Object-centric representation,,,,,,,,,,CVPR,,,,"Michelle Guo, Alireza Fathi, Jiajun Wu, Thomas Funkhouser",guo2020osfs,00000058,"We present a method for composing photorealistic scenes from captured images of objects. Our work builds upon neural radiance fields (NeRFs), which implicitly model the volumetric density and directionally-emitted radiance of a scene. While NeRFs synthesize realistic pictures, they only model static scenes and are closely tied to specific imaging conditions. This property makes NeRFs hard to generalize to new scenarios, including new lighting or new arrangements of objects. Instead of learning a scene radiance field as a NeRF does, we propose to learn object-centric neural scattering functions (OSFs), a representation that models per-object light transport implicitly using a lighting- and view-dependent neural network. This enables rendering scenes even when objects or lights move, without retraining. Combined with a volumetric path tracing procedure, our framework is capable of rendering both intra- and inter-object light transport effects including occlusions, specularities, shadows, and indirect illumination. We evaluate our approach on scene composition and show that it generalizes to novel illumination conditions, producing photorealistic, physically accurate renderings of multi-object scenes.",6,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnufH9MtS9fNvgtaEojxtpna6RZmkgKF5rvaJl9--cH9NnH55lB2OcEmdbCEZWHAYA2I
8/29/2021 17:03:11,Learning Continuous Image Representation with Local Implicit Image Function,LIIF,12/16/2020,https://arxiv.org/pdf/2012.09161.pdf,https://yinboc.github.io/liif/,https://github.com/yinboc/liif,https://www.youtube.com/watch?v=6f2roieSY_8,,,,"@article{chen2020liif,
  AUTHOR = {Yinbo Chen and Sifei Liu and Xiaolong Wang},
  TITLE = {Learning Continuous Image Representation with Local Implicit ImageFunction},
  EPRINT = {2012.09161v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {How to represent an image? While the visual world is presented in acontinuous manner, machines store and see the images in a discrete way with 2Darrays of pixels. In this paper, we seek to learn a continuous representationfor images. Inspired by the recent progress in 3D reconstruction with implicitneural representation, we propose Local Implicit Image Function (LIIF), whichtakes an image coordinate and the 2D deep features around the coordinate asinputs, predicts the RGB value at a given coordinate as an output. Since thecoordinates are continuous, LIIF can be presented in arbitrary resolution. Togenerate the continuous representation for images, we train an encoder withLIIF representation via a self-supervised task with super-resolution. Thelearned continuous representation can be presented in arbitrary resolution evenextrapolate to x30 higher resolution, where the training tasks are notprovided. We further show that LIIF representation builds a bridge betweendiscrete and continuous representation in 2D, it naturally supports thelearning tasks with size-varied image ground-truths and significantlyoutperforms the method with resizing the ground-truths.},
  YEAR = {2020},
  MONTH = {Dec},
  URL = {http://arxiv.org/abs/2012.09161v2},
  FILE = {2012.09161v2.pdf}
 }","Image, Fundamentals","Conditional neural field, Representation, Data-driven component (pre-trained, cross-scene)",SIREN,,,,,,,,,CVPR 2021 Oral,,,Direct,"Yinbo Chen, Sifei Liu, Xiaolong Wang",chen2020liif,00000172,"How to represent an image? While the visual world is presented in a continuous manner, machines store and see the images in a discrete way with 2D arrays of pixels. In this paper, we seek to learn a continuous representation for images. Inspired by the recent progress in 3D reconstruction with implicit neural representation, we propose Local Implicit Image Function (LIIF), which takes an image coordinate and the 2D deep features around the coordinate as inputs, predicts the RGB value at a given coordinate as an output. Since the coordinates are continuous, LIIF can be presented in arbitrary resolution. To generate the continuous representation for images, we train an encoder with LIIF representation via a self-supervised task with super-resolution. The learned continuous representation can be presented in arbitrary resolution even extrapolate to x30 higher resolution, where the training tasks are not provided. We further show that LIIF representation builds a bridge between discrete and continuous representation in 2D, it naturally supports the learning tasks with size-varied image ground-truths and significantly outperforms the method with resizing the ground-truths.",10,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnuc0cf8OovL38qIfGO9rvcoCyQpgTBBSlJraNlg3LLrcGxSKZ0yZJFwefrJbFUo1GT4
5/23/2021 18:14:09,Neural Radiance Flow for 4D View Synthesis and Video Processing,NeRFlow,12/17/2020,https://arxiv.org/pdf/2012.09790.pdf,https://yilundu.github.io/nerflow/,Coming soon,,,,,"@article{du2020nerflow,
  AUTHOR = {Yilun Du and Yinan Zhang and Hong-Xing Yu and Joshua B. Tenenbaum and Jiajun Wu},
  TITLE = {Neural Radiance Flow for 4D View Synthesis and Video Processing},
  EPRINT = {2012.09790v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We present a method, Neural Radiance Flow (NeRFlow),to learn a 4Dspatial-temporal representation of a dynamic scene from a set of RGB images.Key to our approach is the use of a neural implicit representation that learnsto capture the 3D occupancy, radiance, and dynamics of the scene. By enforcingconsistency across different modalities, our representation enables multi-viewrendering in diverse dynamic scenes, including water pouring, roboticinteraction, and real images, outperforming state-of-the-art methods forspatial-temporal view synthesis. Our approach works even when inputs images arecaptured with only one camera. We further demonstrate that the learnedrepresentation can serve as an implicit scene prior, enabling video processingtasks such as image super-resolution and de-noising without any additionalsupervision.},
  YEAR = {2020},
  MONTH = {Dec},
  URL = {http://arxiv.org/abs/2012.09790v1},
  FILE = {2012.09790v1.pdf}
 }",Dynamic,,,,,,,,,,,CVPR,,,,"Yilun Du, Yinan Zhang, Hong-Xing Yu, Joshua B. Tenenbaum, Jiajun Wu",du2020nerflow,00000059,"We present a method, Neural Radiance Flow (NeRFlow),to learn a 4D spatial-temporal representation of a dynamic scene from a set of RGB images. Key to our approach is the use of a neural implicit representation that learns to capture the 3D occupancy, radiance, and dynamics of the scene. By enforcing consistency across different modalities, our representation enables multi-view rendering in diverse dynamic scenes, including water pouring, robotic interaction, and real images, outperforming state-of-the-art methods for spatial-temporal view synthesis. Our approach works even when inputs images are captured with only one camera. We further demonstrate that the learned representation can serve as an implicit scene prior, enabling video processing tasks such as image super-resolution and de-noising without any additional supervision.",5,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnuehe9a2imk4g82Lx6dDBF4x92biMR6nSMEOy0RPcApn0N5wYw7-vRIzKOGP6s1yYvo
7/19/2021 16:53:32,Learning Compositional Radiance Fields of Dynamic Human Heads,HybridNeRF,12/17/2020,https://arxiv.org/pdf/2012.09955.pdf,https://ziyanw1.github.io/hybrid_nerf/,,,https://openaccess.thecvf.com/content/CVPR2021/supplemental/Wang_Learning_Compositional_Radiance_CVPR_2021_supplemental.pdf,,,"@article{wang2020learning,
  AUTHOR = {Ziyan Wang and Timur Bagautdinov and Stephen Lombardi and Tomas Simon and Jason Saragih and Jessica Hodgins and Michael Zollhofer},
  TITLE = {Learning Compositional Radiance Fields of Dynamic Human Heads},
  EPRINT = {2012.09955v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Photorealistic rendering of dynamic humans is an important ability fortelepresence systems, virtual shopping, synthetic data generation, and more.Recently, neural rendering methods, which combine techniques from computergraphics and machine learning, have created high-fidelity models of humans andobjects. Some of these methods do not produce results with high-enough fidelityfor driveable human models (Neural Volumes) whereas others have extremely longrendering times (NeRF). We propose a novel compositional 3D representation thatcombines the best of previous methods to produce both higher-resolution andfaster results. Our representation bridges the gap between discrete andcontinuous volumetric representations by combining a coarse 3D-structure-awaregrid of animation codes with a continuous learned scene function that mapsevery position and its corresponding local animation code to its view-dependentemitted radiance and local volume density. Differentiable volume rendering isemployed to compute photo-realistic novel views of the human head and upperbody as well as to train our novel representation end-to-end using only 2Dsupervision. In addition, we show that the learned dynamic radiance field canbe used to synthesize novel unseen expressions based on a global animationcode. Our approach achieves state-of-the-art results for synthesizing novelviews of dynamic human heads and the upper body.},
  YEAR = {2020},
  MONTH = {Dec},
  URL = {http://arxiv.org/abs/2012.09955v1},
  FILE = {2012.09955v1.pdf}
 }","Dynamic, Human head, Generalization","Voxelization, Feature volume, Sampling",,,,,,,,,,CVPR 2021 Oral,,No,,"Ziyan Wang, Timur Bagautdinov, Stephen Lombardi, Tomas Simon, Jason Saragih, Jessica Hodgins, Michael Zollhöfer",wang2020learning,00000060,"Photorealistic rendering of dynamic humans is an important ability for telepresence systems, virtual shopping, synthetic data generation, and more. Recently, neural rendering methods, which combine techniques from computer graphics and machine learning, have created high-fidelity models of humans and objects. Some of these methods do not produce results with high-enough fidelity for driveable human models (Neural Volumes) whereas others have extremely long rendering times (NeRF). We propose a novel compositional 3D representation that combines the best of previous methods to produce both higher-resolution and faster results. Our representation bridges the gap between discrete and continuous volumetric representations by combining a coarse 3D-structure-aware grid of animation codes with a continuous learned scene function that maps every position and its corresponding local animation code to its view-dependent emitted radiance and local volume density. Differentiable volume rendering is employed to compute photo-realistic novel views of the human head and upper body as well as to train our novel representation end-to-end using only 2D supervision. In addition, we show that the learned dynamic radiance field can be used to synthesize novel unseen expressions based on a global animation code. Our approach achieves state-of-the-art results for synthesizing novel views of dynamic human heads and the upper body.",5,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnuf_2Fn46b_7o8rVeg2QI2Tvzi2RnkxOzxaVsBHfM66wyf7W8X2B5XB993LeCmpULag
8/29/2021 16:42:59,HyperSeg: Patch-wise Hypernetwork for Real-time Semantic Segmentation,HyperSeg,12/21/2020,https://arxiv.org/pdf/2012.11582.pdf,,https://github.com/YuvalNirkin/hyperseg,,,,,"@article{nirkin2020hyperseg,
  AUTHOR = {Yuval Nirkin and Lior Wolf and Tal Hassner},
  TITLE = {HyperSeg: Patch-wise Hypernetwork for Real-time Semantic Segmentation},
  EPRINT = {2012.11582v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We present a novel, real-time, semantic segmentation network in which theencoder both encodes and generates the parameters (weights) of the decoder.Furthermore, to allow maximal adaptivity, the weights at each decoder blockvary spatially. For this purpose, we design a new type of hypernetwork,composed of a nested U-Net for drawing higher level context features, amulti-headed weight generating module which generates the weights of each blockin the decoder immediately before they are consumed, for efficient memoryutilization, and a primary network that is composed of novel dynamic patch-wiseconvolutions. Despite the usage of less-conventional blocks, our architectureobtains real-time performance. In terms of the runtime vs. accuracy trade-off,we surpass state of the art (SotA) results on popular semantic segmentationbenchmarks: PASCAL VOC 2012 (val. set) and real-time semantic segmentation onCityscapes, and CamVid. The code is available: https://nirkin.com/hyperseg.},
  YEAR = {2020},
  MONTH = {Dec},
  URL = {http://arxiv.org/abs/2012.11582v2},
  FILE = {2012.11582v2.pdf}
 }",Image,"Hypernetwork, Data-driven component (pre-trained, cross-scene), Coordinate CNN",,,,,,,,,,CVPR 2021,https://github.com/YuvalNirkin/hyperseg,,"Direct, Indirect","Yuval Nirkin, Lior Wolf, Tal Hassner",nirkin2020hyperseg,00000169,"We present a novel, real-time, semantic segmentation network in which the encoder both encodes and generates the parameters (weights) of the decoder. Furthermore, to allow maximal adaptivity, the weights at each decoder block vary spatially. For this purpose, we design a new type of hypernetwork, composed of a nested U-Net for drawing higher level context features, a multi-headed weight generating module which generates the weights of each block in the decoder immediately before they are consumed, for efficient memory utilization, and a primary network that is composed of novel dynamic patch-wise convolutions. Despite the usage of less-conventional blocks, our architecture obtains real-time performance. In terms of the runtime vs. accuracy trade-off, we surpass state of the art (SotA) results on popular semantic segmentation benchmarks: PASCAL VOC 2012 (val. set) and real-time semantic segmentation on Cityscapes, and CamVid. The code is available: https://nirkin.com/hyperseg.",2,,Yes,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnuf4oc89_cMKdVY39Ua695_vSJN6rY5hXToSNm-S18arnOJzLq_ORnZLcKy8GlPVGbI
5/23/2021 18:16:35,Non-Rigid Neural Radiance Fields: Reconstruction and Novel View Synthesis of a Dynamic Scene From Monocular Video,NR-NeRF,12/22/2020,https://arxiv.org/pdf/2012.12247.pdf,,,,,,,"@article{tretschk2020nrnerf,
  AUTHOR = {Edgar Tretschk and Ayush Tewari and Vladislav Golyanik and Michael Zollhofer and Christoph Lassner and Christian Theobalt},
  TITLE = {Non-Rigid Neural Radiance Fields: Reconstruction and Novel ViewSynthesis of a Dynamic Scene From Monocular Video},
  EPRINT = {2012.12247v4},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We present Non-Rigid Neural Radiance Fields (NR-NeRF), a reconstruction andnovel view synthesis approach for general non-rigid dynamic scenes. Ourapproach takes RGB images of a dynamic scene as input (e.g., from a monocularvideo recording), and creates a high-quality space-time geometry and appearancerepresentation. We show that a single handheld consumer-grade camera issufficient to synthesize sophisticated renderings of a dynamic scene from novelvirtual camera views, e.g. a `bullet-time' video effect. NR-NeRF disentanglesthe dynamic scene into a canonical volume and its deformation. Scenedeformation is implemented as ray bending, where straight rays are deformednon-rigidly. We also propose a novel rigidity network to better constrain rigidregions of the scene, leading to more stable results. The ray bending andrigidity network are trained without explicit supervision. Our formulationenables dense correspondence estimation across views and time, and compellingvideo editing applications such as motion exaggeration. Our code will be opensourced.},
  YEAR = {2020},
  MONTH = {Dec},
  URL = {http://arxiv.org/abs/2012.12247v4},
  FILE = {2012.12247v4.pdf}
 }",Dynamic,"Conditional neural field, Warping field/Flow field",,,,,,,,,,CVPR,,,,"Edgar Tretschk, Ayush Tewari, Vladislav Golyanik, Michael Zollhöfer, Christoph Lassner, Christian Theobalt",tretschk2020nrnerf,00000061,"We present Non-Rigid Neural Radiance Fields (NR-NeRF), a reconstruction and novel view synthesis approach for general non-rigid dynamic scenes. Our approach takes RGB images of a dynamic scene as input, e.g., from a monocular video recording, and creates a high-quality space-time geometry and appearance representation. In particular, we show that even a single handheld consumer-grade camera is sufficient to synthesize sophisticated renderings of a dynamic scene from novel virtual camera views, for example a `bullet-time' video effect. Our method disentangles the dynamic scene into a canonical volume and its deformation. Scene deformation is implemented as ray bending, where straight rays are deformed non-rigidly to represent scene motion. We also propose a novel rigidity regression network that enables us to better constrain rigid regions of the scene, which leads to more stable results. The ray bending and rigidity network are trained without any explicit supervision. In addition to novel view synthesis, our formulation enables dense correspondence estimation across views and time, as well as compelling video editing applications such as motion exaggeration. We demonstrate the effectiveness of our method using extensive evaluations, including ablation studies and comparisons to the state of the art. We urge the reader to watch the supplemental video for qualitative results. Our code will be open sourced.",18,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnuepoKfRycIWWX2v1Nh-2qyJNQBq5CiAZV4q4C8F6KwXCJ2G5iGyTZURFI5jjXGMeH4
5/23/2021 18:56:31,STaR: Self-supervised Tracking and Reconstruction of Rigid Objects in Motion with Neural Rendering,STaR,12/22/2020,https://arxiv.org/pdf/2101.01602.pdf,https://wentaoyuan.github.io/star/,Coming soon,,,,,"@article{yuan2020star,
  AUTHOR = {Wentao Yuan and Zhaoyang Lv and Tanner Schmidt and Steven Lovegrove},
  TITLE = {STaR: Self-supervised Tracking and Reconstruction of Rigid Objects inMotion with Neural Rendering},
  EPRINT = {2101.01602v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We present STaR, a novel method that performs Self-supervised Tracking andReconstruction of dynamic scenes with rigid motion from multi-view RGB videoswithout any manual annotation. Recent work has shown that neural networks aresurprisingly effective at the task of compressing many views of a scene into alearned function which maps from a viewing ray to an observed radiance valuevia volume rendering. Unfortunately, these methods lose all their predictivepower once any object in the scene has moved. In this work, we explicitly modelrigid motion of objects in the context of neural representations of radiancefields. We show that without any additional human specified supervision, we canreconstruct a dynamic scene with a single rigid object in motion bysimultaneously decomposing it into its two constituent parts and encoding eachwith its own neural representation. We achieve this by jointly optimizing theparameters of two neural radiance fields and a set of rigid poses which alignthe two fields at each frame. On both synthetic and real world datasets, wedemonstrate that our method can render photorealistic novel views, wherenovelty is measured on both spatial and temporal axes. Our factoredrepresentation furthermore enables animation of unseen object motion.},
  YEAR = {2020},
  MONTH = {Dec},
  URL = {http://arxiv.org/abs/2101.01602v1},
  FILE = {2101.01602v1.pdf}
 }","Dynamic, Segmentation/composition",,,,,,,,,,,,,,,"Wentao Yuan, Zhaoyang Lv, Tanner Schmidt, Steven Lovegrove",yuan2020star,00000062,"We present STaR, a novel method that performs Self-supervised Tracking and Reconstruction of dynamic scenes with rigid motion from multi-view RGB videos without any manual annotation. Recent work has shown that neural networks are surprisingly effective at the task of compressing many views of a scene into a learned function which maps from a viewing ray to an observed radiance value via volume rendering. Unfortunately, these methods lose all their predictive power once any object in the scene has moved. In this work, we explicitly model rigid motion of objects in the context of neural representations of radiance fields. We show that without any additional human specified supervision, we can reconstruct a dynamic scene with a single rigid object in motion by simultaneously decomposing it into its two constituent parts and encoding each with its own neural representation. We achieve this by jointly optimizing the parameters of two neural radiance fields and a set of rigid poses which align the two fields at each frame. On both synthetic and real world datasets, we demonstrate that our method can render photorealistic novel views, where novelty is measured on both spatial and temporal axes. Our factored representation furthermore enables animation of unseen object motion.",3,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnucgVFNN6uRAwtLeryspP9KnKLcaUwkyBIi4B3ajrmJ2G2Sx-B1s-sEwrVPGtpiP_Wc
5/23/2021 18:58:16,Neural Body: Implicit Neural Representations with Structured Latent Codes for Novel View Synthesis of Dynamic Humans,Neural Body,12/31/2020,https://arxiv.org/pdf/2012.15838.pdf,https://zju3dv.github.io/neuralbody/,https://github.com/zju3dv/neuralbody,,https://github.com/zju3dv/neuralbody/blob/master/supplementary_material.md,,,"@article{peng2020neuralbody,
  AUTHOR = {Sida Peng and Yuanqing Zhang and Yinghao Xu and Qianqian Wang and Qing Shuai and Hujun Bao and Xiaowei Zhou},
  TITLE = {Neural Body: Implicit Neural Representations with Structured LatentCodes for Novel View Synthesis of Dynamic Humans},
  EPRINT = {2012.15838v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {This paper addresses the challenge of novel view synthesis for a humanperformer from a very sparse set of camera views. Some recent works have shownthat learning implicit neural representations of 3D scenes achieves remarkableview synthesis quality given dense input views. However, the representationlearning will be ill-posed if the views are highly sparse. To solve thisill-posed problem, our key idea is to integrate observations over video frames.To this end, we propose Neural Body, a new human body representation whichassumes that the learned neural representations at different frames share thesame set of latent codes anchored to a deformable mesh, so that theobservations across frames can be naturally integrated. The deformable meshalso provides geometric guidance for the network to learn 3D representationsmore efficiently. To evaluate our approach, we create a multi-view datasetnamed ZJU-MoCap that captures performers with complex motions. Experiments onZJU-MoCap show that our approach outperforms prior works by a large margin interms of novel view synthesis quality. We also demonstrate the capability ofour approach to reconstruct a moving person from a monocular video on thePeople-Snapshot dataset. The code and dataset are available athttps://zju3dv.github.io/neuralbody/.},
  YEAR = {2020},
  MONTH = {Dec},
  URL = {http://arxiv.org/abs/2012.15838v2},
  FILE = {2012.15838v2.pdf}
 }","Dynamic, Human body","Feature volume, Articulated",,,,,,,,,,CVPR 2020,https://zjueducn-my.sharepoint.com/:f:/g/personal/pengsida_zju_edu_cn/Eo9zn4x_xcZKmYHZNjzel7gBdWf_d4m-pISHhPWB-GZBYw?e=Hf4mz7,,,"Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, Xiaowei Zhou",peng2020neuralbody,00000063,"This paper addresses the challenge of novel view synthesis for a human performer from a very sparse set of camera views. Some recent works have shown that learning implicit neural representations of 3D scenes achieves remarkable view synthesis quality given dense input views. However, the representation learning will be ill-posed if the views are highly sparse. To solve this ill-posed problem, our key idea is to integrate observations over video frames. To this end, we propose Neural Body, a new human body representation which assumes that the learned neural representations at different frames share the same set of latent codes anchored to a deformable mesh, so that the observations across frames can be naturally integrated. The deformable mesh also provides geometric guidance for the network to learn 3D representations more efficiently. To evaluate our approach, we create a multi-view dataset named ZJU-MoCap that captures performers with complex motions. Experiments on ZJU-MoCap show that our approach outperforms prior works by a large margin in terms of novel view synthesis quality. We also demonstrate the capability of our approach to reconstruct a moving person from a monocular video on the People-Snapshot dataset. The code and dataset are available at https://zju3dv.github.io/neuralbody/.",23,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnucx5idL5k8tHfwXv2DcaqfoSwYkjHldWcJtCoTULiLtPaPk19Ofu_FIzCDkYb_BQxs
5/23/2021 18:43:36,Non-line-of-Sight Imaging via Neural Transient Fields,,1/2/2021,https://arxiv.org/pdf/2101.00373.pdf,,,,,,,"@article{shen2021nonlineofsight,
  AUTHOR = {Siyuan Shen and Zi Wang and Ping Liu and Zhengqing Pan and Ruiqian Li and Tian Gao and Shiying Li and Jingyi Yu},
  TITLE = {Non-line-of-Sight Imaging via Neural Transient Fields},
  EPRINT = {2101.00373v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {eess.IV},
  ABSTRACT = {We present a neural modeling framework for Non-Line-of-Sight (NLOS) imaging.Previous solutions have sought to explicitly recover the 3D geometry (e.g., aspoint clouds) or voxel density (e.g., within a pre-defined volume) of thehidden scene. In contrast, inspired by the recent Neural Radiance Field (NeRF)approach, we use a multi-layer perceptron (MLP) to represent the neuraltransient field or NeTF. However, NeTF measures the transient over sphericalwavefronts rather than the radiance along lines. We therefore formulate aspherical volume NeTF reconstruction pipeline, applicable to both confocal andnon-confocal setups. Compared with NeRF, NeTF samples a much sparser set ofviewpoints (scanning spots) and the sampling is highly uneven. We thusintroduce a Monte Carlo technique to improve the robustness in thereconstruction. Comprehensive experiments on synthetic and real datasetsdemonstrate NeTF provides higher quality reconstruction and preserves finedetails largely missing in the state-of-the-art.},
  YEAR = {2021},
  MONTH = {Jan},
  URL = {http://arxiv.org/abs/2101.00373v2},
  FILE = {2101.00373v2.pdf}
 }",Beyond graphics,,,,,,,,,,,,,,,"Siyuan Shen, Zi Wang, Ping Liu, Zhengqing Pan, Ruiqian Li, Tian Gao, Shiying Li, Jingyi Yu",shen2021nonlineofsight,00000064,"We present a neural modeling framework for Non-Line-of-Sight (NLOS) imaging. Previous solutions have sought to explicitly recover the 3D geometry (e.g., as point clouds) or voxel density (e.g., within a pre-defined volume) of the hidden scene. In contrast, inspired by the recent Neural Radiance Field (NeRF) approach, we use a multi-layer perceptron (MLP) to represent the neural transient field or NeTF. However, NeTF measures the transient over spherical wavefronts rather than the radiance along lines. We therefore formulate a spherical volume NeTF reconstruction pipeline, applicable to both confocal and non-confocal setups. Compared with NeRF, NeTF samples a much sparser set of viewpoints (scanning spots) and the sampling is highly uneven. We thus introduce a Monte Carlo technique to improve the robustness in the reconstruction. Comprehensive experiments on synthetic and real datasets demonstrate NeTF provides higher quality reconstruction and preserves fine details largely missing in the state-of-the-art.",0,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnueJLwl3tW273LENiNn3UuQI-aCZhVuA1fJFdxkN4_lTPvXk5II1dBAorqNn2Ez8yCI
5/23/2021 18:56:46,Pixel-aligned Volumetric Avatars,PVA,1/7/2021,https://arxiv.org/pdf/2101.02697.pdf,,,,,,,"@article{raj2021pva,
  AUTHOR = {Amit Raj and Michael Zollhoefer and Tomas Simon and Jason Saragih and Shunsuke Saito and James Hays and Stephen Lombardi},
  TITLE = {PVA: Pixel-aligned Volumetric Avatars},
  EPRINT = {2101.02697v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Acquisition and rendering of photo-realistic human heads is a highlychallenging research problem of particular importance for virtual telepresence.Currently, the highest quality is achieved by volumetric approaches trained ina person specific manner on multi-view data. These models better represent finestructure, such as hair, compared to simpler mesh-based models. Volumetricmodels typically employ a global code to represent facial expressions, suchthat they can be driven by a small set of animation parameters. While sucharchitectures achieve impressive rendering quality, they can not easily beextended to the multi-identity setting. In this paper, we devise a novelapproach for predicting volumetric avatars of the human head given just a smallnumber of inputs. We enable generalization across identities by a novelparameterization that combines neural radiance fields with local, pixel-alignedfeatures extracted directly from the inputs, thus sidestepping the need forvery deep or complex networks. Our approach is trained in an end-to-end mannersolely based on a photometric re-rendering loss without requiring explicit 3Dsupervision.We demonstrate that our approach outperforms the existing state ofthe art in terms of quality and is able to generate faithful facial expressionsin a multi-identity setting.},
  YEAR = {2021},
  MONTH = {Jan},
  URL = {http://arxiv.org/abs/2101.02697v1},
  FILE = {2101.02697v1.pdf}
 }","Few-shot reconstruction, Generalization","Lifting 2D features to 3D, Image-based rendering, Data-driven component (pre-trained, cross-scene)",,,,,,,,,,CVPR,,,,"Amit Raj, Michael Zollhoefer, Tomas Simon, Jason Saragih, Shunsuke Saito, James Hays, Stephen Lombardi",raj2021pva,00000065,"Acquisition and rendering of photo-realistic human heads is a highly challenging research problem of particular importance for virtual telepresence. Currently, the highest quality is achieved by volumetric approaches trained in a person specific manner on multi-view data. These models better represent fine structure, such as hair, compared to simpler mesh-based models. Volumetric models typically employ a global code to represent facial expressions, such that they can be driven by a small set of animation parameters. While such architectures achieve impressive rendering quality, they can not easily be extended to the multi-identity setting. In this paper, we devise a novel approach for predicting volumetric avatars of the human head given just a small number of inputs. We enable generalization across identities by a novel parameterization that combines neural radiance fields with local, pixel-aligned features extracted directly from the inputs, thus sidestepping the need for very deep or complex networks. Our approach is trained in an end-to-end manner solely based on a photometric re-rendering loss without requiring explicit 3D supervision.We demonstrate that our approach outperforms the existing state of the art in terms of quality and is able to generate faithful facial expressions in a multi-identity setting.",6,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnudxATwMvzHUQ_x1-LfF-uKxFrPxw4nR3vWT_jmtJr7NjgiU-PfHDyhnBnN3-r-bsS8
9/17/2021 11:46:52,"S3: Neural Shape, Skeleton, and Skinning Fields for 3D Human Modeling",S3,1/17/2021,https://arxiv.org/pdf/2101.06571.pdf,,,,,,,"@article{yang2021s3,
  AUTHOR = {Ze Yang and Shenlong Wang and Sivabalan Manivasagam and Zeng Huang and Wei-Chiu Ma and Xinchen Yan and Ersin Yumer and Raquel Urtasun},
  TITLE = {S3: Neural Shape, Skeleton, and Skinning Fields for 3D Human Modeling},
  EPRINT = {2101.06571v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Constructing and animating humans is an important component for buildingvirtual worlds in a wide variety of applications such as virtual reality orrobotics testing in simulation. As there are exponentially many variations ofhumans with different shape, pose and clothing, it is critical to developmethods that can automatically reconstruct and animate humans at scale fromreal world data. Towards this goal, we represent the pedestrian's shape, poseand skinning weights as neural implicit functions that are directly learnedfrom data. This representation enables us to handle a wide variety of differentpedestrian shapes and poses without explicitly fitting a human parametric bodymodel, allowing us to handle a wider range of human geometries and topologies.We demonstrate the effectiveness of our approach on various datasets and showthat our reconstructions outperform existing state-of-the-art methods.Furthermore, our re-animation experiments show that we can generate 3D humananimations at scale from a single RGB image (and/or an optional LiDAR sweep) asinput.},
  YEAR = {2021},
  MONTH = {Jan},
  URL = {http://arxiv.org/abs/2101.06571v1},
  FILE = {2101.06571v1.pdf}
 }","Human body, Editable","Conditional neural field, Voxelization, Feature volume",,Occupancy,Category-level,,,,,,,CVPR 2021,,Yes,Direct,"Ze Yang, Shenlong Wang, Sivabalan Manivasagam, Zeng Huang, Wei-Chiu Ma, Xinchen Yan, Ersin Yumer, Raquel Urtasun",yang2021s3,00000197,"Constructing and animating humans is an important component for building virtual worlds in a wide variety of applications such as virtual reality or robotics testing in simulation. As there are exponentially many variations of humans with different shape, pose and clothing, it is critical to develop methods that can automatically reconstruct and animate humans at scale from real world data. Towards this goal, we represent the pedestrian's shape, pose and skinning weights as neural implicit functions that are directly learned from data. This representation enables us to handle a wide variety of different pedestrian shapes and poses without explicitly fitting a human parametric body model, allowing us to handle a wider range of human geometries and topologies. We demonstrate the effectiveness of our approach on various datasets and show that our reconstructions outperform existing state-of-the-art methods. Furthermore, our re-animation experiments show that we can generate 3D human animations at scale from a single RGB image (and/or an optional LiDAR sweep) as input.",,No,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnueRZAOQxTf4Z-onuhtpnYt4cFX8RELKjQmgzy23U_t_HpLu2Jg4Y64OSMxjIj8lpeI
6/29/2021 15:40:58,Neural Geometric Level of Detail: Real-time Rendering with Implicit 3D Shapes,,1/26/2021,https://arxiv.org/pdf/2101.10994.pdf,nv-tlabs.github.io/nglod,https://github.com/nv-tlabs/nglod,https://www.youtube.com/watch?v=Pi7W6XrFtMs,,,,"@article{takikawa2021neural,
  AUTHOR = {Towaki Takikawa and Joey Litalien and Kangxue Yin and Karsten Kreis and Charles Loop and Derek Nowrouzezahrai and Alec Jacobson and Morgan McGuire and Sanja Fidler},
  TITLE = {Neural Geometric Level of Detail: Real-time Rendering with Implicit 3DShapes},
  EPRINT = {2101.10994v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Neural signed distance functions (SDFs) are emerging as an effectiverepresentation for 3D shapes. State-of-the-art methods typically encode the SDFwith a large, fixed-size neural network to approximate complex shapes withimplicit surfaces. Rendering with these large networks is, however,computationally expensive since it requires many forward passes through thenetwork for every pixel, making these representations impractical for real-timegraphics. We introduce an efficient neural representation that, for the firsttime, enables real-time rendering of high-fidelity neural SDFs, while achievingstate-of-the-art geometry reconstruction quality. We represent implicitsurfaces using an octree-based feature volume which adaptively fits shapes withmultiple discrete levels of detail (LODs), and enables continuous LOD with SDFinterpolation. We further develop an efficient algorithm to directly render ournovel neural SDF representation in real-time by querying only the necessaryLODs with sparse octree traversal. We show that our representation is 2-3orders of magnitude more efficient in terms of rendering speed compared toprevious works. Furthermore, it produces state-of-the-art reconstructionquality for complex shapes under both 3D geometric and 2D image-space metrics.},
  YEAR = {2021},
  MONTH = {Jan},
  URL = {http://arxiv.org/abs/2101.10994v1},
  FILE = {2101.10994v1.pdf}
 }","Performance (training), Performance (rendering), Generalization","Conditional neural field, Coarse-to-fine, Sampling, Voxelization, Representation, Volume partitioning",None,SDF,Category-level,,,,,,,,,Yes,,"Towaki Takikawa, Joey Litalien, Kangxue Yin, Karsten Kreis, Charles Loop, Derek Nowrouzezahrai, Alec Jacobson, Morgan McGuire, Sanja Fidler",takikawa2021neural,00000066,"Neural signed distance functions (SDFs) are emerging as an effective representation for 3D shapes. State-of-the-art methods typically encode the SDF with a large, fixed-size neural network to approximate complex shapes with implicit surfaces. Rendering with these large networks is, however, computationally expensive since it requires many forward passes through the network for every pixel, making these representations impractical for real-time graphics. We introduce an efficient neural representation that, for the first time, enables real-time rendering of high-fidelity neural SDFs, while achieving state-of-the-art geometry reconstruction quality. We represent implicit surfaces using an octree-based feature volume which adaptively fits shapes with multiple discrete levels of detail (LODs), and enables continuous LOD with SDF interpolation. We further develop an efficient algorithm to directly render our novel neural SDF representation in real-time by querying only the necessary LODs with sparse octree traversal. We show that our representation is 2-3 orders of magnitude more efficient in terms of rendering speed compared to previous works. Furthermore, it produces state-of-the-art reconstruction quality for complex shapes under both 3D geometric and 2D image-space metrics.",12,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnueMudgb4UH3QQkqNjYL2InqZx4ptfgRL2Vow20OwPXIbqi5cN3o_AmSa6iqS7xFkYo
5/23/2021 18:53:44,Towards Generalising Neural Implicit Representations,,1/29/2021,https://arxiv.org/pdf/2101.12690.pdf,,,,,,,"@article{costain2021towards,
  AUTHOR = {Theo W. Costain and Victor Adrian Prisacariu},
  TITLE = {Towards Generalising Neural Implicit Representations},
  EPRINT = {2101.12690v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Neural implicit representations have shown substantial improvements inefficiently storing 3D data, when compared to conventional formats. However,the focus of existing work has mainly been on storage and subsequentreconstruction. In this work, we show that training neural representations forreconstruction tasks alongside conventional tasks can produce more generalencodings that admit equal quality reconstructions to single task training,whilst improving results on conventional tasks when compared to single taskencodings. We reformulate the semantic segmentation task, creating a morerepresentative task for implicit representation contexts, and throughmulti-task experiments on reconstruction, classification, and segmentation,show our approach learns feature rich encodings that admit equal performancefor each task.},
  YEAR = {2021},
  MONTH = {Jan},
  URL = {http://arxiv.org/abs/2101.12690v2},
  FILE = {2101.12690v2.pdf}
 }","Generalization, Beyond graphics",Conditional neural field,,,,,,,,,,,,,,"Theo W. Costain, Victor Adrian Prisacariu",costain2021towards,00000067,"Neural implicit representations have shown substantial improvements in efficiently storing 3D data, when compared to conventional formats. However, the focus of existing work has mainly been on storage and subsequent reconstruction. In this work, we show that training neural representations for reconstruction tasks alongside conventional tasks can produce more general encodings that admit equal quality reconstructions to single task training, whilst improving results on conventional tasks when compared to single task encodings. We reformulate the semantic segmentation task, creating a more representative task for implicit representation contexts, and through multi-task experiments on reconstruction, classification, and segmentation, show our approach learns feature rich encodings that admit equal performance for each task.",0,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnudviRcPoMX8F_mpW8FZ2e_5_Z77ixQJ1d5VyfcGqIt0_Iwf6L-8jK9u-0nAtv_DTYM
7/19/2021 21:41:13,CoIL: Coordinate-based Internal Learning for Imaging Inverse Problems,CoIL,2/9/2021,https://arxiv.org/pdf/2102.05181.pdf,,,,,,,"@article{sun2021coil,
  AUTHOR = {Yu Sun and Jiaming Liu and Mingyang Xie and Brendt Wohlberg and Ulugbek S. Kamilov},
  TITLE = {CoIL: Coordinate-based Internal Learning for Imaging Inverse Problems},
  EPRINT = {2102.05181v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {eess.IV},
  ABSTRACT = {We propose Coordinate-based Internal Learning (CoIL) as a new deep-learning(DL) methodology for the continuous representation of measurements. Unliketraditional DL methods that learn a mapping from the measurements to thedesired image, CoIL trains a multilayer perceptron (MLP) to encode the completemeasurement field by mapping the coordinates of the measurements to theirresponses. CoIL is a self-supervised method that requires no training examplesbesides the measurements of the test object itself. Once the MLP is trained,CoIL generates new measurements that can be used within a majority of imagereconstruction methods. We validate CoIL on sparse-view computed tomographyusing several widely-used reconstruction methods, including purely model-basedmethods and those based on DL. Our results demonstrate the ability of CoIL toconsistently improve the performance of all the considered methods by providinghigh-fidelity measurement fields.},
  YEAR = {2021},
  MONTH = {Feb},
  URL = {http://arxiv.org/abs/2102.05181v1},
  FILE = {2102.05181v1.pdf}
 }","Beyond graphics, Science and engineering, Alternative imaging",,,,,,,,,,,,,,,"Yu Sun, Jiaming Liu, Mingyang Xie, Brendt Wohlberg, Ulugbek S. Kamilov",sun2021coil,00000068,"We propose Coordinate-based Internal Learning (CoIL) as a new deep-learning (DL) methodology for the continuous representation of measurements. Unlike traditional DL methods that learn a mapping from the measurements to the desired image, CoIL trains a multilayer perceptron (MLP) to encode the complete measurement field by mapping the coordinates of the measurements to their responses. CoIL is a self-supervised method that requires no training examples besides the measurements of the test object itself. Once the MLP is trained, CoIL generates new measurements that can be used within a majority of image reconstruction methods. We validate CoIL on sparse-view computed tomography using several widely-used reconstruction methods, including purely model-based methods and those based on DL. Our results demonstrate the ability of CoIL to consistently improve the performance of all the considered methods by providing high-fidelity measurement fields.",3,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnufRGXOo_tCttv0hOUodh8t2BIERGuhqR7HL0OvApyrzzYNDELFLNmpgSRxQU5GcGzk
5/23/2021 18:54:28,A-NeRF: Surface-free Human 3D Pose Refinement via Neural Rendering,A-NeRF,2/11/2021,https://arxiv.org/pdf/2102.06199.pdf,,,,,,,"@article{su2021anerf,
  AUTHOR = {Shih-Yang Su and Frank Yu and Michael Zollhoefer and Helge Rhodin},
  TITLE = {A-NeRF: Surface-free Human 3D Pose Refinement via Neural Rendering},
  EPRINT = {2102.06199v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {While deep learning has reshaped the classical motion capture pipeline,generative, analysis-by-synthesis elements are still in use to recover finedetails if a high-quality 3D model of the user is available. Unfortunately,obtaining such a model for every user a priori is challenging, time-consuming,and limits the application scenarios. We propose a novel test-time optimizationapproach for monocular motion capture that learns a volumetric body model ofthe user in a self-supervised manner. To this end, our approach combines theadvantages of neural radiance fields with an articulated skeletonrepresentation. Our proposed skeleton embedding serves as a common referencethat links constraints across time, thereby reducing the number of requiredcamera views from traditionally dozens of calibrated cameras, down to a singleuncalibrated one. As a starting point, we employ the output of an off-the-shelfmodel that predicts the 3D skeleton pose. The volumetric body shape andappearance is then learned from scratch, while jointly refining the initialpose estimate. Our approach is self-supervised and does not require anyadditional ground truth labels for appearance, pose, or 3D shape. Wedemonstrate that our novel combination of a discriminative pose estimationtechnique with surface-free analysis-by-synthesis outperforms purelydiscriminative monocular pose estimation approaches and generalizes well tomultiple views.},
  YEAR = {2021},
  MONTH = {Feb},
  URL = {http://arxiv.org/abs/2102.06199v1},
  FILE = {2102.06199v1.pdf}
 }","Dynamic, Human body",Articulated,,,,,,,,,,CVPR,,,,"Shih-Yang Su, Frank Yu, Michael Zollhoefer, Helge Rhodin",su2021anerf,00000069,"While deep learning has reshaped the classical motion capture pipeline, generative, analysis-by-synthesis elements are still in use to recover fine details if a high-quality 3D model of the user is available. Unfortunately, obtaining such a model for every user a priori is challenging, time-consuming, and limits the application scenarios. We propose a novel test-time optimization approach for monocular motion capture that learns a volumetric body model of the user in a self-supervised manner. To this end, our approach combines the advantages of neural radiance fields with an articulated skeleton representation. Our proposed skeleton embedding serves as a common reference that links constraints across time, thereby reducing the number of required camera views from traditionally dozens of calibrated cameras, down to a single uncalibrated one. As a starting point, we employ the output of an off-the-shelf model that predicts the 3D skeleton pose. The volumetric body shape and appearance is then learned from scratch, while jointly refining the initial pose estimate. Our approach is self-supervised and does not require any additional ground truth labels for appearance, pose, or 3D shape. We demonstrate that our novel combination of a discriminative pose estimation technique with surface-free analysis-by-synthesis outperforms purely discriminative monocular pose estimation approaches and generalizes well to multiple views.",1,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnue2QAfOqOzGmCZRB05-JuDWhaIhFyuQbAoiGf5OWj-lqI3yu32RzCbuGnvucAgHkSc
5/23/2021 18:54:55,NeRF−−: Neural Radiance Fields Without Known Camera Parameters,NeRF−−,2/14/2021,https://arxiv.org/pdf/2102.07064.pdf,,https://github.com/ActiveVisionLab/nerfmm,,,,,"@article{wang2021nerf--,
  AUTHOR = {Zirui Wang and Shangzhe Wu and Weidi Xie and Min Chen and Victor Adrian Prisacariu},
  TITLE = {NeRF--: Neural Radiance Fields Without Known Camera Parameters},
  EPRINT = {2102.07064v3},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {This paper tackles the problem of novel view synthesis (NVS) from 2D imageswithout known camera poses and intrinsics. Among various NVS techniques, NeuralRadiance Field (NeRF) has recently gained popularity due to its remarkablesynthesis quality. Existing NeRF-based approaches assume that the cameraparameters associated with each input image are either directly accessible attraining, or can be accurately estimated with conventional techniques based oncorrespondences, such as Structure-from-Motion. In this work, we propose anend-to-end framework, termed NeRF--, for training NeRF models given only RGBimages, without pre-computed camera parameters. Specifically, we show that thecamera parameters, including both intrinsics and extrinsics, can beautomatically discovered via joint optimisation during the training of the NeRFmodel. On the standard LLFF benchmark, our model achieves comparable novel viewsynthesis results compared to the baseline trained with COLMAP pre-computedcamera parameters. We also conduct extensive analyses to understand the modelbehaviour under different camera trajectories, and show that in scenarios whereCOLMAP fails, our model still produces robust results.},
  YEAR = {2021},
  MONTH = {Feb},
  URL = {http://arxiv.org/abs/2102.07064v3},
  FILE = {2102.07064v3.pdf}
 }",Camera parameter estimation,,,,,,,,,,,CVPR,,,,"Zirui Wang, Shangzhe Wu, Weidi Xie, Min Chen, Victor Adrian Prisacariu",wang2021nerf--,00000070,"This paper tackles the problem of novel view synthesis (NVS) from 2D images without known camera poses and intrinsics. Among various NVS techniques, Neural Radiance Field (NeRF) has recently gained popularity due to its remarkable synthesis quality. Existing NeRF-based approaches assume that the camera parameters associated with each input image are either directly accessible at training, or can be accurately estimated with conventional techniques based on correspondences, such as Structure-from-Motion. In this work, we propose an end-to-end framework, termed NeRF--, for training NeRF models given only RGB images, without pre-computed camera parameters. Specifically, we show that the camera parameters, including both intrinsics and extrinsics, can be automatically discovered via joint optimisation during the training of the NeRF model. On the standard LLFF benchmark, our model achieves comparable novel view synthesis results compared to the baseline trained with COLMAP pre-computed camera parameters. We also conduct extensive analyses to understand the model behaviour under different camera trajectories, and show that in scenarios where COLMAP fails, our model still produces robust results.",9,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnud19BQxPwYpBBqetX2TWbfhybhQisCXu1gpdk5sObNDBQIkEEVGyWxEKUBc8NPGhTc
7/26/2021 14:37:27,ShaRF: Shape-conditioned Radiance Fields from a Single View,ShaRF,2/17/2021,https://arxiv.org/pdf/2102.08860.pdf,,,,,,,"@article{rematas2021sharf,
  AUTHOR = {Konstantinos Rematas and Ricardo Martin-Brualla and Vittorio Ferrari},
  TITLE = {ShaRF: Shape-conditioned Radiance Fields from a Single View},
  EPRINT = {2102.08860v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We present a method for estimating neural scenes representations of objectsgiven only a single image. The core of our method is the estimation of ageometric scaffold for the object and its use as a guide for the reconstructionof the underlying radiance field. Our formulation is based on a generativeprocess that first maps a latent code to a voxelized shape, and then renders itto an image, with the object appearance being controlled by a second latentcode. During inference, we optimize both the latent codes and the networks tofit a test image of a new object. The explicit disentanglement of shape andappearance allows our model to be fine-tuned given a single image. We can thenrender new views in a geometrically consistent manner and they representfaithfully the input object. Additionally, our method is able to generalize toimages outside of the training domain (more realistic renderings and even realphotographs). Finally, the inferred geometric scaffold is itself an accurateestimate of the object's 3D shape. We demonstrate in several experiments theeffectiveness of our approach in both synthetic and real images.},
  YEAR = {2021},
  MONTH = {Feb},
  URL = {http://arxiv.org/abs/2102.08860v2},
  FILE = {2102.08860v2.pdf}
 }","Few-shot reconstruction, Generalization","Generative/adversarial formulation, Sampling, Voxelization, Data-driven component (pre-trained, cross-scene)",NeRF,Density,,,,,,,,CVPR,,No,Direct,"Konstantinos Rematas, Ricardo Martin-Brualla, Vittorio Ferrari",rematas2021sharf,00000071,"We present a method for estimating neural scenes representations of objects given only a single image. The core of our method is the estimation of a geometric scaffold for the object and its use as a guide for the reconstruction of the underlying radiance field. Our formulation is based on a generative process that first maps a latent code to a voxelized shape, and then renders it to an image, with the object appearance being controlled by a second latent code. During inference, we optimize both the latent codes and the networks to fit a test image of a new object. The explicit disentanglement of shape and appearance allows our model to be fine-tuned given a single image. We can then render new views in a geometrically consistent manner and they represent faithfully the input object. Additionally, our method is able to generalize to images outside of the training domain (more realistic renderings and even real photographs). Finally, the inferred geometric scaffold is itself an accurate estimate of the object's 3D shape. We demonstrate in several experiments the effectiveness of our approach in both synthetic and real images.",5,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnuc1F6UOVFlPZsZCFPID2cuNZC5V991gu33cql9rOWB7PY2G0EiXNSXNsbYYHQYvPjk
5/23/2021 18:53:27,NTopo: Mesh-free Topolibogy Optimization using Implicit Neural Representations,NTopo,2/22/2021,https://arxiv.org/pdf/2102.10782.pdf,,,,,,,"@article{zehnder2021ntopo,
  AUTHOR = {Jonas Zehnder and Yue Li and Stelian Coros and Bernhard Thomaszewski},
  TITLE = {NTopo: Mesh-free Topology Optimization using Implicit NeuralRepresentations},
  EPRINT = {2102.10782v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.LG},
  ABSTRACT = {Recent advances in implicit neural representations show great promise when itcomes to generating numerical solutions to partial differential equations(PDEs). Compared to conventional alternatives, such representations employparameterized neural networks to define, in a mesh-free manner, signals thatare highly-detailed, continuous, and fully differentiable. Most prior works aimto exploit these benefits in order to solve PDE-governed forward problems, orassociated inverse problems that are defined by a small number of parameters.In this work, we present a novel machine learning approach to tackle topologyoptimization (TO) problems. Topology optimization refers to an important classof inverse problems that typically feature very high-dimensional parameterspaces and objective landscapes which are highly non-linear. To effectivelyleverage neural representations in the context of TO problems, we usemultilayer perceptrons (MLPs) to parameterize both density and displacementfields. Using sensitivity analysis with a moving mean squared error, we showthat our formulation can be used to efficiently minimize traditional structuralcompliance objectives. As we show through our experiments, a major benefit ofour approach is that it enables self-supervised learning of continuous solutionspaces to topology optimization problems.},
  YEAR = {2021},
  MONTH = {Feb},
  URL = {http://arxiv.org/abs/2102.10782v1},
  FILE = {2102.10782v1.pdf}
 }","Beyond graphics, Science and engineering",PDE,,,,,,,,,,CVPR,,,,"Jonas Zehnder, Yue Li, Stelian Coros, Bernhard Thomaszewski",zehnder2021ntopo,00000072,"Recent advances in implicit neural representations show great promise when it comes to generating numerical solutions to partial differential equations (PDEs). Compared to conventional alternatives, such representations employ parameterized neural networks to define, in a mesh-free manner, signals that are highly-detailed, continuous, and fully differentiable. Most prior works aim to exploit these benefits in order to solve PDE-governed forward problems, or associated inverse problems that are defined by a small number of parameters. In this work, we present a novel machine learning approach to tackle topology optimization (TO) problems. Topology optimization refers to an important class of inverse problems that typically feature very high-dimensional parameter spaces and objective landscapes which are highly non-linear. To effectively leverage neural representations in the context of TO problems, we use multilayer perceptrons (MLPs) to parameterize both density and displacement fields. Using sensitivity analysis with a moving mean squared error, we show that our formulation can be used to efficiently minimize traditional structural compliance objectives. As we show through our experiments, a major benefit of our approach is that it enables self-supervised learning of continuous solution spaces to topology optimization problems.",0,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnue6ZRNB8D8xtWOCq6361tYPdLzVN9on395pvt_2cyLvBbj5ZzVxWTMZ09V9iZfNSXI
5/23/2021 18:52:47,IBRNet: Learning Multi-View Image-Based Rendering,IBRNet,2/25/2021,https://arxiv.org/pdf/2102.13090.pdf,https://ibrnet.github.io/,,,,,,"@article{wang2021ibrnet,
  AUTHOR = {Qianqian Wang and Zhicheng Wang and Kyle Genova and Pratul Srinivasan and Howard Zhou and Jonathan T. Barron and Ricardo Martin-Brualla and Noah Snavely and Thomas Funkhouser},
  TITLE = {IBRNet: Learning Multi-View Image-Based Rendering},
  EPRINT = {2102.13090v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We present a method that synthesizes novel views of complex scenes byinterpolating a sparse set of nearby views. The core of our method is a networkarchitecture that includes a multilayer perceptron and a ray transformer thatestimates radiance and volume density at continuous 5D locations (3D spatiallocations and 2D viewing directions), drawing appearance information on the flyfrom multiple source views. By drawing on source views at render time, ourmethod hearkens back to classic work on image-based rendering (IBR), and allowsus to render high-resolution imagery. Unlike neural scene representation workthat optimizes per-scene functions for rendering, we learn a generic viewinterpolation function that generalizes to novel scenes. We render images usingclassic volume rendering, which is fully differentiable and allows us to trainusing only multi-view posed images as supervision. Experiments show that ourmethod outperforms recent novel view synthesis methods that also seek togeneralize to novel scenes. Further, if fine-tuned on each scene, our method iscompetitive with state-of-the-art single-scene neural rendering methods.Project page: https://ibrnet.github.io/},
  YEAR = {2021},
  MONTH = {Feb},
  URL = {http://arxiv.org/abs/2102.13090v2},
  FILE = {2102.13090v2.pdf}
 }","Performance (training), Generalization","Lifting 2D features to 3D, Image-based rendering, Data-driven component (pre-trained, cross-scene)",,,,,,,,,,CVPR 2020,,,,"Qianqian Wang, Zhicheng Wang, Kyle Genova, Pratul Srinivasan, Howard Zhou, Jonathan T. Barron, Ricardo Martin-Brualla, Noah Snavely, Thomas Funkhouser",wang2021ibrnet,00000073,"We present a method that synthesizes novel views of complex scenes by interpolating a sparse set of nearby views. The core of our method is a network architecture that includes a multilayer perceptron and a ray transformer that estimates radiance and volume density at continuous 5D locations (3D spatial locations and 2D viewing directions), drawing appearance information on the fly from multiple source views. By drawing on source views at render time, our method hearkens back to classic work on image-based rendering (IBR), and allows us to render high-resolution imagery. Unlike neural scene representation work that optimizes per-scene functions for rendering, we learn a generic view interpolation function that generalizes to novel scenes. We render images using classic volume rendering, which is fully differentiable and allows us to train using only multi-view posed images as supervision. Experiments show that our method outperforms recent novel view synthesis methods that also seek to generalize to novel scenes. Further, if fine-tuned on each scene, our method is competitive with state-of-the-art single-scene neural rendering methods. Project page: https://ibrnet.github.io/",20,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnudyc0pp1X8xSHsKO-WcO3Jve7_6up7iAW3hOPjICdEK3vZ81Md8oDtLnhrHa4HMP9Q
5/23/2021 18:56:02,NeuTex: Neural Texture Mapping for Volumetric Neural Rendering,NeuTex,3/1/2021,https://arxiv.org/pdf/2103.00762.pdf,,,,,,,"@article{xiang2021neutex,
  AUTHOR = {Fanbo Xiang and Zexiang Xu and Milos Hasan and Yannick Hold-Geoffroy and Kalyan Sunkavalli and Hao Su},
  TITLE = {NeuTex: Neural Texture Mapping for Volumetric Neural Rendering},
  EPRINT = {2103.00762v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Recent work has demonstrated that volumetric scene representations combinedwith differentiable volume rendering can enable photo-realistic rendering forchallenging scenes that mesh reconstruction fails on. However, these methodsentangle geometry and appearance in a ""black-box"" volume that cannot be edited.Instead, we present an approach that explicitly disentanglesgeometry--represented as a continuous 3D volume--from appearance--representedas a continuous 2D texture map. We achieve this by introducing a 3D-to-2Dtexture mapping (or surface parameterization) network into volumetricrepresentations. We constrain this texture mapping network using an additional2D-to-3D inverse mapping network and a novel cycle consistency loss to make 3Dsurface points map to 2D texture points that map back to the original 3Dpoints. We demonstrate that this representation can be reconstructed using onlymulti-view image supervision and generates high-quality rendering results. Moreimportantly, by separating geometry and texture, we allow users to editappearance by simply editing 2D texture maps.},
  YEAR = {2021},
  MONTH = {Mar},
  URL = {http://arxiv.org/abs/2103.00762v1},
  FILE = {2103.00762v1.pdf}
 }",Fundamentals,,,,,,,,,,,,,,,"Fanbo Xiang, Zexiang Xu, Miloš Hašan, Yannick Hold-Geoffroy, Kalyan Sunkavalli, Hao Su",xiang2021neutex,00000074,"Recent work has demonstrated that volumetric scene representations combined with differentiable volume rendering can enable photo-realistic rendering for challenging scenes that mesh reconstruction fails on. However, these methods entangle geometry and appearance in a ""black-box"" volume that cannot be edited. Instead, we present an approach that explicitly disentangles geometry--represented as a continuous 3D volume--from appearance--represented as a continuous 2D texture map. We achieve this by introducing a 3D-to-2D texture mapping (or surface parameterization) network into volumetric representations. We constrain this texture mapping network using an additional 2D-to-3D inverse mapping network and a novel cycle consistency loss to make 3D surface points map to 2D texture points that map back to the original 3D points. We demonstrate that this representation can be reconstructed using only multi-view image supervision and generates high-quality rendering results. More importantly, by separating geometry and texture, we allow users to edit appearance by simply editing 2D texture maps.",2,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnueo72MgZHt-K4t-Il8Q8RICbf7NNhJVI8NA9KE2qKFjWfzqt5iwI6v2XObRdIj1gZA
5/23/2021 18:52:14,Mixture of Volumetric Primitives for Efficient Neural Rendering,MVP,3/2/2021,https://arxiv.org/pdf/2103.01954.pdf,,,,,,,"@article{lombardi2021mvp,
  AUTHOR = {Stephen Lombardi and Tomas Simon and Gabriel Schwartz and Michael Zollhoefer and Yaser Sheikh and Jason Saragih},
  TITLE = {Mixture of Volumetric Primitives for Efficient Neural Rendering},
  EPRINT = {2103.01954v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.GR},
  ABSTRACT = {Real-time rendering and animation of humans is a core function in games,movies, and telepresence applications. Existing methods have a number ofdrawbacks we aim to address with our work. Triangle meshes have difficultymodeling thin structures like hair, volumetric representations like NeuralVolumes are too low-resolution given a reasonable memory budget, andhigh-resolution implicit representations like Neural Radiance Fields are tooslow for use in real-time applications. We present Mixture of VolumetricPrimitives (MVP), a representation for rendering dynamic 3D content thatcombines the completeness of volumetric representations with the efficiency ofprimitive-based rendering, e.g., point-based or mesh-based methods. Ourapproach achieves this by leveraging spatially shared computation with adeconvolutional architecture and by minimizing computation in empty regions ofspace with volumetric primitives that can move to cover only occupied regions.Our parameterization supports the integration of correspondence and trackingconstraints, while being robust to areas where classical tracking fails, suchas around thin or translucent structures and areas with large topologicalvariability. MVP is a hybrid that generalizes both volumetric andprimitive-based representations. Through a series of extensive experiments wedemonstrate that it inherits the strengths of each, while avoiding many oftheir limitations. We also compare our approach to several state-of-the-artmethods and demonstrate that MVP produces superior results in terms of qualityand runtime performance.},
  YEAR = {2021},
  MONTH = {Mar},
  URL = {http://arxiv.org/abs/2103.01954v2},
  FILE = {2103.01954v2.pdf}
 }","Performance (training), Performance (rendering)","Sampling, Voxelization, Feature volume, Representation",,,,,,,,,,SIGGRAPH 2020,,,Indirect,"Stephen Lombardi, Tomas Simon, Gabriel Schwartz, Michael Zollhoefer, Yaser Sheikh, Jason Saragih",lombardi2021mvp,00000075,"Real-time rendering and animation of humans is a core function in games, movies, and telepresence applications. Existing methods have a number of drawbacks we aim to address with our work. Triangle meshes have difficulty modeling thin structures like hair, volumetric representations like Neural Volumes are too low-resolution given a reasonable memory budget, and high-resolution implicit representations like Neural Radiance Fields are too slow for use in real-time applications. We present Mixture of Volumetric Primitives (MVP), a representation for rendering dynamic 3D content that combines the completeness of volumetric representations with the efficiency of primitive-based rendering, e.g., point-based or mesh-based methods. Our approach achieves this by leveraging spatially shared computation with a deconvolutional architecture and by minimizing computation in empty regions of space with volumetric primitives that can move to cover only occupied regions. Our parameterization supports the integration of correspondence and tracking constraints, while being robust to areas where classical tracking fails, such as around thin or translucent structures and areas with large topological variability. MVP is a hybrid that generalizes both volumetric and primitive-based representations. Through a series of extensive experiments we demonstrate that it inherits the strengths of each, while avoiding many of their limitations. We also compare our approach to several state-of-the-art methods and demonstrate that MVP produces superior results in terms of quality and runtime performance.",6,,No,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnuerrAjFLHW_oQgGR-ABmAvPNBNXUJFxttUW4SPx8phEtTfamWwm-NLrM6Wr9KYktmo
5/23/2021 18:50:30,COIN: COmpression with Implicit Neural representations,COIN,3/3/2021,https://arxiv.org/pdf/2103.03123.pdf,,,,,,,"@article{dupont2021coin,
  AUTHOR = {Emilien Dupont and Adam Golinski and Milad Alizadeh and Yee Whye Teh and Arnaud Doucet},
  TITLE = {COIN: COmpression with Implicit Neural representations},
  EPRINT = {2103.03123v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {eess.IV},
  ABSTRACT = {We propose a new simple approach for image compression: instead of storingthe RGB values for each pixel of an image, we store the weights of a neuralnetwork overfitted to the image. Specifically, to encode an image, we fit itwith an MLP which maps pixel locations to RGB values. We then quantize andstore the weights of this MLP as a code for the image. To decode the image, wesimply evaluate the MLP at every pixel location. We found that this simpleapproach outperforms JPEG at low bit-rates, even without entropy coding orlearning a distribution over weights. While our framework is not yetcompetitive with state of the art compression methods, we show that it hasvarious attractive properties which could make it a viable alternative to otherneural data compression approaches.},
  YEAR = {2021},
  MONTH = {Mar},
  URL = {http://arxiv.org/abs/2103.03123v2},
  FILE = {2103.03123v2.pdf}
 }",Compression,,SIREN,,,,,,,,,SIGGRAPH,,,,"Emilien Dupont, Adam Goliński, Milad Alizadeh, Yee Whye Teh, Arnaud Doucet",dupont2021coin,00000076,"We propose a new simple approach for image compression: instead of storing the RGB values for each pixel of an image, we store the weights of a neural network overfitted to the image. Specifically, to encode an image, we fit it with an MLP which maps pixel locations to RGB values. We then quantize and store the weights of this MLP as a code for the image. To decode the image, we simply evaluate the MLP at every pixel location. We found that this simple approach outperforms JPEG at low bit-rates, even without entropy coding or learning a distribution over weights. While our framework is not yet competitive with state of the art compression methods, we show that it has various attractive properties which could make it a viable alternative to other neural data compression approaches.",0,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnufoFFLu34JYheofFkNuKaCmsKCnU-Gbr3AKbW19FnVSxM8zIKDSM-1mQnsPEMp8Q3Q
5/23/2021 18:50:45,Neural 3D Video Synthesis,DyNeRF,3/3/2021,https://arxiv.org/pdf/2103.02597.pdf,,,,,,,"@article{li2021dynerf,
  AUTHOR = {Tianye Li and Mira Slavcheva and Michael Zollhoefer and Simon Green and Christoph Lassner and Changil Kim and Tanner Schmidt and Steven Lovegrove and Michael Goesele and Zhaoyang Lv},
  TITLE = {Neural 3D Video Synthesis},
  EPRINT = {2103.02597v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We propose a novel approach for 3D video synthesis that is able to representmulti-view video recordings of a dynamic real-world scene in a compact, yetexpressive representation that enables high-quality view synthesis and motioninterpolation. Our approach takes the high quality and compactness of staticneural radiance fields in a new direction: to a model-free, dynamic setting. Atthe core of our approach is a novel time-conditioned neural radiance fieldsthat represents scene dynamics using a set of compact latent codes. To exploitthe fact that changes between adjacent frames of a video are typically smalland locally consistent, we propose two novel strategies for efficient trainingof our neural network: 1) An efficient hierarchical training scheme, and 2) animportance sampling strategy that selects the next rays for training based onthe temporal variation of the input videos. In combination, these twostrategies significantly boost the training speed, lead to fast convergence ofthe training process, and enable high quality results. Our learnedrepresentation is highly compact and able to represent a 10 second 30 FPSmulti-view video recording by 18 cameras with a model size of just 28MB. Wedemonstrate that our method can render high-fidelity wide-angle novel views atover 1K resolution, even for highly complex and dynamic scenes. We perform anextensive qualitative and quantitative evaluation that shows that our approachoutperforms the current state of the art. We include additional video andinformation at: https://neural-3d-video.github.io/},
  YEAR = {2021},
  MONTH = {Mar},
  URL = {http://arxiv.org/abs/2103.02597v1},
  FILE = {2103.02597v1.pdf}
 }",Dynamic,Conditional neural field,,,,,,,,,,SIGGRAPH,,,,"Tianye Li, Mira Slavcheva, Michael Zollhoefer, Simon Green, Christoph Lassner, Changil Kim, Tanner Schmidt, Steven Lovegrove, Michael Goesele, Zhaoyang Lv",li2021dynerf,00000077,"We propose a novel approach for 3D video synthesis that is able to represent multi-view video recordings of a dynamic real-world scene in a compact, yet expressive representation that enables high-quality view synthesis and motion interpolation. Our approach takes the high quality and compactness of static neural radiance fields in a new direction: to a model-free, dynamic setting. At the core of our approach is a novel time-conditioned neural radiance fields that represents scene dynamics using a set of compact latent codes. To exploit the fact that changes between adjacent frames of a video are typically small and locally consistent, we propose two novel strategies for efficient training of our neural network: 1) An efficient hierarchical training scheme, and 2) an importance sampling strategy that selects the next rays for training based on the temporal variation of the input videos. In combination, these two strategies significantly boost the training speed, lead to fast convergence of the training process, and enable high quality results. Our learned representation is highly compact and able to represent a 10 second 30 FPS multi-view video recording by 18 cameras with a model size of just 28MB. We demonstrate that our method can render high-fidelity wide-angle novel views at over 1K resolution, even for highly complex and dynamic scenes. We perform an extensive qualitative and quantitative evaluation that shows that our approach outperforms the current state of the art. We include additional video and information at: https://neural-3d-video.github.io/",3,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnuepnK4G93pm4xmZg3OgoMxPEjUThHW8aaTBjvSPT4WFja4cZ-tZhzuDlggNajiRwdo
5/25/2021 0:11:31,DONeRF: Towards Real-Time Rendering of Neural Radiance Fields using Depth Oracle Networks,DONeRF,3/4/2021,https://arxiv.org/pdf/2103.03231.pdf,,,,,,,"@article{neff2021donerf,
  AUTHOR = {Thomas Neff and Pascal Stadlbauer and Mathias Parger and Andreas Kurz and Joerg H. Mueller and Chakravarty R. Alla Chaitanya and Anton Kaplanyan and Markus Steinberger},
  TITLE = {DONeRF: Towards Real-Time Rendering of Compact Neural Radiance Fieldsusing Depth Oracle Networks},
  EPRINT = {2103.03231v4},
  DOI = {10.1111/cgf.14340},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {The recent research explosion around implicit neural representations, such asNeRF, shows that there is immense potential for implicitly storing high-qualityscene and lighting information in compact neural networks. However, one majorlimitation preventing the use of NeRF in real-time rendering applications isthe prohibitive computational cost of excessive network evaluations along eachview ray, requiring dozens of petaFLOPS. In this work, we bring compact neuralrepresentations closer to practical rendering of synthetic content in real-timeapplications, such as games and virtual reality. We show that the number ofsamples required for each view ray can be significantly reduced when samplesare placed around surfaces in the scene without compromising image quality. Tothis end, we propose a depth oracle network that predicts ray sample locationsfor each view ray with a single network evaluation. We show that using aclassification network around logarithmically discretized and sphericallywarped depth values is essential to encode surface locations rather thandirectly estimating depth. The combination of these techniques leads to DONeRF,our compact dual network design with a depth oracle network as its first stepand a locally sampled shading network for ray accumulation. With DONeRF, wereduce the inference costs by up to 48x compared to NeRF when conditioning onavailable ground truth depth information. Compared to concurrent accelerationmethods for raymarching-based neural representations, DONeRF does not requireadditional memory for explicit caching or acceleration structures, and canrender interactively (20 frames per second) on a single GPU.},
  YEAR = {2021},
  MONTH = {Mar},
  NOTE = {Computer Graphics Forum Volume 40, Issue 4, 2021},
  URL = {http://arxiv.org/abs/2103.03231v4},
  FILE = {2103.03231v4.pdf}
 }",Performance (rendering),"Sampling, Data-driven component (pre-trained, cross-scene)",,,,,,,,,,EGSR 2020,,,,"Thomas Neff, Pascal Stadlbauer, Mathias Parger, Andreas Kurz, Joerg H. Mueller, Chakravarty R. Alla Chaitanya, Anton Kaplanyan, Markus Steinberger",neff2021donerf,00000078,"The recent research explosion around implicit neural representations, such as NeRF, shows that there is immense potential for implicitly storing high-quality scene and lighting information in compact neural networks. However, one major limitation preventing the use of NeRF in real-time rendering applications is the prohibitive computational cost of excessive network evaluations along each view ray, requiring dozens of petaFLOPS. In this work, we bring compact neural representations closer to practical rendering of synthetic content in real-time applications, such as games and virtual reality. We show that the number of samples required for each view ray can be significantly reduced when samples are placed around surfaces in the scene without compromising image quality. To this end, we propose a depth oracle network that predicts ray sample locations for each view ray with a single network evaluation. We show that using a classification network around logarithmically discretized and spherically warped depth values is essential to encode surface locations rather than directly estimating depth. The combination of these techniques leads to DONeRF, our compact dual network design with a depth oracle network as its first step and a locally sampled shading network for ray accumulation. With DONeRF, we reduce the inference costs by up to 48x compared to NeRF when conditioning on available ground truth depth information. Compared to concurrent acceleration methods for raymarching-based neural representations, DONeRF does not require additional memory for explicit caching or acceleration structures, and can render interactively (20 frames per second) on a single GPU.",6,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnuf5FOkh6DPToODe-phxD5ZMhEPG8LPiOE7vWJ3u9gIE1DiNtSm47d_KnJ-UZKnVCBY
5/23/2021 18:48:47,NeX: Real-time View Synthesis with Neural Basis Expansion,NeX,3/9/2021,https://arxiv.org/pdf/2103.05606.pdf,https://nex-mpi.github.io/,https://github.com/nex-mpi/nex-code/,,,,,"@article{wizadwongsa2021nex,
  AUTHOR = {Suttisak Wizadwongsa and Pakkapon Phongthawee and Jiraphon Yenphraphai and Supasorn Suwajanakorn},
  TITLE = {NeX: Real-time View Synthesis with Neural Basis Expansion},
  EPRINT = {2103.05606v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We present NeX, a new approach to novel view synthesis based on enhancementsof multiplane image (MPI) that can reproduce next-level view-dependent effects-- in real time. Unlike traditional MPI that uses a set of simple RGB$\alpha$planes, our technique models view-dependent effects by instead parameterizingeach pixel as a linear combination of basis functions learned from a neuralnetwork. Moreover, we propose a hybrid implicit-explicit modeling strategy thatimproves upon fine detail and produces state-of-the-art results. Our method isevaluated on benchmark forward-facing datasets as well as our newly-introduceddataset designed to test the limit of view-dependent modeling withsignificantly more challenging effects such as rainbow reflections on a CD. Ourmethod achieves the best overall scores across all major metrics on thesedatasets with more than 1000$\times$ faster rendering time than the state ofthe art. For real-time demos, visit https://nex-mpi.github.io/},
  YEAR = {2021},
  MONTH = {Mar},
  URL = {http://arxiv.org/abs/2103.05606v2},
  FILE = {2103.05606v2.pdf}
 }",Performance (rendering),Representation,,,,,,,,,,CVPR 2020,https://vistec-my.sharepoint.com/personal/pakkapon_p_s19_vistec_ac_th/_layouts/15/onedrive.aspx?id=%2Fpersonal%2Fpakkapon%5Fp%5Fs19%5Fvistec%5Fac%5Fth%2FDocuments%2Fpublic%2FVLL%2FNeX%2Fshiny%5Fdatasets&originalPath=aHR0cHM6Ly92aXN0ZWMtbXkuc2hhcmVwb2ludC5jb20vOmY6L2cvcGVyc29uYWwvcGFra2Fwb25fcF9zMTlfdmlzdGVjX2FjX3RoL0VuSVVoc1JWSk9kTnNaXzRzbWRoeWUwQjh6MFZseHFPUjM1SVIzYnAwdUd1cFE%5FcnRpbWU9c1hYTTNEd2UyVWc,,,"Suttisak Wizadwongsa, Pakkapon Phongthawee, Jiraphon Yenphraphai, Supasorn Suwajanakorn",wizadwongsa2021nex,00000079,"We present NeX, a new approach to novel view synthesis based on enhancements of multiplane image (MPI) that can reproduce next-level view-dependent effects -- in real time. Unlike traditional MPI that uses a set of simple RGB$\alpha$ planes, our technique models view-dependent effects by instead parameterizing each pixel as a linear combination of basis functions learned from a neural network. Moreover, we propose a hybrid implicit-explicit modeling strategy that improves upon fine detail and produces state-of-the-art results. Our method is evaluated on benchmark forward-facing datasets as well as our newly-introduced dataset designed to test the limit of view-dependent modeling with significantly more challenging effects such as rainbow reflections on a CD. Our method achieves the best overall scores across all major metrics on these datasets with more than 1000$\times$ faster rendering time than the state of the art. For real-time demos, visit https://nex-mpi.github.io/",9,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnudoO15pLeUGNIV1WiNk-tAfI0TC3WHeZxd3fRVkNlYkrhXP4gsdSd-8xxlShEr4yO4
9/17/2021 11:42:58,SMPLicit: Topology-aware Generative Model for Clothed People,SMPLicit,3/11/2021,https://arxiv.org/pdf/2103.06871.pdf,http://www.iri.upc.edu/people/ecorona/smplicit/,https://github.com/enriccorona/SMPLicit,,,,,"@article{corona2021smplicit,
  AUTHOR = {Enric Corona and Albert Pumarola and Guillem Alenya and Gerard Pons-Moll and Francesc Moreno-Noguer},
  TITLE = {SMPLicit: Topology-aware Generative Model for Clothed People},
  EPRINT = {2103.06871v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {In this paper we introduce SMPLicit, a novel generative model to jointlyrepresent body pose, shape and clothing geometry. In contrast to existinglearning-based approaches that require training specific models for each typeof garment, SMPLicit can represent in a unified manner different garmenttopologies (e.g. from sleeveless tops to hoodies and to open jackets), whilecontrolling other properties like the garment size or tightness/looseness. Weshow our model to be applicable to a large variety of garments includingT-shirts, hoodies, jackets, shorts, pants, skirts, shoes and even hair. Therepresentation flexibility of SMPLicit builds upon an implicit modelconditioned with the SMPL human body parameters and a learnable latent spacewhich is semantically interpretable and aligned with the clothing attributes.The proposed model is fully differentiable, allowing for its use into largerend-to-end trainable systems. In the experimental section, we demonstrateSMPLicit can be readily used for fitting 3D scans and for 3D reconstruction inimages of dressed people. In both cases we are able to go beyond state of theart, by retrieving complex garment geometries, handling situations withmultiple clothing layers and providing a tool for easy outfit editing. Tostimulate further research in this direction, we will make our code and modelpublicly available at http://www.iri.upc.edu/people/ecorona/smplicit/.},
  YEAR = {2021},
  MONTH = {Mar},
  URL = {http://arxiv.org/abs/2103.06871v2},
  FILE = {2103.06871v2.pdf}
 }","Human body, Editable","Generative/adversarial formulation, Conditional neural field",NeRF,UDF,Category-level,,,,,,,CVPR 2021,,No,Direct,"Enric Corona, Albert Pumarola, Guillem Alenyà, Gerard Pons-Moll, Francesc Moreno-Noguer",corona2021smplicit,00000196,"In this paper we introduce SMPLicit, a novel generative model to jointly represent body pose, shape and clothing geometry. In contrast to existing learning-based approaches that require training specific models for each type of garment, SMPLicit can represent in a unified manner different garment topologies (e.g. from sleeveless tops to hoodies and to open jackets), while controlling other properties like the garment size or tightness/looseness. We show our model to be applicable to a large variety of garments including T-shirts, hoodies, jackets, shorts, pants, skirts, shoes and even hair. The representation flexibility of SMPLicit builds upon an implicit model conditioned with the SMPL human body parameters and a learnable latent space which is semantically interpretable and aligned with the clothing attributes. The proposed model is fully differentiable, allowing for its use into larger end-to-end trainable systems. In the experimental section, we demonstrate SMPLicit can be readily used for fitting 3D scans and for 3D reconstruction in images of dressed people. In both cases we are able to go beyond state of the art, by retrieving complex garment geometries, handling situations with multiple clothing layers and providing a tool for easy outfit editing. To stimulate further research in this direction, we will make our code and model publicly available at http://www.iri.upc.edu/people/ecorona/smplicit/.",,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnueo8XHlBKQaPa9xK1wF8neP6jEtXfv4IArMaiLQaaNazdZaXh5yCujzHCJzyR5U59w
5/23/2021 18:44:44,AD-NeRF: Audio Driven Neural Radiance Fields for Talking Head Synthesis,AD-NeRF,3/20/2021,https://arxiv.org/pdf/2103.11078.pdf,,,https://www.youtube.com/watch?v=TQO2EBYXLyU,,,,"@article{guo2021adnerf,
  AUTHOR = {Yudong Guo and Keyu Chen and Sen Liang and Yong-Jin Liu and Hujun Bao and Juyong Zhang},
  TITLE = {AD-NeRF: Audio Driven Neural Radiance Fields for Talking Head Synthesis},
  EPRINT = {2103.11078v3},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Generating high-fidelity talking head video by fitting with the input audiosequence is a challenging problem that receives considerable attentionsrecently. In this paper, we address this problem with the aid of neural scenerepresentation networks. Our method is completely different from existingmethods that rely on intermediate representations like 2D landmarks or 3D facemodels to bridge the gap between audio input and video output. Specifically,the feature of input audio signal is directly fed into a conditional implicitfunction to generate a dynamic neural radiance field, from which ahigh-fidelity talking-head video corresponding to the audio signal issynthesized using volume rendering. Another advantage of our framework is thatnot only the head (with hair) region is synthesized as previous methods did,but also the upper body is generated via two individual neural radiance fields.Experimental results demonstrate that our novel framework can (1) producehigh-fidelity and natural results, and (2) support free adjustment of audiosignals, viewing directions, and background images. Code is available athttps://github.com/YudongGuo/AD-NeRF.},
  YEAR = {2021},
  MONTH = {Mar},
  URL = {http://arxiv.org/abs/2103.11078v3},
  FILE = {2103.11078v3.pdf}
 }","Dynamic, Beyond graphics","Conditional neural field, Volume partitioning",,,,,,,,,,CVPR,,,,"Yudong Guo, Keyu Chen, Sen Liang, Yongjin Liu, Hujun Bao, Juyong Zhang",guo2021adnerf,00000080,"Generating high-fidelity talking head video by fitting with the input audio sequence is a challenging problem that receives considerable attentions recently. In this paper, we address this problem with the aid of neural scene representation networks. Our method is completely different from existing methods that rely on intermediate representations like 2D landmarks or 3D face models to bridge the gap between audio input and video output. Specifically, the feature of input audio signal is directly fed into a conditional implicit function to generate a dynamic neural radiance field, from which a high-fidelity talking-head video corresponding to the audio signal is synthesized using volume rendering. Another advantage of our framework is that not only the head (with hair) region is synthesized as previous methods did, but also the upper body is generated via two individual neural radiance fields. Experimental results demonstrate that our novel framework can (1) produce high-fidelity and natural results, and (2) support free adjustment of audio signals, viewing directions, and background images. Code is available at https://github.com/YudongGuo/AD-NeRF.",0,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnucGOj9SLLKX-4HAv60eBR8vFlSPQr0VenEoP4xsVwgvHtn0cRAJRfog6ffnnDSuTMw
6/21/2021 16:43:15,Neural Lumigraph Rendering,NLR,3/22/2021,https://arxiv.org/pdf/2103.11571.pdf,http://www.computationalimaging.org/publications/nlr/,,https://www.youtube.com/watch?v=maVF-7x9644,https://openaccess.thecvf.com/content/CVPR2021/supplemental/Kellnhofer_Neural_Lumigraph_Rendering_CVPR_2021_supplemental.pdf,,,"@article{kellnhofer2021nlr,
  AUTHOR = {Petr Kellnhofer and Lars Jebe and Andrew Jones and Ryan Spicer and Kari Pulli and Gordon Wetzstein},
  TITLE = {Neural Lumigraph Rendering},
  EPRINT = {2103.11571v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Novel view synthesis is a challenging and ill-posed inverse renderingproblem. Neural rendering techniques have recently achieved photorealisticimage quality for this task. State-of-the-art (SOTA) neural volume renderingapproaches, however, are slow to train and require minutes of inference (i.e.,rendering) time for high image resolutions. We adopt high-capacity neural scenerepresentations with periodic activations for jointly optimizing an implicitsurface and a radiance field of a scene supervised exclusively with posed 2Dimages. Our neural rendering pipeline accelerates SOTA neural volume renderingby about two orders of magnitude and our implicit surface representation isunique in allowing us to export a mesh with view-dependent texture information.Thus, like other implicit surface representations, ours is compatible withtraditional graphics pipelines, enabling real-time rendering rates, whileachieving unprecedented image quality compared to other surface methods. Weassess the quality of our approach using existing datasets as well ashigh-quality 3D face data captured with a custom multi-camera rig.},
  YEAR = {2021},
  MONTH = {Mar},
  URL = {http://arxiv.org/abs/2103.11571v1},
  FILE = {2103.11571v1.pdf}
 }",Performance (rendering),Image-based rendering,,SDF,,,,,,,,CVPR,,,,"Petr Kellnhofer, Lars Jebe, Andrew Jones, Ryan Spicer, Kari Pulli, Gordon Wetzstein",kellnhofer2021nlr,00000081,"Novel view synthesis is a challenging and ill-posed inverse rendering problem. Neural rendering techniques have recently achieved photorealistic image quality for this task. State-of-the-art (SOTA) neural volume rendering approaches, however, are slow to train and require minutes of inference (i.e., rendering) time for high image resolutions. We adopt high-capacity neural scene representations with periodic activations for jointly optimizing an implicit surface and a radiance field of a scene supervised exclusively with posed 2D images. Our neural rendering pipeline accelerates SOTA neural volume rendering by about two orders of magnitude and our implicit surface representation is unique in allowing us to export a mesh with view-dependent texture information. Thus, like other implicit surface representations, ours is compatible with traditional graphics pipelines, enabling real-time rendering rates, while achieving unprecedented image quality compared to other surface methods. We assess the quality of our approach using existing datasets as well as high-quality 3D face data captured with a custom multi-camera rig.",8,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnud7J3eL9ZOsP874PT0PGHo1ysVzSkvq8fnh9DcrnVDpjPLQ_TlaarOaMKAQocZ-W9g
5/23/2021 18:47:09,iMAP: Implicit Mapping and Positioning in Real-Time,iMAP,3/23/2021,https://arxiv.org/pdf/2103.12352.pdf,https://edgarsucar.github.io/iMAP/,,,,,,"@article{sucar2021imap,
  AUTHOR = {Edgar Sucar and Shikun Liu and Joseph Ortiz and Andrew J. Davison},
  TITLE = {iMAP: Implicit Mapping and Positioning in Real-Time},
  EPRINT = {2103.12352v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We show for the first time that a multilayer perceptron (MLP) can serve asthe only scene representation in a real-time SLAM system for a handheld RGB-Dcamera. Our network is trained in live operation without prior data, building adense, scene-specific implicit 3D model of occupancy and colour which is alsoimmediately used for tracking.Achieving real-time SLAM via continual training of a neural network against alive image stream requires significant innovation. Our iMAP algorithm uses akeyframe structure and multi-processing computation flow, with dynamicinformation-guided pixel sampling for speed, with tracking at 10 Hz and globalmap updating at 2 Hz. The advantages of an implicit MLP over standard denseSLAM techniques include efficient geometry representation with automatic detailcontrol and smooth, plausible filling-in of unobserved regions such as the backsurfaces of objects.},
  YEAR = {2021},
  MONTH = {Mar},
  URL = {http://arxiv.org/abs/2103.12352v1},
  FILE = {2103.12352v1.pdf}
 }",Camera parameter estimation,Sampling,,,,,,,,,,,,,,"Edgar Sucar, Shikun Liu, Joseph Ortiz, Andrew J. Davison",sucar2021imap,00000082,"We show for the first time that a multilayer perceptron (MLP) can serve as the only scene representation in a real-time SLAM system for a handheld RGB-D camera. Our network is trained in live operation without prior data, building a dense, scene-specific implicit 3D model of occupancy and colour which is also immediately used for tracking. Achieving real-time SLAM via continual training of a neural network against a live image stream requires significant innovation. Our iMAP algorithm uses a keyframe structure and multi-processing computation flow, with dynamic information-guided pixel sampling for speed, with tracking at 10 Hz and global map updating at 2 Hz. The advantages of an implicit MLP over standard dense SLAM techniques include efficient geometry representation with automatic detail control and smooth, plausible filling-in of unobserved regions such as the back surfaces of objects.",3,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnucyWE5VuEa_0hkOiebVTGHbEl-d-MEXmRqT9hForDl4JOY1tbpCXSTAZmSLaH8DuPs
5/23/2021 18:46:10,Mip-NeRF: A Multiscale Representation for Anti-Aliasing Neural Radiance Fields,Mip-NeRF,3/24/2021,https://arxiv.org/pdf/2103.13415.pdf,https://jonbarron.info/mipnerf/,,https://www.youtube.com/watch?v=EpH175PY1A0,,,,"@article{barron2021mipnerf,
  AUTHOR = {Jonathan T. Barron and Ben Mildenhall and Matthew Tancik and Peter Hedman and Ricardo Martin-Brualla and Pratul P. Srinivasan},
  TITLE = {Mip-NeRF: A Multiscale Representation for Anti-Aliasing Neural RadianceFields},
  EPRINT = {2103.13415v3},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {The rendering procedure used by neural radiance fields (NeRF) samples a scenewith a single ray per pixel and may therefore produce renderings that areexcessively blurred or aliased when training or testing images observe scenecontent at different resolutions. The straightforward solution of supersamplingby rendering with multiple rays per pixel is impractical for NeRF, becauserendering each ray requires querying a multilayer perceptron hundreds of times.Our solution, which we call ""mip-NeRF"" (a la ""mipmap""), extends NeRF torepresent the scene at a continuously-valued scale. By efficiently renderinganti-aliased conical frustums instead of rays, mip-NeRF reduces objectionablealiasing artifacts and significantly improves NeRF's ability to represent finedetails, while also being 7% faster than NeRF and half the size. Compared toNeRF, mip-NeRF reduces average error rates by 17% on the dataset presented withNeRF and by 60% on a challenging multiscale variant of that dataset that wepresent. Mip-NeRF is also able to match the accuracy of a brute-forcesupersampled NeRF on our multiscale dataset while being 22x faster.},
  YEAR = {2021},
  MONTH = {Mar},
  URL = {http://arxiv.org/abs/2103.13415v3},
  FILE = {2103.13415v3.pdf}
 }",Fundamentals,Sampling,,,,,,,,,,,,,,"Jonathan T. Barron, Ben Mildenhall, Matthew Tancik, Peter Hedman, Ricardo Martin-Brualla, Pratul P. Srinivasan",barron2021mipnerf,00000083,"The rendering procedure used by neural radiance fields (NeRF) samples a scene with a single ray per pixel and may therefore produce renderings that are excessively blurred or aliased when training or testing images observe scene content at different resolutions. The straightforward solution of supersampling by rendering with multiple rays per pixel is impractical for NeRF, because rendering each ray requires querying a multilayer perceptron hundreds of times. Our solution, which we call ""mip-NeRF"" (a la ""mipmap""), extends NeRF to represent the scene at a continuously-valued scale. By efficiently rendering anti-aliased conical frustums instead of rays, mip-NeRF reduces objectionable aliasing artifacts and significantly improves NeRF's ability to represent fine details, while also being 7% faster than NeRF and half the size. Compared to NeRF, mip-NeRF reduces average error rates by 17% on the dataset presented with NeRF and by 60% on a challenging multiscale variant of that dataset that we present. Mip-NeRF is also able to match the accuracy of a brute-force supersampled NeRF on our multiscale dataset while being 22x faster.",3,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnudXtC9ePS9BhYtPjcLkX3Yap-3zxJNNU4U3h-rfYeaN7FMmX2lysTp3A-HHyrM9TSI
5/25/2021 15:13:46,PlenOctrees for Real-time Rendering of Neural Radiance Fields,"NeRF-SH, PlenOctrees",3/25/2021,https://arxiv.org/pdf/2103.14024.pdf,https://alexyu.net/plenoctrees/,"https://github.com/sxyu/plenoctree, https://github.com/sxyu/volrend",,,,,"@article{yu2021nerfsh, plenoctrees,
  AUTHOR = {Alex Yu and Ruilong Li and Matthew Tancik and Hao Li and Ren Ng and Angjoo Kanazawa},
  TITLE = {PlenOctrees for Real-time Rendering of Neural Radiance Fields},
  EPRINT = {2103.14024v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We introduce a method to render Neural Radiance Fields (NeRFs) in real timeusing PlenOctrees, an octree-based 3D representation which supportsview-dependent effects. Our method can render 800x800 images at more than 150FPS, which is over 3000 times faster than conventional NeRFs. We do so withoutsacrificing quality while preserving the ability of NeRFs to performfree-viewpoint rendering of scenes with arbitrary geometry and view-dependenteffects. Real-time performance is achieved by pre-tabulating the NeRF into aPlenOctree. In order to preserve view-dependent effects such as specularities,we factorize the appearance via closed-form spherical basis functions.Specifically, we show that it is possible to train NeRFs to predict a sphericalharmonic representation of radiance, removing the viewing direction as an inputto the neural network. Furthermore, we show that PlenOctrees can be directlyoptimized to further minimize the reconstruction loss, which leads to equal orbetter quality compared to competing methods. Moreover, this octreeoptimization step can be used to reduce the training time, as we no longer needto wait for the NeRF training to converge fully. Our real-time neural renderingapproach may potentially enable new applications such as 6-DOF industrial andproduct visualizations, as well as next generation AR/VR systems. PlenOctreesare amenable to in-browser rendering as well; please visit the project page forthe interactive online demo, as well as video and code:https://alexyu.net/plenoctrees},
  YEAR = {2021},
  MONTH = {Mar},
  URL = {http://arxiv.org/abs/2103.14024v2},
  FILE = {2103.14024v2.pdf}
 }","Performance (rendering), Material/lighting estimation","Per-instance fine-tuning, Sampling, Representation, Caching",,,,,,,,,,ICCV 2021,,,,"Alex Yu, Ruilong Li, Matthew Tancik, Hao Li, Ren Ng, Angjoo Kanazawa",yu2021nerfsh,00000084,"We introduce a method to render Neural Radiance Fields (NeRFs) in real time using PlenOctrees, an octree-based 3D representation which supports view-dependent effects. Our method can render 800x800 images at more than 150 FPS, which is over 3000 times faster than conventional NeRFs. We do so without sacrificing quality while preserving the ability of NeRFs to perform free-viewpoint rendering of scenes with arbitrary geometry and view-dependent effects. Real-time performance is achieved by pre-tabulating the NeRF into a PlenOctree. In order to preserve view-dependent effects such as specularities, we factorize the appearance via closed-form spherical basis functions. Specifically, we show that it is possible to train NeRFs to predict a spherical harmonic representation of radiance, removing the viewing direction as an input to the neural network. Furthermore, we show that PlenOctrees can be directly optimized to further minimize the reconstruction loss, which leads to equal or better quality compared to competing methods. Moreover, this octree optimization step can be used to reduce the training time, as we no longer need to wait for the NeRF training to converge fully. Our real-time neural rendering approach may potentially enable new applications such as 6-DOF industrial and product visualizations, as well as next generation AR/VR systems. PlenOctrees are amenable to in-browser rendering as well; please visit the project page for the interactive online demo, as well as video and code: https://alexyu.net/plenoctrees",12,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnufjgWqdP_M0Q7t9BWRuCIIOF46-gbMml5Hc3Xwi41v_4wzVvektELUZCSYglm0_vps
8/5/2021 15:51:57,KiloNeRF: Speeding up Neural Radiance Fields with Thousands of Tiny MLPs,KiloNeRF,3/25/2021,https://arxiv.org/pdf/2103.13744.pdf,,,,,,,"@article{reiser2021kilonerf,
  AUTHOR = {Christian Reiser and Songyou Peng and Yiyi Liao and Andreas Geiger},
  TITLE = {KiloNeRF: Speeding up Neural Radiance Fields with Thousands of Tiny MLPs},
  EPRINT = {2103.13744v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {NeRF synthesizes novel views of a scene with unprecedented quality by fittinga neural radiance field to RGB images. However, NeRF requires querying a deepMulti-Layer Perceptron (MLP) millions of times, leading to slow renderingtimes, even on modern GPUs. In this paper, we demonstrate that real-timerendering is possible by utilizing thousands of tiny MLPs instead of one singlelarge MLP. In our setting, each individual MLP only needs to represent parts ofthe scene, thus smaller and faster-to-evaluate MLPs can be used. By combiningthis divide-and-conquer strategy with further optimizations, rendering isaccelerated by three orders of magnitude compared to the original NeRF modelwithout incurring high storage costs. Further, using teacher-studentdistillation for training, we show that this speed-up can be achieved withoutsacrificing visual quality.},
  YEAR = {2021},
  MONTH = {Mar},
  URL = {http://arxiv.org/abs/2103.13744v2},
  FILE = {2103.13744v2.pdf}
 }",Performance (rendering),"Voxelization, Representation, Sampling, Volume partitioning",NeRF,Density,Per-scene,,,,,,,ICCV 2021,,No,Direct,"Christian Reiser, Songyou Peng, Yiyi Liao, Andreas Geiger",reiser2021kilonerf,00000085,"NeRF synthesizes novel views of a scene with unprecedented quality by fitting a neural radiance field to RGB images. However, NeRF requires querying a deep Multi-Layer Perceptron (MLP) millions of times, leading to slow rendering times, even on modern GPUs. In this paper, we demonstrate that real-time rendering is possible by utilizing thousands of tiny MLPs instead of one single large MLP. In our setting, each individual MLP only needs to represent parts of the scene, thus smaller and faster-to-evaluate MLPs can be used. By combining this divide-and-conquer strategy with further optimizations, rendering is accelerated by three orders of magnitude compared to the original NeRF model without incurring high storage costs. Further, using teacher-student distillation for training, we show that this speed-up can be achieved without sacrificing visual quality.",6,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnucsfhsEjvHSt0CzA-m0ZlisvMKH3Po8rDhYRAQ30QQuzVAPqWuMOsNDhsHwxUWud_4
5/23/2021 18:46:37,Baking Neural Radiance Fields for Real-Time View Synthesis,SNeRG,3/26/2021,https://arxiv.org/pdf/2103.14645.pdf,https://phog.github.io/snerg/,,,,,,"@article{hedman2021snerg,
  AUTHOR = {Peter Hedman and Pratul P. Srinivasan and Ben Mildenhall and Jonathan T. Barron and Paul Debevec},
  TITLE = {Baking Neural Radiance Fields for Real-Time View Synthesis},
  EPRINT = {2103.14645v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Neural volumetric representations such as Neural Radiance Fields (NeRF) haveemerged as a compelling technique for learning to represent 3D scenes fromimages with the goal of rendering photorealistic images of the scene fromunobserved viewpoints. However, NeRF's computational requirements areprohibitive for real-time applications: rendering views from a trained NeRFrequires querying a multilayer perceptron (MLP) hundreds of times per ray. Wepresent a method to train a NeRF, then precompute and store (i.e. ""bake"") it asa novel representation called a Sparse Neural Radiance Grid (SNeRG) thatenables real-time rendering on commodity hardware. To achieve this, weintroduce 1) a reformulation of NeRF's architecture, and 2) a sparse voxel gridrepresentation with learned feature vectors. The resulting scene representationretains NeRF's ability to render fine geometric details and view-dependentappearance, is compact (averaging less than 90 MB per scene), and can berendered in real-time (higher than 30 frames per second on a laptop GPU).Actual screen captures are shown in our video.},
  YEAR = {2021},
  MONTH = {Mar},
  URL = {http://arxiv.org/abs/2103.14645v1},
  FILE = {2103.14645v1.pdf}
 }","Performance (rendering), Compression","Per-instance fine-tuning, Voxelization, Feature volume, Caching",,,,,,,,,,ICCV,,,,"Peter Hedman, Pratul P. Srinivasan, Ben Mildenhall, Jonathan T. Barron, Paul Debevec",hedman2021snerg,00000086,"Neural volumetric representations such as Neural Radiance Fields (NeRF) have emerged as a compelling technique for learning to represent 3D scenes from images with the goal of rendering photorealistic images of the scene from unobserved viewpoints. However, NeRF's computational requirements are prohibitive for real-time applications: rendering views from a trained NeRF requires querying a multilayer perceptron (MLP) hundreds of times per ray. We present a method to train a NeRF, then precompute and store (i.e. ""bake"") it as a novel representation called a Sparse Neural Radiance Grid (SNeRG) that enables real-time rendering on commodity hardware. To achieve this, we introduce 1) a reformulation of NeRF's architecture, and 2) a sparse voxel grid representation with learned feature vectors. The resulting scene representation retains NeRF's ability to render fine geometric details and view-dependent appearance, is compact (averaging less than 90 MB per scene), and can be rendered in real-time (higher than 30 frames per second on a laptop GPU). Actual screen captures are shown in our video.",2,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnuek1GX_3croFGl-9mYeOHDvvgtSd3MB4Xv30yMBabTev16d_Y-Nj2Jb1coPSir2xD4
8/31/2021 16:19:57,MINE: Towards Continuous Depth MPI with NeRF for Novel View Synthesis,MINE,3/27/2021,https://arxiv.org/pdf/2103.14910.pdf,https://vincentfung13.github.io/projects/mine/,https://github.com/vincentfung13/MINE,,,,,"@article{li2021mine,
  AUTHOR = {Jiaxin Li and Zijian Feng and Qi She and Henghui Ding and Changhu Wang and Gim Hee Lee},
  TITLE = {MINE: Towards Continuous Depth MPI with NeRF for Novel View Synthesis},
  EPRINT = {2103.14910v3},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {In this paper, we propose MINE to perform novel view synthesis and depthestimation via dense 3D reconstruction from a single image. Our approach is acontinuous depth generalization of the Multiplane Images (MPI) by introducingthe NEural radiance fields (NeRF). Given a single image as input, MINE predictsa 4-channel image (RGB and volume density) at arbitrary depth values to jointlyreconstruct the camera frustum and fill in occluded contents. The reconstructedand inpainted frustum can then be easily rendered into novel RGB or depth viewsusing differentiable rendering. Extensive experiments on RealEstate10K, KITTIand Flowers Light Fields show that our MINE outperforms state-of-the-art by alarge margin in novel view synthesis. We also achieve competitive results indepth estimation on iBims-1 and NYU-v2 without annotated depth supervision. Oursource code is available at https://github.com/vincentfung13/MINE},
  YEAR = {2021},
  MONTH = {Mar},
  URL = {http://arxiv.org/abs/2103.14910v3},
  FILE = {2103.14910v3.pdf}
 }","Few-shot reconstruction, Generalization","Image-based rendering, Representation, Data-driven component (pre-trained, cross-scene), Coordinate CNN",NeRF,MPI,,,,,,,,ICCV 2021,,,"Direct, Indirect","Jiaxin Li, Zijian Feng, Qi She, Henghui Ding, Changhu Wang, Gim Hee Lee",li2021mine,00000167,"In this paper, we propose MINE to perform novel view synthesis and depth estimation via dense 3D reconstruction from a single image. Our approach is a continuous depth generalization of the Multiplane Images (MPI) by introducing the NEural radiance fields (NeRF). Given a single image as input, MINE predicts a 4-channel image (RGB and volume density) at arbitrary depth values to jointly reconstruct the camera frustum and fill in occluded contents. The reconstructed and inpainted frustum can then be easily rendered into novel RGB or depth views using differentiable rendering. Extensive experiments on RealEstate10K, KITTI and Flowers Light Fields show that our MINE outperforms state-of-the-art by a large margin in novel view synthesis. We also achieve competitive results in depth estimation on iBims-1 and NYU-v2 without annotated depth supervision. Our source code is available at https://github.com/vincentfung13/MINE",0,,No,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnuf-8w9eB3ooWKj4HPEHiO4XwgLDsxQe3aHo68AX1XTZ_ENfzStaYOZTyuQSqxNMNuE
6/15/2021 16:07:55,In-Place Scene Labelling and Understanding with Implicit Scene Representation,Semantic-NeRF,3/29/2021,https://arxiv.org/pdf/2103.15875.pdf,,,,,,,"@article{zhi2021semanticnerf,
  AUTHOR = {Shuaifeng Zhi and Tristan Laidlow and Stefan Leutenegger and Andrew J. Davison},
  TITLE = {In-Place Scene Labelling and Understanding with Implicit SceneRepresentation},
  EPRINT = {2103.15875v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Semantic labelling is highly correlated with geometry and radiancereconstruction, as scene entities with similar shape and appearance are morelikely to come from similar classes. Recent implicit neural reconstructiontechniques are appealing as they do not require prior training data, but thesame fully self-supervised approach is not possible for semantics becauselabels are human-defined properties.We extend neural radiance fields (NeRF) to jointly encode semantics withappearance and geometry, so that complete and accurate 2D semantic labels canbe achieved using a small amount of in-place annotations specific to the scene.The intrinsic multi-view consistency and smoothness of NeRF benefit semanticsby enabling sparse labels to efficiently propagate. We show the benefit of thisapproach when labels are either sparse or very noisy in room-scale scenes. Wedemonstrate its advantageous properties in various interesting applicationssuch as an efficient scene labelling tool, novel semantic view synthesis, labeldenoising, super-resolution, label interpolation and multi-view semantic labelfusion in visual semantic mapping systems.},
  YEAR = {2021},
  MONTH = {Mar},
  URL = {http://arxiv.org/abs/2103.15875v2},
  FILE = {2103.15875v2.pdf}
 }","Segmentation/composition, Beyond graphics",,,,,,,,,,,ICCV,,,,"Shuaifeng Zhi, Tristan Laidlow, Stefan Leutenegger, Andrew J. Davison",zhi2021semanticnerf,00000088,"Semantic labelling is highly correlated with geometry and radiance reconstruction, as scene entities with similar shape and appearance are more likely to come from similar classes. Recent implicit neural reconstruction techniques are appealing as they do not require prior training data, but the same fully self-supervised approach is not possible for semantics because labels are human-defined properties. We extend neural radiance fields (NeRF) to jointly encode semantics with appearance and geometry, so that complete and accurate 2D semantic labels can be achieved using a small amount of in-place annotations specific to the scene. The intrinsic multi-view consistency and smoothness of NeRF benefit semantics by enabling sparse labels to efficiently propagate. We show the benefit of this approach when labels are either sparse or very noisy in room-scale scenes. We demonstrate its advantageous properties in various interesting applications such as an efficient scene labelling tool, novel semantic view synthesis, label denoising, super-resolution, label interpolation and multi-view semantic label fusion in visual semantic mapping systems.",1,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnud-1tEEydj5TUe_upzxjfX-bagtLrKh5AnFrjK8j7K3vi1PNKqM6JG2Ym2LNhoEpV4
5/23/2021 18:42:35,MVSNeRF: Fast Generalizable Radiance Field Reconstruction from Multi-View Stereo,NVSNeRF,3/29/2021,https://arxiv.org/pdf/2103.15595.pdf,https://apchenstu.github.io/mvsnerf/,https://github.com/apchenstu/mvsnerf,https://www.youtube.com/watch?v=68N21TacPxw,,,,"@article{chen2021nvsnerf,
  AUTHOR = {Anpei Chen and Zexiang Xu and Fuqiang Zhao and Xiaoshuai Zhang and Fanbo Xiang and Jingyi Yu and Hao Su},
  TITLE = {MVSNeRF: Fast Generalizable Radiance Field Reconstruction fromMulti-View Stereo},
  EPRINT = {2103.15595v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We present MVSNeRF, a novel neural rendering approach that can efficientlyreconstruct neural radiance fields for view synthesis. Unlike prior works onneural radiance fields that consider per-scene optimization on densely capturedimages, we propose a generic deep neural network that can reconstruct radiancefields from only three nearby input views via fast network inference. Ourapproach leverages plane-swept cost volumes (widely used in multi-view stereo)for geometry-aware scene reasoning, and combines this with physically basedvolume rendering for neural radiance field reconstruction. We train our networkon real objects in the DTU dataset, and test it on three different datasets toevaluate its effectiveness and generalizability. Our approach can generalizeacross scenes (even indoor scenes, completely different from our trainingscenes of objects) and generate realistic view synthesis results using onlythree input images, significantly outperforming concurrent works ongeneralizable radiance field reconstruction. Moreover, if dense images arecaptured, our estimated radiance field representation can be easily fine-tuned;this leads to fast per-scene reconstruction with higher rendering quality andsubstantially less optimization time than NeRF.},
  YEAR = {2021},
  MONTH = {Mar},
  URL = {http://arxiv.org/abs/2103.15595v2},
  FILE = {2103.15595v2.pdf}
 }","Performance (training), Few-shot reconstruction, Generalization","Per-instance fine-tuning, Lifting 2D features to 3D, Data-driven component (pre-trained, cross-scene)",,,,,,,,,,,,,,"Anpei Chen, Zexiang Xu, Fuqiang Zhao, Xiaoshuai Zhang, Fanbo Xiang, Jingyi Yu, Hao Su",chen2021nvsnerf,00000089,"We present MVSNeRF, a novel neural rendering approach that can efficiently reconstruct neural radiance fields for view synthesis. Unlike prior works on neural radiance fields that consider per-scene optimization on densely captured images, we propose a generic deep neural network that can reconstruct radiance fields from only three nearby input views via fast network inference. Our approach leverages plane-swept cost volumes (widely used in multi-view stereo) for geometry-aware scene reasoning, and combines this with physically based volume rendering for neural radiance field reconstruction. We train our network on real objects in the DTU dataset, and test it on three different datasets to evaluate its effectiveness and generalizability. Our approach can generalize across scenes (even indoor scenes, completely different from our training scenes of objects) and generate realistic view synthesis results using only three input images, significantly outperforming concurrent works on generalizable radiance field reconstruction. Moreover, if dense images are captured, our estimated radiance field representation can be easily fine-tuned; this leads to fast per-scene reconstruction with higher rendering quality and substantially less optimization time than NeRF.",4,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnufCRGfRnJXH1x8ppIEODnJZ2SkUhSoaFPrUvWxSYWkSYx9q-KETXEM__ghI77RuyEs
5/23/2021 18:43:52,GNeRF: GAN-based Neural Radiance Field without Posed Camera,GNeRF,3/29/2021,https://arxiv.org/pdf/2103.15606.pdf,,,,,,,"@article{meng2021gnerf,
  AUTHOR = {Quan Meng and Anpei Chen and Haimin Luo and Minye Wu and Hao Su and Lan Xu and Xuming He and Jingyi Yu},
  TITLE = {GNeRF: GAN-based Neural Radiance Field without Posed Camera},
  EPRINT = {2103.15606v3},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We introduce GNeRF, a framework to marry Generative Adversarial Networks(GAN) with Neural Radiance Field (NeRF) reconstruction for the complexscenarios with unknown and even randomly initialized camera poses. RecentNeRF-based advances have gained popularity for remarkable realistic novel viewsynthesis. However, most of them heavily rely on accurate camera posesestimation, while few recent methods can only optimize the unknown camera posesin roughly forward-facing scenes with relatively short camera trajectories andrequire rough camera poses initialization. Differently, our GNeRF only utilizesrandomly initialized poses for complex outside-in scenarios. We propose a noveltwo-phases end-to-end framework. The first phase takes the use of GANs into thenew realm for optimizing coarse camera poses and radiance fields jointly, whilethe second phase refines them with additional photometric loss. We overcomelocal minima using a hybrid and iterative optimization scheme. Extensiveexperiments on a variety of synthetic and natural scenes demonstrate theeffectiveness of GNeRF. More impressively, our approach outperforms thebaselines favorably in those scenes with repeated patterns or even low texturesthat are regarded as extremely challenging before.},
  YEAR = {2021},
  MONTH = {Mar},
  URL = {http://arxiv.org/abs/2103.15606v3},
  FILE = {2103.15606v3.pdf}
 }",Camera parameter estimation,Generative/adversarial formulation,,,,,,,,,,ICCV 2021,,,,"Quan Meng, Anpei Chen, Haimin Luo, Minye Wu, Hao Su, Lan Xu, Xuming He, Jingyi Yu",meng2021gnerf,00000090,"We introduce GNeRF, a framework to marry Generative Adversarial Networks (GAN) with Neural Radiance Field (NeRF) reconstruction for the complex scenarios with unknown and even randomly initialized camera poses. Recent NeRF-based advances have gained popularity for remarkable realistic novel view synthesis. However, most of them heavily rely on accurate camera poses estimation, while few recent methods can only optimize the unknown camera poses in roughly forward-facing scenes with relatively short camera trajectories and require rough camera poses initialization. Differently, our GNeRF only utilizes randomly initialized poses for complex outside-in scenarios. We propose a novel two-phases end-to-end framework. The first phase takes the use of GANs into the new realm for optimizing coarse camera poses and radiance fields jointly, while the second phase refines them with additional photometric loss. We overcome local minima using a hybrid and iterative optimization scheme. Extensive experiments on a variety of synthetic and natural scenes demonstrate the effectiveness of GNeRF. More impressively, our approach outperforms the baselines favorably in those scenes with repeated patterns or even low textures that are regarded as extremely challenging before.",1,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnudBRMM7qaYw7IxQSYrJgS-QRbT8NBjelU5_gbRUh398AbTUbfhIPwjDlbKFxp30A2I
5/23/2021 18:41:27,Unsupervised Learning of 3D Object Categories from Videos in the Wild,,3/30/2021,https://arxiv.org/pdf/2103.16552.pdf,https://henzler.github.io/publication/unsupervised_videos/,,,,,,"@article{henzler2021unsupervised,
  AUTHOR = {Philipp Henzler and Jeremy Reizenstein and Patrick Labatut and Roman Shapovalov and Tobias Ritschel and Andrea Vedaldi and David Novotny},
  TITLE = {Unsupervised Learning of 3D Object Categories from Videos in the Wild},
  EPRINT = {2103.16552v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Our goal is to learn a deep network that, given a small number of images ofan object of a given category, reconstructs it in 3D. While several recentworks have obtained analogous results using synthetic data or assuming theavailability of 2D primitives such as keypoints, we are interested in workingwith challenging real data and with no manual annotations. We thus focus onlearning a model from multiple views of a large collection of object instances.We contribute with a new large dataset of object centric videos suitable fortraining and benchmarking this class of models. We show that existingtechniques leveraging meshes, voxels, or implicit surfaces, which work well forreconstructing isolated objects, fail on this challenging data. Finally, wepropose a new neural network design, called warp-conditioned ray embedding(WCR), which significantly improves reconstruction while obtaining a detailedimplicit representation of the object surface and texture, also compensatingfor the noise in the initial SfM reconstruction that bootstrapped the learningprocess. Our evaluation demonstrates performance improvements over several deepmonocular reconstruction baselines on existing benchmarks and on our noveldataset.},
  YEAR = {2021},
  MONTH = {Mar},
  URL = {http://arxiv.org/abs/2103.16552v1},
  FILE = {2103.16552v1.pdf}
 }",Generalization,"Lifting 2D features to 3D, Data-driven component (pre-trained, cross-scene)",,,,,,,,,,,,,,"Philipp Henzler, Jeremy Reizenstein, Patrick Labatut, Roman Shapovalov, Tobias Ritschel, Andrea Vedaldi, David Novotny",henzler2021unsupervised,00000091,"Our goal is to learn a deep network that, given a small number of images of an object of a given category, reconstructs it in 3D. While several recent works have obtained analogous results using synthetic data or assuming the availability of 2D primitives such as keypoints, we are interested in working with challenging real data and with no manual annotations. We thus focus on learning a model from multiple views of a large collection of object instances. We contribute with a new large dataset of object centric videos suitable for training and benchmarking this class of models. We show that existing techniques leveraging meshes, voxels, or implicit surfaces, which work well for reconstructing isolated objects, fail on this challenging data. Finally, we propose a new neural network design, called warp-conditioned ray embedding (WCR), which significantly improves reconstruction while obtaining a detailed implicit representation of the object surface and texture, also compensating for the noise in the initial SfM reconstruction that bootstrapped the learning process. Our evaluation demonstrates performance improvements over several deep monocular reconstruction baselines on existing benchmarks and on our novel dataset.",2,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnueQI_iKPOE-mNo9gmmudgMtVr1za2DeWetBPI2KTxV5KZDn-HRIJ6htoJYZHhGXttk
5/23/2021 18:42:52,Foveated Neural Radiance Fields for Real-Time and Egocentric Virtual Reality,,3/30/2021,https://arxiv.org/pdf/2103.16365.pdf,,,,,,,"@article{deng2021foveated,
  AUTHOR = {Nianchen Deng and Zhenyi He and Jiannan Ye and Praneeth Chakravarthula and Xubo Yang and Qi Sun},
  TITLE = {Foveated Neural Radiance Fields for Real-Time and Egocentric VirtualReality},
  EPRINT = {2103.16365v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.GR},
  ABSTRACT = {Traditional high-quality 3D graphics requires large volumes of fine-detailedscene data for rendering. This demand compromises computational efficiency andlocal storage resources. Specifically, it becomes more concerning for futurewearable and portable virtual and augmented reality (VR/AR) displays. Recentapproaches to combat this problem include remote rendering/streaming and neuralrepresentations of 3D assets. These approaches have redefined the traditionallocal storage-rendering pipeline by distributed computing or compression oflarge data. However, these methods typically suffer from high latency or lowquality for practical visualization of large immersive virtual scenes, notablywith extra high resolution and refresh rate requirements for VR applicationssuch as gaming and design.Tailored for the future portable, low-storage, and energy-efficient VRplatforms, we present the first gaze-contingent 3D neural representation andview synthesis method. We incorporate the human psychophysics of visual- andstereo-acuity into an egocentric neural representation of 3D scenery.Furthermore, we jointly optimize the latency/performance and visual quality,while mutually bridging human perception and neural scene synthesis, to achieveperceptually high-quality immersive interaction. Both objective analysis andsubjective study demonstrate the effectiveness of our approach in significantlyreducing local storage volume and synthesis latency (up to 99% reduction inboth data size and computational time), while simultaneously presentinghigh-fidelity rendering, with perceptual quality identical to that of fullylocally stored and rendered high-quality imagery.},
  YEAR = {2021},
  MONTH = {Mar},
  URL = {http://arxiv.org/abs/2103.16365v1},
  FILE = {2103.16365v1.pdf}
 }","Performance (rendering), Compression","Sampling, Representation",,,,,,,,,,,,,,"Nianchen Deng, Zhenyi He, Jiannan Ye, Praneeth Chakravarthula, Xubo Yang, Qi Sun",deng2021foveated,00000092,"Traditional high-quality 3D graphics requires large volumes of fine-detailed scene data for rendering. This demand compromises computational efficiency and local storage resources. Specifically, it becomes more concerning for future wearable and portable virtual and augmented reality (VR/AR) displays. Recent approaches to combat this problem include remote rendering/streaming and neural representations of 3D assets. These approaches have redefined the traditional local storage-rendering pipeline by distributed computing or compression of large data. However, these methods typically suffer from high latency or low quality for practical visualization of large immersive virtual scenes, notably with extra high resolution and refresh rate requirements for VR applications such as gaming and design. Tailored for the future portable, low-storage, and energy-efficient VR platforms, we present the first gaze-contingent 3D neural representation and view synthesis method. We incorporate the human psychophysics of visual- and stereo-acuity into an egocentric neural representation of 3D scenery. Furthermore, we jointly optimize the latency/performance and visual quality, while mutually bridging human perception and neural scene synthesis, to achieve perceptually high-quality immersive interaction. Both objective analysis and subjective study demonstrate the effectiveness of our approach in significantly reducing local storage volume and synthesis latency (up to 99% reduction in both data size and computational time), while simultaneously presenting high-fidelity rendering, with perceptual quality identical to that of fully locally stored and rendered high-quality imagery.",0,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnucFfDlUBieCXztSVteti5w5Nm_Pl-_FvPtBeGUQxKbAiZYkURFZ9tFNxDP9KgJHMts
5/23/2021 18:43:20,CAMPARI: Camera-Aware Decomposed Generative Neural Radiance Fields,CAMPARI,3/31/2021,https://arxiv.org/pdf/2103.17269.pdf,,,,,,,"@article{niemeyer2021campari,
  AUTHOR = {Michael Niemeyer and Andreas Geiger},
  TITLE = {CAMPARI: Camera-Aware Decomposed Generative Neural Radiance Fields},
  EPRINT = {2103.17269v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Tremendous progress in deep generative models has led to photorealistic imagesynthesis. While achieving compelling results, most approaches operate in thetwo-dimensional image domain, ignoring the three-dimensional nature of ourworld. Several recent works therefore propose generative models which are3D-aware, i.e., scenes are modeled in 3D and then rendered differentiably tothe image plane. This leads to impressive 3D consistency, but incorporatingsuch a bias comes at a price: the camera needs to be modeled as well. Currentapproaches assume fixed intrinsics and a predefined prior over camera poseranges. As a result, parameter tuning is typically required for real-worlddata, and results degrade if the data distribution is not matched. Our keyhypothesis is that learning a camera generator jointly with the image generatorleads to a more principled approach to 3D-aware image synthesis. Further, wepropose to decompose the scene into a background and foreground model, leadingto more efficient and disentangled scene representations. While training fromraw, unposed image collections, we learn a 3D- and camera-aware generativemodel which faithfully recovers not only the image but also the camera datadistribution. At test time, our model generates images with explicit controlover the camera as well as the shape and appearance of the scene.},
  YEAR = {2021},
  MONTH = {Mar},
  URL = {http://arxiv.org/abs/2103.17269v1},
  FILE = {2103.17269v1.pdf}
 }",Image,"Generative/adversarial formulation, Conditional neural field, Lifting 2D features to 3D, Volume partitioning, Data-driven component (pre-trained, cross-scene)",,,,,,,,,,,,,,"Michael Niemeyer, Andreas Geiger",niemeyer2021campari,00000093,"Tremendous progress in deep generative models has led to photorealistic image synthesis. While achieving compelling results, most approaches operate in the two-dimensional image domain, ignoring the three-dimensional nature of our world. Several recent works therefore propose generative models which are 3D-aware, i.e., scenes are modeled in 3D and then rendered differentiably to the image plane. This leads to impressive 3D consistency, but incorporating such a bias comes at a price: the camera needs to be modeled as well. Current approaches assume fixed intrinsics and a predefined prior over camera pose ranges. As a result, parameter tuning is typically required for real-world data, and results degrade if the data distribution is not matched. Our key hypothesis is that learning a camera generator jointly with the image generator leads to a more principled approach to 3D-aware image synthesis. Further, we propose to decompose the scene into a background and foreground model, leading to more efficient and disentangled scene representations. While training from raw, unposed image collections, we learn a 3D- and camera-aware generative model which faithfully recovers not only the image but also the camera data distribution. At test time, our model generates images with explicit control over the camera as well as the shape and appearance of the scene.",2,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnuewjJrUlO5ufAa76jpu9zQcP9dANhUsP0jZ7TGXS9psvo1ZYusKtoEjPBpPSLg9UoE
5/23/2021 18:37:12,NPMs: Neural Parametric Models for 3D Deformable Shapes,NPMs,4/1/2021,https://arxiv.org/pdf/2104.00702.pdf,,,,,,,"@article{palafox2021npms,
  AUTHOR = {Pablo Palafox and Aljaz Bozic and Justus Thies and Matthias Niessner and Angela Dai},
  TITLE = {NPMs: Neural Parametric Models for 3D Deformable Shapes},
  EPRINT = {2104.00702v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Parametric 3D models have enabled a wide variety of tasks in computergraphics and vision, such as modeling human bodies, faces, and hands. However,the construction of these parametric models is often tedious, as it requiresheavy manual tweaking, and they struggle to represent additional complexity anddetails such as wrinkles or clothing. To this end, we propose Neural ParametricModels (NPMs), a novel, learned alternative to traditional, parametric 3Dmodels, which does not require hand-crafted, object-specific constraints. Inparticular, we learn to disentangle 4D dynamics into latent-spacerepresentations of shape and pose, leveraging the flexibility of recentdevelopments in learned implicit functions. Crucially, once learned, our neuralparametric models of shape and pose enable optimization over the learned spacesto fit to new observations, similar to the fitting of a traditional parametricmodel, e.g., SMPL. This enables NPMs to achieve a significantly more accurateand detailed representation of observed deformable sequences. We show that NPMsimprove notably over both parametric and non-parametric state of the art inreconstruction and tracking of monocular depth sequences of clothed humans andhands. Latent-space interpolation as well as shape/pose transfer experimentsfurther demonstrate the usefulness of NPMs. Code is publicly available athttps://pablopalafox.github.io/npms.},
  YEAR = {2021},
  MONTH = {Apr},
  URL = {http://arxiv.org/abs/2104.00702v2},
  FILE = {2104.00702v2.pdf}
 }",Human body,"Conditional neural field, Warping field/Flow field",,SDF,,,,,,,,ICCV,,,,"Pablo Palafox, Aljaž Božič, Justus Thies, Matthias Nießner, Angela Dai",palafox2021npms,00000094,"Parametric 3D models have enabled a wide variety of tasks in computer graphics and vision, such as modeling human bodies, faces, and hands. However, the construction of these parametric models is often tedious, as it requires heavy manual tweaking, and they struggle to represent additional complexity and details such as wrinkles or clothing. To this end, we propose Neural Parametric Models (NPMs), a novel, learned alternative to traditional, parametric 3D models, which does not require hand-crafted, object-specific constraints. In particular, we learn to disentangle 4D dynamics into latent-space representations of shape and pose, leveraging the flexibility of recent developments in learned implicit functions. Crucially, once learned, our neural parametric models of shape and pose enable optimization over the learned spaces to fit to new observations, similar to the fitting of a traditional parametric model, e.g., SMPL. This enables NPMs to achieve a significantly more accurate and detailed representation of observed deformable sequences. We show that NPMs improve notably over both parametric and non-parametric state of the art in reconstruction and tracking of monocular depth sequences of clothed humans and hands. Latent-space interpolation as well as shape/pose transfer experiments further demonstrate the usefulness of NPMs. Code is publicly available at https://pablopalafox.github.io/npms.",2,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnufBg_lPvwXTxdJBlp3fWFnD-1pRfBvNIzv1y97a0G9eJXboSouwehufNAEww64CVh8
5/23/2021 18:39:26,RGB-D Local Implicit Function for Depth Completion of Transparent Objects,,4/1/2021,https://arxiv.org/pdf/2104.00622.pdf,https://research.nvidia.com/publication/2021-03_RGB-D-Local-Implicit,Coming soon,,,,,"@article{zhu2021rgbd,
  AUTHOR = {Luyang Zhu and Arsalan Mousavian and Yu Xiang and Hammad Mazhar and Jozef van Eenbergen and Shoubhik Debnath and Dieter Fox},
  TITLE = {RGB-D Local Implicit Function for Depth Completion of TransparentObjects},
  EPRINT = {2104.00622v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Majority of the perception methods in robotics require depth informationprovided by RGB-D cameras. However, standard 3D sensors fail to capture depthof transparent objects due to refraction and absorption of light. In thispaper, we introduce a new approach for depth completion of transparent objectsfrom a single RGB-D image. Key to our approach is a local implicit neuralrepresentation built on ray-voxel pairs that allows our method to generalize tounseen objects and achieve fast inference speed. Based on this representation,we present a novel framework that can complete missing depth given noisy RGB-Dinput. We further improve the depth estimation iteratively using aself-correcting refinement model. To train the whole pipeline, we build a largescale synthetic dataset with transparent objects. Experiments demonstrate thatour method performs significantly better than the current state-of-the-artmethods on both synthetic and real world data. In addition, our approachimproves the inference speed by a factor of 20 compared to the previous bestmethod, ClearGrasp. Code and dataset will be released athttps://research.nvidia.com/publication/2021-03_RGB-D-Local-Implicit.},
  YEAR = {2021},
  MONTH = {Apr},
  URL = {http://arxiv.org/abs/2104.00622v1},
  FILE = {2104.00622v1.pdf}
 }","Challenging materials (fur, hair, transparency), Beyond graphics",Voxelization,,,,,,,,,,CVPR 2021,,,,"Luyang Zhu, Arsalan Mousavian, Yu Xiang, Hammad Mazhar, Jozef van Eenbergen, Shoubhik Debnath, Dieter Fox",zhu2021rgbd,00000095,"Majority of the perception methods in robotics require depth information provided by RGB-D cameras. However, standard 3D sensors fail to capture depth of transparent objects due to refraction and absorption of light. In this paper, we introduce a new approach for depth completion of transparent objects from a single RGB-D image. Key to our approach is a local implicit neural representation built on ray-voxel pairs that allows our method to generalize to unseen objects and achieve fast inference speed. Based on this representation, we present a novel framework that can complete missing depth given noisy RGB-D input. We further improve the depth estimation iteratively using a self-correcting refinement model. To train the whole pipeline, we build a large scale synthetic dataset with transparent objects. Experiments demonstrate that our method performs significantly better than the current state-of-the-art methods on both synthetic and real world data. In addition, our approach improves the inference speed by a factor of 20 compared to the previous best method, ClearGrasp. Code and dataset will be released at https://research.nvidia.com/publication/2021-03_RGB-D-Local-Implicit.",1,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnuc2W9jUM1S8IXOAVQPfg4wMNCmtenNV3UFX73-_zBSpILVq2Ciolwvo8fZC5L7hdl8
5/25/2021 14:56:49,PhySG: Inverse Rendering with Spherical Gaussians for Physics-based Material Editing and Relighting,PhySG,4/1/2021,https://arxiv.org/pdf/2104.00674.pdf,https://kai-46.github.io/PhySG-website/,Coming soon,Coming soon,,,,"@article{zhang2021physg,
  AUTHOR = {Kai Zhang and Fujun Luan and Qianqian Wang and Kavita Bala and Noah Snavely},
  TITLE = {PhySG: Inverse Rendering with Spherical Gaussians for Physics-basedMaterial Editing and Relighting},
  EPRINT = {2104.00674v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We present PhySG, an end-to-end inverse rendering pipeline that includes afully differentiable renderer and can reconstruct geometry, materials, andillumination from scratch from a set of RGB input images. Our frameworkrepresents specular BRDFs and environmental illumination using mixtures ofspherical Gaussians, and represents geometry as a signed distance functionparameterized as a Multi-Layer Perceptron. The use of spherical Gaussiansallows us to efficiently solve for approximate light transport, and our methodworks on scenes with challenging non-Lambertian reflectance captured undernatural, static illumination. We demonstrate, with both synthetic and realdata, that our reconstructions not only enable rendering of novel viewpoints,but also physics-based appearance editing of materials and illumination.},
  YEAR = {2021},
  MONTH = {Apr},
  URL = {http://arxiv.org/abs/2104.00674v1},
  FILE = {2104.00674v1.pdf}
 }",Material/lighting estimation,,,SDF,,,,,,,,CVPR 2021,,,,"Kai Zhang, Fujun Luan, Qianqian Wang, Kavita Bala, Noah Snavely",zhang2021physg,00000096,"We present PhySG, an end-to-end inverse rendering pipeline that includes a fully differentiable renderer and can reconstruct geometry, materials, and illumination from scratch from a set of RGB input images. Our framework represents specular BRDFs and environmental illumination using mixtures of spherical Gaussians, and represents geometry as a signed distance function parameterized as a Multi-Layer Perceptron. The use of spherical Gaussians allows us to efficiently solve for approximate light transport, and our method works on scenes with challenging non-Lambertian reflectance captured under natural, static illumination. We demonstrate, with both synthetic and real data, that our reconstructions not only enable rendering of novel viewpoints, but also physics-based appearance editing of materials and illumination.",2,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnufNW5UI-hqKb1XR6wOsa9dFkSgTelnmb1A3JB_hbC4uCS5fK3z4ufQGt142vqFRoyA
5/23/2021 18:39:59,NeRF-VAE: A Geometry Aware 3D Scene Generative Model,NeRF-VAE,4/1/2021,https://arxiv.org/pdf/2104.00587.pdf,,,https://www.youtube.com/watch?v=f-T3BLVuXkY,,,,"@article{kosiorek2021nerfvae,
  AUTHOR = {Adam R. Kosiorek and Heiko Strathmann and Daniel Zoran and Pol Moreno and Rosalia Schneider and Sona Mokra and Danilo J. Rezende},
  TITLE = {NeRF-VAE: A Geometry Aware 3D Scene Generative Model},
  EPRINT = {2104.00587v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {stat.ML},
  ABSTRACT = {We propose NeRF-VAE, a 3D scene generative model that incorporates geometricstructure via NeRF and differentiable volume rendering. In contrast to NeRF,our model takes into account shared structure across scenes, and is able toinfer the structure of a novel scene -- without the need to re-train -- usingamortized inference. NeRF-VAE's explicit 3D rendering process further contrastsprevious generative models with convolution-based rendering which lacksgeometric structure. Our model is a VAE that learns a distribution overradiance fields by conditioning them on a latent scene representation. We showthat, once trained, NeRF-VAE is able to infer and rendergeometrically-consistent scenes from previously unseen 3D environments usingvery few input images. We further demonstrate that NeRF-VAE generalizes well toout-of-distribution cameras, while convolutional models do not. Finally, weintroduce and study an attention-based conditioning mechanism of NeRF-VAE'sdecoder, which improves model performance.},
  YEAR = {2021},
  MONTH = {Apr},
  URL = {http://arxiv.org/abs/2104.00587v1},
  FILE = {2104.00587v1.pdf}
 }","Few-shot reconstruction, Generalization, Image","Generative/adversarial formulation, Conditional neural field",,,,,,,,,,CVPR,,,,"Adam R. Kosiorek, Heiko Strathmann, Daniel Zoran, Pol Moreno, Rosalia Schneider, Soňa Mokrá, Danilo J. Rezende",kosiorek2021nerfvae,00000097,"We propose NeRF-VAE, a 3D scene generative model that incorporates geometric structure via NeRF and differentiable volume rendering. In contrast to NeRF, our model takes into account shared structure across scenes, and is able to infer the structure of a novel scene -- without the need to re-train -- using amortized inference. NeRF-VAE's explicit 3D rendering process further contrasts previous generative models with convolution-based rendering which lacks geometric structure. Our model is a VAE that learns a distribution over radiance fields by conditioning them on a latent scene representation. We show that, once trained, NeRF-VAE is able to infer and render geometrically-consistent scenes from previously unseen 3D environments using very few input images. We further demonstrate that NeRF-VAE generalizes well to out-of-distribution cameras, while convolutional models do not. Finally, we introduce and study an attention-based conditioning mechanism of NeRF-VAE's decoder, which improves model performance.",6,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnufZFu2M2l8Ty0_r7hzzy6d6igN-xH6xsN9l1Nc1bWYcR_2l61rIsubU5aN2QaZl56g
5/23/2021 18:40:23,Unconstrained Scene Generation with Locally Conditioned Radiance Fields,,4/1/2021,https://arxiv.org/pdf/2104.00670.pdf,https://apple.github.io/ml-gsn/,https://github.com/apple/ml-gsn,,,,,"@article{devries2021unconstrained,
  AUTHOR = {Terrance DeVries and Miguel Angel Bautista and Nitish Srivastava and Graham W. Taylor and Joshua M. Susskind},
  TITLE = {Unconstrained Scene Generation with Locally Conditioned Radiance Fields},
  EPRINT = {2104.00670v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We tackle the challenge of learning a distribution over complex, realistic,indoor scenes. In this paper, we introduce Generative Scene Networks (GSN),which learns to decompose scenes into a collection of many local radiancefields that can be rendered from a free moving camera. Our model can be used asa prior to generate new scenes, or to complete a scene given only sparse 2Dobservations. Recent work has shown that generative models of radiance fieldscan capture properties such as multi-view consistency and view-dependentlighting. However, these models are specialized for constrained viewing ofsingle objects, such as cars or faces. Due to the size and complexity ofrealistic indoor environments, existing models lack the representationalcapacity to adequately capture them. Our decomposition scheme scales to largerand more complex scenes while preserving details and diversity, and the learnedprior enables high-quality rendering from viewpoints that are significantlydifferent from observed viewpoints. When compared to existing models, GSNproduces quantitatively higher-quality scene renderings across severaldifferent scene datasets.},
  YEAR = {2021},
  MONTH = {Apr},
  URL = {http://arxiv.org/abs/2104.00670v1},
  FILE = {2104.00670v1.pdf}
 }",,"Generative/adversarial formulation, Conditional neural field, Feature volume",,,,,,,,,,,,,,"Terrance DeVries, Miguel Angel Bautista, Nitish Srivastava, Graham W. Taylor, Joshua M. Susskind",devries2021unconstrained,00000098,"We tackle the challenge of learning a distribution over complex, realistic, indoor scenes. In this paper, we introduce Generative Scene Networks (GSN), which learns to decompose scenes into a collection of many local radiance fields that can be rendered from a free moving camera. Our model can be used as a prior to generate new scenes, or to complete a scene given only sparse 2D observations. Recent work has shown that generative models of radiance fields can capture properties such as multi-view consistency and view-dependent lighting. However, these models are specialized for constrained viewing of single objects, such as cars or faces. Due to the size and complexity of realistic indoor environments, existing models lack the representational capacity to adequately capture them. Our decomposition scheme scales to larger and more complex scenes while preserving details and diversity, and the learned prior enables high-quality rendering from viewpoints that are significantly different from observed viewpoints. When compared to existing models, GSN produces quantitatively higher-quality scene renderings across several different scene datasets.",0,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnueHcye4R-QH9p5_pvlLrH25NK8AkChUlfkrqmYRkO9j4EEovKQlAHez8fQaPoG1AJY
5/23/2021 18:41:00,Putting NeRF on a Diet: Semantically Consistent Few-Shot View Synthesis,DietNeRF,4/1/2021,https://arxiv.org/pdf/2104.00677.pdf,https://www.ajayj.com/dietnerf,"Coming soon, https://github.com/codestella/putting-nerf-on-a-diet",https://www.youtube.com/watch?v=RF_3hsNizqw,,,,"@article{jain2021dietnerf,
  AUTHOR = {Ajay Jain and Matthew Tancik and Pieter Abbeel},
  TITLE = {Putting NeRF on a Diet: Semantically Consistent Few-Shot View Synthesis},
  EPRINT = {2104.00677v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We present DietNeRF, a 3D neural scene representation estimated from a fewimages. Neural Radiance Fields (NeRF) learn a continuous volumetricrepresentation of a scene through multi-view consistency, and can be renderedfrom novel viewpoints by ray casting. While NeRF has an impressive ability toreconstruct geometry and fine details given many images, up to 100 forchallenging 360{\deg} scenes, it often finds a degenerate solution to its imagereconstruction objective when only a few input views are available. To improvefew-shot quality, we propose DietNeRF. We introduce an auxiliary semanticconsistency loss that encourages realistic renderings at novel poses. DietNeRFis trained on individual scenes to (1) correctly render given input views fromthe same pose, and (2) match high-level semantic attributes across different,random poses. Our semantic loss allows us to supervise DietNeRF from arbitraryposes. We extract these semantics using a pre-trained visual encoder such asCLIP, a Vision Transformer trained on hundreds of millions of diversesingle-view, 2D photographs mined from the web with natural languagesupervision. In experiments, DietNeRF improves the perceptual quality offew-shot view synthesis when learned from scratch, can render novel views withas few as one observed image when pre-trained on a multi-view dataset, andproduces plausible completions of completely unobserved regions.},
  YEAR = {2021},
  MONTH = {Apr},
  URL = {http://arxiv.org/abs/2104.00677v1},
  FILE = {2104.00677v1.pdf}
 }",Few-shot reconstruction,"Lifting 2D features to 3D, Data-driven component (pre-trained, cross-scene)",,,,,,,,,,,,,,"Ajay Jain, Matthew Tancik, Pieter Abbeel",jain2021dietnerf,00000099,"We present DietNeRF, a 3D neural scene representation estimated from a few images. Neural Radiance Fields (NeRF) learn a continuous volumetric representation of a scene through multi-view consistency, and can be rendered from novel viewpoints by ray casting. While NeRF has an impressive ability to reconstruct geometry and fine details given many images, up to 100 for challenging 360{\deg} scenes, it often finds a degenerate solution to its image reconstruction objective when only a few input views are available. To improve few-shot quality, we propose DietNeRF. We introduce an auxiliary semantic consistency loss that encourages realistic renderings at novel poses. DietNeRF is trained on individual scenes to (1) correctly render given input views from the same pose, and (2) match high-level semantic attributes across different, random poses. Our semantic loss allows us to supervise DietNeRF from arbitrary poses. We extract these semantics using a pre-trained visual encoder such as CLIP, a Vision Transformer trained on hundreds of millions of diverse single-view, 2D photographs mined from the web with natural language supervision. In experiments, DietNeRF improves the perceptual quality of few-shot view synthesis when learned from scratch, can render novel views with as few as one observed image when pre-trained on a multi-view dataset, and produces plausible completions of completely unobserved regions.",0,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnuf9bV4EhH_BpR0snxpYou2lFMdXYIxbZsm6v9GnOWT4ZYFVemQPUpWK2aI0fBc-d5o
7/19/2021 21:26:43,Multi-scene Representation Learning with Neural Radiance Fields,,4/1/2021,https://iopscience.iop.org/article/10.1088/1742-6596/1880/1/012034,,,,,,,"@article{Fu_2021,
  DOI = {10.1088/1742-6596/1880/1/012034},
  URL = {https://doi.org/10.1088/1742-6596/1880/1/012034},
  YEAR = {apr},
  PUBLISHER = {{IOP} Publishing},
  VOLUME = {1880},
  NUMBER = {1},
  PAGES = {012034},
  AUTHOR = {Bofeng Fu and Zheng Wang},
  TITLE = {Multi-scene Representation Learning with Neural Radiance Fields},
  JOURNAL = {Journal of Physics: Conference Series},
  ABSTRACT = {Getting representations of multiple objects or scenes is a raising research topic in Machine Learning (ML) community. Here, we propose a multi-scene representation model that can learn the representation of complex scenes and reconstruct them in high resolution given novel viewing directions. Our method represents a single scene with fully-connected layers. Each set of fully-connected layers are controlled by hyper-networks for multiple scenes modeling. For each scene, we take 3D coordinates (x, y, z) and 2D view-point orientations (I,, E,) as inputs. A set of fully-connected layers output volume density and RGB values at given 3D spatial positions. Then, we render the output volume density and RGB values along the camera rays into images using volume density rendering techniques. During training process, we optimize a continuous volume scene function with a small amount of input viewing directions. By designing versatile embedding module and multi-scene representation networks, our model can render photographic images with novel viewing directions for different complex scenes. Experiment results demonstrate the neural rendering and multi-scene representation abilities of our model. Several thorough experiments show that our method outperforms previous model on both reconstruction precision and scenes generation ability from novel viewing directions.}
 }",Generalization,,NeRF,Density,,,,,,,,,,No,Direct,"Bofeng Fu, Zheng Wang",fu2021multiscene,00000100,"Getting representations of multiple objects or scenes is a raising research topic in Machine Learning (ML) community. Here, we propose a multi-scene representation model that can learn the representation of complex scenes and reconstruct them in high resolution given novel viewing directions. Our method represents a single scene with fully-connected layers. Each set of fully-connected layers are controlled by hyper-networks for multiple scenes modeling. For each scene, we take 3D coordinates (x, y, z) and 2D view-point orientations (θ, ɸ) as inputs. A set of fully-connected layers output volume density and RGB values at given 3D spatial positions. Then, we render the output volume density and RGB values along the camera rays into images using volume density rendering techniques. During training process, we optimize a continuous volume scene function with a small amount of input viewing directions. By designing versatile embedding module and multi-scene representation networks, our model can render photographic images with novel viewing directions for different complex scenes. Experiment results demonstrate the neural rendering and multi-scene representation abilities of our model. Several thorough experiments show that our method outperforms previous model on both reconstruction precision and scenes generation ability from novel viewing directions.",0,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnucu9xHWkIXpEh4VGdR-1xqOAzS2iZigXAhZ7in2kHougqMb6_HoRxNSg2B8DjN4ZTc
5/23/2021 18:37:37,Decomposing 3D Scenes into Objects via Unsupervised Volume Segmentation,OBSuRF,4/2/2021,https://arxiv.org/pdf/2104.01148.pdf,https://stelzner.github.io/obsurf/,Coming soon,,,,,"@article{stelzner2021obsurf,
  AUTHOR = {Karl Stelzner and Kristian Kersting and Adam R. Kosiorek},
  TITLE = {Decomposing 3D Scenes into Objects via Unsupervised Volume Segmentation},
  EPRINT = {2104.01148v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We present ObSuRF, a method which turns a single image of a scene into a 3Dmodel represented as a set of Neural Radiance Fields (NeRFs), with each NeRFcorresponding to a different object. A single forward pass of an encodernetwork outputs a set of latent vectors describing the objects in the scene.These vectors are used independently to condition a NeRF decoder, defining thegeometry and appearance of each object. We make learning more computationallyefficient by deriving a novel loss, which allows training NeRFs on RGB-D inputswithout explicit ray marching. After confirming that the model performs equalor better than state of the art on three 2D image segmentation benchmarks, weapply it to two multi-object 3D datasets: A multiview version of CLEVR, and anovel dataset in which scenes are populated by ShapeNet models. We find thatafter training ObSuRF on RGB-D views of training scenes, it is capable of notonly recovering the 3D geometry of a scene depicted in a single input image,but also to segment it into objects, despite receiving no supervision in thatregard.},
  YEAR = {2021},
  MONTH = {Apr},
  URL = {http://arxiv.org/abs/2104.01148v1},
  FILE = {2104.01148v1.pdf}
 }",Segmentation/composition,"Conditional neural field, Volume partitioning, Object-centric representation",,,,,,,,,,CVPR,Coming soon,,,"Karl Stelzner, Kristian Kersting, Adam R. Kosiorek",stelzner2021obsurf,00000101,"We present ObSuRF, a method which turns a single image of a scene into a 3D model represented as a set of Neural Radiance Fields (NeRFs), with each NeRF corresponding to a different object. A single forward pass of an encoder network outputs a set of latent vectors describing the objects in the scene. These vectors are used independently to condition a NeRF decoder, defining the geometry and appearance of each object. We make learning more computationally efficient by deriving a novel loss, which allows training NeRFs on RGB-D inputs without explicit ray marching. After confirming that the model performs equal or better than state of the art on three 2D image segmentation benchmarks, we apply it to two multi-object 3D datasets: A multiview version of CLEVR, and a novel dataset in which scenes are populated by ShapeNet models. We find that after training ObSuRF on RGB-D views of training scenes, it is capable of not only recovering the 3D geometry of a scene depicted in a single input image, but also to segment it into objects, despite receiving no supervision in that regard.",1,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnufwTgfSTzHUXiHcwsfPmGXn3_oJmEf3RAfVdd4_0BDZMiV9hf_Vl6zQbXtaF7qi4Ik
5/23/2021 18:38:52,Synergies Between Affordance and Geometry: 6-DoF Grasp Detection via Implicit Representations,GIGA,4/4/2021,https://arxiv.org/pdf/2104.01542.pdf,https://sites.google.com/view/rpl-giga2021,https://github.com/UT-Austin-RPL/GIGA,,,,,"@article{jiang2021giga,
  AUTHOR = {Zhenyu Jiang and Yifeng Zhu and Maxwell Svetlik and Kuan Fang and Yuke Zhu},
  TITLE = {Synergies Between Affordance and Geometry: 6-DoF Grasp Detection viaImplicit Representations},
  EPRINT = {2104.01542v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.RO},
  ABSTRACT = {Grasp detection in clutter requires the robot to reason about the 3D scenefrom incomplete and noisy perception. In this work, we draw insight that 3Dreconstruction and grasp learning are two intimately connected tasks, both ofwhich require a fine-grained understanding of local geometry details. We thuspropose to utilize the synergies between grasp affordance and 3D reconstructionthrough multi-task learning of a shared representation. Our model takesadvantage of deep implicit functions, a continuous and memory-efficientrepresentation, to enable differentiable training of both tasks. We train themodel on self-supervised grasp trials data in simulation. Evaluation isconducted on a clutter removal task, where the robot clears cluttered objectsby grasping them one at a time. The experimental results in simulation and onthe real robot have demonstrated that the use of implicit neuralrepresentations and joint learning of grasp affordance and 3D reconstructionhave led to state-of-the-art grasping results. Our method outperforms baselinesby over 10% in terms of grasp success rate. Additional results and videos canbe found at https://sites.google.com/view/rpl-giga2021},
  YEAR = {2021},
  MONTH = {Apr},
  URL = {http://arxiv.org/abs/2104.01542v2},
  FILE = {2104.01542v2.pdf}
 }","Generalization, Beyond graphics, Science and engineering, Robotics","Voxelization, Feature volume, Data-driven component (pre-trained, cross-scene)",,,,,,,,,,,,,,"Zhenyu Jiang, Yifeng Zhu, Maxwell Svetlik, Kuan Fang, Yuke Zhu",jiang2021giga,00000102,"Grasp detection in clutter requires the robot to reason about the 3D scene from incomplete and noisy perception. In this work, we draw insight that 3D reconstruction and grasp learning are two intimately connected tasks, both of which require a fine-grained understanding of local geometry details. We thus propose to utilize the synergies between grasp affordance and 3D reconstruction through multi-task learning of a shared representation. Our model takes advantage of deep implicit functions, a continuous and memory-efficient representation, to enable differentiable training of both tasks. We train the model on self-supervised grasp trials data in simulation. Evaluation is conducted on a clutter removal task, where the robot clears cluttered objects by grasping them one at a time. The experimental results in simulation and on the real robot have demonstrated that the use of implicit neural representations and joint learning of grasp affordance and 3D reconstruction have led to state-of-the-art grasping results. Our method outperforms baselines by over 10% in terms of grasp success rate. Additional results and videos can be found at https://sites.google.com/view/rpl-giga2021",1,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnuciscMB2BnQ3IwmONuEWTrWooUtw-w9rFyXIpRhpomlrhUij3cMK7xurOt4OjcvM10
5/23/2021 18:18:26,pi-GAN: Periodic Implicit Generative Adversarial Networks for 3D-Aware Image Synthesis,pi-GAN,4/5/2021,https://arxiv.org/pdf/2012.00926.pdf,https://marcoamonteiro.github.io/pi-GAN-website/,Coming soon,https://www.youtube.com/watch?v=0HCdof9BGtw,,,,"@article{chan2020pigan,
  AUTHOR = {Eric R. Chan and Marco Monteiro and Petr Kellnhofer and Jiajun Wu and Gordon Wetzstein},
  TITLE = {pi-GAN: Periodic Implicit Generative Adversarial Networks for 3D-AwareImage Synthesis},
  EPRINT = {2012.00926v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We have witnessed rapid progress on 3D-aware image synthesis, leveragingrecent advances in generative visual models and neural rendering. Existingapproaches however fall short in two ways: first, they may lack an underlying3D representation or rely on view-inconsistent rendering, hence synthesizingimages that are not multi-view consistent; second, they often depend uponrepresentation network architectures that are not expressive enough, and theirresults thus lack in image quality. We propose a novel generative model, namedPeriodic Implicit Generative Adversarial Networks ($\pi$-GAN or pi-GAN), forhigh-quality 3D-aware image synthesis. $\pi$-GAN leverages neuralrepresentations with periodic activation functions and volumetric rendering torepresent scenes as view-consistent 3D representations with fine detail. Theproposed approach obtains state-of-the-art results for 3D-aware image synthesiswith multiple real and synthetic datasets.},
  YEAR = {2020},
  MONTH = {Dec},
  URL = {http://arxiv.org/abs/2012.00926v2},
  FILE = {2012.00926v2.pdf}
 }",Generalization,"Generative/adversarial formulation, Conditional neural field",SIREN,,,,,,,,,,,,,"Eric R. Chan, Marco Monteiro, Petr Kellnhofer, Jiajun Wu, Gordon Wetzstein",chan2020pigan,00000103,"We have witnessed rapid progress on 3D-aware image synthesis, leveraging recent advances in generative visual models and neural rendering. Existing approaches however fall short in two ways: first, they may lack an underlying 3D representation or rely on view-inconsistent rendering, hence synthesizing images that are not multi-view consistent; second, they often depend upon representation network architectures that are not expressive enough, and their results thus lack in image quality. We propose a novel generative model, named Periodic Implicit Generative Adversarial Networks ($\pi$-GAN or pi-GAN), for high-quality 3D-aware image synthesis. $\pi$-GAN leverages neural representations with periodic activation functions and volumetric rendering to represent scenes as view-consistent 3D representations with fine detail. The proposed approach obtains state-of-the-art results for 3D-aware image synthesis with multiple real and synthetic datasets.",20,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnuefyKLYDFPTgEe9z6g59cIDu2YqjbwgjXgH-j5aRzVtk2fOHbnbCLnE0mtvwqr0fBs
5/23/2021 18:35:55,Convolutional Neural Opacity Radiance Fields,,4/5/2021,https://arxiv.org/pdf/2104.01772.pdf,,,,,,,"@article{luo2021convolutional,
  AUTHOR = {Haimin Luo and Anpei Chen and Qixuan Zhang and Bai Pang and Minye Wu and Lan Xu and Jingyi Yu},
  TITLE = {Convolutional Neural Opacity Radiance Fields},
  EPRINT = {2104.01772v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Photo-realistic modeling and rendering of fuzzy objects with complex opacityare critical for numerous immersive VR/AR applications, but it suffers fromstrong view-dependent brightness, color. In this paper, we propose a novelscheme to generate opacity radiance fields with a convolutional neural rendererfor fuzzy objects, which is the first to combine both explicit opacitysupervision and convolutional mechanism into the neural radiance fieldframework so as to enable high-quality appearance and global consistent alphamattes generation in arbitrary novel views. More specifically, we propose anefficient sampling strategy along with both the camera rays and image plane,which enables efficient radiance field sampling and learning in a patch-wisemanner, as well as a novel volumetric feature integration scheme that generatesper-patch hybrid feature embeddings to reconstruct the view-consistentfine-detailed appearance and opacity output. We further adopt a patch-wiseadversarial training scheme to preserve both high-frequency appearance andopacity details in a self-supervised framework. We also introduce an effectivemulti-view image capture system to capture high-quality color and alpha mapsfor challenging fuzzy objects. Extensive experiments on existing and our newchallenging fuzzy object dataset demonstrate that our method achievesphoto-realistic, globally consistent, and fined detailed appearance and opacityfree-viewpoint rendering for various fuzzy objects.},
  YEAR = {2021},
  MONTH = {Apr},
  URL = {http://arxiv.org/abs/2104.01772v1},
  FILE = {2104.01772v1.pdf}
 }","Performance (training), Performance (rendering), Challenging materials (fur, hair, transparency)","Generative/adversarial formulation, Sampling, Feature volume",,,,,,,,,,,,,,"Haimin Luo, Anpei Chen, Qixuan Zhang, Bai Pang, Minye Wu, Lan Xu, Jingyi Yu",luo2021convolutional,00000104,"Photo-realistic modeling and rendering of fuzzy objects with complex opacity are critical for numerous immersive VR/AR applications, but it suffers from strong view-dependent brightness, color. In this paper, we propose a novel scheme to generate opacity radiance fields with a convolutional neural renderer for fuzzy objects, which is the first to combine both explicit opacity supervision and convolutional mechanism into the neural radiance field framework so as to enable high-quality appearance and global consistent alpha mattes generation in arbitrary novel views. More specifically, we propose an efficient sampling strategy along with both the camera rays and image plane, which enables efficient radiance field sampling and learning in a patch-wise manner, as well as a novel volumetric feature integration scheme that generates per-patch hybrid feature embeddings to reconstruct the view-consistent fine-detailed appearance and opacity output. We further adopt a patch-wise adversarial training scheme to preserve both high-frequency appearance and opacity details in a self-supervised framework. We also introduce an effective multi-view image capture system to capture high-quality color and alpha maps for challenging fuzzy objects. Extensive experiments on existing and our new challenging fuzzy object dataset demonstrate that our method achieves photo-realistic, globally consistent, and fined detailed appearance and opacity free-viewpoint rendering for various fuzzy objects.",2,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnuegXH4CBWNzgGPhWNq-iLOXKT9w-QM6mPH_iUfkE6YbuF8xpEbTQ74Kg3LmaxWWIxg
5/23/2021 18:36:18,MirrorNeRF: One-shot Neural Portrait Radiance Field from Multi-mirror Catadioptric Imaging,MirrorNeRF,4/6/2021,https://arxiv.org/pdf/2104.02607.pdf,,,,,,,"@article{wang2021mirrornerf,
  AUTHOR = {Ziyu Wang and Liao Wang and Fuqiang Zhao and Minye Wu and Lan Xu and Jingyi Yu},
  TITLE = {MirrorNeRF: One-shot Neural Portrait Radiance Field from Multi-mirrorCatadioptric Imaging},
  EPRINT = {2104.02607v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Photo-realistic neural reconstruction and rendering of the human portrait arecritical for numerous VR/AR applications. Still, existing solutions inherentlyrely on multi-view capture settings, and the one-shot solution to get rid ofthe tedious multi-view synchronization and calibration remains extremelychallenging. In this paper, we propose MirrorNeRF - a one-shot neural portraitfree-viewpoint rendering approach using a catadioptric imaging system withmultiple sphere mirrors and a single high-resolution digital camera, which isthe first to combine neural radiance field with catadioptric imaging so as toenable one-shot photo-realistic human portrait reconstruction and rendering, ina low-cost and casual capture setting. More specifically, we propose alight-weight catadioptric system design with a sphere mirror array to enablediverse ray sampling in the continuous 3D space as well as an effective onlinecalibration for the camera and the mirror array. Our catadioptric imagingsystem can be easily deployed with a low budget and the casual capture abilityfor convenient daily usages. We introduce a novel neural warping radiance fieldrepresentation to learn a continuous displacement field that implicitlycompensates for the misalignment due to our flexible system setting. We furtherpropose a density regularization scheme to leverage the inherent geometryinformation from the catadioptric data in a self-supervision manner, which notonly improves the training efficiency but also provides more effective densitysupervision for higher rendering quality. Extensive experiments demonstrate theeffectiveness and robustness of our scheme to achieve one-shot photo-realisticand high-quality appearance free-viewpoint rendering for human portrait scenes.},
  YEAR = {2021},
  MONTH = {Apr},
  URL = {http://arxiv.org/abs/2104.02607v2},
  FILE = {2104.02607v2.pdf}
 }",Few-shot reconstruction,Warping field/Flow field,,,,,,,,,,,,,,"Ziyu Wang, Liao Wang, Fuqiang Zhao, Minye Wu, Lan Xu, Jingyi Yu",wang2021mirrornerf,00000105,"Photo-realistic neural reconstruction and rendering of the human portrait are critical for numerous VR/AR applications. Still, existing solutions inherently rely on multi-view capture settings, and the one-shot solution to get rid of the tedious multi-view synchronization and calibration remains extremely challenging. In this paper, we propose MirrorNeRF - a one-shot neural portrait free-viewpoint rendering approach using a catadioptric imaging system with multiple sphere mirrors and a single high-resolution digital camera, which is the first to combine neural radiance field with catadioptric imaging so as to enable one-shot photo-realistic human portrait reconstruction and rendering, in a low-cost and casual capture setting. More specifically, we propose a light-weight catadioptric system design with a sphere mirror array to enable diverse ray sampling in the continuous 3D space as well as an effective online calibration for the camera and the mirror array. Our catadioptric imaging system can be easily deployed with a low budget and the casual capture ability for convenient daily usages. We introduce a novel neural warping radiance field representation to learn a continuous displacement field that implicitly compensates for the misalignment due to our flexible system setting. We further propose a density regularization scheme to leverage the inherent geometry information from the catadioptric data in a self-supervision manner, which not only improves the training efficiency but also provides more effective density supervision for higher rendering quality. Extensive experiments demonstrate the effectiveness and robustness of our scheme to achieve one-shot photo-realistic and high-quality appearance free-viewpoint rendering for human portrait scenes.",1,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnuf_EzpTaZxugyLXym6xQQU5A5fAAoZCyRoIG6kODIGoiPYZpqHC6joLPQo6-H-4q8o
5/23/2021 18:34:59,Neural Articulated Radiance Field,NARF,4/7/2021,https://arxiv.org/pdf/2104.03110.pdf,,https://arxiv.org/pdf/2104.03110.pdf,,,,,"@article{noguchi2021narf,
  AUTHOR = {Atsuhiro Noguchi and Xiao Sun and Stephen Lin and Tatsuya Harada},
  TITLE = {Neural Articulated Radiance Field},
  EPRINT = {2104.03110v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We present Neural Articulated Radiance Field (NARF), a novel deformable 3Drepresentation for articulated objects learned from images. While recentadvances in 3D implicit representation have made it possible to learn models ofcomplex objects, learning pose-controllable representations of articulatedobjects remains a challenge, as current methods require 3D shape supervisionand are unable to render appearance. In formulating an implicit representationof 3D articulated objects, our method considers only the rigid transformationof the most relevant object part in solving for the radiance field at each 3Dlocation. In this way, the proposed method represents pose-dependent changeswithout significantly increasing the computational complexity. NARF is fullydifferentiable and can be trained from images with pose annotations. Moreover,through the use of an autoencoder, it can learn appearance variations overmultiple instances of an object class. Experiments show that the proposedmethod is efficient and can generalize well to novel poses. The code isavailable for research purposes at https://github.com/nogu-atsu/NARF},
  YEAR = {2021},
  MONTH = {Apr},
  URL = {http://arxiv.org/abs/2104.03110v2},
  FILE = {2104.03110v2.pdf}
 }",Human body,"Volume partitioning, Articulated",,,,,,,,,,ICCV 2020,,,,"Atsuhiro Noguchi, Xiao Sun, Stephen Lin, Tatsuya Harada",noguchi2021narf,00000106,"We present Neural Articulated Radiance Field (NARF), a novel deformable 3D representation for articulated objects learned from images. While recent advances in 3D implicit representation have made it possible to learn models of complex objects, learning pose-controllable representations of articulated objects remains a challenge, as current methods require 3D shape supervision and are unable to render appearance. In formulating an implicit representation of 3D articulated objects, our method considers only the rigid transformation of the most relevant object part in solving for the radiance field at each 3D location. In this way, the proposed method represents pose-dependent changes without significantly increasing the computational complexity. NARF is fully differentiable and can be trained from images with pose annotations. Moreover, through the use of an autoencoder, it can learn appearance variations over multiple instances of an object class. Experiments show that the proposed method is efficient and can generalize well to novel poses. The code is available for research purposes at https://github.com/nogu-atsu/NARF",2,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnufJXCAm0adjUHR5b5cFRiV_BK7clNsyykOqxtTUOcO3f4y0UCshGMQptgiAE6oVRt4
7/19/2021 22:03:42,SCANimate: Weakly Supervised Learning of Skinned Clothed Avatar Networks,SCANimate,4/7/2021,https://arxiv.org/pdf/2104.03313.pdf,https://scanimate.is.tue.mpg.de,https://github.com/shunsukesaito/SCANimate,https://www.youtube.com/watch?v=ohavL55Oznw,,,,"@article{saito2021scanimate,
  AUTHOR = {Shunsuke Saito and Jinlong Yang and Qianli Ma and Michael J. Black},
  TITLE = {SCANimate: Weakly Supervised Learning of Skinned Clothed Avatar Networks},
  EPRINT = {2104.03313v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We present SCANimate, an end-to-end trainable framework that takes raw 3Dscans of a clothed human and turns them into an animatable avatar. Theseavatars are driven by pose parameters and have realistic clothing that movesand deforms naturally. SCANimate does not rely on a customized mesh template orsurface mesh registration. We observe that fitting a parametric 3D body model,like SMPL, to a clothed human scan is tractable while surface registration ofthe body topology to the scan is often not, because clothing can deviatesignificantly from the body shape. We also observe that articulatedtransformations are invertible, resulting in geometric cycle consistency in theposed and unposed shapes. These observations lead us to a weakly supervisedlearning method that aligns scans into a canonical pose by disentanglingarticulated deformations without template-based surface registration.Furthermore, to complete missing regions in the aligned scans while modelingpose-dependent deformations, we introduce a locally pose-aware implicitfunction that learns to complete and model geometry with learned posecorrectives. In contrast to commonly used global pose embeddings, our localpose conditioning significantly reduces long-range spurious correlations andimproves generalization to unseen poses, especially when training data islimited. Our method can be applied to pose-aware appearance modeling togenerate a fully textured avatar. We demonstrate our approach on variousclothing types with different amounts of training data, outperforming existingsolutions and other variants in terms of fidelity and generality in everysetting. The code is available at https://scanimate.is.tue.mpg.de.},
  YEAR = {2021},
  MONTH = {Apr},
  URL = {http://arxiv.org/abs/2104.03313v2},
  FILE = {2104.03313v2.pdf}
 }","Dynamic, Human body",Warping field/Flow field,,SDF,Category-level,,,,,,,CVPR 2020,,No,Direct,"Shunsuke Saito, Jinlong Yang, Qianli Ma, Michael J. Black",saito2021scanimate,00000107,"We present SCANimate, an end-to-end trainable framework that takes raw 3D scans of a clothed human and turns them into an animatable avatar. These avatars are driven by pose parameters and have realistic clothing that moves and deforms naturally. SCANimate does not rely on a customized mesh template or surface mesh registration. We observe that fitting a parametric 3D body model, like SMPL, to a clothed human scan is tractable while surface registration of the body topology to the scan is often not, because clothing can deviate significantly from the body shape. We also observe that articulated transformations are invertible, resulting in geometric cycle consistency in the posed and unposed shapes. These observations lead us to a weakly supervised learning method that aligns scans into a canonical pose by disentangling articulated deformations without template-based surface registration. Furthermore, to complete missing regions in the aligned scans while modeling pose-dependent deformations, we introduce a locally pose-aware implicit function that learns to complete and model geometry with learned pose correctives. In contrast to commonly used global pose embeddings, our local pose conditioning significantly reduces long-range spurious correlations and improves generalization to unseen poses, especially when training data is limited. Our method can be applied to pose-aware appearance modeling to generate a fully textured avatar. We demonstrate our approach on various clothing types with different amounts of training data, outperforming existing solutions and other variants in terms of fidelity and generality in every setting. The code is available at https://scanimate.is.tue.mpg.de.",7,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnuf6FHUIMGX2zGoEdRbaVh_jLkkewLeC4BOT4AYi2ut7vVZAuCBdiU7wVfgMWP_x7nw
5/23/2021 18:33:32,Direct-PoseNet: Absolute Pose Regression with Photometric Consistency,Direct-PoseNet,4/8/2021,https://arxiv.org/pdf/2104.04073.pdf,,,,,,,"@article{chen2021directposenet,
  AUTHOR = {Shuai Chen and Zirui Wang and Victor Prisacariu},
  TITLE = {Direct-PoseNet: Absolute Pose Regression with Photometric Consistency},
  EPRINT = {2104.04073v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We present a relocalization pipeline, which combines an absolute poseregression (APR) network with a novel view synthesis based direct matchingmodule, offering superior accuracy while maintaining low inference time. Ourcontribution is twofold: i) we design a direct matching module that supplies aphotometric supervision signal to refine the pose regression network viadifferentiable rendering; ii) we modify the rotation representation from theclassical quaternion to SO(3) in pose regression, removing the need forbalancing rotation and translation loss terms. As a result, our networkDirect-PoseNet achieves state-of-the-art performance among all othersingle-image APR methods on the 7-Scenes benchmark and the LLFF dataset.},
  YEAR = {2021},
  MONTH = {Apr},
  URL = {http://arxiv.org/abs/2104.04073v1},
  FILE = {2104.04073v1.pdf}
 }",Camera parameter estimation,,,,,,,,,,,,,,,"Shuai Chen, Zirui Wang, Victor Prisacariu",chen2021directposenet,00000108,"We present a relocalization pipeline, which combines an absolute pose regression (APR) network with a novel view synthesis based direct matching module, offering superior accuracy while maintaining low inference time. Our contribution is twofold: i) we design a direct matching module that supplies a photometric supervision signal to refine the pose regression network via differentiable rendering; ii) we modify the rotation representation from the classical quaternion to SO(3) in pose regression, removing the need for balancing rotation and translation loss terms. As a result, our network Direct-PoseNet achieves state-of-the-art performance among all other single-image APR methods on the 7-Scenes benchmark and the LLFF dataset.",0,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnucGkYBJgD2btAQRyl7FtpVlnEX02OGft3JXp9_-ZoP_WHpxYgLpwcOEAcFelwJiLnQ
5/23/2021 18:35:19,SNARF: Differentiable Forward Skinning for Animating Non-Rigid Neural Implicit Shapes,SNARF,4/8/2021,https://arxiv.org/pdf/2104.03953.pdf,,,,,,,"@article{chen2021snarf,
  AUTHOR = {Xu Chen and Yufeng Zheng and Michael J. Black and Otmar Hilliges and Andreas Geiger},
  TITLE = {SNARF: Differentiable Forward Skinning for Animating Non-Rigid NeuralImplicit Shapes},
  EPRINT = {2104.03953v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Neural implicit surface representations have emerged as a promising paradigmto capture 3D shapes in a continuous and resolution-independent manner.However, adapting them to articulated shapes is non-trivial. Existingapproaches learn a backward warp field that maps deformed to canonical points.However, this is problematic since the backward warp field is pose dependentand thus requires large amounts of data to learn. To address this, we introduceSNARF, which combines the advantages of linear blend skinning (LBS) forpolygonal meshes with those of neural implicit surfaces by learning a forwarddeformation field without direct supervision. This deformation field is definedin canonical, pose-independent space, allowing for generalization to unseenposes. Learning the deformation field from posed meshes alone is challengingsince the correspondences of deformed points are defined implicitly and may notbe unique under changes of topology. We propose a forward skinning model thatfinds all canonical correspondences of any deformed point using iterative rootfinding. We derive analytical gradients via implicit differentiation, enablingend-to-end training from 3D meshes with bone transformations. Compared tostate-of-the-art neural implicit representations, our approach generalizesbetter to unseen poses while preserving accuracy. We demonstrate our method inchallenging scenarios on (clothed) 3D humans in diverse and unseen poses.},
  YEAR = {2021},
  MONTH = {Apr},
  URL = {http://arxiv.org/abs/2104.03953v1},
  FILE = {2104.03953v1.pdf}
 }",Human body,"Articulated, Warping field/Flow field",,,,,,,,,,CVPR,,,,"Xu Chen, Yufeng Zheng, Michael J. Black, Otmar Hilliges, Andreas Geiger",chen2021snarf,00000109,"Neural implicit surface representations have emerged as a promising paradigm to capture 3D shapes in a continuous and resolution-independent manner. However, adapting them to articulated shapes is non-trivial. Existing approaches learn a backward warp field that maps deformed to canonical points. However, this is problematic since the backward warp field is pose dependent and thus requires large amounts of data to learn. To address this, we introduce SNARF, which combines the advantages of linear blend skinning (LBS) for polygonal meshes with those of neural implicit surfaces by learning a forward deformation field without direct supervision. This deformation field is defined in canonical, pose-independent space, allowing for generalization to unseen poses. Learning the deformation field from posed meshes alone is challenging since the correspondences of deformed points are defined implicitly and may not be unique under changes of topology. We propose a forward skinning model that finds all canonical correspondences of any deformed point using iterative root finding. We derive analytical gradients via implicit differentiation, enabling end-to-end training from 3D meshes with bone transformations. Compared to state-of-the-art neural implicit representations, our approach generalizes better to unseen poses while preserving accuracy. We demonstrate our method in challenging scenarios on (clothed) 3D humans in diverse and unseen poses.",1,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnueGP67Mvo9TzTIqQLIWZlIw7VvsVvs-kJDyxRzPsor0-QbEiglD183zR2sKT5M_O5s
5/23/2021 18:35:38,Modulated Periodic Activations for Generalizable Local Functional Representations,,4/8/2021,https://arxiv.org/pdf/2104.03960.pdf,,,,,,,"@article{mehta2021modulated,
  AUTHOR = {Ishit Mehta and Michael Gharbi and Connelly Barnes and Eli Shechtman and Ravi Ramamoorthi and Manmohan Chandraker},
  TITLE = {Modulated Periodic Activations for Generalizable Local FunctionalRepresentations},
  EPRINT = {2104.03960v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Multi-Layer Perceptrons (MLPs) make powerful functional representations forsampling and reconstruction problems involving low-dimensional signals likeimages,shapes and light fields. Recent works have significantly improved theirability to represent high-frequency content by using periodic activations orpositional encodings. This often came at the expense of generalization: modernmethods are typically optimized for a single signal. We present a newrepresentation that generalizes to multiple instances and achievesstate-of-the-art fidelity. We use a dual-MLP architecture to encode thesignals. A synthesis network creates a functional mapping from alow-dimensional input (e.g. pixel-position) to the output domain (e.g. RGBcolor). A modulation network maps a latent code corresponding to the targetsignal to parameters that modulate the periodic activations of the synthesisnetwork. We also propose a local-functional representation which enablesgeneralization. The signal's domain is partitioned into a regular grid,witheach tile represented by a latent code. At test time, the signal is encodedwith high-fidelity by inferring (or directly optimizing) the latent code-book.Our approach produces generalizable functional representations of images,videos and shapes, and achieves higher reconstruction quality than prior worksthat are optimized for a single signal.},
  YEAR = {2021},
  MONTH = {Apr},
  URL = {http://arxiv.org/abs/2104.03960v1},
  FILE = {2104.03960v1.pdf}
 }","Generalization, Compression, Fundamentals","Conditional neural field, Hypernetwork",Other,,,,,,,,,CVPR,,,,"Ishit Mehta, Michaël Gharbi, Connelly Barnes, Eli Shechtman, Ravi Ramamoorthi, Manmohan Chandraker",mehta2021modulated,00000110,"Multi-Layer Perceptrons (MLPs) make powerful functional representations for sampling and reconstruction problems involving low-dimensional signals like images,shapes and light fields. Recent works have significantly improved their ability to represent high-frequency content by using periodic activations or positional encodings. This often came at the expense of generalization: modern methods are typically optimized for a single signal. We present a new representation that generalizes to multiple instances and achieves state-of-the-art fidelity. We use a dual-MLP architecture to encode the signals. A synthesis network creates a functional mapping from a low-dimensional input (e.g. pixel-position) to the output domain (e.g. RGB color). A modulation network maps a latent code corresponding to the target signal to parameters that modulate the periodic activations of the synthesis network. We also propose a local-functional representation which enables generalization. The signal's domain is partitioned into a regular grid,with each tile represented by a latent code. At test time, the signal is encoded with high-fidelity by inferring (or directly optimizing) the latent code-book. Our approach produces generalizable functional representations of images, videos and shapes, and achieves higher reconstruction quality than prior works that are optimized for a single signal.",1,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnuej0-noezBbvYqSep5kkbTiTzqPdYZpJ2ulBWNenotI3zyvhSv89-YIDqRoX6Qawzk
5/23/2021 18:34:02,Neural RGB-D Surface Reconstruction,,4/9/2021,https://arxiv.org/pdf/2104.04532.pdf,,,,,,,"@article{azinovic2021neural,
  AUTHOR = {Dejan Azinovic and Ricardo Martin-Brualla and Dan B Goldman and Matthias Niessner and Justus Thies},
  TITLE = {Neural RGB-D Surface Reconstruction},
  EPRINT = {2104.04532v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {In this work, we explore how to leverage the success of implicit novel viewsynthesis methods for surface reconstruction. Methods which learn a neuralradiance field have shown amazing image synthesis results, but the underlyinggeometry representation is only a coarse approximation of the real geometry. Wedemonstrate how depth measurements can be incorporated into the radiance fieldformulation to produce more detailed and complete reconstruction results thanusing methods based on either color or depth data alone. In contrast to adensity field as the underlying geometry representation, we propose to learn adeep neural network which stores a truncated signed distance field. Using thisrepresentation, we show that one can still leverage differentiable volumerendering to estimate color values of the observed images during training tocompute a reconstruction loss. This is beneficial for learning the signeddistance field in regions with missing depth measurements. Furthermore, wecorrect misalignment errors of the camera, improving the overall reconstructionquality. In several experiments, we showcase our method and compare to existingworks on classical RGB-D fusion and learned representations.},
  YEAR = {2021},
  MONTH = {Apr},
  URL = {http://arxiv.org/abs/2104.04532v1},
  FILE = {2104.04532v1.pdf}
 }",Camera parameter estimation,Conditional neural field,,,,,,,,,,CVPR,,,,"Dejan Azinović, Ricardo Martin-Brualla, Dan B Goldman, Matthias Nießner, Justus Thies",azinovic2021neural,00000111,"In this work, we explore how to leverage the success of implicit novel view synthesis methods for surface reconstruction. Methods which learn a neural radiance field have shown amazing image synthesis results, but the underlying geometry representation is only a coarse approximation of the real geometry. We demonstrate how depth measurements can be incorporated into the radiance field formulation to produce more detailed and complete reconstruction results than using methods based on either color or depth data alone. In contrast to a density field as the underlying geometry representation, we propose to learn a deep neural network which stores a truncated signed distance field. Using this representation, we show that one can still leverage differentiable volume rendering to estimate color values of the observed images during training to compute a reconstruction loss. This is beneficial for learning the signed distance field in regions with missing depth measurements. Furthermore, we correct misalignment errors of the camera, improving the overall reconstruction quality. In several experiments, we showcase our method and compare to existing works on classical RGB-D fusion and learned representations.",1,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnudWb1Zvxf4nelXco5DMwNyKC4si49QcNwZVlPRvB8L1Q28g5gyTAopzBbVRB4ASSBk
5/23/2021 18:33:47,Compressive Neural Representations of Volumetric Scalar Fields,,4/11/2021,https://arxiv.org/pdf/2104.04523.pdf,,,,,,,"@article{lu2021compressive,
  AUTHOR = {Yuzhe Lu and Kairong Jiang and Joshua A. Levine and Matthew Berger},
  TITLE = {Compressive Neural Representations of Volumetric Scalar Fields},
  EPRINT = {2104.04523v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.LG},
  ABSTRACT = {We present an approach for compressing volumetric scalar fields usingimplicit neural representations. Our approach represents a scalar field as alearned function, wherein a neural network maps a point in the domain to anoutput scalar value. By setting the number of weights of the neural network tobe smaller than the input size, we achieve compressed representations of scalarfields, thus framing compression as a type of function approximation. Combinedwith carefully quantizing network weights, we show that this approach yieldshighly compact representations that outperform state-of-the-art volumecompression approaches. The conceptual simplicity of our approach enables anumber of benefits, such as support for time-varying scalar fields, optimizingto preserve spatial gradients, and random-access field evaluation. We study theimpact of network design choices on compression performance, highlighting howsimple network architectures are effective for a broad range of volumes.},
  YEAR = {2021},
  MONTH = {Apr},
  URL = {http://arxiv.org/abs/2104.04523v1},
  FILE = {2104.04523v1.pdf}
 }","Dynamic, Compression, Beyond graphics, Fundamentals",,SIREN,,,,,,,,,EuroVis 2020,,,,"Yuzhe Lu, Kairong Jiang, Joshua A. Levine, Matthew Berger",lu2021compressive,00000112,"We present an approach for compressing volumetric scalar fields using implicit neural representations. Our approach represents a scalar field as a learned function, wherein a neural network maps a point in the domain to an output scalar value. By setting the number of weights of the neural network to be smaller than the input size, we achieve compressed representations of scalar fields, thus framing compression as a type of function approximation. Combined with carefully quantizing network weights, we show that this approach yields highly compact representations that outperform state-of-the-art volume compression approaches. The conceptual simplicity of our approach enables a number of benefits, such as support for time-varying scalar fields, optimizing to preserve spatial gradients, and random-access field evaluation. We study the impact of network design choices on compression performance, highlighting how simple network architectures are effective for a broad range of volumes.",1,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnuecHAkjoX655sRj0eSjXWAb4bulPmfscKWXmYbG1F52QG--j3d7he_ZR90zgfJgpRc
9/17/2021 14:04:23,StereoPIFu: Depth Aware Clothed Human Digitization via Stereo Vision,StereoPIFu,4/12/2021,https://arxiv.org/pdf/2104.05289.pdf,https://hy1995.top/StereoPIFuProject/,https://github.com/CrisHY1995/StereoPIFu_Code,,,,,"@article{hong2021stereopifu,
  AUTHOR = {Yang Hong and Juyong Zhang and Boyi Jiang and Yudong Guo and Ligang Liu and Hujun Bao},
  TITLE = {StereoPIFu: Depth Aware Clothed Human Digitization via Stereo Vision},
  EPRINT = {2104.05289v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {In this paper, we propose StereoPIFu, which integrates the geometricconstraints of stereo vision with implicit function representation of PIFu, torecover the 3D shape of the clothed human from a pair of low-cost rectifiedimages. First, we introduce the effective voxel-aligned features from a stereovision-based network to enable depth-aware reconstruction. Moreover, the novelrelative z-offset is employed to associate predicted high-fidelity human depthand occupancy inference, which helps restore fine-level surface details.Second, a network structure that fully utilizes the geometry information fromthe stereo images is designed to improve the human body reconstruction quality.Consequently, our StereoPIFu can naturally infer the human body's spatiallocation in camera space and maintain the correct relative position ofdifferent parts of the human body, which enables our method to capture humanperformance. Compared with previous works, our StereoPIFu significantlyimproves the robustness, completeness, and accuracy of the clothed humanreconstruction, which is demonstrated by extensive experimental results.},
  YEAR = {2021},
  MONTH = {Apr},
  URL = {http://arxiv.org/abs/2104.05289v2},
  FILE = {2104.05289v2.pdf}
 }",Human body,"Conditional neural field, Lifting 2D features to 3D, Feature volume, Data-driven component (pre-trained, cross-scene)",,Occupancy,Category-level,,,,,,,CVPR 2021,,Yes,,"Yang Hong, Juyong Zhang, Boyi Jiang, Yudong Guo, Ligang Liu, Hujun Bao",hong2021stereopifu,00000204,"In this paper, we propose StereoPIFu, which integrates the geometric constraints of stereo vision with implicit function representation of PIFu, to recover the 3D shape of the clothed human from a pair of low-cost rectified images. First, we introduce the effective voxel-aligned features from a stereo vision-based network to enable depth-aware reconstruction. Moreover, the novel relative z-offset is employed to associate predicted high-fidelity human depth and occupancy inference, which helps restore fine-level surface details. Second, a network structure that fully utilizes the geometry information from the stereo images is designed to improve the human body reconstruction quality. Consequently, our StereoPIFu can naturally infer the human body's spatial location in camera space and maintain the correct relative position of different parts of the human body, which enables our method to capture human performance. Compared with previous works, our StereoPIFu significantly improves the robustness, completeness, and accuracy of the clothed human reconstruction, which is demonstrated by extensive experimental results.",,Yes,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnudTwuF-ycMxqvKOJjqB_7G0cL5KFYYkhSTpg2iOyeIqfDkj739ai5zCHByysGgk8G0
5/23/2021 18:34:37,BARF: Bundle-Adjusting Neural Radiance Fields,BARF,4/13/2021,https://arxiv.org/pdf/2104.06405.pdf,https://chenhsuanlin.bitbucket.io/bundle-adjusting-NeRF/,Coming soon,,,,,"@article{lin2021barf,
  AUTHOR = {Chen-Hsuan Lin and Wei-Chiu Ma and Antonio Torralba and Simon Lucey},
  TITLE = {BARF: Bundle-Adjusting Neural Radiance Fields},
  EPRINT = {2104.06405v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Neural Radiance Fields (NeRF) have recently gained a surge of interest withinthe computer vision community for its power to synthesize photorealistic novelviews of real-world scenes. One limitation of NeRF, however, is its requirementof accurate camera poses to learn the scene representations. In this paper, wepropose Bundle-Adjusting Neural Radiance Fields (BARF) for training NeRF fromimperfect (or even unknown) camera poses -- the joint problem of learningneural 3D representations and registering camera frames. We establish atheoretical connection to classical image alignment and show thatcoarse-to-fine registration is also applicable to NeRF. Furthermore, we showthat na\""ively applying positional encoding in NeRF has a negative impact onregistration with a synthesis-based objective. Experiments on synthetic andreal-world data show that BARF can effectively optimize the neural scenerepresentations and resolve large camera pose misalignment at the same time.This enables view synthesis and localization of video sequences from unknowncamera poses, opening up new avenues for visual localization systems (e.g.SLAM) and potential applications for dense 3D mapping and reconstruction.},
  YEAR = {2021},
  MONTH = {Apr},
  URL = {http://arxiv.org/abs/2104.06405v2},
  FILE = {2104.06405v2.pdf}
 }",Camera parameter estimation,Coarse-to-fine,,,,,,,,,,ICCV 2020,,,,"Chen-Hsuan Lin, Wei-Chiu Ma, Antonio Torralba, Simon Lucey",lin2021barf,00000113,"Neural Radiance Fields (NeRF) have recently gained a surge of interest within the computer vision community for its power to synthesize photorealistic novel views of real-world scenes. One limitation of NeRF, however, is its requirement of accurate camera poses to learn the scene representations. In this paper, we propose Bundle-Adjusting Neural Radiance Fields (BARF) for training NeRF from imperfect (or even unknown) camera poses -- the joint problem of learning neural 3D representations and registering camera frames. We establish a theoretical connection to classical image alignment and show that coarse-to-fine registration is also applicable to NeRF. Furthermore, we show that na\""ively applying positional encoding in NeRF has a negative impact on registration with a synthesis-based objective. Experiments on synthetic and real-world data show that BARF can effectively optimize the neural scene representations and resolve large camera pose misalignment at the same time. This enables view synthesis and localization of video sequences from unknown camera poses, opening up new avenues for visual localization systems (e.g. SLAM) and potential applications for dense 3D mapping and reconstruction.",2,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnudrIYHcoUWJp9qbSFgbFepJU6VwimdVU1K5YsqoSrvcbmacbhXTaQQgqP5C4C1gYNc
5/23/2021 18:36:57,Stereo Radiance Fields (SRF): Learning View Synthesis for Sparse Views of Novel Scenes,SRF,4/14/2021,https://arxiv.org/pdf/2104.06935.pdf,https://virtualhumans.mpi-inf.mpg.de/srf/,,,https://arxiv.org/pdf/2104.06935.pdf,,,"@article{chibane2021srf,
  AUTHOR = {Julian Chibane and Aayush Bansal and Verica Lazova and Gerard Pons-Moll},
  TITLE = {Stereo Radiance Fields (SRF): Learning View Synthesis for Sparse Viewsof Novel Scenes},
  EPRINT = {2104.06935v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Recent neural view synthesis methods have achieved impressive quality andrealism, surpassing classical pipelines which rely on multi-viewreconstruction. State-of-the-Art methods, such as NeRF, are designed to learn asingle scene with a neural network and require dense multi-view inputs. Testingon a new scene requires re-training from scratch, which takes 2-3 days. In thiswork, we introduce Stereo Radiance Fields (SRF), a neural view synthesisapproach that is trained end-to-end, generalizes to new scenes, and requiresonly sparse views at test time. The core idea is a neural architecture inspiredby classical multi-view stereo methods, which estimates surface points byfinding similar image regions in stereo images. In SRF, we predict color anddensity for each 3D point given an encoding of its stereo correspondence in theinput images. The encoding is implicitly learned by an ensemble of pair-wisesimilarities -- emulating classical stereo. Experiments show that SRF learnsstructure instead of overfitting on a scene. We train on multiple scenes of theDTU dataset and generalize to new ones without re-training, requiring only 10sparse and spread-out views as input. We show that 10-15 minutes of fine-tuningfurther improve the results, achieving significantly sharper, more detailedresults than scene-specific models. The code, model, and videos are availableat https://virtualhumans.mpi-inf.mpg.de/srf/.},
  YEAR = {2021},
  MONTH = {Apr},
  NOTE = {IEEE Conference on Computer Vision and Pattern Recognition (CVPR)
  2021},
  URL = {http://arxiv.org/abs/2104.06935v1},
  FILE = {2104.06935v1.pdf}
 }","Performance (training), Few-shot reconstruction, Generalization","Lifting 2D features to 3D, Image-based rendering",,,Universal,,,,,,,CVPR 2020,,,,"Julian Chibane, Aayush Bansal, Verica Lazova, Gerard Pons-Moll",chibane2021srf,00000114,"Recent neural view synthesis methods have achieved impressive quality and realism, surpassing classical pipelines which rely on multi-view reconstruction. State-of-the-Art methods, such as NeRF, are designed to learn a single scene with a neural network and require dense multi-view inputs. Testing on a new scene requires re-training from scratch, which takes 2-3 days. In this work, we introduce Stereo Radiance Fields (SRF), a neural view synthesis approach that is trained end-to-end, generalizes to new scenes, and requires only sparse views at test time. The core idea is a neural architecture inspired by classical multi-view stereo methods, which estimates surface points by finding similar image regions in stereo images. In SRF, we predict color and density for each 3D point given an encoding of its stereo correspondence in the input images. The encoding is implicitly learned by an ensemble of pair-wise similarities -- emulating classical stereo. Experiments show that SRF learns structure instead of overfitting on a scene. We train on multiple scenes of the DTU dataset and generalize to new ones without re-training, requiring only 10 sparse and spread-out views as input. We show that 10-15 minutes of fine-tuning further improve the results, achieving significantly sharper, more detailed results than scene-specific models. The code, model, and videos are available at https://virtualhumans.mpi-inf.mpg.de/srf/.",4,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnueeLT2w5w2u_Gs7GBsTUdnbi1AZONivCmQPtcDjbZdGLCE_jhbzYLmoCMOfoLds4m0
9/17/2021 11:50:12,LEAP: Learning Articulated Occupancy of People,LEAP,4/14/2021,https://arxiv.org/pdf/2104.06849.pdf,https://neuralbodies.github.io/LEAP/,https://github.com/neuralbodies/leap,https://www.youtube.com/watch?v=UVB8A_T5e3c,,,,"@article{mihajlovic2021leap,
  AUTHOR = {Marko Mihajlovic and Yan Zhang and Michael J. Black and Siyu Tang},
  TITLE = {LEAP: Learning Articulated Occupancy of People},
  EPRINT = {2104.06849v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Substantial progress has been made on modeling rigid 3D objects using deepimplicit representations. Yet, extending these methods to learn neural modelsof human shape is still in its infancy. Human bodies are complex and the keychallenge is to learn a representation that generalizes such that it canexpress body shape deformations for unseen subjects in unseen,highly-articulated, poses. To address this challenge, we introduce LEAP(LEarning Articulated occupancy of People), a novel neural occupancyrepresentation of the human body. Given a set of bone transformations (i.e.joint locations and rotations) and a query point in space, LEAP first maps thequery point to a canonical space via learned linear blend skinning (LBS)functions and then efficiently queries the occupancy value via an occupancynetwork that models accurate identity- and pose-dependent deformations in thecanonical space. Experiments show that our canonicalized occupancy estimationwith the learned LBS functions greatly improves the generalization capabilityof the learned occupancy representation across various human shapes and poses,outperforming existing solutions in all settings.},
  YEAR = {2021},
  MONTH = {Apr},
  URL = {http://arxiv.org/abs/2104.06849v1},
  FILE = {2104.06849v1.pdf}
 }","Human body, Editable","Conditional neural field, Articulated, Warping field/Flow field, Data-driven component (pre-trained, cross-scene)",,Occupancy,Category-level,,,,,,,CVPR 2021,,Yes,Direct,"Marko Mihajlovic, Yan Zhang, Michael J. Black, Siyu Tang",mihajlovic2021leap,00000198,"Substantial progress has been made on modeling rigid 3D objects using deep implicit representations. Yet, extending these methods to learn neural models of human shape is still in its infancy. Human bodies are complex and the key challenge is to learn a representation that generalizes such that it can express body shape deformations for unseen subjects in unseen, highly-articulated, poses. To address this challenge, we introduce LEAP (LEarning Articulated occupancy of People), a novel neural occupancy representation of the human body. Given a set of bone transformations (i.e. joint locations and rotations) and a query point in space, LEAP first maps the query point to a canonical space via learned linear blend skinning (LBS) functions and then efficiently queries the occupancy value via an occupancy network that models accurate identity- and pose-dependent deformations in the canonical space. Experiments show that our canonicalized occupancy estimation with the learned LBS functions greatly improves the generalization capability of the learned occupancy representation across various human shapes and poses, outperforming existing solutions in all settings.",,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnufEHrZwA0Yi2K2FbnIRcDURoAq8An4EYleuQfu0oY4cAfYRQ0pxb123FFDj0_erSlk
5/23/2021 18:32:45,GANcraft: Unsupervised 3D Neural Rendering of Minecraft Worlds,GANcraft,4/15/2021,https://arxiv.org/pdf/2104.07659.pdf,https://nvlabs.github.io/GANcraft/,https://github.com/NVlabs/imaginaire,https://www.youtube.com/watch?v=1Hky092CGFQ,,,,"@article{hao2021gancraft,
  AUTHOR = {Zekun Hao and Arun Mallya and Serge Belongie and Ming-Yu Liu},
  TITLE = {GANcraft: Unsupervised 3D Neural Rendering of Minecraft Worlds},
  EPRINT = {2104.07659v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We present GANcraft, an unsupervised neural rendering framework forgenerating photorealistic images of large 3D block worlds such as those createdin Minecraft. Our method takes a semantic block world as input, where eachblock is assigned a semantic label such as dirt, grass, or water. We representthe world as a continuous volumetric function and train our model to renderview-consistent photorealistic images for a user-controlled camera. In theabsence of paired ground truth real images for the block world, we devise atraining technique based on pseudo-ground truth and adversarial training. Thisstands in contrast to prior work on neural rendering for view synthesis, whichrequires ground truth images to estimate scene geometry and view-dependentappearance. In addition to camera trajectory, GANcraft allows user control overboth scene semantics and output style. Experimental results with comparison tostrong baselines show the effectiveness of GANcraft on this novel task ofphotorealistic 3D block world synthesis. The project website is available athttps://nvlabs.github.io/GANcraft/ .},
  YEAR = {2021},
  MONTH = {Apr},
  URL = {http://arxiv.org/abs/2104.07659v1},
  FILE = {2104.07659v1.pdf}
 }",Generalization,"Generative/adversarial formulation, Voxelization, Feature volume, Data-driven component (pre-trained, cross-scene)",,,,,,,,,,,,,,"Zekun Hao, Arun Mallya, Serge Belongie, Ming-Yu Liu",hao2021gancraft,00000115,"We present GANcraft, an unsupervised neural rendering framework for generating photorealistic images of large 3D block worlds such as those created in Minecraft. Our method takes a semantic block world as input, where each block is assigned a semantic label such as dirt, grass, or water. We represent the world as a continuous volumetric function and train our model to render view-consistent photorealistic images for a user-controlled camera. In the absence of paired ground truth real images for the block world, we devise a training technique based on pseudo-ground truth and adversarial training. This stands in contrast to prior work on neural rendering for view synthesis, which requires ground truth images to estimate scene geometry and view-dependent appearance. In addition to camera trajectory, GANcraft allows user control over both scene semantics and output style. Experimental results with comparison to strong baselines show the effectiveness of GANcraft on this novel task of photorealistic 3D block world synthesis. The project website is available at https://nvlabs.github.io/GANcraft/ .",2,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnuf44fRZZtJp13rVFeB74xSbXlNORSFtB6UVeo2n2Kc6IPbr3E7ld8bYoqPI0Z8QJAw
5/23/2021 18:33:15,A-SDF: Learning Disentangled Signed Distance Functions for Articulated Shape Representation,A-SDF,4/15/2021,https://arxiv.org/pdf/2104.07645.pdf,https://jitengmu.github.io/A-SDF/,Coming soon,,,,,"@article{mu2021asdf,
  AUTHOR = {Jiteng Mu and Weichao Qiu and Adam Kortylewski and Alan Yuille and Nuno Vasconcelos and Xiaolong Wang},
  TITLE = {A-SDF: Learning Disentangled Signed Distance Functions for ArticulatedShape Representation},
  EPRINT = {2104.07645v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Recent work has made significant progress on using implicit functions, as acontinuous representation for 3D rigid object shape reconstruction. However,much less effort has been devoted to modeling general articulated objects.Compared to rigid objects, articulated objects have higher degrees of freedom,which makes it hard to generalize to unseen shapes. To deal with the largeshape variance, we introduce Articulated Signed Distance Functions (A-SDF) torepresent articulated shapes with a disentangled latent space, where we haveseparate codes for encoding shape and articulation. We assume no priorknowledge on part geometry, articulation status, joint type, joint axis, andjoint location. With this disentangled continuous representation, wedemonstrate that we can control the articulation input and animate unseeninstances with unseen joint angles. Furthermore, we propose a Test-TimeAdaptation inference algorithm to adjust our model during inference. Wedemonstrate our model generalize well to out-of-distribution and unseen data,e.g., partial point clouds and real-world depth images.},
  YEAR = {2021},
  MONTH = {Apr},
  URL = {http://arxiv.org/abs/2104.07645v1},
  FILE = {2104.07645v1.pdf}
 }",Editable,"Conditional neural field, Articulated",,,,,,,,,,CVPR,,,,"Jiteng Mu, Weichao Qiu, Adam Kortylewski, Alan Yuille, Nuno Vasconcelos, Xiaolong Wang",mu2021asdf,00000116,"Recent work has made significant progress on using implicit functions, as a continuous representation for 3D rigid object shape reconstruction. However, much less effort has been devoted to modeling general articulated objects. Compared to rigid objects, articulated objects have higher degrees of freedom, which makes it hard to generalize to unseen shapes. To deal with the large shape variance, we introduce Articulated Signed Distance Functions (A-SDF) to represent articulated shapes with a disentangled latent space, where we have separate codes for encoding shape and articulation. We assume no prior knowledge on part geometry, articulation status, joint type, joint axis, and joint location. With this disentangled continuous representation, we demonstrate that we can control the articulation input and animate unseen instances with unseen joint angles. Furthermore, we propose a Test-Time Adaptation inference algorithm to adjust our model during inference. We demonstrate our model generalize well to out-of-distribution and unseen data, e.g., partial point clouds and real-world depth images.",2,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnufFh4I8ScaSvsBqexoHbIv8ObGQxbwD9QCGwO9D80NXwsjObz6ksNCv7a2PalT44HU
5/23/2021 18:47:27,FastNeRF: High-Fidelity Neural Rendering at 200FPS,FastNeRF,4/15/2021,https://arxiv.org/pdf/2103.10380.pdf,,,,,,,"@article{garbin2021fastnerf,
  AUTHOR = {Stephan J. Garbin and Marek Kowalski and Matthew Johnson and Jamie Shotton and Julien Valentin},
  TITLE = {FastNeRF: High-Fidelity Neural Rendering at 200FPS},
  EPRINT = {2103.10380v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Recent work on Neural Radiance Fields (NeRF) showed how neural networks canbe used to encode complex 3D environments that can be renderedphotorealistically from novel viewpoints. Rendering these images is verycomputationally demanding and recent improvements are still a long way fromenabling interactive rates, even on high-end hardware. Motivated by scenarioson mobile and mixed reality devices, we propose FastNeRF, the first NeRF-basedsystem capable of rendering high fidelity photorealistic images at 200Hz on ahigh-end consumer GPU. The core of our method is a graphics-inspiredfactorization that allows for (i) compactly caching a deep radiance map at eachposition in space, (ii) efficiently querying that map using ray directions toestimate the pixel values in the rendered image. Extensive experiments showthat the proposed method is 3000 times faster than the original NeRF algorithmand at least an order of magnitude faster than existing work on acceleratingNeRF, while maintaining visual quality and extensibility.},
  YEAR = {2021},
  MONTH = {Mar},
  URL = {http://arxiv.org/abs/2103.10380v2},
  FILE = {2103.10380v2.pdf}
 }",Performance (rendering),Caching,,,,,,,,,,CVPR,,,,"Stephan J. Garbin, Marek Kowalski, Matthew Johnson, Jamie Shotton, Julien Valentin",garbin2021fastnerf,00000117,"Recent work on Neural Radiance Fields (NeRF) showed how neural networks can be used to encode complex 3D environments that can be rendered photorealistically from novel viewpoints. Rendering these images is very computationally demanding and recent improvements are still a long way from enabling interactive rates, even on high-end hardware. Motivated by scenarios on mobile and mixed reality devices, we propose FastNeRF, the first NeRF-based system capable of rendering high fidelity photorealistic images at 200Hz on a high-end consumer GPU. The core of our method is a graphics-inspired factorization that allows for (i) compactly caching a deep radiance map at each position in space, (ii) efficiently querying that map using ray directions to estimate the pixel values in the rendered image. Extensive experiments show that the proposed method is 3000 times faster than the original NeRF algorithm and at least an order of magnitude faster than existing work on accelerating NeRF, while maintaining visual quality and extensibility.",10,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnudd11n0kXe-lH9my_lAaE4-ozTXOx1HXWTOntE5WyoZrKNWfUTVnrdYEr-rnND5C6E
9/17/2021 11:58:40,Locally Aware Piecewise Transformation Fields for 3D Human Mesh Registration,,4/16/2021,https://arxiv.org/pdf/2104.08160.pdf,https://taconite.github.io/PTF/website/PTF.html,https://github.com/taconite/PTF,https://www.youtube.com/watch?v=TvLoGLVF70k,,,,"@article{wang2021locally,
  AUTHOR = {Shaofei Wang and Andreas Geiger and Siyu Tang},
  TITLE = {Locally Aware Piecewise Transformation Fields for 3D Human MeshRegistration},
  EPRINT = {2104.08160v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Registering point clouds of dressed humans to parametric human models is achallenging task in computer vision. Traditional approaches often rely onheavily engineered pipelines that require accurate manual initialization ofhuman poses and tedious post-processing. More recently, learning-based methodsare proposed in hope to automate this process. We observe that poseinitialization is key to accurate registration but existing methods often failto provide accurate pose initialization. One major obstacle is that, regressingjoint rotations from point clouds or images of humans is still verychallenging. To this end, we propose novel piecewise transformation fields(PTF), a set of functions that learn 3D translation vectors to map any querypoint in posed space to its correspond position in rest-pose space. We combinePTF with multi-class occupancy networks, obtaining a novel learning-basedframework that learns to simultaneously predict shape and per-pointcorrespondences between the posed space and the canonical space for clothedhuman. Our key insight is that the translation vector for each query point canbe effectively estimated using the point-aligned local features; consequently,rigid per bone transformations and joint rotations can be obtained efficientlyvia a least-square fitting given the estimated point correspondences,circumventing the challenging task of directly regressing joint rotations fromneural networks. Furthermore, the proposed PTF facilitate canonicalizedoccupancy estimation, which greatly improves generalization capability andresults in more accurate surface reconstruction with only half of theparameters compared with the state-of-the-art. Both qualitative andquantitative studies show that fitting parametric models with poses initializedby our network results in much better registration quality, especially forextreme poses.},
  YEAR = {2021},
  MONTH = {Apr},
  URL = {http://arxiv.org/abs/2104.08160v1},
  FILE = {2104.08160v1.pdf}
 }",Human body,"Voxelization, Feature volume, Volume partitioning, Warping field/Flow field, Data-driven component (pre-trained, cross-scene)",,Occupancy,Category-level,,,,,,,CVPR 2021,,Yes,Direct,"Shaofei Wang, Andreas Geiger, Siyu Tang",wang2021locally,00000200,"Registering point clouds of dressed humans to parametric human models is a challenging task in computer vision. Traditional approaches often rely on heavily engineered pipelines that require accurate manual initialization of human poses and tedious post-processing. More recently, learning-based methods are proposed in hope to automate this process. We observe that pose initialization is key to accurate registration but existing methods often fail to provide accurate pose initialization. One major obstacle is that, regressing joint rotations from point clouds or images of humans is still very challenging. To this end, we propose novel piecewise transformation fields (PTF), a set of functions that learn 3D translation vectors to map any query point in posed space to its correspond position in rest-pose space. We combine PTF with multi-class occupancy networks, obtaining a novel learning-based framework that learns to simultaneously predict shape and per-point correspondences between the posed space and the canonical space for clothed human. Our key insight is that the translation vector for each query point can be effectively estimated using the point-aligned local features; consequently, rigid per bone transformations and joint rotations can be obtained efficiently via a least-square fitting given the estimated point correspondences, circumventing the challenging task of directly regressing joint rotations from neural networks. Furthermore, the proposed PTF facilitate canonicalized occupancy estimation, which greatly improves generalization capability and results in more accurate surface reconstruction with only half of the parameters compared with the state-of-the-art. Both qualitative and quantitative studies show that fitting parametric models with poses initialized by our network results in much better registration quality, especially for extreme poses.",,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnud735hb1_brvWCIYh9WEybeut7gZimQCCINOraM39l8LjBHMD6UTC0cBtbNVoBkzAM
5/23/2021 18:20:16,FiG-NeRF: Figure-Ground Neural Radiance Fields for 3D Object Category Modelling,FiG-NeRF,4/17/2021,https://arxiv.org/pdf/2104.08418.pdf,https://fig-nerf.github.io/,,https://www.youtube.com/watch?v=WtZxuv_hkic,,,,"@article{xie2021fignerf,
  AUTHOR = {Christopher Xie and Keunhong Park and Ricardo Martin-Brualla and Matthew Brown},
  TITLE = {FiG-NeRF: Figure-Ground Neural Radiance Fields for 3D Object CategoryModelling},
  EPRINT = {2104.08418v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We investigate the use of Neural Radiance Fields (NeRF) to learn high quality3D object category models from collections of input images. In contrast toprevious work, we are able to do this whilst simultaneously separatingforeground objects from their varying backgrounds. We achieve this via a2-component NeRF model, FiG-NeRF, that prefers explanation of the scene as ageometrically constant background and a deformable foreground that representsthe object category. We show that this method can learn accurate 3D objectcategory models using only photometric supervision and casually captured imagesof the objects. Additionally, our 2-part decomposition allows the model toperform accurate and crisp amodal segmentation. We quantitatively evaluate ourmethod with view synthesis and image fidelity metrics, using synthetic,lab-captured, and in-the-wild data. Our results demonstrate convincing 3Dobject category modelling that exceed the performance of existing methods.},
  YEAR = {2021},
  MONTH = {Apr},
  URL = {http://arxiv.org/abs/2104.08418v1},
  FILE = {2104.08418v1.pdf}
 }","Generalization, Segmentation/composition, Fundamentals",Conditional neural field,,,Category-level,,,,,,,,,,,"Christopher Xie, Keunhong Park, Ricardo Martin-Brualla, Matthew Brown",xie2021fignerf,00000118,"We investigate the use of Neural Radiance Fields (NeRF) to learn high quality 3D object category models from collections of input images. In contrast to previous work, we are able to do this whilst simultaneously separating foreground objects from their varying backgrounds. We achieve this via a 2-component NeRF model, FiG-NeRF, that prefers explanation of the scene as a geometrically constant background and a deformable foreground that represents the object category. We show that this method can learn accurate 3D object category models using only photometric supervision and casually captured images of the objects. Additionally, our 2-part decomposition allows the model to perform accurate and crisp amodal segmentation. We quantitatively evaluate our method with view synthesis and image fidelity metrics, using synthetic, lab-captured, and in-the-wild data. Our results demonstrate convincing 3D object category modelling that exceed the performance of existing methods.",1,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnueGSyESER0aFFTZ0F2Qmuk6zEPafOunzpbKNCIz8OLecQhPf6qLRrM3kP40gIMAevY
7/19/2021 21:59:14,SAPE: Spatially-Adaptive Progressive Encoding for Neural Optimization,SAPE,4/19/2021,https://arxiv.org/pdf/2104.09125.pdf,,,,,,,"@article{hertz2021sape,
  AUTHOR = {Amir Hertz and Or Perel and Raja Giryes and Olga Sorkine-Hornung and Daniel Cohen-Or},
  TITLE = {SAPE: Spatially-Adaptive Progressive Encoding for Neural Optimization},
  EPRINT = {2104.09125v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.LG},
  ABSTRACT = {Multilayer-perceptrons (MLP) are known to struggle with learning functions ofhigh-frequencies, and in particular cases with wide frequency bands. We presenta spatially adaptive progressive encoding (SAPE) scheme for input signals ofMLP networks, which enables them to better fit a wide range of frequencieswithout sacrificing training stability or requiring any domain specificpreprocessing. SAPE gradually unmasks signal components with increasingfrequencies as a function of time and space. The progressive exposure offrequencies is monitored by a feedback loop throughout the neural optimizationprocess, allowing changes to propagate at different rates among local spatialportions of the signal space. We demonstrate the advantage of SAPE on a varietyof domains and applications, including regression of low dimensional signalsand images, representation learning of occupancy networks, and a geometric taskof mesh transfer between 3D shapes.},
  YEAR = {2021},
  MONTH = {Apr},
  URL = {http://arxiv.org/abs/2104.09125v2},
  FILE = {2104.09125v2.pdf}
 }",Fundamentals,Coarse-to-fine,Other,,,,,,,,,,,,,"Amir Hertz, Or Perel, Raja Giryes, Olga Sorkine-Hornung, Daniel Cohen-Or",hertz2021sape,00000119,"Multilayer-perceptrons (MLP) are known to struggle with learning functions of high-frequencies, and in particular cases with wide frequency bands. We present a spatially adaptive progressive encoding (SAPE) scheme for input signals of MLP networks, which enables them to better fit a wide range of frequencies without sacrificing training stability or requiring any domain specific preprocessing. SAPE gradually unmasks signal components with increasing frequencies as a function of time and space. The progressive exposure of frequencies is monitored by a feedback loop throughout the neural optimization process, allowing changes to propagate at different rates among local spatial portions of the signal space. We demonstrate the advantage of SAPE on a variety of domains and applications, including regression of low dimensional signals and images, representation learning of occupancy networks, and a geometric task of mesh transfer between 3D shapes.",1,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnueLMX_U0Dg_kyK06UDa5Ql_-UPWhjrk9_JJaSNXV2g-zXVLtR7a_bRzVnBgkAxCQGc
7/20/2021 15:00:44,Shadow Neural Radiance Fields for Multi-view Satellite Photogrammetry,S-NeRF,4/20/2021,https://arxiv.org/pdf/2104.09877.pdf,,,,,,,"@article{derksen2021snerf,
  AUTHOR = {Dawa Derksen and Dario Izzo},
  TITLE = {Shadow Neural Radiance Fields for Multi-view Satellite Photogrammetry},
  EPRINT = {2104.09877v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We present a new generic method for shadow-aware multi-view satellitephotogrammetry of Earth Observation scenes. Our proposed method, the ShadowNeural Radiance Field (S-NeRF) follows recent advances in implicit volumetricrepresentation learning. For each scene, we train S-NeRF using very highspatial resolution optical images taken from known viewing angles. The learningrequires no labels or shape priors: it is self-supervised by an imagereconstruction loss. To accommodate for changing light source conditions bothfrom a directional light source (the Sun) and a diffuse light source (the sky),we extend the NeRF approach in two ways. First, direct illumination from theSun is modeled via a local light source visibility field. Second, indirectillumination from a diffuse light source is learned as a non-local color fieldas a function of the position of the Sun. Quantitatively, the combination ofthese factors reduces the altitude and color errors in shaded areas, comparedto NeRF. The S-NeRF methodology not only performs novel view synthesis and full3D shape estimation, it also enables shadow detection, albedo synthesis, andtransient object filtering, without any explicit shape supervision.},
  YEAR = {2021},
  MONTH = {Apr},
  URL = {http://arxiv.org/abs/2104.09877v1},
  FILE = {2104.09877v1.pdf}
 }",Material/lighting estimation,Sampling,,,,,,,,,,CVPR 2020,,,,"Dawa Derksen, Dario Izzo",derksen2021snerf,00000120,"We present a new generic method for shadow-aware multi-view satellite photogrammetry of Earth Observation scenes. Our proposed method, the Shadow Neural Radiance Field (S-NeRF) follows recent advances in implicit volumetric representation learning. For each scene, we train S-NeRF using very high spatial resolution optical images taken from known viewing angles. The learning requires no labels or shape priors: it is self-supervised by an image reconstruction loss. To accommodate for changing light source conditions both from a directional light source (the Sun) and a diffuse light source (the sky), we extend the NeRF approach in two ways. First, direct illumination from the Sun is modeled via a local light source visibility field. Second, indirect illumination from a diffuse light source is learned as a non-local color field as a function of the position of the Sun. Quantitatively, the combination of these factors reduces the altitude and color errors in shaded areas, compared to NeRF. The S-NeRF methodology not only performs novel view synthesis and full 3D shape estimation, it also enables shadow detection, albedo synthesis, and transient object filtering, without any explicit shape supervision.",0,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnuf6QkJKvTXvKqOil8vo2uRBN3TJ6yEoWYOIphGvmWwyLUnwmAEYoToBy_DNgMv8LPA
5/23/2021 18:32:09,UNISURF: Unifying Neural Implicit Surfaces and Radiance Fields for Multi-View Reconstruction,UNISURF,4/20/2021,https://arxiv.org/pdf/2104.10078.pdf,https://arxiv.org/pdf/2104.10078.pdf,,,,,,"@article{oechsle2021unisurf,
  AUTHOR = {Michael Oechsle and Songyou Peng and Andreas Geiger},
  TITLE = {UNISURF: Unifying Neural Implicit Surfaces and Radiance Fields forMulti-View Reconstruction},
  EPRINT = {2104.10078v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Neural implicit 3D representations have emerged as a powerful paradigm forreconstructing surfaces from multi-view images and synthesizing novel views.Unfortunately, existing methods such as DVR or IDR require accurate per-pixelobject masks as supervision. At the same time, neural radiance fields haverevolutionized novel view synthesis. However, NeRF's estimated volume densitydoes not admit accurate surface reconstruction. Our key insight is thatimplicit surface models and radiance fields can be formulated in a unified way,enabling both surface and volume rendering using the same model. This unifiedperspective enables novel, more efficient sampling procedures and the abilityto reconstruct accurate surfaces without input masks. We compare our method onthe DTU, BlendedMVS, and a synthetic indoor dataset. Our experimentsdemonstrate that we outperform NeRF in terms of reconstruction quality whileperforming on par with IDR without requiring masks.},
  YEAR = {2021},
  MONTH = {Apr},
  URL = {http://arxiv.org/abs/2104.10078v1},
  FILE = {2104.10078v1.pdf}
 }",Fundamentals,Representation,,Occupancy,,,,,,,,,,No,,"Michael Oechsle, Songyou Peng, Andreas Geiger",oechsle2021unisurf,00000121,"Neural implicit 3D representations have emerged as a powerful paradigm for reconstructing surfaces from multi-view images and synthesizing novel views. Unfortunately, existing methods such as DVR or IDR require accurate per-pixel object masks as supervision. At the same time, neural radiance fields have revolutionized novel view synthesis. However, NeRF's estimated volume density does not admit accurate surface reconstruction. Our key insight is that implicit surface models and radiance fields can be formulated in a unified way, enabling both surface and volume rendering using the same model. This unified perspective enables novel, more efficient sampling procedures and the ability to reconstruct accurate surfaces without input masks. We compare our method on the DTU, BlendedMVS, and a synthetic indoor dataset. Our experiments demonstrate that we outperform NeRF in terms of reconstruction quality while performing on par with IDR without requiring masks.",7,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnucGp9idfZQdOyqa5rPex2jBkzaRljSYAtt8ZBSu1LyPdtv29tjUkTuYXvRPO3OL1vg
5/23/2021 18:21:02,Dynamic CT Reconstruction from Limited Views with Implicit Neural Representations and Parametric Motion Fields,INR,4/23/2021,https://arxiv.org/pdf/2104.11745.pdf,,,,,,,"@article{reed2021inr,
  AUTHOR = {Albert W. Reed and Hyojin Kim and Rushil Anirudh and K. Aditya Mohan and Kyle Champley and Jingu Kang and Suren Jayasuriya},
  TITLE = {Dynamic CT Reconstruction from Limited Views with Implicit NeuralRepresentations and Parametric Motion Fields},
  EPRINT = {2104.11745v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {eess.IV},
  ABSTRACT = {Reconstructing dynamic, time-varying scenes with computed tomography (4D-CT)is a challenging and ill-posed problem common to industrial and medicalsettings. Existing 4D-CT reconstructions are designed for sparse samplingschemes that require fast CT scanners to capture multiple, rapid revolutionsaround the scene in order to generate high quality results. However, if thescene is moving too fast, then the sampling occurs along a limited view and isdifficult to reconstruct due to spatiotemporal ambiguities. In this work, wedesign a reconstruction pipeline using implicit neural representations coupledwith a novel parametric motion field warping to perform limited view 4D-CTreconstruction of rapidly deforming scenes. Importantly, we utilize adifferentiable analysis-by-synthesis approach to compare with captured x-raysinogram data in a self-supervised fashion. Thus, our resulting optimizationmethod requires no training data to reconstruct the scene. We demonstrate thatour proposed system robustly reconstructs scenes containing deformable andperiodic motion and validate against state-of-the-art baselines. Further, wedemonstrate an ability to reconstruct continuous spatiotemporal representationsof our scenes and upsample them to arbitrary volumes and frame ratespost-optimization. This research opens a new avenue for implicit neuralrepresentations in computed tomography reconstruction in general.},
  YEAR = {2021},
  MONTH = {Apr},
  URL = {http://arxiv.org/abs/2104.11745v1},
  FILE = {2104.11745v1.pdf}
 }","Dynamic, Beyond graphics, Science and engineering",Warping field/Flow field,,,,,,,,,,,,,,"Albert W. Reed, Hyojin Kim, Rushil Anirudh, K. Aditya Mohan, Kyle Champley, Jingu Kang, Suren Jayasuriya",reed2021inr,00000122,"Reconstructing dynamic, time-varying scenes with computed tomography (4D-CT) is a challenging and ill-posed problem common to industrial and medical settings. Existing 4D-CT reconstructions are designed for sparse sampling schemes that require fast CT scanners to capture multiple, rapid revolutions around the scene in order to generate high quality results. However, if the scene is moving too fast, then the sampling occurs along a limited view and is difficult to reconstruct due to spatiotemporal ambiguities. In this work, we design a reconstruction pipeline using implicit neural representations coupled with a novel parametric motion field warping to perform limited view 4D-CT reconstruction of rapidly deforming scenes. Importantly, we utilize a differentiable analysis-by-synthesis approach to compare with captured x-ray sinogram data in a self-supervised fashion. Thus, our resulting optimization method requires no training data to reconstruct the scene. We demonstrate that our proposed system robustly reconstructs scenes containing deformable and periodic motion and validate against state-of-the-art baselines. Further, we demonstrate an ability to reconstruct continuous spatiotemporal representations of our scenes and upsample them to arbitrary volumes and frame rates post-optimization. This research opens a new avenue for implicit neural representations in computed tomography reconstruction in general.",1,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnueqisWqLF-xm-eBwYXf8i5YkWwy_trQEBO-qkxmHaSihSIo4F51EwSrht_cAnxoqZc
8/29/2021 16:25:16,Vector Neurons: A General Framework for SO(3)-Equivariant Networks,Vector Neurons,4/25/2021,https://arxiv.org/pdf/2104.12229.pdf,https://cs.stanford.edu/~congyue/vnn/,"https://github.com/FlyingGiraffe/vnn, https://github.com/FlyingGiraffe/vnn-neural-implicits/",Coming soon,,,,"@article{deng2021vector neurons,
  AUTHOR = {Congyue Deng and Or Litany and Yueqi Duan and Adrien Poulenard and Andrea Tagliasacchi and Leonidas Guibas},
  TITLE = {Vector Neurons: A General Framework for SO(3)-Equivariant Networks},
  EPRINT = {2104.12229v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Invariance and equivariance to the rotation group have been widely discussedin the 3D deep learning community for pointclouds. Yet most proposed methodseither use complex mathematical tools that may limit their accessibility, orare tied to specific input data types and network architectures. In this paper,we introduce a general framework built on top of what we call Vector Neuronrepresentations for creating SO(3)-equivariant neural networks for pointcloudprocessing. Extending neurons from 1D scalars to 3D vectors, our vector neuronsenable a simple mapping of SO(3) actions to latent spaces thereby providing aframework for building equivariance in common neural operations -- includinglinear layers, non-linearities, pooling, and normalizations. Due to theirsimplicity, vector neurons are versatile and, as we demonstrate, can beincorporated into diverse network architecture backbones, allowing them toprocess geometry inputs in arbitrary poses. Despite its simplicity, our methodperforms comparably well in accuracy and generalization with other more complexand specialized state-of-the-art methods on classification and segmentationtasks. We also show for the first time a rotation equivariant reconstructionnetwork.},
  YEAR = {2021},
  MONTH = {Apr},
  URL = {http://arxiv.org/abs/2104.12229v1},
  FILE = {2104.12229v1.pdf}
 }","Generalization, Fundamentals","Data-driven component (pre-trained, cross-scene), Symmetry",,Occupancy,Category-level,,,,,,,,,Yes,Direct,"Congyue Deng, Or Litany, Yueqi Duan, Adrien Poulenard, Andrea Tagliasacchi, Leonidas Guibas",deng2021vector neurons,00000164,"Invariance and equivariance to the rotation group have been widely discussed in the 3D deep learning community for pointclouds. Yet most proposed methods either use complex mathematical tools that may limit their accessibility, or are tied to specific input data types and network architectures. In this paper, we introduce a general framework built on top of what we call Vector Neuron representations for creating SO(3)-equivariant neural networks for pointcloud processing. Extending neurons from 1D scalars to 3D vectors, our vector neurons enable a simple mapping of SO(3) actions to latent spaces thereby providing a framework for building equivariance in common neural operations -- including linear layers, non-linearities, pooling, and normalizations. Due to their simplicity, vector neurons are versatile and, as we demonstrate, can be incorporated into diverse network architecture backbones, allowing them to process geometry inputs in arbitrary poses. Despite its simplicity, our method performs comparably well in accuracy and generalization with other more complex and specialized state-of-the-art methods on classification and segmentation tasks. We also show for the first time a rotation equivariant reconstruction network.",1,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnudRwmBQ2J1-lOyhMDfmHqEewMnlyjfkKuN-XDbvEsLJfttbjlmxr5yXARgrUWW9k1w
5/23/2021 18:19:17,Neural Ray-Tracing: Learning Surfaces and Reflectance for Relighting and View Synthesis,Neural Ray-Tracing,4/28/2021,https://arxiv.org/pdf/2104.13562.pdf,,https://github.com/princeton-computational-imaging/neural_raytracing,,,,,"@article{knodt2021neural raytracing,
  AUTHOR = {Julian Knodt and Seung-Hwan Baek and Felix Heide},
  TITLE = {Neural Ray-Tracing: Learning Surfaces and Reflectance for Relighting andView Synthesis},
  EPRINT = {2104.13562v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Recent neural rendering methods have demonstrated accurate view interpolationby predicting volumetric density and color with a neural network. Although suchvolumetric representations can be supervised on static and dynamic scenes,existing methods implicitly bake the complete scene light transport into asingle neural network for a given scene, including surface modeling,bidirectional scattering distribution functions, and indirect lighting effects.In contrast to traditional rendering pipelines, this prohibits changing surfacereflectance, illumination, or composing other objects in the scene.In this work, we explicitly model the light transport between scene surfacesand we rely on traditional integration schemes and the rendering equation toreconstruct a scene. The proposed method allows BSDF recovery with unknownlight conditions and classic light transports such as pathtracing. By learningdecomposed transport with surface representations established in conventionalrendering methods, the method naturally facilitates editing shape, reflectance,lighting and scene composition. The method outperforms NeRV for relightingunder known lighting conditions, and produces realistic reconstructions forrelit and edited scenes. We validate the proposed approach for scene editing,relighting and reflectance estimation learned from synthetic and captured viewson a subset of NeRV's datasets.},
  YEAR = {2021},
  MONTH = {Apr},
  URL = {http://arxiv.org/abs/2104.13562v1},
  FILE = {2104.13562v1.pdf}
 }","Editable, Material/lighting estimation","Learning residual, Representation",,SDF,Per-scene,,,,,,,CVPR,,No,Direct,"Julian Knodt, Seung-Hwan Baek, Felix Heide",knodt2021neuralraytracing,00000123,"Recent neural rendering methods have demonstrated accurate view interpolation by predicting volumetric density and color with a neural network. Although such volumetric representations can be supervised on static and dynamic scenes, existing methods implicitly bake the complete scene light transport into a single neural network for a given scene, including surface modeling, bidirectional scattering distribution functions, and indirect lighting effects. In contrast to traditional rendering pipelines, this prohibits changing surface reflectance, illumination, or composing other objects in the scene. In this work, we explicitly model the light transport between scene surfaces and we rely on traditional integration schemes and the rendering equation to reconstruct a scene. The proposed method allows BSDF recovery with unknown light conditions and classic light transports such as pathtracing. By learning decomposed transport with surface representations established in conventional rendering methods, the method naturally facilitates editing shape, reflectance, lighting and scene composition. The method outperforms NeRV for relighting under known lighting conditions, and produces realistic reconstructions for relit and edited scenes. We validate the proposed approach for scene editing, relighting and reflectance estimation learned from synthetic and captured views on a subset of NeRV's datasets.",0,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnufPpWAcLchhyZ0sQ3I4df3qkPJgfX6aa-lZuBRy6gpyIFcgTAK5NFKObcL-zQjAviY
5/23/2021 18:18:54,Editable Free-Viewpoint Video using a Layered Neural Representation,ST-NeRF,4/30/2021,https://arxiv.org/pdf/2104.14786.pdf,,,https://arxiv.org/pdf/2104.14786.pdf,,,,"@article{zhang2021stnerf,
  AUTHOR = {Jiakai Zhang and Xinhang Liu and Xinyi Ye and Fuqiang Zhao and Yanshun Zhang and Minye Wu and Yingliang Zhang and Lan Xu and Jingyi Yu},
  TITLE = {Editable Free-viewpoint Video Using a Layered Neural Representation},
  EPRINT = {2104.14786v1},
  DOI = {10.1145/3450626.3459756},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Generating free-viewpoint videos is critical for immersive VR/AR experiencebut recent neural advances still lack the editing ability to manipulate thevisual perception for large dynamic scenes. To fill this gap, in this paper wepropose the first approach for editable photo-realistic free-viewpoint videogeneration for large-scale dynamic scenes using only sparse 16 cameras. Thecore of our approach is a new layered neural representation, where each dynamicentity including the environment itself is formulated into a space-timecoherent neural layered radiance representation called ST-NeRF. Such layeredrepresentation supports fully perception and realistic manipulation of thedynamic scene whilst still supporting a free viewing experience in a widerange. In our ST-NeRF, the dynamic entity/layer is represented as continuousfunctions, which achieves the disentanglement of location, deformation as wellas the appearance of the dynamic entity in a continuous and self-supervisedmanner. We propose a scene parsing 4D label map tracking to disentangle thespatial information explicitly, and a continuous deform module to disentanglethe temporal motion implicitly. An object-aware volume rendering scheme isfurther introduced for the re-assembling of all the neural layers. We adopt anovel layered loss and motion-aware ray sampling strategy to enable efficienttraining for a large dynamic scene with multiple performers, Our frameworkfurther enables a variety of editing functions, i.e., manipulating the scaleand location, duplicating or retiming individual neural layers to createnumerous visual effects while preserving high realism. Extensive experimentsdemonstrate the effectiveness of our approach to achieve high-quality,photo-realistic, and editable free-viewpoint video generation for dynamicscenes.},
  YEAR = {2021},
  MONTH = {Apr},
  URL = {http://arxiv.org/abs/2104.14786v1},
  FILE = {2104.14786v1.pdf}
 }","Dynamic, Editable, Segmentation/composition","Volume partitioning, Warping field/Flow field, Object-centric representation",,,,,,,,,,,,,,"Jiakai Zhang, Xinhang Liu, Xinyi Ye, Fuqiang Zhao, Yanshun Zhang, Minye Wu, Yingliang Zhang, Lan Xu, Jingyi Yu",zhang2021stnerf,00000124,"Generating free-viewpoint videos is critical for immersive VR/AR experience but recent neural advances still lack the editing ability to manipulate the visual perception for large dynamic scenes. To fill this gap, in this paper we propose the first approach for editable photo-realistic free-viewpoint video generation for large-scale dynamic scenes using only sparse 16 cameras. The core of our approach is a new layered neural representation, where each dynamic entity including the environment itself is formulated into a space-time coherent neural layered radiance representation called ST-NeRF. Such layered representation supports fully perception and realistic manipulation of the dynamic scene whilst still supporting a free viewing experience in a wide range. In our ST-NeRF, the dynamic entity/layer is represented as continuous functions, which achieves the disentanglement of location, deformation as well as the appearance of the dynamic entity in a continuous and self-supervised manner. We propose a scene parsing 4D label map tracking to disentangle the spatial information explicitly, and a continuous deform module to disentangle the temporal motion implicitly. An object-aware volume rendering scheme is further introduced for the re-assembling of all the neural layers. We adopt a novel layered loss and motion-aware ray sampling strategy to enable efficient training for a large dynamic scene with multiple performers, Our framework further enables a variety of editing functions, i.e., manipulating the scale and location, duplicating or retiming individual neural layers to create numerous visual effects while preserving high realism. Extensive experiments demonstrate the effectiveness of our approach to achieve high-quality, photo-realistic, and editable free-viewpoint video generation for dynamic scenes.",1,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnudu3kBKWr1o7mc6tCAIMd9sNjjokGEwcP3jlsYrRI3hRBRzYJwr6TwDBvPyFGMMrVM
5/23/2021 18:54:15,3D Scene Compression through Entropy Penalized Neural Representation Functions,cNeRF,5/3/2021,https://arxiv.org/pdf/2104.12456.pdf,,,,,,,"@article{bird2021cnerf,
  AUTHOR = {Thomas Bird and Johannes Balle and Saurabh Singh and Philip A. Chou},
  TITLE = {3D Scene Compression through Entropy Penalized Neural RepresentationFunctions},
  EPRINT = {2104.12456v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Some forms of novel visual media enable the viewer to explore a 3D scene fromarbitrary viewpoints, by interpolating between a discrete set of originalviews. Compared to 2D imagery, these types of applications require much largeramounts of storage space, which we seek to reduce. Existing approaches forcompressing 3D scenes are based on a separation of compression and rendering:each of the original views is compressed using traditional 2D image formats;the receiver decompresses the views and then performs the rendering. We unifythese steps by directly compressing an implicit representation of the scene, afunction that maps spatial coordinates to a radiance vector field, which canthen be queried to render arbitrary viewpoints. The function is implemented asa neural network and jointly trained for reconstruction as well ascompressibility, in an end-to-end manner, with the use of an entropy penalty onthe parameters. Our method significantly outperforms a state-of-the-artconventional approach for scene compression, achieving simultaneously higherquality reconstructions and lower bitrates. Furthermore, we show that theperformance at lower bitrates can be improved by jointly representing multiplescenes using a soft form of parameter sharing.},
  YEAR = {2021},
  MONTH = {Apr},
  URL = {http://arxiv.org/abs/2104.12456v1},
  FILE = {2104.12456v1.pdf}
 }",Compression,,,,,,,,,,,CVPR 2020,,,,"Thomas Bird, Johannes Ballé, Saurabh Singh, Philip A. Chou",bird2021cnerf,00000125,"Some forms of novel visual media enable the viewer to explore a 3D scene from arbitrary viewpoints, by interpolating between a discrete set of original views. Compared to 2D imagery, these types of applications require much larger amounts of storage space, which we seek to reduce. Existing approaches for compressing 3D scenes are based on a separation of compression and rendering: each of the original views is compressed using traditional 2D image formats; the receiver decompresses the views and then performs the rendering. We unify these steps by directly compressing an implicit representation of the scene, a function that maps spatial coordinates to a radiance vector field, which can then be queried to render arbitrary viewpoints. The function is implemented as a neural network and jointly trained for reconstruction as well as compressibility, in an end-to-end manner, with the use of an entropy penalty on the parameters. Our method significantly outperforms a state-of-the-art conventional approach for scene compression, achieving simultaneously higher quality reconstructions and lower bitrates. Furthermore, we show that the performance at lower bitrates can be improved by jointly representing multiple scenes using a soft form of parameter sharing.",0,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnue4tvrjIGefCS4jWCxYrMmJyiOm54qTzHBOTBhjeKmJgRv9KX2k1j_dXUZFEjR1_Rw
5/23/2021 18:17:15,Animatable Neural Radiance Fields for Human Body Modeling,,5/6/2021,https://arxiv.org/pdf/2105.02872.pdf,,https://arxiv.org/pdf/2105.02872.pdf,https://www.youtube.com/watch?v=eWOSWbmfJo4,,,,"@article{peng2021animatable,
  AUTHOR = {Sida Peng and Junting Dong and Qianqian Wang and Shangzhan Zhang and Qing Shuai and Hujun Bao and Xiaowei Zhou},
  TITLE = {Animatable Neural Radiance Fields for Human Body Modeling},
  EPRINT = {2105.02872v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {This paper addresses the challenge of reconstructing an animatable humanmodel from a multi-view video. Some recent works have proposed to decompose adynamic scene into a canonical neural radiance field and a set of deformationfields that map observation-space points to the canonical space, therebyenabling them to learn the dynamic scene from images. However, they representthe deformation field as translational vector field or SE(3) field, which makesthe optimization highly under-constrained. Moreover, these representationscannot be explicitly controlled by input motions. Instead, we introduce neuralblend weight fields to produce the deformation fields. Based on theskeleton-driven deformation, blend weight fields are used with 3D humanskeletons to generate observation-to-canonical and canonical-to-observationcorrespondences. Since 3D human skeletons are more observable, they canregularize the learning of deformation fields. Moreover, the learned blendweight fields can be combined with input skeletal motions to generate newdeformation fields to animate the human model. Experiments show that ourapproach significantly outperforms recent human synthesis methods. The codewill be available at https://zju3dv.github.io/animatable_nerf/.},
  YEAR = {2021},
  MONTH = {May},
  URL = {http://arxiv.org/abs/2105.02872v1},
  FILE = {2105.02872v1.pdf}
 }","Dynamic, Human body",Articulated,,,,,,,,,,3DV,,,,"Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Hujun Bao, Xiaowei Zhou",peng2021animatable,00000126,"This paper addresses the challenge of reconstructing an animatable human model from a multi-view video. Some recent works have proposed to decompose a dynamic scene into a canonical neural radiance field and a set of deformation fields that map observation-space points to the canonical space, thereby enabling them to learn the dynamic scene from images. However, they represent the deformation field as translational vector field or SE(3) field, which makes the optimization highly under-constrained. Moreover, these representations cannot be explicitly controlled by input motions. Instead, we introduce neural blend weight fields to produce the deformation fields. Based on the skeleton-driven deformation, blend weight fields are used with 3D human skeletons to generate observation-to-canonical and canonical-to-observation correspondences. Since 3D human skeletons are more observable, they can regularize the learning of deformation fields. Moreover, the learned blend weight fields can be combined with input skeletal motions to generate new deformation fields to animate the human model. Experiments show that our approach significantly outperforms recent human synthesis methods. The code will be available at https://zju3dv.github.io/animatable_nerf/.",1,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnud83yRHhP7QeUGBGKT_VcJgJ5dWdokbVWmQK4oHEmgEScgTNCKdUeHfNJjd7aQznIM
6/29/2021 16:36:45,acorn: Adaptive Coordinate Networks for Neural Scene Representation,ACORN,5/6/2021,https://arxiv.org/pdf/2105.02788.pdf,,,,,,,"@article{martel2021acorn,
  AUTHOR = {Julien N. P. Martel and David B. Lindell and Connor Z. Lin and Eric R. Chan and Marco Monteiro and Gordon Wetzstein},
  TITLE = {ACORN: Adaptive Coordinate Networks for Neural Scene Representation},
  EPRINT = {2105.02788v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Neural representations have emerged as a new paradigm for applications inrendering, imaging, geometric modeling, and simulation. Compared to traditionalrepresentations such as meshes, point clouds, or volumes they can be flexiblyincorporated into differentiable learning-based pipelines. While recentimprovements to neural representations now make it possible to representsignals with fine details at moderate resolutions (e.g., for images and 3Dshapes), adequately representing large-scale or complex scenes has proven achallenge. Current neural representations fail to accurately represent imagesat resolutions greater than a megapixel or 3D scenes with more than a fewhundred thousand polygons. Here, we introduce a new hybrid implicit-explicitnetwork architecture and training strategy that adaptively allocates resourcesduring training and inference based on the local complexity of a signal ofinterest. Our approach uses a multiscale block-coordinate decomposition,similar to a quadtree or octree, that is optimized during training. The networkarchitecture operates in two stages: using the bulk of the network parameters,a coordinate encoder generates a feature grid in a single forward pass. Then,hundreds or thousands of samples within each block can be efficiently evaluatedusing a lightweight feature decoder. With this hybrid implicit-explicit networkarchitecture, we demonstrate the first experiments that fit gigapixel images tonearly 40 dB peak signal-to-noise ratio. Notably this represents an increase inscale of over 1000x compared to the resolution of previously demonstratedimage-fitting experiments. Moreover, our approach is able to represent 3Dshapes significantly faster and better than previous techniques; it reducestraining times from days to hours or minutes and memory requirements by over anorder of magnitude.},
  YEAR = {2021},
  MONTH = {May},
  URL = {http://arxiv.org/abs/2105.02788v1},
  FILE = {2105.02788v1.pdf}
 }","Performance (training), Performance (rendering)","Conditional neural field, Coarse-to-fine, Sampling, Voxelization, Feature volume, Representation",NeRF,Occupancy,Per-scene,,,,,,,3DV,,Yes,,"Julien N. P. Martel, David B. Lindell, Connor Z. Lin, Eric R. Chan, Marco Monteiro, Gordon Wetzstein",martel2021acorn,00000127,"Neural representations have emerged as a new paradigm for applications in rendering, imaging, geometric modeling, and simulation. Compared to traditional representations such as meshes, point clouds, or volumes they can be flexibly incorporated into differentiable learning-based pipelines. While recent improvements to neural representations now make it possible to represent signals with fine details at moderate resolutions (e.g., for images and 3D shapes), adequately representing large-scale or complex scenes has proven a challenge. Current neural representations fail to accurately represent images at resolutions greater than a megapixel or 3D scenes with more than a few hundred thousand polygons. Here, we introduce a new hybrid implicit-explicit network architecture and training strategy that adaptively allocates resources during training and inference based on the local complexity of a signal of interest. Our approach uses a multiscale block-coordinate decomposition, similar to a quadtree or octree, that is optimized during training. The network architecture operates in two stages: using the bulk of the network parameters, a coordinate encoder generates a feature grid in a single forward pass. Then, hundreds or thousands of samples within each block can be efficiently evaluated using a lightweight feature decoder. With this hybrid implicit-explicit network architecture, we demonstrate the first experiments that fit gigapixel images to nearly 40 dB peak signal-to-noise ratio. Notably this represents an increase in scale of over 1000x compared to the resolution of previously demonstrated image-fitting experiments. Moreover, our approach is able to represent 3D shapes significantly faster and better than previous techniques; it reduces training times from days to hours or minutes and memory requirements by over an order of magnitude.",2,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnuf0_BGyB3qsBHX8bwcbgxs_vOsWALqULd-oM2LM6liA3BHZu_D35CL9iN85aK408yc
5/23/2021 18:17:41,Neural 3D Scene Compression via Model Compression,,5/7/2021,https://arxiv.org/pdf/2105.03120.pdf,,,,,,,"@article{isik2021neural,
  AUTHOR = {Berivan Isik},
  TITLE = {Neural 3D Scene Compression via Model Compression},
  EPRINT = {2105.03120v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Rendering 3D scenes requires access to arbitrary viewpoints from the scene.Storage of such a 3D scene can be done in two ways; (1) storing 2D images takenfrom the 3D scene that can reconstruct the scene back through interpolations,or (2) storing a representation of the 3D scene itself that already encodesviews from all directions. So far, traditional 3D compression methods havefocused on the first type of storage and compressed the original 2D images withimage compression techniques. With this approach, the user first decodes thestored 2D images and then renders the 3D scene. However, this separatedprocedure is inefficient since a large amount of 2D images have to be stored.In this work, we take a different approach and compress a functionalrepresentation of 3D scenes. In particular, we introduce a method to compress3D scenes by compressing the neural networks that represent the scenes asneural radiance fields. Our method provides more efficient storage of 3D scenessince it does not store 2D images -- which are redundant when we render thescene from the neural functional representation.},
  YEAR = {2021},
  MONTH = {May},
  URL = {http://arxiv.org/abs/2105.03120v1},
  FILE = {2105.03120v1.pdf}
 }",Compression,,,,,,,,,,,CVPR 2020,,,,Berivan Isik,isik2021neural,00000128,"Rendering 3D scenes requires access to arbitrary viewpoints from the scene. Storage of such a 3D scene can be done in two ways; (1) storing 2D images taken from the 3D scene that can reconstruct the scene back through interpolations, or (2) storing a representation of the 3D scene itself that already encodes views from all directions. So far, traditional 3D compression methods have focused on the first type of storage and compressed the original 2D images with image compression techniques. With this approach, the user first decodes the stored 2D images and then renders the 3D scene. However, this separated procedure is inefficient since a large amount of 2D images have to be stored. In this work, we take a different approach and compress a functional representation of 3D scenes. In particular, we introduce a method to compress 3D scenes by compressing the neural networks that represent the scenes as neural radiance fields. Our method provides more efficient storage of 3D scenes since it does not store 2D images -- which are redundant when we render the scene from the neural functional representation.",0,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnufIUvxeB2qjL0Wehm4wF7D-4mTzYhTg8UOLRz7xHKNVH73Lyqa3Oqeb66xBxWj0HEA
5/23/2021 18:12:40,Vision-based Neural Scene Representations for Spacecraft,,5/11/2021,https://arxiv.org/pdf/2105.06405.pdf,,,,,,,"@article{mergy2021visionbased,
  AUTHOR = {Anne Mergy and Gurvan Lecuyer and Dawa Derksen and Dario Izzo},
  TITLE = {Vision-based Neural Scene Representations for Spacecraft},
  EPRINT = {2105.06405v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {In advanced mission concepts with high levels of autonomy, spacecraft need tointernally model the pose and shape of nearby orbiting objects. Recent works inneural scene representations show promising results for inferring genericthree-dimensional scenes from optical images. Neural Radiance Fields (NeRF)have shown success in rendering highly specular surfaces using a large numberof images and their pose. More recently, Generative Radiance Fields (GRAF)achieved full volumetric reconstruction of a scene from unposed images only,thanks to the use of an adversarial framework to train a NeRF. In this paper,we compare and evaluate the potential of NeRF and GRAF to render novel viewsand extract the 3D shape of two different spacecraft, the Soil Moisture andOcean Salinity satellite of ESA's Living Planet Programme and a generic cubesat. Considering the best performances of both models, we observe that NeRF hasthe ability to render more accurate images regarding the material specularityof the spacecraft and its pose. For its part, GRAF generates precise novelviews with accurate details even when parts of the satellites are shadowedwhile having the significant advantage of not needing any information about therelative pose.},
  YEAR = {2021},
  MONTH = {May},
  URL = {http://arxiv.org/abs/2105.06405v1},
  FILE = {2105.06405v1.pdf}
 }","Few-shot reconstruction, Science and engineering",Lifting 2D features to 3D,,,,,,,,,,,,,,"Anne Mergy, Gurvan Lecuyer, Dawa Derksen, Dario Izzo",mergy2021visionbased,00000129,"In advanced mission concepts with high levels of autonomy, spacecraft need to internally model the pose and shape of nearby orbiting objects. Recent works in neural scene representations show promising results for inferring generic three-dimensional scenes from optical images. Neural Radiance Fields (NeRF) have shown success in rendering highly specular surfaces using a large number of images and their pose. More recently, Generative Radiance Fields (GRAF) achieved full volumetric reconstruction of a scene from unposed images only, thanks to the use of an adversarial framework to train a NeRF. In this paper, we compare and evaluate the potential of NeRF and GRAF to render novel views and extract the 3D shape of two different spacecraft, the Soil Moisture and Ocean Salinity satellite of ESA's Living Planet Programme and a generic cube sat. Considering the best performances of both models, we observe that NeRF has the ability to render more accurate images regarding the material specularity of the spacecraft and its pose. For its part, GRAF generates precise novel views with accurate details even when parts of the satellites are shadowed while having the significant advantage of not needing any information about the relative pose.",0,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnuc73tCCFtFwesBXRuJhAFCnpqrR85wo6C6-wGpAXRcPGd3QOxeovLelZmcuz8HS3U8
5/19/2021 18:01:42,Electrocardio Panorama: Synthesizing New ECG Views with Self-supervision,Nef-Net,5/12/2021,https://arxiv.org/pdf/2105.06293.pdf,,,,,,,"@article{chen2021nefnet,
  AUTHOR = {Jintai Chen and Xiangshang Zheng and Hongyun Yu and Danny Z. Chen and Jian Wu},
  TITLE = {Electrocardio Panorama: Synthesizing New ECG Views with Self-supervision},
  EPRINT = {2105.06293v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {eess.SP},
  ABSTRACT = {Multi-lead electrocardiogram (ECG) provides clinical information ofheartbeats from several fixed viewpoints determined by the lead positioning.However, it is often not satisfactory to visualize ECG signals in these fixedand limited views, as some clinically useful information is represented onlyfrom a few specific ECG viewpoints. For the first time, we propose a newconcept, Electrocardio Panorama, which allows visualizing ECG signals from anyqueried viewpoints. To build Electrocardio Panorama, we assume that anunderlying electrocardio field exists, representing locations, magnitudes, anddirections of ECG signals. We present a Neural electrocardio field Network(Nef-Net), which first predicts the electrocardio field representation by usinga sparse set of one or few input ECG views and then synthesizes ElectrocardioPanorama based on the predicted representations. Specially, to betterdisentangle electrocardio field information from viewpoint biases, a newAngular Encoding is proposed to process viewpoint angles. Also, we propose aself-supervised learning approach called Standin Learning, which helps modelthe electrocardio field without direct supervision. Further, with very fewmodifications, Nef-Net can also synthesize ECG signals from scratch.Experiments verify that our Nef-Net performs well on Electrocardio Panoramasynthesis, and outperforms the previous work on the auxiliary tasks (ECG viewtransformation and ECG synthesis from scratch). The codes and the divisionlabels of cardiac cycles and ECG deflections on Tianchi ECG and PTB datasetsare available at https://github.com/WhatAShot/Electrocardio-Panorama.},
  YEAR = {2021},
  MONTH = {May},
  NOTE = {the 30th International Joint Conference on Artificial Intelligence
  (2021)},
  URL = {http://arxiv.org/abs/2105.06293v1},
  FILE = {2105.06293v1.pdf}
 }","Beyond graphics, Science and engineering",,,,,,,,,,,,,,,"Jintai Chen, Xiangshang Zheng, Hongyun Yu, Danny Z. Chen, Jian Wu",chen2021nefnet,00000130,"Multi-lead electrocardiogram (ECG) provides clinical information of heartbeats from several fixed viewpoints determined by the lead positioning. However, it is often not satisfactory to visualize ECG signals in these fixed and limited views, as some clinically useful information is represented only from a few specific ECG viewpoints. For the first time, we propose a new concept, Electrocardio Panorama, which allows visualizing ECG signals from any queried viewpoints. To build Electrocardio Panorama, we assume that an underlying electrocardio field exists, representing locations, magnitudes, and directions of ECG signals. We present a Neural electrocardio field Network (Nef-Net), which first predicts the electrocardio field representation by using a sparse set of one or few input ECG views and then synthesizes Electrocardio Panorama based on the predicted representations. Specially, to better disentangle electrocardio field information from viewpoint biases, a new Angular Encoding is proposed to process viewpoint angles. Also, we propose a self-supervised learning approach called Standin Learning, which helps model the electrocardio field without direct supervision. Further, with very few modifications, Nef-Net can also synthesize ECG signals from scratch. Experiments verify that our Nef-Net performs well on Electrocardio Panorama synthesis, and outperforms the previous work on the auxiliary tasks (ECG view transformation and ECG synthesis from scratch). The codes and the division labels of cardiac cycles and ECG deflections on Tianchi ECG and PTB datasets are available at https://github.com/WhatAShot/Electrocardio-Panorama.",0,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnufYh4swePWp6tbdEQxdxfGg0lv-eigMm07OSY0hYpEPRtM_vFyXsoTDXT1vAIHs6f8
5/23/2021 18:13:26,Neural Trajectory Fields for Dynamic Novel View Synthesis,DCT-NeRF,5/12/2021,https://arxiv.org/pdf/2105.05994.pdf,,,,,,,"@article{wang2021dctnerf,
  AUTHOR = {Chaoyang Wang and Ben Eckart and Simon Lucey and Orazio Gallo},
  TITLE = {Neural Trajectory Fields for Dynamic Novel View Synthesis},
  EPRINT = {2105.05994v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Recent approaches to render photorealistic views from a limited set ofphotographs have pushed the boundaries of our interactions with pictures ofstatic scenes. The ability to recreate moments, that is, time-varyingsequences, is perhaps an even more interesting scenario, but it remains largelyunsolved. We introduce DCT-NeRF, a coordinatebased neural representation fordynamic scenes. DCTNeRF learns smooth and stable trajectories over the inputsequence for each point in space. This allows us to enforce consistency betweenany two frames in the sequence, which results in high quality reconstruction,particularly in dynamic regions.},
  YEAR = {2021},
  MONTH = {May},
  URL = {http://arxiv.org/abs/2105.05994v1},
  FILE = {2105.05994v1.pdf}
 }",Dynamic,Warping field/Flow field,,,,,,,,,,,,,,"Chaoyang Wang, Ben Eckart, Simon Lucey, Orazio Gallo",wang2021dctnerf,00000131,"Recent approaches to render photorealistic views from a limited set of photographs have pushed the boundaries of our interactions with pictures of static scenes. The ability to recreate moments, that is, time-varying sequences, is perhaps an even more interesting scenario, but it remains largely unsolved. We introduce DCT-NeRF, a coordinatebased neural representation for dynamic scenes. DCTNeRF learns smooth and stable trajectories over the input sequence for each point in space. This allows us to enforce consistency between any two frames in the sequence, which results in high quality reconstruction, particularly in dynamic regions.",0,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnudJYgJYWiVFQuLgvckrRKeqQcvXk_3xAliR1xTi-PawUofWcpSVHW7yUxSMiJw9C1A
5/23/2021 18:11:30,Editing Conditional Radiance Fields,,5/13/2021,https://arxiv.org/pdf/2105.06466.pdf,,,,,,,"@article{liu2021editing,
  AUTHOR = {Steven Liu and Xiuming Zhang and Zhoutong Zhang and Richard Zhang and Jun-Yan Zhu and Bryan Russell},
  TITLE = {Editing Conditional Radiance Fields},
  EPRINT = {2105.06466v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {A neural radiance field (NeRF) is a scene model supporting high-quality viewsynthesis, optimized per scene. In this paper, we explore enabling user editingof a category-level NeRF - also known as a conditional radiance field - trainedon a shape category. Specifically, we introduce a method for propagating coarse2D user scribbles to the 3D space, to modify the color or shape of a localregion. First, we propose a conditional radiance field that incorporates newmodular network components, including a shape branch that is shared acrossobject instances. Observing multiple instances of the same category, our modellearns underlying part semantics without any supervision, thereby allowing thepropagation of coarse 2D user scribbles to the entire 3D region (e.g., chairseat). Next, we propose a hybrid network update strategy that targets specificnetwork components, which balances efficiency and accuracy. During userinteraction, we formulate an optimization problem that both satisfies theuser's constraints and preserves the original object structure. We demonstrateour approach on various editing tasks over three shape datasets and show thatit outperforms prior neural editing approaches. Finally, we edit the appearanceand shape of a real photograph and show that the edit propagates toextrapolated novel views.},
  YEAR = {2021},
  MONTH = {May},
  URL = {http://arxiv.org/abs/2105.06466v2},
  FILE = {2105.06466v2.pdf}
 }","Generalization, Editable",Per-instance fine-tuning,,,,,,,,,,CVPR,,,,"Steven Liu, Xiuming Zhang, Zhoutong Zhang, Richard Zhang, Jun-Yan Zhu, Bryan Russell",liu2021editing,00000132,"A neural radiance field (NeRF) is a scene model supporting high-quality view synthesis, optimized per scene. In this paper, we explore enabling user editing of a category-level NeRF - also known as a conditional radiance field - trained on a shape category. Specifically, we introduce a method for propagating coarse 2D user scribbles to the 3D space, to modify the color or shape of a local region. First, we propose a conditional radiance field that incorporates new modular network components, including a shape branch that is shared across object instances. Observing multiple instances of the same category, our model learns underlying part semantics without any supervision, thereby allowing the propagation of coarse 2D user scribbles to the entire 3D region (e.g., chair seat). Next, we propose a hybrid network update strategy that targets specific network components, which balances efficiency and accuracy. During user interaction, we formulate an optimization problem that both satisfies the user's constraints and preserves the original object structure. We demonstrate our approach on various editing tasks over three shape datasets and show that it outperforms prior neural editing approaches. Finally, we edit the appearance and shape of a real photograph and show that the edit propagates to extrapolated novel views.",0,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnueLKQga6dt5aN5NvITovoCuZmfvd2ZtZw-maSfcLSiGYESGaY9-9Nc4kAtIb_Ju26k
5/23/2021 18:13:01,Dynamic View Synthesis from Dynamic Monocular Video,,5/13/2021,https://arxiv.org/pdf/2105.06468.pdf,,,,,,,"@article{gao2021dynamic,
  AUTHOR = {Chen Gao and Ayush Saraf and Johannes Kopf and Jia-Bin Huang},
  TITLE = {Dynamic View Synthesis from Dynamic Monocular Video},
  EPRINT = {2105.06468v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We present an algorithm for generating novel views at arbitrary viewpointsand any input time step given a monocular video of a dynamic scene. Our workbuilds upon recent advances in neural implicit representation and usescontinuous and differentiable functions for modeling the time-varying structureand the appearance of the scene. We jointly train a time-invariant static NeRFand a time-varying dynamic NeRF, and learn how to blend the results in anunsupervised manner. However, learning this implicit function from a singlevideo is highly ill-posed (with infinitely many solutions that match the inputvideo). To resolve the ambiguity, we introduce regularization losses toencourage a more physically plausible solution. We show extensive quantitativeand qualitative results of dynamic view synthesis from casually capturedvideos.},
  YEAR = {2021},
  MONTH = {May},
  URL = {http://arxiv.org/abs/2105.06468v1},
  FILE = {2105.06468v1.pdf}
 }",Dynamic,Warping field/Flow field,,,,,,,,,,CVPR,,,,"Chen Gao, Ayush Saraf, Johannes Kopf, Jia-Bin Huang",gao2021dynamic,00000133,"We present an algorithm for generating novel views at arbitrary viewpoints and any input time step given a monocular video of a dynamic scene. Our work builds upon recent advances in neural implicit representation and uses continuous and differentiable functions for modeling the time-varying structure and the appearance of the scene. We jointly train a time-invariant static NeRF and a time-varying dynamic NeRF, and learn how to blend the results in an unsupervised manner. However, learning this implicit function from a single video is highly ill-posed (with infinitely many solutions that match the input video). To resolve the ambiguity, we introduce regularization losses to encourage a more physically plausible solution. We show extensive quantitative and qualitative results of dynamic view synthesis from casually captured videos.",0,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnueSCpKXSGJnUWXZxooz7yc5nw0oU4FO8wLmQE6y5MGdcyZ2ck6uyFX-njj8I5B2N0E
5/23/2021 11:24:49,NeuLF: Efficient Novel View Synthesis with Neural 4D Light Field,NeuLF,5/15/2021,https://arxiv.org/pdf/2105.07112.pdf,,,,,,,"@article{liu2021neulf,
  AUTHOR = {Celong Liu and Zhong Li and Junsong Yuan and Yi Xu},
  TITLE = {NeuLF: Efficient Novel View Synthesis with Neural 4D Light Field},
  EPRINT = {2105.07112v4},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {In this paper, we present an efficient and robust deep learning solution fornovel view synthesis of complex scenes. In our approach, a 3D scene isrepresented as a light field, i.e., a set of rays, each of which has acorresponding color when reaching the image plane. For efficient novel viewrendering, we adopt a 4D parameterization of the light field, where each ray ischaracterized by a 4D parameter. We then formulate the light field as a 4Dfunction that maps 4D coordinates to corresponding color values. We train adeep fully connected network to optimize this implicit function and memorizethe 3D scene. Then, the scene-specific model is used to synthesize novel views.Different from previous light field approaches which require dense viewsampling to reliably render novel views, our method can render novel views bysampling rays and querying the color for each ray from the network directly,thus enabling high-quality light field rendering with a sparser set of trainingimages. Our method achieves state-of-the-art novel view synthesis results whilemaintaining an interactive frame rate.},
  YEAR = {2021},
  MONTH = {May},
  URL = {http://arxiv.org/abs/2105.07112v4},
  FILE = {2105.07112v4.pdf}
 }",Performance (rendering),Representation,NeRF,Light Field,,,,,,,,CVPR,,,,"Celong Liu, Zhong Li, Junsong Yuan, Yi Xu",liu2021neulf,00000134,"In this paper, we present an efficient and robust deep learning solution for novel view synthesis of complex scenes. In our approach, a 3D scene is represented as a light field, i.e., a set of rays, each of which has a corresponding color when reaching the image plane. For efficient novel view rendering, we adopt a 4D parameterization of the light field, where each ray is characterized by a 4D parameter. We then formulate the light field as a 4D function that maps 4D coordinates to corresponding color values. We train a deep fully connected network to optimize this implicit function and memorize the 3D scene. Then, the scene-specific model is used to synthesize novel views. Different from previous light field approaches which require dense view sampling to reliably render novel views, our method can render novel views by sampling rays and querying the color for each ray from the network directly, thus enabling high-quality light field rendering with a sparser set of training images. Our method achieves state-of-the-art novel view synthesis results while maintaining an interactive frame rate.",,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnucrCgLBrBGlExmB-ytxNT75DOTreyjRdKkme8xZGHrFGApTL-c1T_eq5Vs5IoWBORs
5/23/2021 12:29:18,Recursive-NeRF: An Efficient and Dynamically Growing NeRF,Recursive-NeRF,5/19/2021,https://arxiv.org/pdf/2105.09103.pdf,,https://github.com/Gword/Recursive-NeRF,,,,,"@article{yang2021recursivenerf,
  AUTHOR = {Guo-Wei Yang and Wen-Yang Zhou and Hao-Yang Peng and Dun Liang and Tai-Jiang Mu and Shi-Min Hu},
  TITLE = {Recursive-NeRF: An Efficient and Dynamically Growing NeRF},
  EPRINT = {2105.09103v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {View synthesis methods using implicit continuous shape representationslearned from a set of images, such as the Neural Radiance Field (NeRF) method,have gained increasing attention due to their high quality imagery andscalability to high resolution. However, the heavy computation required by itsvolumetric approach prevents NeRF from being useful in practice; minutes aretaken to render a single image of a few megapixels. Now, an image of a scenecan be rendered in a level-of-detail manner, so we posit that a complicatedregion of the scene should be represented by a large neural network while asmall neural network is capable of encoding a simple region, enabling a balancebetween efficiency and quality. Recursive-NeRF is our embodiment of this idea,providing an efficient and adaptive rendering and training approach for NeRF.The core of Recursive-NeRF learns uncertainties for query coordinates,representing the quality of the predicted color and volumetric intensity ateach level. Only query coordinates with high uncertainties are forwarded to thenext level to a bigger neural network with a more powerful representationalcapability. The final rendered image is a composition of results from neuralnetworks of all levels. Our evaluation on three public datasets shows thatRecursive-NeRF is more efficient than NeRF while providing state-of-the-artquality. The code will be available at https://github.com/Gword/Recursive-NeRF.},
  YEAR = {2021},
  MONTH = {May},
  URL = {http://arxiv.org/abs/2105.09103v1},
  FILE = {2105.09103v1.pdf}
 }",,"Coarse-to-fine, Sampling, Volume partitioning",NeRF,Density,Per-scene,,,,,,,CVPR,,,,"Guo-Wei Yang, Wen-Yang Zhou, Hao-Yang Peng, Dun Liang, Tai-Jiang Mu, Shi-Min Hu",yang2021recursivenerf,00000135,"View synthesis methods using implicit continuous shape representations learned from a set of images, such as the Neural Radiance Field (NeRF) method, have gained increasing attention due to their high quality imagery and scalability to high resolution. However, the heavy computation required by its volumetric approach prevents NeRF from being useful in practice; minutes are taken to render a single image of a few megapixels. Now, an image of a scene can be rendered in a level-of-detail manner, so we posit that a complicated region of the scene should be represented by a large neural network while a small neural network is capable of encoding a simple region, enabling a balance between efficiency and quality. Recursive-NeRF is our embodiment of this idea, providing an efficient and adaptive rendering and training approach for NeRF. The core of Recursive-NeRF learns uncertainties for query coordinates, representing the quality of the predicted color and volumetric intensity at each level. Only query coordinates with high uncertainties are forwarded to the next level to a bigger neural network with a more powerful representational capability. The final rendered image is a composition of results from neural networks of all levels. Our evaluation on three public datasets shows that Recursive-NeRF is more efficient than NeRF while providing state-of-the-art quality. The code will be available at https://github.com/Gword/Recursive-NeRF.",,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnudMef7jzAVPv3-q0Nma7WzxkO5b06e6zwo3es8L_lS1MUnmnD_yiBGx-adUq1ovcxk
7/19/2021 21:36:40,Neural Radiosity,,5/26/2021,https://arxiv.org/pdf/2105.12319.pdf,,,,,,,"@article{hadadan2021neural,
  AUTHOR = {Saeed Hadadan and Shuhong Chen and Matthias Zwicker},
  TITLE = {Neural Radiosity},
  EPRINT = {2105.12319v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.GR},
  ABSTRACT = {We introduce Neural Radiosity, an algorithm to solve the rendering equationby minimizing the norm of its residual similar as in traditional radiositytechniques. Traditional basis functions used in radiosity techniques, such aspiecewise polynomials or meshless basis functions are typically limited torepresenting isotropic scattering from diffuse surfaces. Instead, we propose toleverage neural networks to represent the full four-dimensional radiancedistribution, directly optimizing network parameters to minimize the norm ofthe residual. Our approach decouples solving the rendering equation fromrendering (perspective) images similar as in traditional radiosity techniques,and allows us to efficiently synthesize arbitrary views of a scene. Inaddition, we propose a network architecture using geometric learnable featuresthat improves convergence of our solver compared to previous techniques. Ourapproach leads to an algorithm that is simple to implement, and we demonstrateits effectiveness on a variety of scenes with non-diffuse surfaces.},
  YEAR = {2021},
  MONTH = {May},
  URL = {http://arxiv.org/abs/2105.12319v1},
  FILE = {2105.12319v1.pdf}
 }",Material/lighting estimation,"Voxelization, Feature volume",,,,,,,,,,,,,,"Saeed Hadadan, Shuhong Chen, Matthias Zwicker",hadadan2021neural,00000136,"We introduce Neural Radiosity, an algorithm to solve the rendering equation by minimizing the norm of its residual similar as in traditional radiosity techniques. Traditional basis functions used in radiosity techniques, such as piecewise polynomials or meshless basis functions are typically limited to representing isotropic scattering from diffuse surfaces. Instead, we propose to leverage neural networks to represent the full four-dimensional radiance distribution, directly optimizing network parameters to minimize the norm of the residual. Our approach decouples solving the rendering equation from rendering (perspective) images similar as in traditional radiosity techniques, and allows us to efficiently synthesize arbitrary views of a scene. In addition, we propose a network architecture using geometric learnable features that improves convergence of our solver compared to previous techniques. Our approach leads to an algorithm that is simple to implement, and we demonstrate its effectiveness on a variety of scenes with non-diffuse surfaces.",,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnuesbgOGO4eh-GYQVqDDDstsdt71omMid_g52zdAUllfdP7kWkO8iYW-zXomOSN3H64
7/19/2021 21:34:55,Stylizing 3D Scene via Implicit Representation and HyperNetwork,,5/27/2021,https://arxiv.org/pdf/2105.13016.pdf,,,,,,,"@article{chiang2021stylizing,
  AUTHOR = {Pei-Ze Chiang and Meng-Shiun Tsai and Hung-Yu Tseng and Wei-sheng Lai and Wei-Chen Chiu},
  TITLE = {Stylizing 3D Scene via Implicit Representation and HyperNetwork},
  EPRINT = {2105.13016v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {In this work, we aim to address the 3D scene stylization problem - generatingstylized images of the scene at arbitrary novel view angles. A straightforwardsolution is to combine existing novel view synthesis and image/video styletransfer approaches, which often leads to blurry results or inconsistentappearance. Inspired by the high quality results of the neural radiance fields(NeRF) method, we propose a joint framework to directly render novel views withthe desired style. Our framework consists of two components: an implicitrepresentation of the 3D scene with the neural radiance field model, and ahypernetwork to transfer the style information into the scene representation.In particular, our implicit representation model disentangles the scene intothe geometry and appearance branches, and the hypernetwork learns to predictthe parameters of the appearance branch from the reference style image. Toalleviate the training difficulties and memory burden, we propose a two-stagetraining procedure and a patch sub-sampling approach to optimize the style andcontent losses with the neural radiance field model. After optimization, ourmodel is able to render consistent novel views at arbitrary view angles witharbitrary style. Both quantitative evaluation and human subject study havedemonstrated that the proposed method generates faithful stylization resultswith consistent appearance across different views.},
  YEAR = {2021},
  MONTH = {May},
  URL = {http://arxiv.org/abs/2105.13016v2},
  FILE = {2105.13016v2.pdf}
 }",Editable,"Hypernetwork, Data-driven component (pre-trained, cross-scene)",NeRF,Density,,,,,,,,CVPR,,No,Direct,"Pei-Ze Chiang, Meng-Shiun Tsai, Hung-Yu Tseng, Wei-sheng Lai, Wei-Chen Chiu",chiang2021stylizing,00000137,"In this work, we aim to address the 3D scene stylization problem - generating stylized images of the scene at arbitrary novel view angles. A straightforward solution is to combine existing novel view synthesis and image/video style transfer approaches, which often leads to blurry results or inconsistent appearance. Inspired by the high quality results of the neural radiance fields (NeRF) method, we propose a joint framework to directly render novel views with the desired style. Our framework consists of two components: an implicit representation of the 3D scene with the neural radiance field model, and a hypernetwork to transfer the style information into the scene representation. In particular, our implicit representation model disentangles the scene into the geometry and appearance branches, and the hypernetwork learns to predict the parameters of the appearance branch from the reference style image. To alleviate the training difficulties and memory burden, we propose a two-stage training procedure and a patch sub-sampling approach to optimize the style and content losses with the neural radiance field model. After optimization, our model is able to render consistent novel views at arbitrary view angles with arbitrary style. Both quantitative evaluation and human subject study have demonstrated that the proposed method generates faithful stylization results with consistent appearance across different views.",,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnufAk-V0gSiqeqgPNhBtOkFwCSfRwbMCPI3y5b6PmTQbAmi_AIaRHF5sv0iTaCIHpF4
7/19/2021 21:41:31,Geodesy of irregular small bodies via neural density fields: geodesyNets,geodesyNets,5/27/2021,https://arxiv.org/pdf/2105.13031.pdf,https://github.com/darioizzo/geodesynets,,,,,,"@article{izzo2021geodesynets,
  AUTHOR = {Dario Izzo and Pablo Gomez},
  TITLE = {Geodesy of irregular small bodies via neural density fields: geodesyNets},
  EPRINT = {2105.13031v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {astro-ph.EP},
  ABSTRACT = {We present a novel approach based on artificial neural networks, so-calledgeodesyNets, and present compelling evidence of their ability to serve asaccurate geodetic models of highly irregular bodies using minimal priorinformation on the body. The approach does not rely on the body shapeinformation but, if available, can harness it. GeodesyNets learn athree-dimensional, differentiable, function representing the body density,which we call neural density field. The body shape, as well as other geodeticproperties, can easily be recovered. We investigate six different shapesincluding the bodies 101955 Bennu, 67P Churyumov-Gerasimenko, 433 Eros and25143 Itokawa for which shape models developed during close proximity surveysare available. Both heterogeneous and homogeneous mass distributions areconsidered. The gravitational acceleration computed from the trainedgeodesyNets models, as well as the inferred body shape, show great accuracy inall cases with a relative error on the predicted acceleration smaller than 1\%even close to the asteroid surface. When the body shape information isavailable, geodesyNets can seamlessly exploit it and be trained to represent ahigh-fidelity neural density field able to give insights into the internalstructure of the body. This work introduces a new unexplored approach togeodesy, adding a powerful tool to consolidated ones based on sphericalharmonics, mascon models and polyhedral gravity.},
  YEAR = {2021},
  MONTH = {May},
  URL = {http://arxiv.org/abs/2105.13031v1},
  FILE = {2105.13031v1.pdf}
 }","Beyond graphics, Alternative imaging, Science and engineering",,,,,,,,,,,,,,,"Dario Izzo, Pablo Gómez",izzo2021geodesynets,00000138,"We present a novel approach based on artificial neural networks, so-called geodesyNets, and present compelling evidence of their ability to serve as accurate geodetic models of highly irregular bodies using minimal prior information on the body. The approach does not rely on the body shape information but, if available, can harness it. GeodesyNets learn a three-dimensional, differentiable, function representing the body density, which we call neural density field. The body shape, as well as other geodetic properties, can easily be recovered. We investigate six different shapes including the bodies 101955 Bennu, 67P Churyumov-Gerasimenko, 433 Eros and 25143 Itokawa for which shape models developed during close proximity surveys are available. Both heterogeneous and homogeneous mass distributions are considered. The gravitational acceleration computed from the trained geodesyNets models, as well as the inferred body shape, show great accuracy in all cases with a relative error on the predicted acceleration smaller than 1\% even close to the asteroid surface. When the body shape information is available, geodesyNets can seamlessly exploit it and be trained to represent a high-fidelity neural density field able to give insights into the internal structure of the body. This work introduces a new unexplored approach to geodesy, adding a powerful tool to consolidated ones based on spherical harmonics, mascon models and polyhedral gravity.",,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnuePnu28a0HG_KMRDz5GKRd6VVLFDQTkq_r4UEtn_QiEX1JL-uQV18QQ81hUNYDF37A
7/19/2021 21:33:42,NeRFactor: Neural Factorization of Shape and Reflectance Under an Unknown Illumination,NeRFactor,6/3/2021,https://arxiv.org/pdf/2106.01970.pdf,https://people.csail.mit.edu/xiuming/projects/nerfactor/,https://github.com/google/nerfactor,https://www.youtube.com/watch?v=UUVSPJlwhPg,,,,"@article{zhang2021nerfactor,
  AUTHOR = {Xiuming Zhang and Pratul P. Srinivasan and Boyang Deng and Paul Debevec and William T. Freeman and Jonathan T. Barron},
  TITLE = {NeRFactor: Neural Factorization of Shape and Reflectance Under anUnknown Illumination},
  EPRINT = {2106.01970v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We address the problem of recovering the shape and spatially-varyingreflectance of an object from posed multi-view images of the object illuminatedby one unknown lighting condition. This enables the rendering of novel views ofthe object under arbitrary environment lighting and editing of the object'smaterial properties. The key to our approach, which we call Neural RadianceFactorization (NeRFactor), is to distill the volumetric geometry of a NeuralRadiance Field (NeRF) [Mildenhall et al. 2020] representation of the objectinto a surface representation and then jointly refine the geometry whilesolving for the spatially-varying reflectance and the environment lighting.Specifically, NeRFactor recovers 3D neural fields of surface normals, lightvisibility, albedo, and Bidirectional Reflectance Distribution Functions(BRDFs) without any supervision, using only a re-rendering loss, simplesmoothness priors, and a data-driven BRDF prior learned from real-world BRDFmeasurements. By explicitly modeling light visibility, NeRFactor is able toseparate shadows from albedo and synthesize realistic soft or hard shadowsunder arbitrary lighting conditions. NeRFactor is able to recover convincing 3Dmodels for free-viewpoint relighting in this challenging and underconstrainedcapture setup for both synthetic and real scenes. Qualitative and quantitativeexperiments show that NeRFactor outperforms classic and deep learning-basedstate of the art across various tasks. Our code and data are available atpeople.csail.mit.edu/xiuming/projects/nerfactor/.},
  YEAR = {2021},
  MONTH = {Jun},
  URL = {http://arxiv.org/abs/2106.01970v1},
  FILE = {2106.01970v1.pdf}
 }","Editable, Material/lighting estimation","Data-driven component (pre-trained, cross-scene)",NeRF,Density,Per-scene,,,,,,,CVPR,,No,Direct,"Xiuming Zhang, Pratul P. Srinivasan, Boyang Deng, Paul Debevec, William T. Freeman, Jonathan T. Barron",zhang2021nerfactor,00000139,"We address the problem of recovering the shape and spatially-varying reflectance of an object from posed multi-view images of the object illuminated by one unknown lighting condition. This enables the rendering of novel views of the object under arbitrary environment lighting and editing of the object's material properties. The key to our approach, which we call Neural Radiance Factorization (NeRFactor), is to distill the volumetric geometry of a Neural Radiance Field (NeRF) [Mildenhall et al. 2020] representation of the object into a surface representation and then jointly refine the geometry while solving for the spatially-varying reflectance and the environment lighting. Specifically, NeRFactor recovers 3D neural fields of surface normals, light visibility, albedo, and Bidirectional Reflectance Distribution Functions (BRDFs) without any supervision, using only a re-rendering loss, simple smoothness priors, and a data-driven BRDF prior learned from real-world BRDF measurements. By explicitly modeling light visibility, NeRFactor is able to separate shadows from albedo and synthesize realistic soft or hard shadows under arbitrary lighting conditions. NeRFactor is able to recover convincing 3D models for free-viewpoint relighting in this challenging and underconstrained capture setup for both synthetic and real scenes. Qualitative and quantitative experiments show that NeRFactor outperforms classic and deep learning-based state of the art across various tasks. Our code and data are available at people.csail.mit.edu/xiuming/projects/nerfactor/.",,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnuey_yD2CxWj90AgMr9BfenY_JXOfq5rvzJ6AG4EgQlzaH2WYlG_TYb7AJ_FZ-D8e5I
7/19/2021 21:42:23,Spline Positional Encoding for Learning 3D Implicit Signed Distance Fields,,6/3/2021,https://arxiv.org/pdf/2106.01553.pdf,,,,,,,"@article{wang2021spline,
  AUTHOR = {Peng-Shuai Wang and Yang Liu and Yu-Qi Yang and Xin Tong},
  TITLE = {Spline Positional Encoding for Learning 3D Implicit Signed DistanceFields},
  EPRINT = {2106.01553v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Multilayer perceptrons (MLPs) have been successfully used to represent 3Dshapes implicitly and compactly, by mapping 3D coordinates to the correspondingsigned distance values or occupancy values. In this paper, we propose a novelpositional encoding scheme, called Spline Positional Encoding, to map the inputcoordinates to a high dimensional space before passing them to MLPs, forhelping to recover 3D signed distance fields with fine-scale geometric detailsfrom unorganized 3D point clouds. We verified the superiority of our approachover other positional encoding schemes on tasks of 3D shape reconstruction frominput point clouds and shape space learning. The efficacy of our approachextended to image reconstruction is also demonstrated and evaluated.},
  YEAR = {2021},
  MONTH = {Jun},
  URL = {http://arxiv.org/abs/2106.01553v1},
  FILE = {2106.01553v1.pdf}
 }",Fundamentals,,Other,SDF,,,,,,,,IJCAI 2020,,Yes,Direct,"Peng-Shuai Wang, Yang Liu, Yu-Qi Yang, Xin Tong",wang2021spline,00000140,"Multilayer perceptrons (MLPs) have been successfully used to represent 3D shapes implicitly and compactly, by mapping 3D coordinates to the corresponding signed distance values or occupancy values. In this paper, we propose a novel positional encoding scheme, called Spline Positional Encoding, to map the input coordinates to a high dimensional space before passing them to MLPs, for helping to recover 3D signed distance fields with fine-scale geometric details from unorganized 3D point clouds. We verified the superiority of our approach over other positional encoding schemes on tasks of 3D shape reconstruction from input point clouds and shape space learning. The efficacy of our approach extended to image reconstruction is also demonstrated and evaluated.",,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnucvVDfS7O1Has7s1iDoVu5TB5mQD45h8ctX6Y3DppmR8LDNp0RU54gCyaznUdBsmB8
7/19/2021 21:46:34,Neural Actor: Neural Free-view Synthesis of Human Actors with Pose Control,NA,6/3/2021,https://arxiv.org/pdf/2106.02019.pdf,http://gvv.mpi-inf.mpg.de/projects/NeuralActor/,Coming soon,http://gvv.mpi-inf.mpg.de/projects/NeuralActor/mp4/main_video_arxiv3.mp4,,,,"@article{liu2021na,
  AUTHOR = {Lingjie Liu and Marc Habermann and Viktor Rudnev and Kripasindhu Sarkar and Jiatao Gu and Christian Theobalt},
  TITLE = {Neural Actor: Neural Free-view Synthesis of Human Actors with PoseControl},
  EPRINT = {2106.02019v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We propose Neural Actor (NA), a new method for high-quality synthesis ofhumans from arbitrary viewpoints and under arbitrary controllable poses. Ourmethod is built upon recent neural scene representation and rendering workswhich learn representations of geometry and appearance from only 2D images.While existing works demonstrated compelling rendering of static scenes andplayback of dynamic scenes, photo-realistic reconstruction and rendering ofhumans with neural implicit methods, in particular under user-controlled novelposes, is still difficult. To address this problem, we utilize a coarse bodymodel as the proxy to unwarp the surrounding 3D space into a canonical pose. Aneural radiance field learns pose-dependent geometric deformations and pose-and view-dependent appearance effects in the canonical space from multi-viewvideo input. To synthesize novel views of high fidelity dynamic geometry andappearance, we leverage 2D texture maps defined on the body model as latentvariables for predicting residual deformations and the dynamic appearance.Experiments demonstrate that our method achieves better quality than thestate-of-the-arts on playback as well as novel pose synthesis, and can evengeneralize well to new poses that starkly differ from the training poses.Furthermore, our method also supports body shape control of the synthesizedresults.},
  YEAR = {2021},
  MONTH = {Jun},
  URL = {http://arxiv.org/abs/2106.02019v1},
  FILE = {2106.02019v1.pdf}
 }","Human body, Editable",Lifting 2D features to 3D,,Density,,,,,,,,,Coming soon,No,Direct,"Lingjie Liu, Marc Habermann, Viktor Rudnev, Kripasindhu Sarkar, Jiatao Gu, Christian Theobalt",liu2021na,00000141,"We propose Neural Actor (NA), a new method for high-quality synthesis of humans from arbitrary viewpoints and under arbitrary controllable poses. Our method is built upon recent neural scene representation and rendering works which learn representations of geometry and appearance from only 2D images. While existing works demonstrated compelling rendering of static scenes and playback of dynamic scenes, photo-realistic reconstruction and rendering of humans with neural implicit methods, in particular under user-controlled novel poses, is still difficult. To address this problem, we utilize a coarse body model as the proxy to unwarp the surrounding 3D space into a canonical pose. A neural radiance field learns pose-dependent geometric deformations and pose- and view-dependent appearance effects in the canonical space from multi-view video input. To synthesize novel views of high fidelity dynamic geometry and appearance, we leverage 2D texture maps defined on the body model as latent variables for predicting residual deformations and the dynamic appearance. Experiments demonstrate that our method achieves better quality than the state-of-the-arts on playback as well as novel pose synthesis, and can even generalize well to new poses that starkly differ from the training poses. Furthermore, our method also supports body shape control of the synthesized results.",,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnucdX5uorgMWX0SjmK6AfGv7ax9P7CLWBk9MSWT6paSEWg3OmvRciID60QYk__4cVXQ
7/19/2021 21:38:52,Light Field Networks: Neural Scene Representations with Single-Evaluation Rendering,LFNs,6/4/2021,https://arxiv.org/pdf/2106.02634.pdf,https://vsitzmann.github.io/lfns/,Coming soon,https://www.youtube.com/watch?v=x3sSreTNFw4,,,,"@article{sitzmann2021lfns,
  AUTHOR = {Vincent Sitzmann and Semon Rezchikov and William T. Freeman and Joshua B. Tenenbaum and Fredo Durand},
  TITLE = {Light Field Networks: Neural Scene Representations withSingle-Evaluation Rendering},
  EPRINT = {2106.02634v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Inferring representations of 3D scenes from 2D observations is a fundamentalproblem of computer graphics, computer vision, and artificial intelligence.Emerging 3D-structured neural scene representations are a promising approach to3D scene understanding. In this work, we propose a novel neural scenerepresentation, Light Field Networks or LFNs, which represent both geometry andappearance of the underlying 3D scene in a 360-degree, four-dimensional lightfield parameterized via a neural implicit representation. Rendering a ray froman LFN requires only a *single* network evaluation, as opposed to hundreds ofevaluations per ray for ray-marching or volumetric based renderers in3D-structured neural scene representations. In the setting of simple scenes, weleverage meta-learning to learn a prior over LFNs that enables multi-viewconsistent light field reconstruction from as little as a single imageobservation. This results in dramatic reductions in time and memory complexity,and enables real-time rendering. The cost of storing a 360-degree light fieldvia an LFN is two orders of magnitude lower than conventional methods such asthe Lumigraph. Utilizing the analytical differentiability of neural implicitrepresentations and a novel parameterization of light space, we furtherdemonstrate the extraction of sparse depth maps from LFNs.},
  YEAR = {2021},
  MONTH = {Jun},
  URL = {http://arxiv.org/abs/2106.02634v1},
  FILE = {2106.02634v1.pdf}
 }","Performance (training), Performance (rendering), Generalization","Conditional neural field, Hypernetwork, Representation, Sampling",,,,,,,,,,IJCAI,Coming soon,,,"Vincent Sitzmann, Semon Rezchikov, William T. Freeman, Joshua B. Tenenbaum, Fredo Durand",sitzmann2021lfns,00000142,"Inferring representations of 3D scenes from 2D observations is a fundamental problem of computer graphics, computer vision, and artificial intelligence. Emerging 3D-structured neural scene representations are a promising approach to 3D scene understanding. In this work, we propose a novel neural scene representation, Light Field Networks or LFNs, which represent both geometry and appearance of the underlying 3D scene in a 360-degree, four-dimensional light field parameterized via a neural implicit representation. Rendering a ray from an LFN requires only a *single* network evaluation, as opposed to hundreds of evaluations per ray for ray-marching or volumetric based renderers in 3D-structured neural scene representations. In the setting of simple scenes, we leverage meta-learning to learn a prior over LFNs that enables multi-view consistent light field reconstruction from as little as a single image observation. This results in dramatic reductions in time and memory complexity, and enables real-time rendering. The cost of storing a 360-degree light field via an LFN is two orders of magnitude lower than conventional methods such as the Lumigraph. Utilizing the analytical differentiability of neural implicit representations and a novel parameterization of light space, we further demonstrate the extraction of sparse depth maps from LFNs.",,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnufvBHIey_OALmznYDZ7Yq-uGOflgc6bBah2hhmZw8KnPj-hPyivF8A_-2PAbfzr2Vw
6/29/2021 15:23:25,Deep Medial Fields,DMF,6/7/2021,https://arxiv.org/pdf/2106.03804.pdf,,,,,,,"@article{rebain2021dmf,
  AUTHOR = {Daniel Rebain and Ke Li and Vincent Sitzmann and Soroosh Yazdani and Kwang Moo Yi and Andrea Tagliasacchi},
  TITLE = {Deep Medial Fields},
  EPRINT = {2106.03804v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.GR},
  ABSTRACT = {Implicit representations of geometry, such as occupancy fields or signeddistance fields (SDF), have recently re-gained popularity in encoding 3D solidshape in a functional form. In this work, we introduce medial fields: a fieldfunction derived from the medial axis transform (MAT) that makes availableinformation about the underlying 3D geometry that is immediately useful for anumber of downstream tasks. In particular, the medial field encodes the localthickness of a 3D shape, and enables O(1) projection of a query point onto themedial axis. To construct the medial field we require nothing but the SDF ofthe shape itself, thus allowing its straightforward incorporation in anyapplication that relies on signed distance fields. Working in unison with theO(1) surface projection supported by the SDF, the medial field opens the doorfor an entirely new set of efficient, shape-aware operations on implicitrepresentations. We present three such applications, including a modificationto sphere tracing that renders implicit representations with better convergenceproperties, a fast construction method for memory-efficient rigid-bodycollision proxies, and an efficient approximation of ambient occlusion thatremains stable with respect to viewpoint variations.},
  YEAR = {2021},
  MONTH = {Jun},
  URL = {http://arxiv.org/abs/2106.03804v1},
  FILE = {2106.03804v1.pdf}
 }",Fundamentals,Representation,NeRF,Medial Field,Per-scene,,,,,,,,,Yes,,"Daniel Rebain, Ke Li, Vincent Sitzmann, Soroosh Yazdani, Kwang Moo Yi, Andrea Tagliasacchi",rebain2021dmf,00000143,"Implicit representations of geometry, such as occupancy fields or signed distance fields (SDF), have recently re-gained popularity in encoding 3D solid shape in a functional form. In this work, we introduce medial fields: a field function derived from the medial axis transform (MAT) that makes available information about the underlying 3D geometry that is immediately useful for a number of downstream tasks. In particular, the medial field encodes the local thickness of a 3D shape, and enables O(1) projection of a query point onto the medial axis. To construct the medial field we require nothing but the SDF of the shape itself, thus allowing its straightforward incorporation in any application that relies on signed distance fields. Working in unison with the O(1) surface projection supported by the SDF, the medial field opens the door for an entirely new set of efficient, shape-aware operations on implicit representations. We present three such applications, including a modification to sphere tracing that renders implicit representations with better convergence properties, a fast construction method for memory-efficient rigid-body collision proxies, and an efficient approximation of ambient occlusion that remains stable with respect to viewpoint variations.",,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnucrNcYA5XBlaiBDBpYN8sfWZdTN9WOP0MiZmrILhvvGwL-3uOv5AvUEMxswG2FXEOc
7/19/2021 21:51:54,MoCo-Flow: Neural Motion Consensus Flow for Dynamic Humans in Stationary Monocular Cameras,MoCo-Flow,6/8/2021,https://arxiv.org/pdf/2106.04477.pdf,https://wyysf-98.github.io/MoCo_Flow/,Coming soon,,,,,"@article{chen2021mocoflow,
  AUTHOR = {Xuelin Chen and Weiyu Li and Daniel Cohen-Or and Niloy J. Mitra and Baoquan Chen},
  TITLE = {MoCo-Flow: Neural Motion Consensus Flow for Dynamic Humans in StationaryMonocular Cameras},
  EPRINT = {2106.04477v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Synthesizing novel views of dynamic humans from stationary monocular camerasis a popular scenario. This is particularly attractive as it does not requirestatic scenes, controlled environments, or specialized hardware. In contrast totechniques that exploit multi-view observations to constrain the modeling,given a single fixed viewpoint only, the problem of modeling the dynamic sceneis significantly more under-constrained and ill-posed. In this paper, weintroduce Neural Motion Consensus Flow (MoCo-Flow), a representation thatmodels the dynamic scene using a 4D continuous time-variant function. Theproposed representation is learned by an optimization which models a dynamicscene that minimizes the error of rendering all observation images. At theheart of our work lies a novel optimization formulation, which is constrainedby a motion consensus regularization on the motion flow. We extensivelyevaluate MoCo-Flow on several datasets that contain human motions of varyingcomplexity, and compare, both qualitatively and quantitatively, to severalbaseline methods and variants of our methods. Pretrained model, code, and datawill be released for research purposes upon paper acceptance.},
  YEAR = {2021},
  MONTH = {Jun},
  URL = {http://arxiv.org/abs/2106.04477v1},
  FILE = {2106.04477v1.pdf}
 }",Human body,"Coarse-to-fine, Warping field/Flow field",NeRF,Density,Category-level,,,,,,,,Coming soon,No,Direct,"Xuelin Chen, Weiyu Li, Daniel Cohen-Or, Niloy J. Mitra, Baoquan Chen",chen2021mocoflow,00000144,"Synthesizing novel views of dynamic humans from stationary monocular cameras is a popular scenario. This is particularly attractive as it does not require static scenes, controlled environments, or specialized hardware. In contrast to techniques that exploit multi-view observations to constrain the modeling, given a single fixed viewpoint only, the problem of modeling the dynamic scene is significantly more under-constrained and ill-posed. In this paper, we introduce Neural Motion Consensus Flow (MoCo-Flow), a representation that models the dynamic scene using a 4D continuous time-variant function. The proposed representation is learned by an optimization which models a dynamic scene that minimizes the error of rendering all observation images. At the heart of our work lies a novel optimization formulation, which is constrained by a motion consensus regularization on the motion flow. We extensively evaluate MoCo-Flow on several datasets that contain human motions of varying complexity, and compare, both qualitatively and quantitatively, to several baseline methods and variants of our methods. Pretrained model, code, and data will be released for research purposes upon paper acceptance.",,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnudxwEnTCiBZza5h6qsq8f_sCBmR9XVYA_qvo4dmWBr3rYEOs8Snd5VPhZdNwaYHZhg
7/28/2021 23:02:16,DoubleField: Bridging the Neural Surface and Radiance Fields for High-fidelity Human Rendering,DoubleField,6/8/2021,https://arxiv.org/pdf/2106.03798.pdf,http://www.liuyebin.com/dbfield/dbfield.html,Coming soon,,,http://www.liuyebin.com/dbfield/assets/supp2.mp4,,"@article{shao2021doublefield,
  AUTHOR = {Ruizhi Shao and Hongwen Zhang and He Zhang and Yanpei Cao and Tao Yu and Yebin Liu},
  TITLE = {DoubleField: Bridging the Neural Surface and Radiance Fields forHigh-fidelity Human Rendering},
  EPRINT = {2106.03798v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We introduce DoubleField, a novel representation combining the merits of bothsurface field and radiance field for high-fidelity human rendering. WithinDoubleField, the surface field and radiance field are associated together by ashared feature embedding and a surface-guided sampling strategy. In this way,DoubleField has a continuous but disentangled learning space for geometry andappearance modeling, which supports fast training, inference, and finetuning.To achieve high-fidelity free-viewpoint rendering, DoubleField is furtheraugmented to leverage ultra-high-resolution inputs, where a view-to-viewtransformer and a transfer learning scheme are introduced for more efficientlearning and finetuning from sparse-view inputs at original resolutions. Theefficacy of DoubleField is validated by the quantitative evaluations on severaldatasets and the qualitative results in a real-world sparse multi-view system,showing its superior capability for photo-realistic free-viewpoint humanrendering. For code and demo video, please refer to our project page:http://www.liuyebin.com/dbfield/dbfield.html.},
  YEAR = {2021},
  MONTH = {Jun},
  URL = {http://arxiv.org/abs/2106.03798v2},
  FILE = {2106.03798v2.pdf}
 }","Performance (training), Human body, Few-shot reconstruction",Lifting 2D features to 3D,NeRF,Density,Per-scene,,,,,,,,,No,Direct,"Ruizhi Shao, Hongwen Zhang, He Zhang, Yanpei Cao, Tao Yu, Yebin Liu",shao2021doublefield,00000145,"We introduce DoubleField, a novel representation combining the merits of both surface field and radiance field for high-fidelity human rendering. Within DoubleField, the surface field and radiance field are associated together by a shared feature embedding and a surface-guided sampling strategy. In this way, DoubleField has a continuous but disentangled learning space for geometry and appearance modeling, which supports fast training, inference, and finetuning. To achieve high-fidelity free-viewpoint rendering, DoubleField is further augmented to leverage ultra-high-resolution inputs, where a view-to-view transformer and a transfer learning scheme are introduced for more efficient learning and finetuning from sparse-view inputs at original resolutions. The efficacy of DoubleField is validated by the quantitative evaluations on several datasets and the qualitative results in a real-world sparse multi-view system, showing its superior capability for photo-realistic free-viewpoint human rendering. For code and demo video, please refer to our project page: http://www.liuyebin.com/dbfield/dbfield.html.",,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnueNbV87PwYqzVLrPoGEvNb35a2aapHayncS1mU4AoUCJm_4TV8RDmNaqWoKnPmc_Ac
6/29/2021 15:35:04,Geometry-Consistent Neural Shape Representation with Implicit Displacement Fields,IDF,6/9/2021,https://arxiv.org/pdf/2106.05187.pdf,https://yifita.github.io/publication/idf/,Coming soon,https://www.youtube.com/watch?v=fl4Rje8HM3I,,,,"@article{yifan2021idf,
  AUTHOR = {Wang Yifan and Lukas Rahmann and Olga Sorkine-Hornung},
  TITLE = {Geometry-Consistent Neural Shape Representation with ImplicitDisplacement Fields},
  EPRINT = {2106.05187v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We present implicit displacement fields, a novel representation for detailed3D geometry. Inspired by a classic surface deformation technique, displacementmapping, our method represents a complex surface as a smooth base surface plusa displacement along the base's normal directions, resulting in afrequency-based shape decomposition, where the high frequency signal isconstrained geometrically by the low frequency signal. Importantly, thisdisentanglement is unsupervised thanks to a tailored architectural design thathas an innate frequency hierarchy by construction. We explore implicitdisplacement field surface reconstruction and detail transfer and demonstratesuperior representational power, training stability and generalizability.},
  YEAR = {2021},
  MONTH = {Jun},
  URL = {http://arxiv.org/abs/2106.05187v2},
  FILE = {2106.05187v2.pdf}
 }",Fundamentals,"Coarse-to-fine, Representation, Volume partitioning, Data-driven component (pre-trained, cross-scene)",SIREN,SDF,Per-scene,,,,,,,IJCAI,,Yes,,"Wang Yifan, Lukas Rahmann, Olga Sorkine-Hornung",yifan2021idf,00000146,"We present implicit displacement fields, a novel representation for detailed 3D geometry. Inspired by a classic surface deformation technique, displacement mapping, our method represents a complex surface as a smooth base surface plus a displacement along the base's normal directions, resulting in a frequency-based shape decomposition, where the high frequency signal is constrained geometrically by the low frequency signal. Importantly, this disentanglement is unsupervised thanks to a tailored architectural design that has an innate frequency hierarchy by construction. We explore implicit displacement field surface reconstruction and detail transfer and demonstrate superior representational power, training stability and generalizability.",,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnucK82bcfeskdcji2RqhQ0rOBeQ8uCk4pb0QQQy5RXe8NDk-xyWLsGVlKo1aS6d6jwQ
9/18/2021 11:01:55,Implicit-PDF: Non-Parametric Representation of Probability Distributions on the Rotation Manifold,Implicit-PDF,6/10/2021,https://arxiv.org/pdf/2106.05965.pdf,https://implicit-pdf.github.io/,https://github.com/google-research/google-research/tree/master/implicit_pdf,https://www.youtube.com/watch?v=Y-MlRRy0xJA,,,,"@article{murphy2021implicitpdf,
  AUTHOR = {Kieran Murphy and Carlos Esteves and Varun Jampani and Srikumar Ramalingam and Ameesh Makadia},
  TITLE = {Implicit-PDF: Non-Parametric Representation of Probability Distributionson the Rotation Manifold},
  EPRINT = {2106.05965v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Single image pose estimation is a fundamental problem in many vision androbotics tasks, and existing deep learning approaches suffer by not completelymodeling and handling: i) uncertainty about the predictions, and ii) symmetricobjects with multiple (sometimes infinite) correct poses. To this end, weintroduce a method to estimate arbitrary, non-parametric distributions onSO(3). Our key idea is to represent the distributions implicitly, with a neuralnetwork that estimates the probability given the input image and a candidatepose. Grid sampling or gradient ascent can be used to find the most likelypose, but it is also possible to evaluate the probability at any pose, enablingreasoning about symmetries and uncertainty. This is the most general way ofrepresenting distributions on manifolds, and to showcase the rich expressivepower, we introduce a dataset of challenging symmetric and nearly-symmetricobjects. We require no supervision on pose uncertainty -- the model trains onlywith a single pose per example. Nonetheless, our implicit model is highlyexpressive to handle complex distributions over 3D poses, while still obtainingaccurate pose estimation on standard non-ambiguous environments, achievingstate-of-the-art performance on Pascal3D+ and ModelNet10-SO(3) benchmarks.},
  YEAR = {2021},
  MONTH = {Jun},
  URL = {http://arxiv.org/abs/2106.05965v1},
  FILE = {2106.05965v1.pdf}
 }","Camera parameter estimation, Beyond graphics, Fundamentals",,,,,,,,,,,ICML 2021,https://www.tensorflow.org/datasets/catalog/symmetric_solids,,,"Kieran Murphy, Carlos Esteves, Varun Jampani, Srikumar Ramalingam, Ameesh Makadia",murphy2021implicitpdf,00000226,"Single image pose estimation is a fundamental problem in many vision and robotics tasks, and existing deep learning approaches suffer by not completely modeling and handling: i) uncertainty about the predictions, and ii) symmetric objects with multiple (sometimes infinite) correct poses. To this end, we introduce a method to estimate arbitrary, non-parametric distributions on SO(3). Our key idea is to represent the distributions implicitly, with a neural network that estimates the probability given the input image and a candidate pose. Grid sampling or gradient ascent can be used to find the most likely pose, but it is also possible to evaluate the probability at any pose, enabling reasoning about symmetries and uncertainty. This is the most general way of representing distributions on manifolds, and to showcase the rich expressive power, we introduce a dataset of challenging symmetric and nearly-symmetric objects. We require no supervision on pose uncertainty -- the model trains only with a single pose per example. Nonetheless, our implicit model is highly expressive to handle complex distributions over 3D poses, while still obtaining accurate pose estimation on standard non-ambiguous environments, achieving state-of-the-art performance on Pascal3D+ and ModelNet10-SO(3) benchmarks.",,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnufvbwDjnFb8SmgwXg-GSGNt7z4Q6TbDBipgPwQgaaS6w_N-bYfUTtyfpi5t9hoI71s
7/19/2021 22:06:59,Unsupervised Video Prediction from a Single Frame by Estimating 3D Dynamic Scene Structure,,6/16/2021,https://arxiv.org/pdf/2106.09051.pdf,http://pmh47.net/vipl4s/,,,,,,"@article{henderson2021unsupervised,
  AUTHOR = {Paul Henderson and Christoph H. Lampert and Bernd Bickel},
  TITLE = {Unsupervised Video Prediction from a Single Frame by Estimating 3DDynamic Scene Structure},
  EPRINT = {2106.09051v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Our goal in this work is to generate realistic videos given just one initialframe as input. Existing unsupervised approaches to this task do not considerthe fact that a video typically shows a 3D environment, and that this shouldremain coherent from frame to frame even as the camera and objects move. Weaddress this by developing a model that first estimates the latent 3D structureof the scene, including the segmentation of any moving objects. It thenpredicts future frames by simulating the object and camera dynamics, andrendering the resulting views. Importantly, it is trained end-to-end using onlythe unsupervised objective of predicting future frames, without any 3Dinformation nor segmentation annotations. Experiments on two challengingdatasets of natural videos show that our model can estimate 3D structure andmotion segmentation from a single frame, and hence generate plausible andvaried predictions.},
  YEAR = {2021},
  MONTH = {Jun},
  URL = {http://arxiv.org/abs/2106.09051v1},
  FILE = {2106.09051v1.pdf}
 }","Dynamic, Beyond graphics","Conditional neural field, Volume partitioning",,,,,,,,,,,,,,"Paul Henderson, Christoph H. Lampert, Bernd Bickel",henderson2021unsupervised,00000147,"Our goal in this work is to generate realistic videos given just one initial frame as input. Existing unsupervised approaches to this task do not consider the fact that a video typically shows a 3D environment, and that this should remain coherent from frame to frame even as the camera and objects move. We address this by developing a model that first estimates the latent 3D structure of the scene, including the segmentation of any moving objects. It then predicts future frames by simulating the object and camera dynamics, and rendering the resulting views. Importantly, it is trained end-to-end using only the unsupervised objective of predicting future frames, without any 3D information nor segmentation annotations. Experiments on two challenging datasets of natural videos show that our model can estimate 3D structure and motion segmentation from a single frame, and hence generate plausible and varied predictions.",,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnufuoYKfAArHXuZ0nNg49hpZZCMNJF2EjsI1jO5CNH9la01PX7008azL8-5Ak2VWLsg
7/19/2021 21:16:24,NeuS: Learning Neural Implicit Surfaces by Volume Rendering for Multi-view Reconstruction,NeuS,6/20/2021,https://arxiv.org/pdf/2106.10689.pdf,https://lingjie0206.github.io/papers/NeuS/index.htm,Coming soon,,,,,"@article{wang2021neus,
  AUTHOR = {Peng Wang and Lingjie Liu and Yuan Liu and Christian Theobalt and Taku Komura and Wenping Wang},
  TITLE = {NeuS: Learning Neural Implicit Surfaces by Volume Rendering forMulti-view Reconstruction},
  EPRINT = {2106.10689v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We present a novel neural surface reconstruction method, called NeuS, forreconstructing objects and scenes with high fidelity from 2D image inputs.Existing neural surface reconstruction approaches, such as DVR and IDR, requireforeground mask as supervision, easily get trapped in local minima, andtherefore struggle with the reconstruction of objects with severeself-occlusion or thin structures. Meanwhile, recent neural methods for novelview synthesis, such as NeRF and its variants, use volume rendering to producea neural scene representation with robustness of optimization, even for highlycomplex objects. However, extracting high-quality surfaces from this learnedimplicit representation is difficult because there are not sufficient surfaceconstraints in the representation. In NeuS, we propose to represent a surfaceas the zero-level set of a signed distance function (SDF) and develop a newvolume rendering method to train a neural SDF representation. We observe thatthe conventional volume rendering method causes inherent geometric errors (i.e.bias) for surface reconstruction, and therefore propose a new formulation thatis free of bias in the first order of approximation, thus leading to moreaccurate surface reconstruction even without the mask supervision. Experimentson the DTU dataset and the BlendedMVS dataset show that NeuS outperforms thestate-of-the-arts in high-quality surface reconstruction, especially forobjects and scenes with complex structures and self-occlusion.},
  YEAR = {2021},
  MONTH = {Jun},
  URL = {http://arxiv.org/abs/2106.10689v1},
  FILE = {2106.10689v1.pdf}
 }",,"Representation, Sampling",,Other,,,,,,,,IJCAI 2022,Coming soon,,,"Peng Wang, Lingjie Liu, Yuan Liu, Christian Theobalt, Taku Komura, Wenping Wang",wang2021neus,00000148,"We present a novel neural surface reconstruction method, called NeuS, for reconstructing objects and scenes with high fidelity from 2D image inputs. Existing neural surface reconstruction approaches, such as DVR and IDR, require foreground mask as supervision, easily get trapped in local minima, and therefore struggle with the reconstruction of objects with severe self-occlusion or thin structures. Meanwhile, recent neural methods for novel view synthesis, such as NeRF and its variants, use volume rendering to produce a neural scene representation with robustness of optimization, even for highly complex objects. However, extracting high-quality surfaces from this learned implicit representation is difficult because there are not sufficient surface constraints in the representation. In NeuS, we propose to represent a surface as the zero-level set of a signed distance function (SDF) and develop a new volume rendering method to train a neural SDF representation. We observe that the conventional volume rendering method causes inherent geometric errors (i.e. bias) for surface reconstruction, and therefore propose a new formulation that is free of bias in the first order of approximation, thus leading to more accurate surface reconstruction even without the mask supervision. Experiments on the DTU dataset and the BlendedMVS dataset show that NeuS outperforms the state-of-the-arts in high-quality surface reconstruction, especially for objects and scenes with complex structures and self-occlusion.",,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnudpar5L2k_n7x-FVH0PXyTbKNLpMt8XxQgGpRpL10bwAvy7rYXC46_ZJSZFggIcL6g
7/19/2021 21:19:05,Moving in a 360 World: Synthesizing Panoramic Parallaxes from a Single Panorama,OmniNeRF,6/21/2021,https://arxiv.org/pdf/2106.10859.pdf,,,,,,,"@article{hsu2021omninerf,
  AUTHOR = {Ching-Yu Hsu and Cheng Sun and Hwann-Tzong Chen},
  TITLE = {Moving in a 360 World: Synthesizing Panoramic Parallaxes from a SinglePanorama},
  EPRINT = {2106.10859v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We present Omnidirectional Neural Radiance Fields (OmniNeRF), the firstmethod to the application of parallax-enabled novel panoramic view synthesis.Recent works for novel view synthesis focus on perspective images with limitedfield-of-view and require sufficient pictures captured in a specific condition.Conversely, OmniNeRF can generate panorama images for unknown viewpoints givena single equirectangular image as training data. To this end, we propose toaugment the single RGB-D panorama by projecting back and forth between a 3Dworld and different 2D panoramic coordinates at different virtual camerapositions. By doing so, we are able to optimize an Omnidirectional NeuralRadiance Field with visible pixels collecting from omnidirectional viewingangles at a fixed center for the estimation of new viewing angles from varyingcamera positions. As a result, the proposed OmniNeRF achieves convincingrenderings of novel panoramic views that exhibit the parallax effect. Weshowcase the effectiveness of each of our proposals on both synthetic andreal-world datasets.},
  YEAR = {2021},
  MONTH = {Jun},
  URL = {http://arxiv.org/abs/2106.10859v1},
  FILE = {2106.10859v1.pdf}
 }",,Representation,NeRF,Density,Per-scene,,,,,,,,,No,Direct,"Ching-Yu Hsu, Cheng Sun, Hwann-Tzong Chen",hsu2021omninerf,00000149,"We present Omnidirectional Neural Radiance Fields (OmniNeRF), the first method to the application of parallax-enabled novel panoramic view synthesis. Recent works for novel view synthesis focus on perspective images with limited field-of-view and require sufficient pictures captured in a specific condition. Conversely, OmniNeRF can generate panorama images for unknown viewpoints given a single equirectangular image as training data. To this end, we propose to augment the single RGB-D panorama by projecting back and forth between a 3D world and different 2D panoramic coordinates at different virtual camera positions. By doing so, we are able to optimize an Omnidirectional Neural Radiance Field with visible pixels collecting from omnidirectional viewing angles at a fixed center for the estimation of new viewing angles from varying camera positions. As a result, the proposed OmniNeRF achieves convincing renderings of novel panoramic views that exhibit the parallax effect. We showcase the effectiveness of each of our proposals on both synthetic and real-world datasets.",,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnudBVLOJWffC9pdvdU-wE5Bwa-F-wdWT6MLTijR596cm0T2A_t9bC8sgJJC4GMRxGnY
6/29/2021 17:02:24,Volume Rendering of Neural Implicit Surfaces,VolSDF,6/22/2021,https://arxiv.org/pdf/2106.12052.pdf,,,,,,,"@article{yariv2021volsdf,
  AUTHOR = {Lior Yariv and Jiatao Gu and Yoni Kasten and Yaron Lipman},
  TITLE = {Volume Rendering of Neural Implicit Surfaces},
  EPRINT = {2106.12052v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Neural volume rendering became increasingly popular recently due to itssuccess in synthesizing novel views of a scene from a sparse set of inputimages. So far, the geometry learned by neural volume rendering techniques wasmodeled using a generic density function. Furthermore, the geometry itself wasextracted using an arbitrary level set of the density function leading to anoisy, often low fidelity reconstruction. The goal of this paper is to improvegeometry representation and reconstruction in neural volume rendering. Weachieve that by modeling the volume density as a function of the geometry. Thisis in contrast to previous work modeling the geometry as a function of thevolume density. In more detail, we define the volume density function asLaplace's cumulative distribution function (CDF) applied to a signed distancefunction (SDF) representation. This simple density representation has threebenefits: (i) it provides a useful inductive bias to the geometry learned inthe neural volume rendering process; (ii) it facilitates a bound on the opacityapproximation error, leading to an accurate sampling of the viewing ray.Accurate sampling is important to provide a precise coupling of geometry andradiance; and (iii) it allows efficient unsupervised disentanglement of shapeand appearance in volume rendering. Applying this new density representation tochallenging scene multiview datasets produced high quality geometryreconstructions, outperforming relevant baselines. Furthermore, switching shapeand appearance between scenes is possible due to the disentanglement of thetwo.},
  YEAR = {2021},
  MONTH = {Jun},
  URL = {http://arxiv.org/abs/2106.12052v1},
  FILE = {2106.12052v1.pdf}
 }",Fundamentals,Representation,None,SDF/Density Hybrid,Per-scene,,,,,,,,,No,,"Lior Yariv, Jiatao Gu, Yoni Kasten, Yaron Lipman",yariv2021volsdf,00000150,"Neural volume rendering became increasingly popular recently due to its success in synthesizing novel views of a scene from a sparse set of input images. So far, the geometry learned by neural volume rendering techniques was modeled using a generic density function. Furthermore, the geometry itself was extracted using an arbitrary level set of the density function leading to a noisy, often low fidelity reconstruction. The goal of this paper is to improve geometry representation and reconstruction in neural volume rendering. We achieve that by modeling the volume density as a function of the geometry. This is in contrast to previous work modeling the geometry as a function of the volume density. In more detail, we define the volume density function as Laplace's cumulative distribution function (CDF) applied to a signed distance function (SDF) representation. This simple density representation has three benefits: (i) it provides a useful inductive bias to the geometry learned in the neural volume rendering process; (ii) it facilitates a bound on the opacity approximation error, leading to an accurate sampling of the viewing ray. Accurate sampling is important to provide a precise coupling of geometry and radiance; and (iii) it allows efficient unsupervised disentanglement of shape and appearance in volume rendering. Applying this new density representation to challenging scene multiview datasets produced high quality geometry reconstructions, outperforming relevant baselines. Furthermore, switching shape and appearance between scenes is possible due to the disentanglement of the two.",,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnudz5cSlZKYU5WMK1KHEvVOBRxFcnMMcuZbdOH-iw1kBsO2easrankLugoDubHI6S_8
7/19/2021 21:50:34,MetaAvatar: Learning Animatable Clothed Human Models from Few Depth Images,MetaAvatar,6/22/2021,https://arxiv.org/pdf/2106.11944.pdf,https://neuralbodies.github.io/metavatar/,,,,"https://www.youtube.com/watch?v=SXv1sBRwm4U, https://www.youtube.com/watch?v=eLZH-h1VOm8, https://www.youtube.com/watch?v=MMQStRgWJUE",,"@article{wang2021metaavatar,
  AUTHOR = {Shaofei Wang and Marko Mihajlovic and Qianli Ma and Andreas Geiger and Siyu Tang},
  TITLE = {MetaAvatar: Learning Animatable Clothed Human Models from Few DepthImages},
  EPRINT = {2106.11944v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {In this paper, we aim to create generalizable and controllable neural signeddistance fields (SDFs) that represent clothed humans from monocular depthobservations. Recent advances in deep learning, especially neural implicitrepresentations, have enabled human shape reconstruction and controllableavatar generation from different sensor inputs. However, to generate realisticcloth deformations from novel input poses, watertight meshes or dense full-bodyscans are usually needed as inputs. Furthermore, due to the difficulty ofeffectively modeling pose-dependent cloth deformations for diverse body shapesand cloth types, existing approaches resort to per-subject/cloth-typeoptimization from scratch, which is computationally expensive. In contrast, wepropose an approach that can quickly generate realistic clothed human avatars,represented as controllable neural SDFs, given only monocular depth images. Weachieve this by using meta-learning to learn an initialization of ahypernetwork that predicts the parameters of neural SDFs. The hypernetwork isconditioned on human poses and represents a clothed neural avatar that deformsnon-rigidly according to the input poses. Meanwhile, it is meta-learned toeffectively incorporate priors of diverse body shapes and cloth types and thuscan be much faster to fine-tune, compared to models trained from scratch. Wequalitatively and quantitatively show that our approach outperformsstate-of-the-art approaches that require complete meshes as inputs while ourapproach requires only depth frames as inputs and runs orders of magnitudesfaster. Furthermore, we demonstrate that our meta-learned hypernetwork is veryrobust, being the first to generate avatars with realistic dynamic clothdeformations given as few as 8 monocular depth frames.},
  YEAR = {2021},
  MONTH = {Jun},
  URL = {http://arxiv.org/abs/2106.11944v1},
  FILE = {2106.11944v1.pdf}
 }",Human body,"Hypernetwork, Warping field/Flow field",SIREN,SDF,Category-level,,,,,,,IJCAI,,Yes,Direct,"Shaofei Wang, Marko Mihajlovic, Qianli Ma, Andreas Geiger, Siyu Tang",wang2021metaavatar,00000151,"In this paper, we aim to create generalizable and controllable neural signed distance fields (SDFs) that represent clothed humans from monocular depth observations. Recent advances in deep learning, especially neural implicit representations, have enabled human shape reconstruction and controllable avatar generation from different sensor inputs. However, to generate realistic cloth deformations from novel input poses, watertight meshes or dense full-body scans are usually needed as inputs. Furthermore, due to the difficulty of effectively modeling pose-dependent cloth deformations for diverse body shapes and cloth types, existing approaches resort to per-subject/cloth-type optimization from scratch, which is computationally expensive. In contrast, we propose an approach that can quickly generate realistic clothed human avatars, represented as controllable neural SDFs, given only monocular depth images. We achieve this by using meta-learning to learn an initialization of a hypernetwork that predicts the parameters of neural SDFs. The hypernetwork is conditioned on human poses and represents a clothed neural avatar that deforms non-rigidly according to the input poses. Meanwhile, it is meta-learned to effectively incorporate priors of diverse body shapes and cloth types and thus can be much faster to fine-tune, compared to models trained from scratch. We qualitatively and quantitatively show that our approach outperforms state-of-the-art approaches that require complete meshes as inputs while our approach requires only depth frames as inputs and runs orders of magnitudes faster. Furthermore, we demonstrate that our meta-learned hypernetwork is very robust, being the first to generate avatars with realistic dynamic cloth deformations given as few as 8 monocular depth frames.",,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnuf_A4imA6Eltgo1yBBFAPH-mLBTjiWFt9OVGFElazLdX03PwXnQH10SjhO84GaYDqU
7/19/2021 17:55:31,Real-time Neural Radiance Caching for Path Tracing,,6/23/2021,https://arxiv.org/pdf/2106.12372.pdf,https://tom94.net/,,,,,,"@article{muller2021realtime,
  AUTHOR = {Thomas Muller and Fabrice Rousselle and Jan Novak and Alexander Keller},
  TITLE = {Real-time Neural Radiance Caching for Path Tracing},
  EPRINT = {2106.12372v2},
  DOI = {10.1145/3450626.3459812},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.GR},
  ABSTRACT = {We present a real-time neural radiance caching method for path-traced globalillumination. Our system is designed to handle fully dynamic scenes, and makesno assumptions about the lighting, geometry, and materials. The data-drivennature of our approach sidesteps many difficulties of caching algorithms, suchas locating, interpolating, and updating cache points. Since pretraining neuralnetworks to handle novel, dynamic scenes is a formidable generalizationchallenge, we do away with pretraining and instead achieve generalization viaadaptation, i.e. we opt for training the radiance cache while rendering. Weemploy self-training to provide low-noise training targets and simulateinfinite-bounce transport by merely iterating few-bounce training updates. Theupdates and cache queries incur a mild overhead -- about 2.6ms on full HDresolution -- thanks to a streaming implementation of the neural network thatfully exploits modern hardware. We demonstrate significant noise reduction atthe cost of little induced bias, and report state-of-the-art, real-timeperformance on a number of challenging scenarios.},
  YEAR = {2021},
  MONTH = {Jun},
  URL = {http://arxiv.org/abs/2106.12372v2},
  FILE = {2106.12372v2.pdf}
 }","Performance (rendering), Material/lighting estimation",Caching,,,,,,,,,,SIGGRAPH 2020,,,,"Thomas Müller, Fabrice Rousselle, Jan Novák, Alexander Keller",muller2021realtime,00000152,"We present a real-time neural radiance caching method for path-traced global illumination. Our system is designed to handle fully dynamic scenes, and makes no assumptions about the lighting, geometry, and materials. The data-driven nature of our approach sidesteps many difficulties of caching algorithms, such as locating, interpolating, and updating cache points. Since pretraining neural networks to handle novel, dynamic scenes is a formidable generalization challenge, we do away with pretraining and instead achieve generalization via adaptation, i.e. we opt for training the radiance cache while rendering. We employ self-training to provide low-noise training targets and simulate infinite-bounce transport by merely iterating few-bounce training updates. The updates and cache queries incur a mild overhead -- about 2.6ms on full HD resolution -- thanks to a streaming implementation of the neural network that fully exploits modern hardware. We demonstrate significant noise reduction at the cost of little induced bias, and report state-of-the-art, real-time performance on a number of challenging scenarios.",,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnucEB7FXF4qBtvN2fvOUXz9YV2yTGFz0QFa-9O5SxVuvwPDU4fUkVroyf9QPP2yrYOA
7/19/2021 22:00:47,HyperNeRF: A Higher-Dimensional Representation for Topologically Varying Neural Radiance Fields,HyperNeRF,6/24/2021,https://arxiv.org/pdf/2106.13228.pdf,,,,,,,"@article{park2021hypernerf,
  AUTHOR = {Keunhong Park and Utkarsh Sinha and Peter Hedman and Jonathan T. Barron and Sofien Bouaziz and Dan B Goldman and Ricardo Martin-Brualla and Steven M. Seitz},
  TITLE = {HyperNeRF: A Higher-Dimensional Representation for Topologically VaryingNeural Radiance Fields},
  EPRINT = {2106.13228v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Neural Radiance Fields (NeRF) are able to reconstruct scenes withunprecedented fidelity, and various recent works have extended NeRF to handledynamic scenes. A common approach to reconstruct such non-rigid scenes isthrough the use of a learned deformation field mapping from coordinates in eachinput image into a canonical template coordinate space. However, thesedeformation-based approaches struggle to model changes in topology, astopological changes require a discontinuity in the deformation field, but thesedeformation fields are necessarily continuous. We address this limitation bylifting NeRFs into a higher dimensional space, and by representing the 5Dradiance field corresponding to each individual input image as a slice throughthis ""hyper-space"". Our method is inspired by level set methods, which modelthe evolution of surfaces as slices through a higher dimensional surface. Weevaluate our method on two tasks: (i) interpolating smoothly between ""moments"",i.e., configurations of the scene, seen in the input images while maintainingvisual plausibility, and (ii) novel-view synthesis at fixed moments. We showthat our method, which we dub HyperNeRF, outperforms existing methods on bothtasks by significant margins. Compared to Nerfies, HyperNeRF reduces averageerror rates by 8.6% for interpolation and 8.8% for novel-view synthesis, asmeasured by LPIPS.},
  YEAR = {2021},
  MONTH = {Jun},
  URL = {http://arxiv.org/abs/2106.13228v1},
  FILE = {2106.13228v1.pdf}
 }","Dynamic, Fundamentals",Conditional neural field,NeRF,Density,Per-scene,,,,,,,SIGGRAPH,,No,Direct,"Keunhong Park, Utkarsh Sinha, Peter Hedman, Jonathan T. Barron, Sofien Bouaziz, Dan B Goldman, Ricardo Martin-Brualla, Steven M. Seitz",park2021hypernerf,00000153,"Neural Radiance Fields (NeRF) are able to reconstruct scenes with unprecedented fidelity, and various recent works have extended NeRF to handle dynamic scenes. A common approach to reconstruct such non-rigid scenes is through the use of a learned deformation field mapping from coordinates in each input image into a canonical template coordinate space. However, these deformation-based approaches struggle to model changes in topology, as topological changes require a discontinuity in the deformation field, but these deformation fields are necessarily continuous. We address this limitation by lifting NeRFs into a higher dimensional space, and by representing the 5D radiance field corresponding to each individual input image as a slice through this ""hyper-space"". Our method is inspired by level set methods, which model the evolution of surfaces as slices through a higher dimensional surface. We evaluate our method on two tasks: (i) interpolating smoothly between ""moments"", i.e., configurations of the scene, seen in the input images while maintaining visual plausibility, and (ii) novel-view synthesis at fixed moments. We show that our method, which we dub HyperNeRF, outperforms existing methods on both tasks by significant margins. Compared to Nerfies, HyperNeRF reduces average error rates by 8.6% for interpolation and 8.8% for novel-view synthesis, as measured by LPIPS.",,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnucOyoH3EWkc7oYofQ4az6X2UeWPRQgXON1ySIXkGxR-neipgQtRwMfeV73kzx0fR5o
7/19/2021 21:14:02,Animatable Neural Radiance Fields from Monocular RGB Video,,6/25/2021,https://arxiv.org/pdf/2106.13629.pdf,,,,,,,"@article{chen2021animatable,
  AUTHOR = {Jianchuan Chen and Ying Zhang and Di Kang and Xuefei Zhe and Linchao Bao and Huchuan Lu},
  TITLE = {Animatable Neural Radiance Fields from Monocular RGB Video},
  EPRINT = {2106.13629v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We present animatable neural radiance fields for detailed human avatarcreation from monocular videos. Our approach extends neural radiance fields(NeRF) to the dynamic scenes with human movements via introducing explicitpose-guided deformation while learning the scene representation network. Inparticular, we estimate the human pose for each frame and learn a constantcanonical space for the detailed human template, which enables natural shapedeformation from the observation space to the canonical space under theexplicit control of the pose parameters. To compensate for inaccurate poseestimation, we introduce the pose refinement strategy that updates the initialpose during the learning process, which not only helps to learn more accuratehuman reconstruction but also accelerates the convergence. In experiments weshow that the proposed approach achieves 1) implicit human geometry andappearance reconstruction with high-quality details, 2) photo-realisticrendering of the human from arbitrary views, and 3) animation of the human witharbitrary poses.},
  YEAR = {2021},
  MONTH = {Jun},
  URL = {http://arxiv.org/abs/2106.13629v1},
  FILE = {2106.13629v1.pdf}
 }","Dynamic, Human body",,,,,,,,,,,SIGGRAPH,,,,"Jianchuan Chen, Ying Zhang, Di Kang, Xuefei Zhe, Linchao Bao, Huchuan Lu",chen2021animatable,00000154,"We present animatable neural radiance fields for detailed human avatar creation from monocular videos. Our approach extends neural radiance fields (NeRF) to the dynamic scenes with human movements via introducing explicit pose-guided deformation while learning the scene representation network. In particular, we estimate the human pose for each frame and learn a constant canonical space for the detailed human template, which enables natural shape deformation from the observation space to the canonical space under the explicit control of the pose parameters. To compensate for inaccurate pose estimation, we introduce the pose refinement strategy that updates the initial pose during the learning process, which not only helps to learn more accurate human reconstruction but also accelerates the convergence. In experiments we show that the proposed approach achieves 1) implicit human geometry and appearance reconstruction with high-quality details, 2) photo-realistic rendering of the human from arbitrary views, and 3) animation of the human with arbitrary poses.",,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnufCHNY5kRDNR3sX7Q1GLACcPxU7q1fIFNvtWnnk3-p4gWlMVe6Pe3iKWmFRjaTGLhQ
7/19/2021 21:56:40,Fast Training of Neural Lumigraph Representations using Meta Learning,MetaNLR++,6/28/2021,https://arxiv.org/pdf/2106.14942.pdf,http://www.computationalimaging.org/publications/metanlr/,,https://www.youtube.com/watch?v=5pBFwyUyW6o,,,,"@article{bergman2021metanlr++,
  AUTHOR = {Alexander W. Bergman and Petr Kellnhofer and Gordon Wetzstein},
  TITLE = {Fast Training of Neural Lumigraph Representations using Meta Learning},
  EPRINT = {2106.14942v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Novel view synthesis is a long-standing problem in machine learning andcomputer vision. Significant progress has recently been made in developingneural scene representations and rendering techniques that synthesizephotorealistic images from arbitrary views. These representations, however, areextremely slow to train and often also slow to render. Inspired by neuralvariants of image-based rendering, we develop a new neural rendering approachwith the goal of quickly learning a high-quality representation which can alsobe rendered in real-time. Our approach, MetaNLR++, accomplishes this by using aunique combination of a neural shape representation and 2D CNN-based imagefeature extraction, aggregation, and re-projection. To push representationconvergence times down to minutes, we leverage meta learning to learn neuralshape and image feature priors which accelerate training. The optimized shapeand image features can then be extracted using traditional graphics techniquesand rendered in real time. We show that MetaNLR++ achieves similar or betternovel view synthesis results in a fraction of the time that competing methodsrequire.},
  YEAR = {2021},
  MONTH = {Jun},
  URL = {http://arxiv.org/abs/2106.14942v1},
  FILE = {2106.14942v1.pdf}
 }","Performance (training), Performance (rendering), Generalization","Hypernetwork, Lifting 2D features to 3D, Image-based rendering",SIREN,SDF,Universal,,,,,,,SIGGRAPH,,No,Direct,"Alexander W. Bergman, Petr Kellnhofer, Gordon Wetzstein",bergman2021metanlr++,00000155,"Novel view synthesis is a long-standing problem in machine learning and computer vision. Significant progress has recently been made in developing neural scene representations and rendering techniques that synthesize photorealistic images from arbitrary views. These representations, however, are extremely slow to train and often also slow to render. Inspired by neural variants of image-based rendering, we develop a new neural rendering approach with the goal of quickly learning a high-quality representation which can also be rendered in real-time. Our approach, MetaNLR++, accomplishes this by using a unique combination of a neural shape representation and 2D CNN-based image feature extraction, aggregation, and re-projection. To push representation convergence times down to minutes, we leverage meta learning to learn neural shape and image feature priors which accelerate training. The optimized shape and image features can then be extracted using traditional graphics techniques and rendered in real time. We show that MetaNLR++ achieves similar or better novel view synthesis results in a fraction of the time that competing methods require.",,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnue_-wuaulufbyBJreOxl2pbL4HZgwNtOQOz2-1Arn-r4qdL3YpsVw3tgt0k2Hkxbjo
7/19/2021 21:15:17,IREM: High-Resolution Magnetic Resonance Image Reconstruction via Implicit Neural Representation,IREM,6/29/2021,https://arxiv.org/pdf/2106.15097.pdf,,,,,,,"@article{wu2021irem,
  AUTHOR = {Qing Wu and Yuwei Li and Lan Xu and Ruiming Feng and Hongjiang Wei and Qing Yang and Boliang Yu and Xiaozhao Liu and Jingyi Yu and Yuyao Zhang},
  TITLE = {IREM: High-Resolution Magnetic Resonance (MR) Image Reconstruction viaImplicit Neural Representation},
  EPRINT = {2106.15097v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {eess.IV},
  ABSTRACT = {For collecting high-quality high-resolution (HR) MR image, we propose a novelimage reconstruction network named IREM, which is trained on multiplelow-resolution (LR) MR images and achieve an arbitrary up-sampling rate for HRimage reconstruction. In this work, we suppose the desired HR image as animplicit continuous function of the 3D image spatial coordinate and thethick-slice LR images as several sparse discrete samplings of this function.Then the super-resolution (SR) task is to learn the continuous volumetricfunction from a limited observations using an fully-connected neural networkcombined with Fourier feature positional encoding. By simply minimizing theerror between the network prediction and the acquired LR image intensity acrosseach imaging plane, IREM is trained to represent a continuous model of theobserved tissue anatomy. Experimental results indicate that IREM succeeds inrepresenting high frequency image feature, and in real scene data collection,IREM reduces scan time and achieves high-quality high-resolution MR imaging interms of SNR and local image detail.},
  YEAR = {2021},
  MONTH = {Jun},
  URL = {http://arxiv.org/abs/2106.15097v1},
  FILE = {2106.15097v1.pdf}
 }",,,,,,,,,,,,SIGGRAPH,,,,"Qing Wu, Yuwei Li, Lan Xu, Ruiming Feng, Hongjiang Wei, Qing Yang, Boliang Yu, Xiaozhao Liu, Jingyi Yu, Yuyao Zhang",wu2021irem,00000156,"For collecting high-quality high-resolution (HR) MR image, we propose a novel image reconstruction network named IREM, which is trained on multiple low-resolution (LR) MR images and achieve an arbitrary up-sampling rate for HR image reconstruction. In this work, we suppose the desired HR image as an implicit continuous function of the 3D image spatial coordinate and the thick-slice LR images as several sparse discrete samplings of this function. Then the super-resolution (SR) task is to learn the continuous volumetric function from a limited observations using an fully-connected neural network combined with Fourier feature positional encoding. By simply minimizing the error between the network prediction and the acquired LR image intensity across each imaging plane, IREM is trained to represent a continuous model of the observed tissue anatomy. Experimental results indicate that IREM succeeds in representing high frequency image feature, and in real scene data collection, IREM reduces scan time and achieves high-quality high-resolution MR imaging in terms of SNR and local image detail.",,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnudUtwYaIxnfgBqAh4b72PLnjMIVWRN-NcBg25xyQ74kDnckbNe8ic9NXAV1UXUYYGE
7/19/2021 21:20:02,Rethinking positional encoding,,7/6/2021,https://arxiv.org/pdf/2107.02561.pdf,,https://github.com/osiriszjq/Rethinking-positional-encoding,,,,,"@article{zheng2021rethinking,
  AUTHOR = {Jianqiao Zheng and Sameera Ramasinghe and Simon Lucey},
  TITLE = {Rethinking Positional Encoding},
  EPRINT = {2107.02561v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.LG},
  ABSTRACT = {It is well noted that coordinate based MLPs benefit greatly -- in terms ofpreserving high-frequency information -- through the encoding of coordinatepositions as an array of Fourier features. Hitherto, the rationale for theeffectiveness of these positional encodings has been solely studied through aFourier lens. In this paper, we strive to broaden this understanding by showingthat alternative non-Fourier embedding functions can indeed be used forpositional encoding. Moreover, we show that their performance is entirelydetermined by a trade-off between the stable rank of the embedded matrix andthe distance preservation between embedded coordinates. We further establishthat the now ubiquitous Fourier feature mapping of position is a special casethat fulfills these conditions. Consequently, we present a more general theoryto analyze positional encoding in terms of shifted basis functions. To thisend, we develop the necessary theoretical formulae and empirically verify thatour theoretical claims hold in practice. Codes available athttps://github.com/osiriszjq/Rethinking-positional-encoding.},
  YEAR = {2021},
  MONTH = {Jul},
  URL = {http://arxiv.org/abs/2107.02561v2},
  FILE = {2107.02561v2.pdf}
 }",Fundamentals,,,,,,,,,,,,,,,"Jianqiao Zheng, Sameera Ramasinghe, Simon Lucey",zheng2021rethinking,00000157,"It is well noted that coordinate based MLPs benefit greatly -- in terms of preserving high-frequency information -- through the encoding of coordinate positions as an array of Fourier features. Hitherto, the rationale for the effectiveness of these positional encodings has been solely studied through a Fourier lens. In this paper, we strive to broaden this understanding by showing that alternative non-Fourier embedding functions can indeed be used for positional encoding. Moreover, we show that their performance is entirely determined by a trade-off between the stable rank of the embedded matrix and the distance preservation between embedded coordinates. We further establish that the now ubiquitous Fourier feature mapping of position is a special case that fulfills these conditions. Consequently, we present a more general theory to analyze positional encoding in terms of shifted basis functions. To this end, we develop the necessary theoretical formulae and empirically verify that our theoretical claims hold in practice. Codes available at https://github.com/osiriszjq/Rethinking-positional-encoding.",,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnuf7IT1U5c1-YxmrQ3knUF7D1IO7TxK0vUkSzBGQ8_VGkpKi1hstD2BJ0Tb-5gbeMLY
7/19/2021 21:49:10,Depth-supervised NeRF: Fewer Views and Faster Training for Free,DS-NeRF,7/6/2021,https://arxiv.org/pdf/2107.02791.pdf,https://www.cs.cmu.edu/~dsnerf/,https://github.com/dunbar12138/DSNeRF,https://www.youtube.com/watch?v=84LFxCo7ogk,,,,"@article{deng2021dsnerf,
  AUTHOR = {Kangle Deng and Andrew Liu and Jun-Yan Zhu and Deva Ramanan},
  TITLE = {Depth-supervised NeRF: Fewer Views and Faster Training for Free},
  EPRINT = {2107.02791v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {One common failure mode of Neural Radiance Field (NeRF) models is fittingincorrect geometries when given an insufficient number of input views. Wepropose DS-NeRF (Depth-supervised Neural Radiance Fields), a loss for learningneural radiance fields that takes advantage of readily-available depthsupervision. Our key insight is that sparse depth supervision can be used toregularize the learned geometry, a crucial component for effectively renderingnovel views using NeRF. We exploit the fact that current NeRF pipelines requireimages with known camera poses that are typically estimated by runningstructure-from-motion (SFM). Crucially, SFM also produces sparse 3D points thatcan be used as ``free"" depth supervision during training: we simply add a lossto ensure that depth rendered along rays that intersect these 3D points isclose to the observed depth. We find that DS-NeRF can render more accurateimages given fewer training views while training 2-6x faster. With only twotraining views on real-world images, DS-NeRF significantly outperforms NeRF aswell as other sparse-view variants. We show that our loss is compatible withthese NeRF models, demonstrating that depth is a cheap and easily digestiblesupervisory signal. Finally, we show that DS-NeRF supports other types of depthsupervision such as scanned depth sensors and RGBD reconstruction outputs.},
  YEAR = {2021},
  MONTH = {Jul},
  URL = {http://arxiv.org/abs/2107.02791v1},
  FILE = {2107.02791v1.pdf}
 }","Performance (training), Few-shot reconstruction",,NeRF,Density,Per-scene,,,,,,,SIGGRAPH 2021,,No,Direct,"Kangle Deng, Andrew Liu, Jun-Yan Zhu, Deva Ramanan",deng2021depthsupervised,00000158,"One common failure mode of Neural Radiance Field (NeRF) models is fitting incorrect geometries when given an insufficient number of input views. We propose DS-NeRF (Depth-supervised Neural Radiance Fields), a loss for learning neural radiance fields that takes advantage of readily-available depth supervision. Our key insight is that sparse depth supervision can be used to regularize the learned geometry, a crucial component for effectively rendering novel views using NeRF. We exploit the fact that current NeRF pipelines require images with known camera poses that are typically estimated by running structure-from-motion (SFM). Crucially, SFM also produces sparse 3D points that can be used as ``free"" depth supervision during training: we simply add a loss to ensure that depth rendered along rays that intersect these 3D points is close to the observed depth. We find that DS-NeRF can render more accurate images given fewer training views while training 2-6x faster. With only two training views on real-world images, DS-NeRF significantly outperforms NeRF as well as other sparse-view variants. We show that our loss is compatible with these NeRF models, demonstrating that depth is a cheap and easily digestible supervisory signal. Finally, we show that DS-NeRF supports other types of depth supervision such as scanned depth sensors and RGBD reconstruction outputs.",,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnueqtBZ36XDFKHEwuOgdqrBHWWSwQM5LTCjsQw8EGp626EU98CBeiB4MFG9dnnC7nSA
7/19/2021 21:44:49,3D Neural Scene Representations for Visuomotor Control,,7/8/2021,https://arxiv.org/pdf/2107.04004.pdf,https://3d-representation-learning.github.io/nerf-dy/,,,,"https://www.youtube.com/watch?v=boKF-q6qofQ, https://www.youtube.com/watch?v=GFkb1x6Oxgo, https://www.youtube.com/watch?v=2fSkcTOvl5M, https://www.youtube.com/watch?v=nckvx1S7-cw",,"@article{li20213d,
  AUTHOR = {Yunzhu Li and Shuang Li and Vincent Sitzmann and Pulkit Agrawal and Antonio Torralba},
  TITLE = {3D Neural Scene Representations for Visuomotor Control},
  EPRINT = {2107.04004v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.RO},
  ABSTRACT = {Humans have a strong intuitive understanding of the 3D environment around us.The mental model of the physics in our brain applies to objects of differentmaterials and enables us to perform a wide range of manipulation tasks that arefar beyond the reach of current robots. In this work, we desire to learn modelsfor dynamic 3D scenes purely from 2D visual observations. Our model combinesNeural Radiance Fields (NeRF) and time contrastive learning with anautoencoding framework, which learns viewpoint-invariant 3D-aware scenerepresentations. We show that a dynamics model, constructed over the learnedrepresentation space, enables visuomotor control for challenging manipulationtasks involving both rigid bodies and fluids, where the target is specified ina viewpoint different from what the robot operates on. When coupled with anauto-decoding framework, it can even support goal specification from cameraviewpoints that are outside the training distribution. We further demonstratethe richness of the learned 3D dynamics model by performing future predictionand novel view synthesis. Finally, we provide detailed ablation studiesregarding different system designs and qualitative analysis of the learnedrepresentations.},
  YEAR = {2021},
  MONTH = {Jul},
  URL = {http://arxiv.org/abs/2107.04004v1},
  FILE = {2107.04004v1.pdf}
 }","Beyond graphics, Science and engineering, Robotics",,,,,,,,,,,SIGGRAPH,,,,"Yunzhu Li, Shuang Li, Vincent Sitzmann, Pulkit Agrawal, Antonio Torralba",li20213d,00000159,"Humans have a strong intuitive understanding of the 3D environment around us. The mental model of the physics in our brain applies to objects of different materials and enables us to perform a wide range of manipulation tasks that are far beyond the reach of current robots. In this work, we desire to learn models for dynamic 3D scenes purely from 2D visual observations. Our model combines Neural Radiance Fields (NeRF) and time contrastive learning with an autoencoding framework, which learns viewpoint-invariant 3D-aware scene representations. We show that a dynamics model, constructed over the learned representation space, enables visuomotor control for challenging manipulation tasks involving both rigid bodies and fluids, where the target is specified in a viewpoint different from what the robot operates on. When coupled with an auto-decoding framework, it can even support goal specification from camera viewpoints that are outside the training distribution. We further demonstrate the richness of the learned 3D dynamics model by performing future prediction and novel view synthesis. Finally, we provide detailed ablation studies regarding different system designs and qualitative analysis of the learned representations.",,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnudAVY4LwBVOrydc0wcfc2QBTNmXCiuk6RygsC0YPG_Gw0B1Tx9o8x3ZESDKYf82JrE
7/19/2021 21:43:10,Adaptive weight matrix and phantom intensity learning for computed tomography of chemiluminescence,,7/12/2021,https://www.osapublishing.org/oe/fulltext.cfm?uri=oe-29-15-23682&id=453213,,,,,,,"@article{Pan:21,
  AUTHOR = {Hujie Pan and Di Xiao and Fuhao Zhang and Xuesong Li and Min Xu},
  JOURNAL = {Opt. Express},
  KEYWORDS = {Computational imaging; Computed tomography; Light fields; Light propagation; Neural networks; Propagation methods},
  NUMBER = {15},
  PAGES = {23682--23700},
  PUBLISHER = {OSA},
  TITLE = {Adaptive weight matrix and phantom intensity learning for computed tomography of chemiluminescence},
  VOLUME = {29},
  MONTH = {Jul},
  YEAR = {2021},
  URL = {http://www.opticsexpress.org/abstract.cfm?URI=oe-29-15-23682},
  DOI = {10.1364/OE.427459},
  ABSTRACT = {Classic algebraic reconstruction technique (ART) for computed tomography requires pre-determined weights of the voxels for the projected pixel values to build the equations. However, such weights cannot be accurately obtained in the application of chemiluminescence measurements due to the high physical complexity and computation resources required. Moreover, streaks arise in the results from ART method especially with imperfect projections. In this study, we propose a semi-case-wise learning-based method named Weight Encode Reconstruction Network (WERNet) to co-learn the target phantom intensities and the adaptive weight matrix of the case without labeling the target voxel set and thus offers a more applicable solution for computed tomography problems. Both numerical and experimental validations were conducted to evaluate the algorithm. In the numerical test, with the help of gradient normalization, the WERNet reconstructed voxel set with a high accuracy and showed a higher capability of denoising compared to the classic ART methods. In the experimental test, WERNet produces comparable results to the ART method while having a better performance in avoiding the streaks. Furthermore, with the adaptive weight matrix, WERNet is not sensitive to the ensemble intensity of the projection which shows much better robustness than ART method.}
 }","Beyond graphics, Alternative imaging, Science and engineering",,,,,,,,,,,,,,,"Hujie Pan, Di Xiao, Fuhao Zhang, Xuesong Li, Min Xu",pan2021adaptive,00000160,"Classic algebraic reconstruction technique (ART) for computed tomography requires pre-determined weights of the voxels for the projected pixel values to build the equations. However, such weights cannot be accurately obtained in the application of chemiluminescence measurements due to the high physical complexity and computation resources required. Moreover, streaks arise in the results from ART method especially with imperfect projections. In this study, we propose a semi-case-wise learning-based method named Weight Encode Reconstruction Network (WERNet) to co-learn the target phantom intensities and the adaptive weight matrix of the case without labeling the target voxel set and thus offers a more applicable solution for computed tomography problems. Both numerical and experimental validations were conducted to evaluate the algorithm. In the numerical test, with the help of gradient normalization, the WERNet reconstructed voxel set with a high accuracy and showed a higher capability of denoising compared to the classic ART methods. In the experimental test, WERNet produces comparable results to the ART method while having a better performance in avoiding the streaks. Furthermore, with the adaptive weight matrix, WERNet is not sensitive to the ensemble intensity of the projection which shows much better robustness than ART method.",,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnudgQ4m9eA_NzjPwCi-m3I1T-x3lmk9XJ2cWgIJ1i62xWTmawwMX5y7bRYWwcpVKa4Q
8/29/2021 20:31:22,Unsupervised Discovery of Object Radiance Fields,uORF,7/16/2021,https://arxiv.org/pdf/2107.07905.pdf,https://kovenyu.com/uorf/,https://github.com/KovenYu/uORF,https://www.youtube.com/watch?v=6J9OpvT4dCA,https://kovenyu.com/uorf/static/uORF_supp.pdf,,,"@article{yu2021uorf,
  AUTHOR = {Hong-Xing Yu and Leonidas J. Guibas and Jiajun Wu},
  TITLE = {Unsupervised Discovery of Object Radiance Fields},
  EPRINT = {2107.07905v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We study the problem of inferring an object-centric scene representation froma single image, aiming to derive a representation that explains the imageformation process, captures the scene's 3D nature, and is learned withoutsupervision. Most existing methods on scene decomposition lack one or more ofthese characteristics, due to the fundamental challenge in integrating thecomplex 3D-to-2D image formation process into powerful inference schemes likedeep networks. In this paper, we propose unsupervised discovery of ObjectRadiance Fields (uORF), integrating recent progresses in neural 3D scenerepresentations and rendering with deep inference networks for unsupervised 3Dscene decomposition. Trained on multi-view RGB images without annotations, uORFlearns to decompose complex scenes with diverse, textured background from asingle image. We show that uORF performs well on unsupervised 3D scenesegmentation, novel view synthesis, and scene editing on three datasets.},
  YEAR = {2021},
  MONTH = {Jul},
  URL = {http://arxiv.org/abs/2107.07905v1},
  FILE = {2107.07905v1.pdf}
  }","Generalization, Editable, Segmentation/composition","Conditional neural field, Lifting 2D features to 3D, Volume partitioning, Object-centric representation, Segmentation, Data-driven component (pre-trained, cross-scene)",NeRF,Density,Category-level,,,,,,,,,No,Direct,"Hong-Xing Yu, Leonidas J. Guibas, Jiajun Wu",yu2021uorf,00000178,"We study the problem of inferring an object-centric scene representation from a single image, aiming to derive a representation that explains the image formation process, captures the scene's 3D nature, and is learned without supervision. Most existing methods on scene decomposition lack one or more of these characteristics, due to the fundamental challenge in integrating the complex 3D-to-2D image formation process into powerful inference schemes like deep networks. In this paper, we propose unsupervised discovery of Object Radiance Fields (uORF), integrating recent progresses in neural 3D scene representations and rendering with deep inference networks for unsupervised 3D scene decomposition. Trained on multi-view RGB images without annotations, uORF learns to decompose complex scenes with diverse, textured background from a single image. We show that uORF performs well on unsupervised 3D scene segmentation, novel view synthesis, and scene editing on three datasets.",,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnucPWiPZxCTzeTAXDv6o47rcF6AIN0LoQ_CFIyqpQPOw4fHX7VIvoMyObMZex6Lc3SM
8/29/2021 19:19:51,A Deep Signed Directional Distance Function for Object Shape Representation,,7/23/2021,https://arxiv.org/pdf/2107.11024.pdf,,,,,,,"@article{zobeidi2021a,
  AUTHOR = {Ehsan Zobeidi and Nikolay Atanasov},
  TITLE = {A Deep Signed Directional Distance Function for Object ShapeRepresentation},
  EPRINT = {2107.11024v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Neural networks that map 3D coordinates to signed distance function (SDF) oroccupancy values have enabled high-fidelity implicit representations of objectshape. This paper develops a new shape model that allows synthesizing noveldistance views by optimizing a continuous signed directional distance function(SDDF). Similar to deep SDF models, our SDDF formulation can represent wholecategories of shapes and complete or interpolate across shapes from partialinput data. Unlike an SDF, which measures distance to the nearest surface inany direction, an SDDF measures distance in a given direction. This allowstraining an SDDF model without 3D shape supervision, using only distancemeasurements, readily available from depth camera or Lidar sensors. Our modelalso removes post-processing steps like surface extraction or rendering bydirectly predicting distance at arbitrary locations and viewing directions.Unlike deep view-synthesis techniques, such as Neural Radiance Fields, whichtrain high-capacity black-box models, our model encodes by construction theproperty that SDDF values decrease linearly along the viewing direction. Thisstructure constraint not only results in dimensionality reduction but alsoprovides analytical confidence about the accuracy of SDDF predictions,regardless of the distance to the object surface.},
  YEAR = {2021},
  MONTH = {Jul},
  URL = {http://arxiv.org/abs/2107.11024v1},
  FILE = {2107.11024v1.pdf}
  }",,,,Signed Directional Distance Field (SDDF),Category-level,,,,,,,,,Yes,Direct,"Ehsan Zobeidi, Nikolay Atanasov",zobeidi2021a,00000175,"Neural networks that map 3D coordinates to signed distance function (SDF) or occupancy values have enabled high-fidelity implicit representations of object shape. This paper develops a new shape model that allows synthesizing novel distance views by optimizing a continuous signed directional distance function (SDDF). Similar to deep SDF models, our SDDF formulation can represent whole categories of shapes and complete or interpolate across shapes from partial input data. Unlike an SDF, which measures distance to the nearest surface in any direction, an SDDF measures distance in a given direction. This allows training an SDDF model without 3D shape supervision, using only distance measurements, readily available from depth camera or Lidar sensors. Our model also removes post-processing steps like surface extraction or rendering by directly predicting distance at arbitrary locations and viewing directions. Unlike deep view-synthesis techniques, such as Neural Radiance Fields, which train high-capacity black-box models, our model encodes by construction the property that SDDF values decrease linearly along the viewing direction. This structure constraint not only results in dimensionality reduction but also provides analytical confidence about the accuracy of SDDF predictions, regardless of the distance to the object surface.",,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnuefWEGFNp6sezYTA7JdVGGS2yBnt1wUbAdIahJ8BBtZoeL_302TurXUk-Aqmo-R6VI
8/29/2021 17:44:47,H3D-Net: Few-Shot High-Fidelity 3D Head Reconstruction,H3D-Net,7/26/2021,https://arxiv.org/pdf/2107.12512.pdf,https://crisalixsa.github.io/h3d-net/,https://github.com/CrisalixSA/h3ds,,,,,"@article{ramon2021h3dnet,
  AUTHOR = {Eduard Ramon and Gil Triginer and Janna Escur and Albert Pumarola and Jaime Garcia and Xavier Giro-i-Nieto and Francesc Moreno-Noguer},
  TITLE = {H3D-Net: Few-Shot High-Fidelity 3D Head Reconstruction},
  EPRINT = {2107.12512v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Recent learning approaches that implicitly represent surface geometry usingcoordinate-based neural representations have shown impressive results in theproblem of multi-view 3D reconstruction. The effectiveness of these techniquesis, however, subject to the availability of a large number (several tens) ofinput views of the scene, and computationally demanding optimizations. In thispaper, we tackle these limitations for the specific problem of few-shot full 3Dhead reconstruction, by endowing coordinate-based representations with aprobabilistic shape prior that enables faster convergence and bettergeneralization when using few input images (down to three). First, we learn ashape model of 3D heads from thousands of incomplete raw scans using implicitrepresentations. At test time, we jointly overfit two coordinate-based neuralnetworks to the scene, one modeling the geometry and another estimating thesurface radiance, using implicit differentiable rendering. We devise atwo-stage optimization strategy in which the learned prior is used toinitialize and constrain the geometry during an initial optimization phase.Then, the prior is unfrozen and fine-tuned to the scene. By doing this, weachieve high-fidelity head reconstructions, including hair and shoulders, andwith a high level of detail that consistently outperforms both state-of-the-art3D Morphable Models methods in the few-shot scenario, and non-parametricmethods when large sets of views are available.},
  YEAR = {2021},
  MONTH = {Jul},
  URL = {http://arxiv.org/abs/2107.12512v1},
  FILE = {2107.12512v1.pdf}
  }","Human head, Few-shot reconstruction","Conditional neural field, Per-instance fine-tuning, Data-driven component (pre-trained, cross-scene)",NeRF,SDF,Category-level,,,,,,,,https://github.com/CrisalixSA/h3ds,No,Direct,"Eduard Ramon, Gil Triginer, Janna Escur, Albert Pumarola, Jaime Garcia, Xavier Giro-i-Nieto, Francesc Moreno-Noguer",ramon2021h3dnet,00000174,"Recent learning approaches that implicitly represent surface geometry using coordinate-based neural representations have shown impressive results in the problem of multi-view 3D reconstruction. The effectiveness of these techniques is, however, subject to the availability of a large number (several tens) of input views of the scene, and computationally demanding optimizations. In this paper, we tackle these limitations for the specific problem of few-shot full 3D head reconstruction, by endowing coordinate-based representations with a probabilistic shape prior that enables faster convergence and better generalization when using few input images (down to three). First, we learn a shape model of 3D heads from thousands of incomplete raw scans using implicit representations. At test time, we jointly overfit two coordinate-based neural networks to the scene, one modeling the geometry and another estimating the surface radiance, using implicit differentiable rendering. We devise a two-stage optimization strategy in which the learned prior is used to initialize and constrain the geometry during an initial optimization phase. Then, the prior is unfrozen and fine-tuned to the scene. By doing this, we achieve high-fidelity head reconstructions, including hair and shoulders, and with a high level of detail that consistently outperforms both state-of-the-art 3D Morphable Models methods in the few-shot scenario, and non-parametric methods when large sets of views are available.",,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnudLbcUmfk4XpKlS9cL3pm01cCXhJo38D9fdoEDA3yMAHQEQu9Rm0_BI2NXRCO4WmjM
9/17/2021 19:35:45,NeLF: Neural Light-transport Field for Portrait View Synthesis and Relighting,NeLF,7/26/2021,https://arxiv.org/pdf/2107.12351.pdf,,,,,,,"@article{sun2021nelf,
  AUTHOR = {Tiancheng Sun and Kai-En Lin and Sai Bi and Zexiang Xu and Ravi Ramamoorthi},
  TITLE = {NeLF: Neural Light-transport Field for Portrait View Synthesis andRelighting},
  EPRINT = {2107.12351v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Human portraits exhibit various appearances when observed from differentviews under different lighting conditions. We can easily imagine how the facewill look like in another setup, but computer algorithms still fail on thisproblem given limited observations. To this end, we present a system forportrait view synthesis and relighting: given multiple portraits, we use aneural network to predict the light-transport field in 3D space, and from thepredicted Neural Light-transport Field (NeLF) produce a portrait from a newcamera view under a new environmental lighting. Our system is trained on alarge number of synthetic models, and can generalize to different synthetic andreal portraits under various lighting conditions. Our method achievessimultaneous view synthesis and relighting given multi-view portraits as theinput, and achieves state-of-the-art results.},
  YEAR = {2021},
  MONTH = {Jul},
  URL = {http://arxiv.org/abs/2107.12351v1},
  FILE = {2107.12351v1.pdf}
 }","Human head, Material/lighting estimation","Lifting 2D features to 3D, Feature volume, Data-driven component (pre-trained, cross-scene)",,,Category-level,,,,,,,EGSR 2021,,No,,"Tiancheng Sun, Kai-En Lin, Sai Bi, Zexiang Xu, Ravi Ramamoorthi",sun2021nelf,00000211,"Human portraits exhibit various appearances when observed from different views under different lighting conditions. We can easily imagine how the face will look like in another setup, but computer algorithms still fail on this problem given limited observations. To this end, we present a system for portrait view synthesis and relighting: given multiple portraits, we use a neural network to predict the light-transport field in 3D space, and from the predicted Neural Light-transport Field (NeLF) produce a portrait from a new camera view under a new environmental lighting. Our system is trained on a large number of synthetic models, and can generalize to different synthetic and real portraits under various lighting conditions. Our method achieves simultaneous view synthesis and relighting given multi-view portraits as the input, and achieves state-of-the-art results.",,Yes,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnueCtXCzkg_yk-PqoXG5OyovlNn_v7tS5DYS41yCJFYxxPG5ERYHKNXkdjXJPEYdJJg
8/29/2021 16:46:20,Neural Image Representations for Multi-Image Fusion and Layer Separation,,8/2/2021,https://arxiv.org/pdf/2108.01199.pdf,,,,,,,"@article{nam2021neural,
  AUTHOR = {Seonghyeon Nam and Marcus A. Brubaker and Michael S. Brown},
  TITLE = {Neural Image Representations for Multi-Image Fusion and Layer Separation},
  EPRINT = {2108.01199v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We propose a framework for aligning and fusing multiple images into a singlecoordinate-based neural representations. Our framework targets burst imagesthat have misalignment due to camera ego motion and small changes in the scene.We describe different strategies for alignment depending on the assumption ofthe scene motion, namely, perspective planar (i.e., homography), optical flowwith minimal scene change, and optical flow with notable occlusion anddisocclusion. Our framework effectively combines the multiple inputs into asingle neural implicit function without the need for selecting one of theimages as a reference frame. We demonstrate how to use this multi-frame fusionframework for various layer separation tasks.},
  YEAR = {2021},
  MONTH = {Aug},
  URL = {http://arxiv.org/abs/2108.01199v2},
  FILE = {2108.01199v2.pdf}
  }","Dynamic, Image",Warping field/Flow field,,,Per-scene,,,,,,,,,,Direct,"Seonghyeon Nam, Marcus A. Brubaker, Michael S. Brown",nam2021neural,00000171,"We propose a framework for aligning and fusing multiple images into a single coordinate-based neural representations. Our framework targets burst images that have misalignment due to camera ego motion and small changes in the scene. We describe different strategies for alignment depending on the assumption of the scene motion, namely, perspective planar (i.e., homography), optical flow with minimal scene change, and optical flow with notable occlusion and disocclusion. Our framework effectively combines the multiple inputs into a single neural implicit function without the need for selecting one of the images as a reference frame. We demonstrate how to use this multi-frame fusion framework for various layer separation tasks.",,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnuedQu0kRtV8fZhN08ACUoKeOI7AEiu5mwj5l5TV1HGZu25ceWDQQQnM64R_iEDMQac
8/30/2021 15:08:06,View Synthesis In Casually Captured Scenes Using a Cylindrical Neural Radiance Field With Exposure Compensation,,8/9/2021,https://dl.acm.org/doi/pdf/10.1145/3450618.3469147,,,,,,,"@inproceedings{khademi2021cylindrical,
 author = {Khademi, Wesley and Ventura, Jonathan},
 title = {View Synthesis In Casually Captured Scenes Using a Cylindrical Neural Radiance Field With Exposure Compensation},
 year = {2021},
 isbn = {9781450383714},
 publisher = {Association for Computing Machinery},
 address = {New York, NY, USA},
 url = {https://doi.org/10.1145/3450618.3469147},
 doi = {10.1145/3450618.3469147},
 abstract = { We extend Neural Radiance Fields (NeRF) with a cylindrical parameterization that
 enables rendering photorealistic novel views of 360° outward facing scenes. We further
 introduce a learned exposure compensation parameter to account for the varying exposure
 in training images that may occur from casually capturing a scene. We evaluate our
 method on a variety of 360° casually captured scenes.},
 booktitle = {ACM SIGGRAPH 2021 Posters},
 articleno = {28},
 numpages = {2},
 location = {Virtual Event, USA},
 series = {SIGGRAPH '21}
 }",,"Representation, Sampling",NeRF,Density,Per-scene,,,,,,,SIGGRAPH 2021,,No,Direct,"Wesley Khademi, Jonathan Ventura",khademi2021cylindrical,00000190,We extend Neural Radiance Fields (NeRF) with a cylindrical parameterization that enables rendering photorealistic novel views of 360◦ outward facing scenes. We further introduce a learned exposure compensation parameter to account for the varying exposure in training images that may occur from casually capturing a scene. We evaluate our method on a variety of 360◦ casually captured scenes.,,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnucwYw--bnUM0WnsqqAvSYjM9jUra2Y_-Iaysjwq3bchuJB9yVOPTvQH-m-pmlD5ilY
8/30/2021 14:56:53,FLAME-in-NeRF: Neural control of Radiance Fields for Free View Face Animation,FLAME-in-NeRF,8/10/2021,https://arxiv.org/pdf/2108.04913.pdf,,,,,,,"@article{athar2021flameinnerf,
  AUTHOR = {ShahRukh Athar and Zhixin Shu and Dimitris Samaras},
  TITLE = {FLAME-in-NeRF : Neural control of Radiance Fields for Free View FaceAnimation},
  EPRINT = {2108.04913v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {This paper presents a neural rendering method for controllable portrait videosynthesis. Recent advances in volumetric neural rendering, such as neuralradiance fields (NeRF), has enabled the photorealistic novel view synthesis ofstatic scenes with impressive results. However, modeling dynamic andcontrollable objects as part of a scene with such scene representations isstill challenging. In this work, we design a system that enables both novelview synthesis for portrait video, including the human subject and the scenebackground, and explicit control of the facial expressions through alow-dimensional expression representation. We leverage the expression space ofa 3D morphable face model (3DMM) to represent the distribution of human facialexpressions, and use it to condition the NeRF volumetric function. Furthermore,we impose a spatial prior brought by 3DMM fitting to guide the network to learndisentangled control for scene appearance and facial actions. We demonstratethe effectiveness of our method on free view synthesis of portrait videos withexpression controls. To train a scene, our method only requires a short videoof a subject captured by a mobile device.},
  YEAR = {2021},
  MONTH = {Aug},
  URL = {http://arxiv.org/abs/2108.04913v1},
  FILE = {2108.04913v1.pdf}
  }","Dynamic, Human head, Generalization, Editable","Conditional neural field, Coarse-to-fine, Warping field/Flow field, Data-driven component (pre-trained, cross-scene)",NeRF,Density,Category-level,,,,,,,,,No,Direct,"ShahRukh Athar, Zhixin Shu, Dimitris Samaras",athar2021flameinnerf,00000189,"This paper presents a neural rendering method for controllable portrait video synthesis. Recent advances in volumetric neural rendering, such as neural radiance fields (NeRF), has enabled the photorealistic novel view synthesis of static scenes with impressive results. However, modeling dynamic and controllable objects as part of a scene with such scene representations is still challenging. In this work, we design a system that enables both novel view synthesis for portrait video, including the human subject and the scene background, and explicit control of the facial expressions through a low-dimensional expression representation. We leverage the expression space of a 3D morphable face model (3DMM) to represent the distribution of human facial expressions, and use it to condition the NeRF volumetric function. Furthermore, we impose a spatial prior brought by 3DMM fitting to guide the network to learn disentangled control for scene appearance and facial actions. We demonstrate the effectiveness of our method on free view synthesis of portrait videos with expression controls. To train a scene, our method only requires a short video of a subject captured by a mobile device.",,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnufV6foTllHAlcynWScf0OGMUCkPLKI2h4L-LhWhtMIuZW3kE5_SB_7A-J_RT9EP5Mg
8/30/2021 11:59:11,SIDER: Single-Image Neural Optimization for Facial Geometric Detail Recovery,SIDER,8/11/2021,https://arxiv.org/pdf/2108.05465.pdf,,,,,,,"@article{chatziagapi2021sider,
  AUTHOR = {Aggelina Chatziagapi and ShahRukh Athar and Francesc Moreno-Noguer and Dimitris Samaras},
  TITLE = {SIDER: Single-Image Neural Optimization for Facial Geometric DetailRecovery},
  EPRINT = {2108.05465v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We present SIDER(Single-Image neural optimization for facial geometric DEtailRecovery), a novel photometric optimization method that recovers detailedfacial geometry from a single image in an unsupervised manner. Inspired byclassical techniques of coarse-to-fine optimization and recent advances inimplicit neural representations of 3D shape, SIDER combines a geometry priorbased on statistical models and Signed Distance Functions (SDFs) to recoverfacial details from single images. First, it estimates a coarse geometry usinga morphable model represented as an SDF. Next, it reconstructs facial geometrydetails by optimizing a photometric loss with respect to the ground truthimage. In contrast to prior work, SIDER does not rely on any dataset priors anddoes not require additional supervision from multiple views, lighting changesor ground truth 3D shape. Extensive qualitative and quantitative evaluationdemonstrates that our method achieves state-of-the-art on facial geometricdetail recovery, using only a single in-the-wild image.},
  YEAR = {2021},
  MONTH = {Aug},
  URL = {http://arxiv.org/abs/2108.05465v1},
  FILE = {2108.05465v1.pdf}
  }","Human head, Few-shot reconstruction, Generalization","Coarse-to-fine, Data-driven component (pre-trained, cross-scene)",,SDF,Category-level,,,,,,,,,No,Direct,"Aggelina Chatziagapi, ShahRukh Athar, Francesc Moreno-Noguer, Dimitris Samaras",chatziagapi2021sider,00000187,"We present SIDER(Single-Image neural optimization for facial geometric DEtail Recovery), a novel photometric optimization method that recovers detailed facial geometry from a single image in an unsupervised manner. Inspired by classical techniques of coarse-to-fine optimization and recent advances in implicit neural representations of 3D shape, SIDER combines a geometry prior based on statistical models and Signed Distance Functions (SDFs) to recover facial details from single images. First, it estimates a coarse geometry using a morphable model represented as an SDF. Next, it reconstructs facial geometry details by optimizing a photometric loss with respect to the ground truth image. In contrast to prior work, SIDER does not rely on any dataset priors and does not require additional supervision from multiple views, lighting changes or ground truth 3D shape. Extensive qualitative and quantitative evaluation demonstrates that our method achieves state-of-the-art on facial geometric detail recovery, using only a single in-the-wild image.",,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnueSnWp-C8RFcJKnAfMsIm-DALevHqTx_FYekpXpN1NLz_TtHQ_3-_4rnkADFN-W3hE
8/30/2021 13:31:02,Continual Neural Mapping: Learning An Implicit Scene Representation from Sequential Observations,,8/12/2021,https://arxiv.org/pdf/2108.05851.pdf,,,https://zikeyan.github.io/videos/iccv2021.mp4,,,,"@article{yan2021continual,
  AUTHOR = {Zike Yan and Yuxin Tian and Xuesong Shi and Ping Guo and Peng Wang and Hongbin Zha},
  TITLE = {Continual Neural Mapping: Learning An Implicit Scene Representation fromSequential Observations},
  EPRINT = {2108.05851v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Recent advances have enabled a single neural network to serve as an implicitscene representation, establishing the mapping function between spatialcoordinates and scene properties. In this paper, we make a further step towardscontinual learning of the implicit scene representation directly fromsequential observations, namely Continual Neural Mapping. The proposed problemsetting bridges the gap between batch-trained implicit neural representationsand commonly used streaming data in robotics and vision communities. Weintroduce an experience replay approach to tackle an exemplary task ofcontinual neural mapping: approximating a continuous signed distance function(SDF) from sequential depth images as a scene geometry representation. We showfor the first time that a single network can represent scene geometry over timecontinually without catastrophic forgetting, while achieving promisingtrade-offs between accuracy and efficiency.},
  YEAR = {2021},
  MONTH = {Aug},
  URL = {http://arxiv.org/abs/2108.05851v1},
  FILE = {2108.05851v1.pdf}
  }","Robotics, Multi-task/Continual/Transfer learning","Conditional neural field, Volume partitioning",,SDF,,,,,,,,ICCV 2021,,Yes,Direct,"Zike Yan, Yuxin Tian, Xuesong Shi, Ping Guo, Peng Wang, Hongbin Zha",yan2021continual,00000188,"Recent advances have enabled a single neural network to serve as an implicit scene representation, establishing the mapping function between spatial coordinates and scene properties. In this paper, we make a further step towards continual learning of the implicit scene representation directly from sequential observations, namely Continual Neural Mapping. The proposed problem setting bridges the gap between batch-trained implicit neural representations and commonly used streaming data in robotics and vision communities. We introduce an experience replay approach to tackle an exemplary task of continual neural mapping: approximating a continuous signed distance function (SDF) from sequential depth images as a scene geometry representation. We show for the first time that a single network can represent scene geometry over time continually without catastrophic forgetting, while achieving promising trade-offs between accuracy and efficiency.",,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnueL7aPFzJ8YhDl0H59GO33RYdtE9Gc-wUrKUzKQitnE_GUqYXEVxBpRTM4OlsERLkU
9/18/2021 10:30:51,Unsupervised Non-Rigid Image Distortion Removal via Grid Deformation,,8/16/2021,https://ivlab.cse.lsu.edu/pub/iccv_21_distortion_removal.pdf,,https://github.com/Nianyi-Li/unsupervised-NDIR,,,,,"@article{liunsupervised,
  ABSTRACT = {Many computer vision problems face difficulties when imaging through turbulent refractive media (eg, air and water) due to the refraction and scattering of light. These effects cause geometric distortion that requires either handcrafted physical priors or supervised learning methods to remove. In this paper, we present a novel unsupervised network to recover the latent distortion-free image. The key idea is to model non-rigid distortions as deformable grids. Our network consists of a grid deformer that estimates the distortion field and an image},
  AUTHOR = {Li, Nianyi and Thapa, Simron and Whyte, Cameron and Reed, Albert and Jayasuriya, Suren and Ye, Jinwei},
  PUB_YEAR = {NA},
  TITLE = {Unsupervised Non-Rigid Image Distortion Removal via Grid Deformation},
  VENUE = {NA}
 }","Image, Beyond graphics, Alternative imaging",,,,,,,,,,,ICCV 2021,,,"Direct, Indirect","Nianyi Li, Simron Thapa, Cameron Whyte, Albert Reed, Suren Jayasuriya, Jinwei Ye",liunsupervised,00000225,"Many computer vision problems face difficulties when imaging through turbulent refractive media (eg, air and water) due to the refraction and scattering of light. These effects cause geometric distortion that requires either handcrafted physical priors or supervised learning methods to remove. In this paper, we present a novel unsupervised network to recover the latent distortion-free image. The key idea is to model non-rigid distortions as deformable grids. Our network consists of a grid deformer that estimates the distortion field and an image",,,Yes,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnufGIbNbqU1lxFQfaYoGO_kQ37_CX7swaM5cE-bkzULODCdFrRi24X_dgfaOGH5E-tw
9/17/2021 14:20:27,ARCH++: Animation-Ready Clothed Human Reconstruction Revisited,ARCH++,8/17/2021,https://arxiv.org/pdf/2108.07845.pdf,,,,,,,"@article{he2021arch++,
  AUTHOR = {Tong He and Yuanlu Xu and Shunsuke Saito and Stefano Soatto and Tony Tung},
  TITLE = {ARCH++: Animation-Ready Clothed Human Reconstruction Revisited},
  EPRINT = {2108.07845v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We present ARCH++, an image-based method to reconstruct 3D avatars witharbitrary clothing styles. Our reconstructed avatars are animation-ready andhighly realistic, in both the visible regions from input views and the unseenregions. While prior work shows great promise of reconstructing animatableclothed humans with various topologies, we observe that there exist fundamentallimitations resulting in sub-optimal reconstruction quality. In this paper, werevisit the major steps of image-based avatar reconstruction and address thelimitations with ARCH++. First, we introduce an end-to-end point based geometryencoder to better describe the semantics of the underlying 3D human body, inreplacement of previous hand-crafted features. Second, in order to address theoccupancy ambiguity caused by topological changes of clothed humans in thecanonical pose, we propose a co-supervising framework with cross-spaceconsistency to jointly estimate the occupancy in both the posed and canonicalspaces. Last, we use image-to-image translation networks to further refinedetailed geometry and texture on the reconstructed surface, which improves thefidelity and consistency across arbitrary viewpoints. In the experiments, wedemonstrate improvements over the state of the art on both public benchmarksand user studies in reconstruction quality and realism.},
  YEAR = {2021},
  MONTH = {Aug},
  URL = {http://arxiv.org/abs/2108.07845v1},
  FILE = {2108.07845v1.pdf}
 }","Human body, Few-shot reconstruction, Editable","Conditional neural field, Lifting 2D features to 3D, Voxelization, Feature volume, Data-driven component (pre-trained, cross-scene)",,,Category-level,,,,,,,ICCV 2021,,No,,"Tong He, Yuanlu Xu, Shunsuke Saito, Stefano Soatto, Tony Tung",he2021arch++,00000207,"We present ARCH++, an image-based method to reconstruct 3D avatars with arbitrary clothing styles. Our reconstructed avatars are animation-ready and highly realistic, in both the visible regions from input views and the unseen regions. While prior work shows great promise of reconstructing animatable clothed humans with various topologies, we observe that there exist fundamental limitations resulting in sub-optimal reconstruction quality. In this paper, we revisit the major steps of image-based avatar reconstruction and address the limitations with ARCH++. First, we introduce an end-to-end point based geometry encoder to better describe the semantics of the underlying 3D human body, in replacement of previous hand-crafted features. Second, in order to address the occupancy ambiguity caused by topological changes of clothed humans in the canonical pose, we propose a co-supervising framework with cross-space consistency to jointly estimate the occupancy in both the posed and canonical spaces. Last, we use image-to-image translation networks to further refine detailed geometry and texture on the reconstructed surface, which improves the fidelity and consistency across arbitrary viewpoints. In the experiments, we demonstrate improvements over the state of the art on both public benchmarks and user studies in reconstruction quality and realism.",,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnuf9CjswqXq_Hm_PWfP_GCcHairpxscXm98u7fu89kokde-qv3TqUM7UMmUJAVAosdo
8/29/2021 16:11:17,Neural-GIF: Neural Generalized Implicit Functions for Animating People in Clothing,Neural-GIF,8/19/2021,https://arxiv.org/pdf/2108.08807.pdf,,,,,,,"@article{tiwari2021neuralgif,
  AUTHOR = {Garvita Tiwari and Nikolaos Sarafianos and Tony Tung and Gerard Pons-Moll},
  TITLE = {Neural-GIF: Neural Generalized Implicit Functions for Animating Peoplein Clothing},
  EPRINT = {2108.08807v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We present Neural Generalized Implicit Functions(Neural-GIF), to animatepeople in clothing as a function of the body pose. Given a sequence of scans ofa subject in various poses, we learn to animate the character for new poses.Existing methods have relied on template-based representations of the humanbody (or clothing). However such models usually have fixed and limitedresolutions, require difficult data pre-processing steps and cannot be usedwith complex clothing. We draw inspiration from template-based methods, whichfactorize motion into articulation and non-rigid deformation, but generalizethis concept for implicit shape learning to obtain a more flexible model. Welearn to map every point in the space to a canonical space, where a learneddeformation field is applied to model non-rigid effects, before evaluating thesigned distance field. Our formulation allows the learning of complex andnon-rigid deformations of clothing and soft tissue, without computing atemplate registration as it is common with current approaches. Neural-GIF canbe trained on raw 3D scans and reconstructs detailed complex surface geometryand deformations. Moreover, the model can generalize to new poses. We evaluateour method on a variety of characters from different public datasets in diverseclothing styles and show significant improvements over baseline methods,quantitatively and qualitatively. We also extend our model to multiple shapesetting. To stimulate further research, we will make the model, code and datapublicly available at: https://virtualhumans.mpi-inf.mpg.de/neuralgif/},
  YEAR = {2021},
  MONTH = {Aug},
  URL = {http://arxiv.org/abs/2108.08807v2},
  FILE = {2108.08807v2.pdf}
  }",Human body,Warping field/Flow field,,SDF,Category-level,,,,,,,,,Yes,,"Garvita Tiwari, Nikolaos Sarafianos, Tony Tung, Gerard Pons-Moll",tiwari2021neuralgif,00000161,"We present Neural Generalized Implicit Functions(Neural-GIF), to animate people in clothing as a function of the body pose. Given a sequence of scans of a subject in various poses, we learn to animate the character for new poses. Existing methods have relied on template-based representations of the human body (or clothing). However such models usually have fixed and limited resolutions, require difficult data pre-processing steps and cannot be used with complex clothing. We draw inspiration from template-based methods, which factorize motion into articulation and non-rigid deformation, but generalize this concept for implicit shape learning to obtain a more flexible model. We learn to map every point in the space to a canonical space, where a learned deformation field is applied to model non-rigid effects, before evaluating the signed distance field. Our formulation allows the learning of complex and non-rigid deformations of clothing and soft tissue, without computing a template registration as it is common with current approaches. Neural-GIF can be trained on raw 3D scans and reconstructs detailed complex surface geometry and deformations. Moreover, the model can generalize to new poses. We evaluate our method on a variety of characters from different public datasets in diverse clothing styles and show significant improvements over baseline methods, quantitatively and qualitatively. We also extend our model to multiple shape setting. To stimulate further research, we will make the model, code and data publicly available at: https://virtualhumans.mpi-inf.mpg.de/neuralgif/",,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnucRISTnmm3x_keTkutQUzq6_UCWbD-iDZQI47FJypjhNrqbgNo8tr-mDnf3b4IwceU
8/29/2021 16:44:39,Augmenting Implicit Neural Shape Representations with Explicit Deformation Fields,,8/19/2021,https://arxiv.org/pdf/2108.08931.pdf,,,,,,,"@article{atzmon2021augmenting,
  AUTHOR = {Matan Atzmon and David Novotny and Andrea Vedaldi and Yaron Lipman},
  TITLE = {Augmenting Implicit Neural Shape Representations with ExplicitDeformation Fields},
  EPRINT = {2108.08931v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Implicit neural representation is a recent approach to learn shapecollections as zero level-sets of neural networks, where each shape isrepresented by a latent code. So far, the focus has been shape reconstruction,while shape generalization was mostly left to generic encoder-decoder orauto-decoder regularization.In this paper we advocate deformation-aware regularization for implicitneural representations, aiming at producing plausible deformations as latentcode changes. The challenge is that implicit representations do not capturecorrespondences between different shapes, which makes it difficult to representand regularize their deformations. Thus, we propose to pair the implicitrepresentation of the shapes with an explicit, piecewise linear deformationfield, learned as an auxiliary function. We demonstrate that, by regularizingthese deformation fields, we can encourage the implicit neural representationto induce natural deformations in the learned shape space, such asas-rigid-as-possible deformations.},
  YEAR = {2021},
  MONTH = {Aug},
  URL = {http://arxiv.org/abs/2108.08931v1},
  FILE = {2108.08931v1.pdf}
  }","Dynamic, Human body",Warping field/Flow field,,,,,,,,,,,,Yes,,"Matan Atzmon, David Novotny, Andrea Vedaldi, Yaron Lipman",atzmon2021augmenting,00000170,"Implicit neural representation is a recent approach to learn shape collections as zero level-sets of neural networks, where each shape is represented by a latent code. So far, the focus has been shape reconstruction, while shape generalization was mostly left to generic encoder-decoder or auto-decoder regularization. In this paper we advocate deformation-aware regularization for implicit neural representations, aiming at producing plausible deformations as latent code changes. The challenge is that implicit representations do not capture correspondences between different shapes, which makes it difficult to represent and regularize their deformations. Thus, we propose to pair the implicit representation of the shapes with an explicit, piecewise linear deformation field, learned as an auxiliary function. We demonstrate that, by regularizing these deformation fields, we can encourage the implicit neural representation to induce natural deformations in the learned shape space, such as as-rigid-as-possible deformations.",,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnud3SZEcDAPRnTm3B9ey4HytRiEd5CpXK_4ld_s7-J_nn1oyilBfrfljJajLq3tVJAA
8/29/2021 16:33:37,Learning Deeper Non-Monotonic Networks by Softly Transferring Solution Space,,8/20/2021,https://www.ijcai.org/proceedings/2021/0440.pdf,,,,,,,"@article{wulearning,
  ABSTRACT = {Different from popular neural networks using quasiconvex activations, non-monotonic networks activated by periodic nonlinearities have emerged as a more competitive paradigm, offering revolutionary benefits: 1) compactly characterizing highfrequency patterns; 2) precisely representing highorder derivatives. Nevertheless, they are also wellknown for being hard to train, due to easily overfitting dissonant noise and only allowing for tiny architectures (shallower than 5 layers). The fundamental bottleneck is that the},
  AUTHOR = {Wu, Zheng-Fan and Xue, Hui and Bai, Weimin},
  PUB_YEAR = {NA},
  TITLE = {Learning Deeper Non-Monotonic Networks by Softly Transferring Solution Space},
  VENUE = {NA}
 }",Fundamentals,,Other,,,,,,,,,IJCAI 2021,,,,"Zheng-Fan Wu, Hui Xue, Weimin Bai",wu2021learning,00000166,"Different from popular neural networks using quasiconvex activations, non-monotonic networks activated by periodic nonlinearities have emerged as a more competitive paradigm, offering revolutionary benefits: 1) compactly characterizing highfrequency patterns; 2) precisely representing highorder derivatives. Nevertheless, they are also wellknown for being hard to train, due to easily overfitting dissonant noise and only allowing for tiny architectures (shallower than 5 layers). The fundamental bottleneck is that the periodicity leads to many poor and dense local minima in solution space. The direction and norm of gradient oscillate continually during error backpropagation. Thus non-monotonic networks are prematurely stuck in these local minima, and leave out effective error feedback. To alleviate the optimization dilemma, in this paper, we propose a non-trivial soft transfer approach. It smooths their solution space close to that of monotonic ones in the beginning, and then improve their representational properties by transferring the solutions from the neural space of monotonic neurons to the Fourier space of nonmonotonic neurons as the training continues. The soft transfer consists of two core components: 1) a rectified concrete gate is constructed to characterize the state of each neuron; 2) a variational Bayesian learning framework is proposed to dynamically balance the empirical risk and the intensity of transfer. We provide comprehensive empirical evidence showing that the soft transfer not only reduces the risk of non-monotonic networks on over-fitting noise, but also helps them scale to much deeper architectures (more than 100 layers) achieving the new state-of-the-art performance.",,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnudzf5QiTMMln7ouKyhqKNeYksT10dq3Z5z57GtwEVHb7b7sIYeU_zeV-7wvGQNEIOc
8/29/2021 16:15:58,Implicit Neural Representations for Deconvolving SAS Images,,8/23/2021,https://web.asu.edu/sites/default/files/imaging-lyceum/files/2021151090.pdf,,,,,,,"@article{reed2021implicit,
  ABSTRACT = {Synthetic aperture sonar (SAS) image resolution is constrained by waveform bandwidth and array geometry. Specifically, the waveform bandwidth determines a point spread function (PSF) that blurs the locations of point scatterers in the scene. In theory, deconvolving the reconstructed SAS image with the scene PSF restores the original distribution of scatterers and yields sharper reconstructions. However, deconvolution is an ill-posed operation that is highly sensitive to noise. In this work, we leverage implicit neural representations (INRs)},
  AUTHOR = {Reed, Albert and Blanford, Thomas and Brown, Daniel C and Jayasuriya, Suren},
  PUB_YEAR = {NA},
  TITLE = {Implicit Neural Representations for Deconvolving SAS Images},
  VENUE = {NA}
 }","Beyond graphics, Science and engineering, Alternative imaging",,,,,,,,,,,,,,Direct,"Albert Reed, Thomas Blanford, Daniel C Brown, Suren Jayasuriya",reed2021implicit,00000162,"Synthetic aperture sonar (SAS) image resolution is constrained by waveform bandwidth and array geometry. Specifically, the waveform bandwidth determines a point spread function (PSF) that blurs the locations of point scatterers in the scene. In theory, deconvolving the reconstructed SAS image with the scene PSF restores the original distribution of scatterers and yields sharper reconstructions. However, deconvolution is an ill-posed operation that is highly sensitive to noise. In this work, we leverage implicit neural representations (INRs), shown to be strong priors for the natural image space, to deconvolve SAS images. Importantly, our method does not require training data, as we perform our deconvolution through an analysis-bysynthesis optimization in a self-supervised fashion. We validate our method on simulated SAS data created with a point scattering model and real data captured with an in-air circular SAS. This work is an important first step towards applying neural networks for SAS image deconvolution.",,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnucT5ZJ2L152EUy6DWBVffWDDsEu6hcudMlTNUqnBFTFut9anoxBW6mMFIi9glyaf8k
8/29/2021 16:40:15,Learning Signed Distance Field for Multi-view Surface Reconstruction,,8/23/2021,https://arxiv.org/pdf/2108.09964.pdf,,,,,,,"@article{zhang2021learning,
  AUTHOR = {Jingyang Zhang and Yao Yao and Long Quan},
  TITLE = {Learning Signed Distance Field for Multi-view Surface Reconstruction},
  EPRINT = {2108.09964v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Recent works on implicit neural representations have shown promising resultsfor multi-view surface reconstruction. However, most approaches are limited torelatively simple geometries and usually require clean object masks forreconstructing complex and concave objects. In this work, we introduce a novelneural surface reconstruction framework that leverages the knowledge of stereomatching and feature consistency to optimize the implicit surfacerepresentation. More specifically, we apply a signed distance field (SDF) and asurface light field to represent the scene geometry and appearancerespectively. The SDF is directly supervised by geometry from stereo matching,and is refined by optimizing the multi-view feature consistency and thefidelity of rendered images. Our method is able to improve the robustness ofgeometry estimation and support reconstruction of complex scene topologies.Extensive experiments have been conducted on DTU, EPFL and Tanks and Templesdatasets. Compared to previous state-of-the-art methods, our method achievesbetter mesh reconstruction in wide open scenes without masks as input.},
  YEAR = {2021},
  MONTH = {Aug},
  URL = {http://arxiv.org/abs/2108.09964v1},
  FILE = {2108.09964v1.pdf}
  }",,"Data-driven component (pre-trained, cross-scene)",,SDF,,,,,,,,ICCV 2021,,No,Direct,"Jingyang Zhang, Yao Yao, Long Quan",zhang2021learning,00000168,"Recent works on implicit neural representations have shown promising results for multi-view surface reconstruction. However, most approaches are limited to relatively simple geometries and usually require clean object masks for reconstructing complex and concave objects. In this work, we introduce a novel neural surface reconstruction framework that leverages the knowledge of stereo matching and feature consistency to optimize the implicit surface representation. More specifically, we apply a signed distance field (SDF) and a surface light field to represent the scene geometry and appearance respectively. The SDF is directly supervised by geometry from stereo matching, and is refined by optimizing the multi-view feature consistency and the fidelity of rendered images. Our method is able to improve the robustness of geometry estimation and support reconstruction of complex scene topologies. Extensive experiments have been conducted on DTU, EPFL and Tanks and Temples datasets. Compared to previous state-of-the-art methods, our method achieves better mesh reconstruction in wide open scenes without masks as input.",,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnud7NQ3u8yr_E5pERXBkMJG3fu5hbjC-G8OJSlCmpYEkSSgaAknDW8s5MBalWQnriQA
8/30/2021 17:53:56,NeRP: Implicit Neural Representation Learning with Prior Embedding for Sparsely Sampled Image Reconstruction,NeRP,8/24/2021,https://arxiv.org/ftp/arxiv/papers/2108/2108.10991.pdf,,,,,,,"@article{shen2021nerp,
  AUTHOR = {Liyue Shen and John Pauly and Lei Xing},
  TITLE = {NeRP: Implicit Neural Representation Learning with Prior Embedding for Sparsely Sampled Image Reconstruction},
  EPRINT = {2108.10991v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {eess.IV},
  ABSTRACT = {Image reconstruction is an inverse problem that solves for a computationalimage based on sampled sensor measurement. Sparsely sampled imagereconstruction poses addition challenges due to limited measurements. In thiswork, we propose an implicit Neural Representation learning methodology withPrior embedding (NeRP) to reconstruct a computational image from sparselysampled measurements. The method differs fundamentally from previous deeplearning-based image reconstruction approaches in that NeRP exploits theinternal information in an image prior, and the physics of the sparsely sampledmeasurements to produce a representation of the unknown subject. No large-scaledata is required to train the NeRP except for a prior image and sparselysampled measurements. In addition, we demonstrate that NeRP is a generalmethodology that generalizes to different imaging modalities such as CT andMRI. We also show that NeRP can robustly capture the subtle yet significantimage changes required for assessing tumor progression.},
  YEAR = {2021},
  MONTH = {Aug},
  URL = {http://arxiv.org/abs/2108.10991v1},
  FILE = {2108.10991v1.pdf}
  }","Generalization, Image, Alternative imaging, Science and engineering","Per-instance fine-tuning, Data-driven component (pre-trained, cross-scene)",,,Category-level,,,,,,,,,,Direct,"Liyue Shen, John Pauly, Lei Xing",shen2021nerp,00000191,"Image reconstruction is an inverse problem that solves for a computational image based on sampled sensor measurement. Sparsely sampled image reconstruction poses addition challenges due to limited measurements. In this work, we propose an implicit Neural Representation learning methodology with Prior embedding (NeRP) to reconstruct a computational image from sparsely sampled measurements. The method differs fundamentally from previous deep learning-based image reconstruction approaches in that NeRP exploits the internal information in an image prior, and the physics of the sparsely sampled measurements to produce a representation of the unknown subject. No large-scale data is required to train the NeRP except for a prior image and sparsely sampled measurements. In addition, we demonstrate that NeRP is a general methodology that generalizes to different imaging modalities such as CT and MRI. We also show that NeRP can robustly capture the subtle yet significant image changes required for assessing tumor progression.",,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnucUZFMmfu2bppryAI-AkEnvR_KGcgljUFwipJwj9-xW-gD3E2V3ztmGQ_CHFKpmvfs
9/17/2021 11:34:22,imGHUM: Implicit Generative Models of 3D Human Shape and Articulated Pose,imGHUM,8/24/2021,https://arxiv.org/pdf/2108.10842.pdf,https://research.google/pubs/pub50642/,,,,,,"@article{alldieck2021imghum,
  AUTHOR = {Thiemo Alldieck and Hongyi Xu and Cristian Sminchisescu},
  TITLE = {imGHUM: Implicit Generative Models of 3D Human Shape and ArticulatedPose},
  EPRINT = {2108.10842v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We present imGHUM, the first holistic generative model of 3D human shape andarticulated pose, represented as a signed distance function. In contrast toprior work, we model the full human body implicitly as a functionzero-level-set and without the use of an explicit template mesh. We propose anovel network architecture and a learning paradigm, which make it possible tolearn a detailed implicit generative model of human pose, shape, and semantics,on par with state-of-the-art mesh-based models. Our model features desireddetail for human models, such as articulated pose including hand motion andfacial expressions, a broad spectrum of shape variations, and can be queried atarbitrary resolutions and spatial locations. Additionally, our model hasattached spatial semantics making it straightforward to establishcorrespondences between different shape instances, thus enabling applicationsthat are difficult to tackle using classical implicit representations. Inextensive experiments, we demonstrate the model accuracy and its applicabilityto current research problems.},
  YEAR = {2021},
  MONTH = {Aug},
  URL = {http://arxiv.org/abs/2108.10842v1},
  FILE = {2108.10842v1.pdf}
 }",Human body,"Conditional neural field, Volume partitioning",,SDF,,,,,,,,CVPR 2021,,No,Direct,"Thiemo Alldieck, Hongyi Xu, Cristian Sminchisescu",alldieck2021imghum,00000194,"We present imGHUM, the first holistic generative model of 3D human shape and articulated pose, represented as a signed distance function. In contrast to prior work, we model the full human body implicitly as a function zero-level-set and without the use of an explicit template mesh. We propose a novel network architecture and a learning paradigm, which make it possible to learn a detailed implicit generative model of human pose, shape, and semantics, on par with state-of-the-art mesh-based models. Our model features desired detail for human models, such as articulated pose including hand motion and facial expressions, a broad spectrum of shape variations, and can be queried at arbitrary resolutions and spatial locations. Additionally, our model has attached spatial semantics making it straightforward to establish correspondences between different shape instances, thus enabling applications that are difficult to tackle using classical implicit representations. In extensive experiments, we demonstrate the model accuracy and its applicability to current research problems.",,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnucZoo_zkhaeRTrMV19ilTBO6src3Yu6R1wmj8PkABf5YsB2hGGUyevGHWCOjGqgNb8
8/30/2021 18:01:26,Self-Calibrating Neural Radiance Fields,,8/30/2021,http://jaesik.info/publications/data/21_iccv1.pdf,,https://github.com/POSTECH-CVLab/SCNeRF,,,,,"@article{jeong2021self,
  ABSTRACT = {In this work, we propose a camera self-calibration algorithm for generic cameras with arbitrary non-linear distortions. We jointly learn the geometry of the scene and the accurate camera parameters without any calibration objects. Our camera model consists of a pinhole model, a fourth order radial distortion, and a generic noise model that can learn arbitrary non-linear camera distortions. While traditional self-calibration algorithms mostly rely on geometric constraints, we additionally incorporate photometric consistency. This requires},
  AUTHOR = {Jeong, Yoonwoo and Ahn, Seokjun and Choy, Christopher and Anandkumar, Animashree and Cho, Minsu and Park, Jaesik},
  JOURNAL = {arXiv preprint arXiv:2108.13826},
  PUB_YEAR = {2021},
  TITLE = {Self-Calibrating Neural Radiance Fields},
  VENUE = {arXiv preprint arXiv …}
 }",Camera parameter estimation,Coarse-to-fine,NeRF,Density,Per-scene,,,,,,,ICCV 2021,https://github.com/POSTECH-CVLab/SCNeRF,No,Direct,"Yoonwoo Jeong, Seokjun Ahn, Christopher Choy, Animashree Anandkumar, Minsu Cho, Jaesik Park",jeong2021self,00000192,"In this work, we propose a camera self-calibration algorithm for generic cameras with arbitrary non-linear distortions. We jointly learn the geometry of the scene and the accurate camera parameters without any calibration objects. Our camera model consists of a pinhole model, a fourth order radial distortion, and a generic noise model that can learn arbitrary non-linear camera distortions. While traditional self-calibration algorithms mostly rely on geometric constraints, we additionally incorporate photometric consistency. This requires learning the geometry of the scene, and we use Neural Radiance Fields (NeRF). We also propose a new geometric loss function, viz., projected ray distance loss, to incorporate geometric consistency for complex non-linear camera models. We validate our approach on standard real image datasets and demonstrate that our model can learn the camera intrinsics and extrinsics (pose) from scratch without COLMAP initialization. Also, we show that learning accurate camera models in a differentiable manner allows us to improve PSNR over baselines. Our module is an easy-to-use plugin that can be applied to NeRF variants to improve performance. The code and data are currently available at https://github.com/POSTECHCVLab/SCNeRF",,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnudu8-3apyntMu4IdCS8QZ7kTZGHsEETz-wNbhdGdYt68FXwzvBua-YaWK1arweWzXU
9/18/2021 9:42:49,A modified physics-informed neural network with positional encoding,,9/1/2021,https://library.seg.org/doi/pdf/10.1190/segam2021-3584127.1,,,,,,,"@inproceedings{huang2021modified,
  ABSTRACT = {Recently developed physics-informed neural network (PINN) for solving for the scattered wavefield in the Helmholtz equation showed large potential in seismic modeling because of its flexibility, low memory requirement, and no limitations on the shape of the solution space. However, the predicted solutions were somewhat smooth and the convergence of the training was slow. Thus, we propose a modified PINN using sinusoidal activation functions and positional encoding, aiming to accelerate the convergence and fit better. We transform},
  AUTHOR = {Huang, Xinquan and Alkhalifah, Tariq and Song, Chao},
  BOOKTITLE = {First International Meeting for Applied Geoscience \& Energy},
  ORGANIZATION = {Society of Exploration Geophysicists},
  PAGES = {2480--2484},
  PUB_YEAR = {2021},
  TITLE = {A modified physics-informed neural network with positional encoding},
  VENUE = {First International Meeting for ...}
 }","Beyond graphics, Fundamentals, Science and engineering, PDE",,NeRF,,,,,,,,,,,,Direct,"Xinquan Huang, Tariq Alkhalifah, Chao Song",huang2021modified,00000220,"Recently developed physics-informed neural network (PINN) for solving for the scattered wavefield in the Helmholtz equation showed large potential in seismic modeling because of its flexibility, low memory requirement, and no limitations on the shape of the solution space. However, the predicted solutions were somewhat smooth and the convergence of the training was slow. Thus, we propose a modified PINN using sinusoidal activation functions and positional encoding, aiming to accelerate the convergence and fit better. We transform",,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnueXrGSBBSg2Q9j7bzwcVEIS60_gKQRrmo25MPkzwZ0uomWEl8hCfWwG2T9kD87LUoY
9/18/2021 10:00:17,Seeing Implicit Neural Representations as Fourier Series,,9/1/2021,https://arxiv.org/pdf/2109.00249.pdf,,,,,,,"@article{benbarka2021seeing,
  AUTHOR = {Nuri Benbarka and Timon Hofer and Hamd ul-moqeet Riaz and Andreas Zell},
  TITLE = {Seeing Implicit Neural Representations as Fourier Series},
  EPRINT = {2109.00249v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Implicit Neural Representations (INR) use multilayer perceptrons to representhigh-frequency functions in low-dimensional problem domains. Recently theserepresentations achieved state-of-the-art results on tasks related to complex3D objects and scenes. A core problem is the representation of highly detailedsignals, which is tackled using networks with periodic activation functions(SIRENs) or applying Fourier mappings to the input. This work analyzes theconnection between the two methods and shows that a Fourier mapped perceptronis structurally like one hidden layer SIREN. Furthermore, we identify therelationship between the previously proposed Fourier mapping and the generald-dimensional Fourier series, leading to an integer lattice mapping. Moreover,we modify a progressive training strategy to work on arbitrary Fourier mappingsand show that it improves the generalization of the interpolation task. Lastly,we compare the different mappings on the image regression and novel viewsynthesis tasks. We confirm the previous finding that the main contributor tothe mapping performance is the size of the embedding and standard deviation ofits elements.},
  YEAR = {2021},
  MONTH = {Sep},
  URL = {http://arxiv.org/abs/2109.00249v1},
  FILE = {2109.00249v1.pdf}
 }","Image, Beyond graphics, Fundamentals",Coarse-to-fine,Other,,,,,,,,,,,,,"Nuri Benbarka, Timon Höfer, Hamd ul-moqeet Riaz, Andreas Zell",benbarka2021seeing,00000221,"Implicit Neural Representations (INR) use multilayer perceptrons to represent high-frequency functions in low-dimensional problem domains. Recently these representations achieved state-of-the-art results on tasks related to complex 3D objects and scenes. A core problem is the representation of highly detailed signals, which is tackled using networks with periodic activation functions (SIRENs) or applying Fourier mappings to the input. This work analyzes the connection between the two methods and shows that a Fourier mapped perceptron is structurally like one hidden layer SIREN. Furthermore, we identify the relationship between the previously proposed Fourier mapping and the general d-dimensional Fourier series, leading to an integer lattice mapping. Moreover, we modify a progressive training strategy to work on arbitrary Fourier mappings and show that it improves the generalization of the interpolation task. Lastly, we compare the different mappings on the image regression and novel view synthesis tasks. We confirm the previous finding that the main contributor to the mapping performance is the size of the embedding and standard deviation of its elements.",,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnuc547Fhb1qDXYatJXvyHp4hd0ZNJfez-UE3GEmVCVqJdZ7VJgWtDeU46gpJZUNhyR8
9/18/2021 10:13:34,Common Objects in 3D: Large-Scale Learning and Evaluation of Real-life 3D Category Reconstruction,CO3D,9/1/2021,https://arxiv.org/pdf/2109.00512.pdf,,https://github.com/facebookresearch/co3d,https://www.youtube.com/watch?v=hMx9nzG50xQ,,,,"@article{reizenstein2021co3d,
  AUTHOR = {Jeremy Reizenstein and Roman Shapovalov and Philipp Henzler and Luca Sbordone and Patrick Labatut and David Novotny},
  TITLE = {Common Objects in 3D: Large-Scale Learning and Evaluation of Real-life3D Category Reconstruction},
  EPRINT = {2109.00512v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Traditional approaches for learning 3D object categories have beenpredominantly trained and evaluated on synthetic datasets due to theunavailability of real 3D-annotated category-centric data. Our main goal is tofacilitate advances in this field by collecting real-world data in a magnitudesimilar to the existing synthetic counterparts. The principal contribution ofthis work is thus a large-scale dataset, called Common Objects in 3D, with realmulti-view images of object categories annotated with camera poses and groundtruth 3D point clouds. The dataset contains a total of 1.5 million frames fromnearly 19,000 videos capturing objects from 50 MS-COCO categories and, as such,it is significantly larger than alternatives both in terms of the number ofcategories and objects. We exploit this new dataset to conduct one of the firstlarge-scale ""in-the-wild"" evaluations of several new-view-synthesis andcategory-centric 3D reconstruction methods. Finally, we contribute NerFormer -a novel neural rendering method that leverages the powerful Transformer toreconstruct an object given a small number of its views. The CO3D dataset isavailable at https://github.com/facebookresearch/co3d .},
  YEAR = {2021},
  MONTH = {Sep},
  NOTE = {International Conference on Computer Vision, 2021},
  URL = {http://arxiv.org/abs/2109.00512v1},
  FILE = {2109.00512v1.pdf}
 }","Few-shot reconstruction, Generalization","Conditional neural field, Data-driven component (pre-trained, cross-scene), Transformer",,Density,,,,,,,,CVPR 2021,https://ai.facebook.com/datasets/co3d-downloads/,No,Direct,"Jeremy Reizenstein, Roman Shapovalov, Philipp Henzler, Luca Sbordone, Patrick Labatut, David Novotny",reizenstein2021co3d,00000223,"Traditional approaches for learning 3D object categories have been predominantly trained and evaluated on synthetic datasets due to the unavailability of real 3D-annotated category-centric data. Our main goal is to facilitate advances in this field by collecting real-world data in a magnitude similar to the existing synthetic counterparts. The principal contribution of this work is thus a large-scale dataset, called Common Objects in 3D, with real multi-view images of object categories annotated with camera poses and ground truth 3D point clouds. The dataset contains a total of 1.5 million frames from nearly 19,000 videos capturing objects from 50 MS-COCO categories and, as such, it is significantly larger than alternatives both in terms of the number of categories and objects. We exploit this new dataset to conduct one of the first large-scale ""in-the-wild"" evaluations of several new-view-synthesis and category-centric 3D reconstruction methods. Finally, we contribute NerFormer - a novel neural rendering method that leverages the powerful Transformer to reconstruct an object given a small number of its views. The CO3D dataset is available at https://github.com/facebookresearch/co3d .",,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnufkbbJ8jAIcaKRywTg6DcWdp7zDAmSs67GEBVQik8_QgjvvodCT-YaUExEPOYnUy3s
9/18/2021 8:55:15,NerfingMVS: Guided Optimization of Neural Radiance Fields for Indoor Multi-view Stereo,NerfingMVS,9/2/2021,https://arxiv.org/pdf/2109.01129.pdf,https://weiyithu.github.io/NerfingMVS/,https://github.com/weiyithu/NerfingMVS,https://www.youtube.com/watch?v=i-b5lPnYipA,,,,"@article{wei2021nerfingmvs,
  AUTHOR = {Yi Wei and Shaohui Liu and Yongming Rao and Wang Zhao and Jiwen Lu and Jie Zhou},
  TITLE = {NerfingMVS: Guided Optimization of Neural Radiance Fields for IndoorMulti-view Stereo},
  EPRINT = {2109.01129v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {In this work, we present a new multi-view depth estimation method thatutilizes both conventional SfM reconstruction and learning-based priors overthe recently proposed neural radiance fields (NeRF). Unlike existing neuralnetwork based optimization method that relies on estimated correspondences, ourmethod directly optimizes over implicit volumes, eliminating the challengingstep of matching pixels in indoor scenes. The key to our approach is to utilizethe learning-based priors to guide the optimization process of NeRF. Our systemfirstly adapts a monocular depth network over the target scene by finetuning onits sparse SfM reconstruction. Then, we show that the shape-radiance ambiguityof NeRF still exists in indoor environments and propose to address the issue byemploying the adapted depth priors to monitor the sampling process of volumerendering. Finally, a per-pixel confidence map acquired by error computation onthe rendered image can be used to further improve the depth quality.Experiments show that our proposed framework significantly outperformsstate-of-the-art methods on indoor scenes, with surprising findings presentedon the effectiveness of correspondence-based optimization and NeRF-basedoptimization over the adapted depth priors. In addition, we show that theguided optimization scheme does not sacrifice the original synthesis capabilityof neural radiance fields, improving the rendering quality on both seen andnovel views. Code is available at https://github.com/weiyithu/NerfingMVS.},
  YEAR = {2021},
  MONTH = {Sep},
  URL = {http://arxiv.org/abs/2109.01129v2},
  FILE = {2109.01129v2.pdf}
 }",,"Lifting 2D features to 3D, Sampling, Data-driven component (pre-trained, cross-scene)",NeRF,Density,,,,,,,,ICCV 2021 Oral,https://drive.google.com/drive/folders/1X_w57Q_MIFlI3lzhRt7Z8C5X9tNS8cg-,No,Direct,"Yi Wei, Shaohui Liu, Yongming Rao, Wang Zhao, Jiwen Lu, Jie Zhou",wei2021nerfingmvs,00000218,"In this work, we present a new multi-view depth estimation method that utilizes both conventional SfM reconstruction and learning-based priors over the recently proposed neural radiance fields (NeRF). Unlike existing neural network based optimization method that relies on estimated correspondences, our method directly optimizes over implicit volumes, eliminating the challenging step of matching pixels in indoor scenes. The key to our approach is to utilize the learning-based priors to guide the optimization process of NeRF. Our system firstly adapts a monocular depth network over the target scene by finetuning on its sparse SfM reconstruction. Then, we show that the shape-radiance ambiguity of NeRF still exists in indoor environments and propose to address the issue by employing the adapted depth priors to monitor the sampling process of volume rendering. Finally, a per-pixel confidence map acquired by error computation on the rendered image can be used to further improve the depth quality. Experiments show that our proposed framework significantly outperforms state-of-the-art methods on indoor scenes, with surprising findings presented on the effectiveness of correspondence-based optimization and NeRF-based optimization over the adapted depth priors. In addition, we show that the guided optimization scheme does not sacrifice the original synthesis capability of neural radiance fields, improving the rendering quality on both seen and novel views. Code is available at https://github.com/weiyithu/NerfingMVS.",,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnuee3diLaZ747OhpmsjBJOTLznSphf_bFmIeCcsN2z5VsBQ3BqpLRT-G-9a4MCkvdp0
9/18/2021 10:03:21,IntraTomo: Self-supervised Learning-based Tomography via Sinogram Synthesis and Prediction,IntraTomo,9/2/2021,https://vccimaging.org/Publications/Zang2021IntraTomo/Zang2021IntraTomo.pdf,https://vccimaging.org/Publications/Zang2021IntraTomo/,https://github.com/gmzang/IntraTomo,,https://vccimaging.org/Publications/Zang2021IntraTomo/Zang2021IntraTomo-supp.pdf,,,"@article{zang2021intratomo,
  ABSTRACT = {We propose IntraTomo, a powerful framework that combines the benefits of learning-based and model-based approaches for solving highly ill-posed inverse problems, in the Computed Tomography (CT) context. IntraTomo is composed of two core modules: a novel sinogram prediction module and a geometry refinement module, which are applied iteratively. In the first module, the unknown density field is represented as a continuous and differentiable function, parameterized by a deep neural network. This network is learned, in},
  AUTHOR = {Zang, Guangming and Idoughi, Ramzi and Li, Rui and Wonka, Peter and Heidrich, Wolfgang},
  PUB_YEAR = {2021},
  PUBLISHER = {IEEE},
  TITLE = {IntraTomo: Self-supervised Learning-based Tomography via Sinogram Synthesis and Prediction},
  VENUE = {NA}
 }","Beyond graphics, Alternative imaging, Science and engineering",,NeRF,,,,,,,,,ICCV 2021,https://github.com/gmzang/IntraTomo,,Direct,"Guangming Zang, Ramzi Idoughi, Rui Li, Peter Wonka, Wolfgang Heidrich",zang2021intratomo,00000222,"We propose IntraTomo, a powerful framework that combines the benefits of learning-based and model-based approaches for solving highly ill-posed inverse problems, in the Computed Tomography (CT) context. IntraTomo is composed of two core modules: a novel sinogram prediction module and a geometry refinement module, which are applied iteratively. In the first module, the unknown density field is represented as a continuous and differentiable function, parameterized by a deep neural network. This network is learned, in",,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnueuWp61dpYUOM_4iY7WqfLVfTzYYnFAzYwD6bW45HQd5vaYxlIdiL6p7W84DP3DfPE
9/17/2021 22:57:44,CodeNeRF: Disentangled Neural Radiance Fields for Object Categories,CodeNeRF,9/3/2021,https://arxiv.org/pdf/2109.01750.pdf,https://sites.google.com/view/wbjang/home/codenerf,https://github.com/wayne1123/code-nerf,https://user-images.githubusercontent.com/32883157/130004248-0ff74d4e-993e-43f2-91ee-bd25776e65bc.mp4,,,,"@article{jang2021codenerf,
  AUTHOR = {Wonbong Jang and Lourdes Agapito},
  TITLE = {CodeNeRF: Disentangled Neural Radiance Fields for Object Categories},
  EPRINT = {2109.01750v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.GR},
  ABSTRACT = {CodeNeRF is an implicit 3D neural representation that learns the variation ofobject shapes and textures across a category and can be trained, from a set ofposed images, to synthesize novel views of unseen objects. Unlike the originalNeRF, which is scene specific, CodeNeRF learns to disentangle shape and textureby learning separate embeddings. At test time, given a single unposed image ofan unseen object, CodeNeRF jointly estimates camera viewpoint, and shape andappearance codes via optimization. Unseen objects can be reconstructed from asingle image, and then rendered from new viewpoints or their shape and textureedited by varying the latent codes. We conduct experiments on the SRNbenchmark, which show that CodeNeRF generalises well to unseen objects andachieves on-par performance with methods that require known camera pose at testtime. Our results on real-world images demonstrate that CodeNeRF can bridge thesim-to-real gap. Project page: \url{https://github.com/wayne1123/code-nerf}},
  YEAR = {2021},
  MONTH = {Sep},
  URL = {http://arxiv.org/abs/2109.01750v1},
  FILE = {2109.01750v1.pdf}
 }","Generalization, Editable","Conditional neural field, Per-instance fine-tuning, Data-driven component (pre-trained, cross-scene)",NeRF,Density,Category-level,,,,,,,ICCV 2021,,No,Direct,"Wonbong Jang, Lourdes Agapito",jang2021codenerf,00000214,"CodeNeRF is an implicit 3D neural representation that learns the variation of object shapes and textures across a category and can be trained, from a set of posed images, to synthesize novel views of unseen objects. Unlike the original NeRF, which is scene specific, CodeNeRF learns to disentangle shape and texture by learning separate embeddings. At test time, given a single unposed image of an unseen object, CodeNeRF jointly estimates camera viewpoint, and shape and appearance codes via optimization. Unseen objects can be reconstructed from a single image, and then rendered from new viewpoints or their shape and texture edited by varying the latent codes. We conduct experiments on the SRN benchmark, which show that CodeNeRF generalises well to unseen objects and achieves on-par performance with methods that require known camera pose at test time. Our results on real-world images demonstrate that CodeNeRF can bridge the sim-to-real gap. Project page: \url{https://github.com/wayne1123/code-nerf}",,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnueJ4x3F4fHychMCKn5hQk3ycOjLMx1wOSbB8mTNVUZx0ZaYqjWaxbqOgACqV_dj8vA
9/17/2021 21:31:17,Learning Object-Compositional Neural Radiance Field for Editable Scene Rendering,,9/4/2021,https://arxiv.org/pdf/2109.01847.pdf,https://zju3dv.github.io/object_nerf/,https://github.com/zju3dv/object_nerf,https://www.youtube.com/watch?v=VTEROu-Yz04,http://www.cad.zju.edu.cn/home/gfzhang/papers/object_nerf/object_nerf_supp.pdf,,,"@article{yang2021learning,
  AUTHOR = {Bangbang Yang and Yinda Zhang and Yinghao Xu and Yijin Li and Han Zhou and Hujun Bao and Guofeng Zhang and Zhaopeng Cui},
  TITLE = {Learning Object-Compositional Neural Radiance Field for Editable SceneRendering},
  EPRINT = {2109.01847v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Implicit neural rendering techniques have shown promising results for novelview synthesis. However, existing methods usually encode the entire scene as awhole, which is generally not aware of the object identity and limits theability to the high-level editing tasks such as moving or adding furniture. Inthis paper, we present a novel neural scene rendering system, which learns anobject-compositional neural radiance field and produces realistic renderingwith editing capability for a clustered and real-world scene. Specifically, wedesign a novel two-pathway architecture, in which the scene branch encodes thescene geometry and appearance, and the object branch encodes each standaloneobject conditioned on learnable object activation codes. To survive thetraining in heavily cluttered scenes, we propose a scene-guided trainingstrategy to solve the 3D space ambiguity in the occluded regions and learnsharp boundaries for each object. Extensive experiments demonstrate that oursystem not only achieves competitive performance for static scene novel-viewsynthesis, but also produces realistic rendering for object-level editing.},
  YEAR = {2021},
  MONTH = {Sep},
  URL = {http://arxiv.org/abs/2109.01847v1},
  FILE = {2109.01847v1.pdf}
 }",Editable,"Conditional neural field, Voxelization, Volume partitioning, Object-centric representation, Segmentation",NeRF,Density,,,,,,,,ICCV 2021,,No,Direct,"Bangbang Yang, Yinda Zhang, Yinghao Xu, Yijin Li, Han Zhou, Hujun Bao, Guofeng Zhang, Zhaopeng Cui",yang2021learning,00000212,"Implicit neural rendering techniques have shown promising results for novel view synthesis. However, existing methods usually encode the entire scene as a whole, which is generally not aware of the object identity and limits the ability to the high-level editing tasks such as moving or adding furniture. In this paper, we present a novel neural scene rendering system, which learns an object-compositional neural radiance field and produces realistic rendering with editing capability for a clustered and real-world scene. Specifically, we design a novel two-pathway architecture, in which the scene branch encodes the scene geometry and appearance, and the object branch encodes each standalone object conditioned on learnable object activation codes. To survive the training in heavily cluttered scenes, we propose a scene-guided training strategy to solve the 3D space ambiguity in the occluded regions and learn sharp boundaries for each object. Extensive experiments demonstrate that our system not only achieves competitive performance for static scene novel-view synthesis, but also produces realistic rendering for object-level editing.",,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnudtZ0By4WU9QOx2hDFVJC7jg4FTcOXqkBFO2WWieWjT2YW-IgYoNA2k54FUkp5yg9Y
9/17/2021 17:49:15,Stochastic Neural Radiance Fields: Quantifying Uncertainty in Implicit 3D Representations,S-NeRF,9/5/2021,https://arxiv.org/pdf/2109.02123.pdf,,,,,,,"@article{shen2021snerf,
  AUTHOR = {Jianxiong Shen and Adria Ruiz and Antonio Agudo and Francesc Moreno},
  TITLE = {Stochastic Neural Radiance Fields:Quantifying Uncertainty in Implicit 3DRepresentations},
  EPRINT = {2109.02123v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Neural Radiance Fields (NeRF) has become a popular framework for learningimplicit 3D representations and addressing different tasks such as novel-viewsynthesis or depth-map estimation. However, in downstream applications wheredecisions need to be made based on automatic predictions, it is critical toleverage the confidence associated with the model estimations. Whereasuncertainty quantification is a long-standing problem in Machine Learning, ithas been largely overlooked in the recent NeRF literature. In this context, wepropose Stochastic Neural Radiance Fields (S-NeRF), a generalization ofstandard NeRF that learns a probability distribution over all the possibleradiance fields modeling the scene. This distribution allows to quantify theuncertainty associated with the scene information provided by the model. S-NeRFoptimization is posed as a Bayesian learning problem which is efficientlyaddressed using the Variational Inference framework. Exhaustive experimentsover benchmark datasets demonstrate that S-NeRF is able to provide morereliable predictions and confidence values than generic approaches previouslyproposed for uncertainty estimation in other domains.},
  YEAR = {2021},
  MONTH = {Sep},
  URL = {http://arxiv.org/abs/2109.02123v1},
  FILE = {2109.02123v1.pdf}
 }",Fundamentals,,NeRF,Density,Per-scene,,,,,,,,,No,Direct,"Jianxiong Shen, Adria Ruiz, Antonio Agudo, Francesc Moreno",shen2021snerf,00000209,"Neural Radiance Fields (NeRF) has become a popular framework for learning implicit 3D representations and addressing different tasks such as novel-view synthesis or depth-map estimation. However, in downstream applications where decisions need to be made based on automatic predictions, it is critical to leverage the confidence associated with the model estimations. Whereas uncertainty quantification is a long-standing problem in Machine Learning, it has been largely overlooked in the recent NeRF literature. In this context, we propose Stochastic Neural Radiance Fields (S-NeRF), a generalization of standard NeRF that learns a probability distribution over all the possible radiance fields modeling the scene. This distribution allows to quantify the uncertainty associated with the scene information provided by the model. S-NeRF optimization is posed as a Bayesian learning problem which is efficiently addressed using the Variational Inference framework. Exhaustive experiments over benchmark datasets demonstrate that S-NeRF is able to provide more reliable predictions and confidence values than generic approaches previously proposed for uncertainty estimation in other domains.",,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnucZ0vrwUrcA_Pd6srEDC0BcOZLZC9UhZbZv6QdhgyJS48kiVOhf_FbOdscAQt8feF0
9/17/2021 18:10:41,NEAT: Neural Attention Fields for End-to-End Autonomous Driving,NEAT,9/9/2021,https://arxiv.org/pdf/2109.04456.pdf,,https://github.com/autonomousvision/neat,https://www.youtube.com/watch?v=gtO-ghjKkRs,http://www.cvlibs.net/publications/Chitta2021ICCV_supplementary.pdf,,,"@article{chitta2021neat,
  AUTHOR = {Kashyap Chitta and Aditya Prakash and Andreas Geiger},
  TITLE = {NEAT: Neural Attention Fields for End-to-End Autonomous Driving},
  EPRINT = {2109.04456v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Efficient reasoning about the semantic, spatial, and temporal structure of ascene is a crucial prerequisite for autonomous driving. We present NEuralATtention fields (NEAT), a novel representation that enables such reasoning forend-to-end imitation learning models. NEAT is a continuous function which mapslocations in Bird's Eye View (BEV) scene coordinates to waypoints andsemantics, using intermediate attention maps to iteratively compresshigh-dimensional 2D image features into a compact representation. This allowsour model to selectively attend to relevant regions in the input while ignoringinformation irrelevant to the driving task, effectively associating the imageswith the BEV representation. In a new evaluation setting involving adverseenvironmental conditions and challenging scenarios, NEAT outperforms severalstrong baselines and achieves driving scores on par with the privileged CARLAexpert used to generate its training data. Furthermore, visualizing theattention maps for models with NEAT intermediate representations providesimproved interpretability.},
  YEAR = {2021},
  MONTH = {Sep},
  URL = {http://arxiv.org/abs/2109.04456v1},
  FILE = {2109.04456v1.pdf}
 }","Dynamic, Beyond graphics, Science and engineering","Data-driven component (pre-trained, cross-scene)",,,,,,,,,,ICCV 2021,,,Direct,"Kashyap Chitta, Aditya Prakash, Andreas Geiger",chitta2021neat,00000210,"Efficient reasoning about the semantic, spatial, and temporal structure of a scene is a crucial prerequisite for autonomous driving. We present NEural ATtention fields (NEAT), a novel representation that enables such reasoning for end-to-end imitation learning models. NEAT is a continuous function which maps locations in Bird's Eye View (BEV) scene coordinates to waypoints and semantics, using intermediate attention maps to iteratively compress high-dimensional 2D image features into a compact representation. This allows our model to selectively attend to relevant regions in the input while ignoring information irrelevant to the driving task, effectively associating the images with the BEV representation. In a new evaluation setting involving adverse environmental conditions and challenging scenarios, NEAT outperforms several strong baselines and achieves driving scores on par with the privileged CARLA expert used to generate its training data. Furthermore, visualizing the attention maps for models with NEAT intermediate representations provides improved interpretability.",,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnueApOF85N3QRL_z9exTWINC9uAnW2ilZaiKe_U2H8bBzN5yrp77n3GBRA2Fd4tgzSM
9/18/2021 8:37:18,Multiresolution Deep Implicit Functions for 3D Shape Representation,MDIF,9/12/2021,https://arxiv.org/pdf/2109.05591.pdf,,,,,,,"@article{chen2021mdif,
  AUTHOR = {Zhang Chen and Yinda Zhang and Kyle Genova and Sean Fanello and Sofien Bouaziz and Christian Haene and Ruofei Du and Cem Keskin and Thomas Funkhouser and Danhang Tang},
  TITLE = {Multiresolution Deep Implicit Functions for 3D Shape Representation},
  EPRINT = {2109.05591v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We introduce Multiresolution Deep Implicit Functions (MDIF), a hierarchicalrepresentation that can recover fine geometry detail, while being able toperform global operations such as shape completion. Our model represents acomplex 3D shape with a hierarchy of latent grids, which can be decoded intodifferent levels of detail and also achieve better accuracy. For shapecompletion, we propose latent grid dropout to simulate partial data in thelatent space and therefore defer the completing functionality to the decoderside. This along with our multires design significantly improves the shapecompletion quality under decoder-only latent optimization. To the best of ourknowledge, MDIF is the first deep implicit function model that can at the sametime (1) represent different levels of detail and allow progressive decoding;(2) support both encoder-decoder inference and decoder-only latentoptimization, and fulfill multiple applications; (3) perform detaileddecoder-only shape completion. Experiments demonstrate its superior performanceagainst prior art in various 3D reconstruction tasks.},
  YEAR = {2021},
  MONTH = {Sep},
  URL = {http://arxiv.org/abs/2109.05591v2},
  FILE = {2109.05591v2.pdf}
 }",Surface reconstruction,"Coarse-to-fine, Learning residual, Voxelization, Feature volume",,,,,,,,,,ICCV 2021,,Yes,,"Zhang Chen, Yinda Zhang, Kyle Genova, Sean Fanello, Sofien Bouaziz, Christian Haene, Ruofei Du, Cem Keskin, Thomas Funkhouser, Danhang Tang",chen2021mdif,00000215,"We introduce Multiresolution Deep Implicit Functions (MDIF), a hierarchical representation that can recover fine geometry detail, while being able to perform global operations such as shape completion. Our model represents a complex 3D shape with a hierarchy of latent grids, which can be decoded into different levels of detail and also achieve better accuracy. For shape completion, we propose latent grid dropout to simulate partial data in the latent space and therefore defer the completing functionality to the decoder side. This along with our multires design significantly improves the shape completion quality under decoder-only latent optimization. To the best of our knowledge, MDIF is the first deep implicit function model that can at the same time (1) represent different levels of detail and allow progressive decoding; (2) support both encoder-decoder inference and decoder-only latent optimization, and fulfill multiple applications; (3) perform detailed decoder-only shape completion. Experiments demonstrate its superior performance against prior art in various 3D reconstruction tasks.",,Yes,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnudSNXqiA7JXUB_cYa3iQJej9WTQ-5Oa-7atmVmQuCJ1I6512LywVCwuLyCYAn277Dc
9/18/2021 8:42:31,Pose with Style: Detail-Preserving Pose-Guided Image Synthesis with Conditional StyleGAN,Pose with Style,9/13/2021,https://arxiv.org/pdf/2109.06166.pdf,https://pose-with-style.github.io/,Coming soon,,,https://www.youtube.com/watch?v=d_ETeAVLilw,,"@article{albahar2021pose with style,
  AUTHOR = {Badour AlBahar and Jingwan Lu and Jimei Yang and Zhixin Shu and Eli Shechtman and Jia-Bin Huang},
  TITLE = {Pose with Style: Detail-Preserving Pose-Guided Image Synthesis withConditional StyleGAN},
  EPRINT = {2109.06166v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We present an algorithm for re-rendering a person from a single image underarbitrary poses. Existing methods often have difficulties in hallucinatingoccluded contents photo-realistically while preserving the identity and finedetails in the source image. We first learn to inpaint the correspondence fieldbetween the body surface texture and the source image with a human bodysymmetry prior. The inpainted correspondence field allows us to transfer/warplocal features extracted from the source to the target view even under largepose changes. Directly mapping the warped local features to an RGB image usinga simple CNN decoder often leads to visible artifacts. Thus, we extend theStyleGAN generator so that it takes pose as input (for controlling poses) andintroduces a spatially varying modulation for the latent space using the warpedlocal features (for controlling appearances). We show that our method comparesfavorably against the state-of-the-art algorithms in both quantitativeevaluation and visual comparison.},
  YEAR = {2021},
  MONTH = {Sep},
  URL = {http://arxiv.org/abs/2109.06166v1},
  FILE = {2109.06166v1.pdf}
 }","Human body, Image",Generative/adversarial formulation,,,Category-level,,,,,,,SIGGRAPH 2021,,,Indirect,"Badour AlBahar, Jingwan Lu, Jimei Yang, Zhixin Shu, Eli Shechtman, Jia-Bin Huang",albahar2021pose with style,00000216,"We present an algorithm for re-rendering a person from a single image under arbitrary poses. Existing methods often have difficulties in hallucinating occluded contents photo-realistically while preserving the identity and fine details in the source image. We first learn to inpaint the correspondence field between the body surface texture and the source image with a human body symmetry prior. The inpainted correspondence field allows us to transfer/warp local features extracted from the source to the target view even under large pose changes. Directly mapping the warped local features to an RGB image using a simple CNN decoder often leads to visible artifacts. Thus, we extend the StyleGAN generator so that it takes pose as input (for controlling poses) and introduces a spatially varying modulation for the latent space using the warped local features (for controlling appearances). We show that our method compares favorably against the state-of-the-art algorithms in both quantitative evaluation and visual comparison.",,,Yes,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnufKNnfS6KzEtQba-fc7Vxa39u2iLsgvjt0vqpDZE-XH1_xugWTdm47hOrHe8mP9rHY
9/18/2021 8:47:01,Neural Cameras: Learning Camera Characteristics for Coherent Mixed Reality Rendering,Neural Cameras,9/18/2021,https://www.hci.otago.ac.nz/papers/MandlIEEEISMAR2021.pdf,,,,,,,"@article{mandlneural,
  ABSTRACT = {Coherent rendering is important for generating plausible Mixed Reality presentations of virtual objects within a user's real-world environment. Besides photo-realistic rendering and correct lighting, visual coherence requires simulating the imaging system that is used to capture the real environment. While existing approaches either focus on a specific camera or a specific component of the imaging system, we introduce Neural Cameras, the first approach that jointly simulates all major components of an arbitrary modern camera using},
  AUTHOR = {Mandl, David and Mohr, Peter and Langlotz, Tobias and Ebner, Christoph and Mori, Shohei and Zollmann, Stefanie and Roth, Peter M and Kalkofen, Denis},
  PUB_YEAR = {NA},
  TITLE = {Neural Cameras: Learning Camera Characteristics for Coherent Mixed Reality Rendering},
  VENUE = {NA}
 }","Camera parameter estimation, Beyond graphics",,,,,,,,,,,,,,,"David Mandl, Peter Mohr, Tobias Langlotz, Christoph Ebner, Shohei Mori, Stefanie Zollmann, Peter M Roth, Denis Kalkofen",mandlneural,00000217,"Coherent rendering is important for generating plausible Mixed Reality presentations of virtual objects within a user's real-world environment. Besides photo-realistic rendering and correct lighting, visual coherence requires simulating the imaging system that is used to capture the real environment. While existing approaches either focus on a specific camera or a specific component of the imaging system, we introduce Neural Cameras, the first approach that jointly simulates all major components of an arbitrary modern camera using",,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnueW5ORQ_TGinn-7URFGjneV88kI53FkZOl0MURf3K-oDErzpSLFdZmy9njV25Fr8OU