Timestamp,Title,Nickname,Date,PDF,Project Webpage,Code Release,Talk/Video,Supplement (pdf),Supplement (video),Paper summary figure #,Citation,Task,Techniques,Frequency Encoding,Geometry proxy,Generalization,Training time (hr),Rendering time (FPS),Dataset(s) used,# of views,Lighting,Inputs,Venue,Data Release,Geometry only,Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z),Authors,Bibtex Name,UID,Abstract,Citation Count,"Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)",Coordinates all at once,,,
8/29/2021 20:09:57,Approximating Reflectance Functions using Neural Networks,,6/29/1998,https://link.springer.com/chapter/10.1007/978-3-7091-6453-2_3,,,,,,,"@article{gargan1998approximating,
  PUBLISHER = {The Eurographics Association and John Wiley & Sons Ltd.},
  JOURNAL = {Computer Graphics Forum},
  ID = {gargan1998approximating},
  ENTRYTYPE = {article},
  TITLE = {Approximating reflectance functions using neural networks},
  AUTHOR = {David Gargan and Francis Neelamkavil},
  BOOKTITLE = {Eurographics Workshop on Rendering Techniques},
  PAGES = {23--34},
  YEAR = {1998},
  ORGANIZATION = {Springer}
 }",Material/lighting estimation,,None,,,,,,,,,EGSR 1998,,,,"David Gargan, Francis Neelamkavil",gargan1998approximating,00000176,"We present a new representation for the storage and reconstruction of arbitrary reflectance functions. This non-linear representation, based on a neural network model, accurately captures the spectral and spatial variation of these functions. It is both computationally efficient and concise, yet expressive. We reconstruct the subtle reflection characteristics of an analytic reflection model as well as measured and simulated reflection data",11,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnuf1YMAGP9Cmsm8aMpGiVTOx68DFhN77pwCt1WXI7CuM84pJpN6_MY2Ibdmt_ezOQrw
6/29/2021 15:19:16,3D Object Reconstruction and Representation Using Neural Networks,,6/15/2004,http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.101.6810&rep=rep1&type=pdf,,,,,,,"@book{lim20043d,
  ID = {lim20043d},
  ENTRYTYPE = {book},
  TITLE = {3D Object Reconstruction and Representation Using Neural Networks},
  AUTHOR = {Wen Peng Lim and Siti Mariyam Shamsuddin},
  YEAR = {2004},
  PUBLISHER = {Universiti Teknologi Malaysia},
  BOOKTITLE = {International Conference on Computer Graphics and Interactive Techniques in Australasia and South East Asia}
 }",Fundamentals,,None,Other,Per-scene,,,,,,,GRAPHITE 2004,,Yes,,"Lim Wen Peng, Siti Mariyam Shamsuddin",lim20043d,00000000,"3D object reconstruction is frequent used in various fields such as product design, engineering, medical and artistic applications. Numerous reconstruction techniques and software were introduced and developed. However, the purpose of this paper is to fully integrate an adaptive artificial neural network (ANN) based method in reconstructing and representing 3D objects. This study explores the ability of neural networks in learning through experience when reconstructing an object by estimating it’s z-coordinate. Neural networks’ capability in representing most classes of 3D objects used in computer graphics is also proven. Simple affined transformation is applied on different objects using this approach and compared with the real objects. The results show that neural network is a promising approach for reconstruction and representation of 3D objects.",23,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnudnSrtQeTInFMc9JVQbhnPHwLuoOgHm3rPC2rryGdkCj1qQ7_Tcpcn6GFMD3S5bFDw
7/19/2021 22:02:18,FoldingNet: Point Cloud Auto-encoder via Deep Grid Deformation,FoldingNet,12/19/2017,https://arxiv.org/pdf/1712.07262.pdf,https://ai4ce.github.io/publication/yang-2018-foldingnet/,https://www.merl.com/research/license#FoldingNet,https://www.youtube.com/embed/WrEKJeK-Wow?rel=0&start=4130&end=4365,,,,"@inproceedings{yang2018foldingnet,
  BOOKTITLE = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  ID = {yang2018foldingnet},
  ENTRYTYPE = {inproceedings},
  AUTHOR = {Yaoqing Yang and Chen Feng and Yiru Shen and Dong Tian},
  TITLE = {FoldingNet: Point Cloud Auto-encoder via Deep Grid Deformation},
  EPRINT = {1712.07262v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Recent deep networks that directly handle points in a point set, e.g., PointNet, have been state-of-the-art for supervised learning tasks on point clouds such as classification and segmentation. In this work, a novel end-to-end deep auto-encoder is proposed to address unsupervised learning challenges on point clouds. On the encoder side, a graph-based enhancement is enforced to promote local structures on top of PointNet. Then, a novel folding-based decoder deforms a canonical 2D grid onto the underlying 3D object surface of a point cloud, achieving low reconstruction errors even for objects with delicate structures. The proposed decoder only uses about 7% parameters of a decoder with fully-connected neural networks, yet leads to a more discriminative representation that achieves higher linear SVM classification accuracy than the benchmark. In addition, the proposed decoder structure is shown, in theory, to be a generic architecture that is able to reconstruct an arbitrary point cloud from a 2D grid. Our code is available at http://www.merl.com/research/license#FoldingNet},
  YEAR = {2018},
  URL = {http://arxiv.org/abs/1712.07262v2},
  FILE = {1712.07262v2.pdf}
 }",Surface reconstruction,,None,Atlas,Per-scene,,,,,,,CVPR 2018,,Yes,Direct,"Yaoqing Yang, Chen Feng, Yiru Shen, Dong Tian",yang2018foldingnet,00000001,"Recent deep networks that directly handle points in a point set, e.g., PointNet, have been state-of-the-art for supervised learning tasks on point clouds such as classification and segmentation. In this work, a novel end-to-end deep auto-encoder is proposed to address unsupervised learning challenges on point clouds. On the encoder side, a graph-based enhancement is enforced to promote local structures on top of PointNet. Then, a novel folding-based decoder deforms a canonical 2D grid onto the underlying 3D object surface of a point cloud, achieving low reconstruction errors even for objects with delicate structures. The proposed decoder only uses about 7% parameters of a decoder with fully-connected neural networks, yet leads to a more discriminative representation that achieves higher linear SVM classification accuracy than the benchmark. In addition, the proposed decoder structure is shown, in theory, to be a generic architecture that is able to reconstruct an arbitrary point cloud from a 2D grid. Our code is available at http://www.merl.com/research/license#FoldingNet",433,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnuek-Fyp-QpxmpzXbybZvUEJ3m6_4VXUfE-ROdsgIVvjwtAE6yHl1ZxVyZNJ5JMQdOQ
5/23/2021 19:23:20,AtlasNet: A Papier-Mâché Approach to Learning 3D Surface Generation,AtlasNet,2/15/2018,https://arxiv.org/pdf/1802.05384.pdf,http://imagine.enpc.fr/~groueixt/atlasnet/,https://github.com/ThibaultGROUEIX/AtlasNet,"http://imagine.enpc.fr/~groueixt/atlasnet/atlasnet_slides_spotlight_CVPR.pptx, http://imagine.enpc.fr/~groueixt/atlasnet/atlasnet_poster.pdf",,,,"@inproceedings{groueix2018atlasnet,
  BOOKTITLE = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  ID = {groueix2018atlasnet},
  ENTRYTYPE = {inproceedings},
  AUTHOR = {Thibault Groueix and Matthew Fisher and Vladimir G. Kim and Bryan C. Russell and Mathieu Aubry},
  TITLE = {AtlasNet: A Papier-Mache Approach to Learning 3D Surface Generation},
  EPRINT = {1802.05384v3},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We introduce a method for learning to generate the surface of 3D shapes. Our approach represents a 3D shape as a collection of parametric surface elements and, in contrast to methods generating voxel grids or point clouds, naturally infers a surface representation of the shape. Beyond its novelty, our new shape generation framework, AtlasNet, comes with significant advantages, such as improved precision and generalization capabilities, and the possibility to generate a shape of arbitrary resolution without memory issues. We demonstrate these benefits and compare to strong baselines on the ShapeNet benchmark for two applications: (i) auto-encoding shapes, and (ii) single-view reconstruction from a still image. We also provide results showing its potential for other applications, such as morphing, parametrization, super-resolution, matching, and co-segmentation.},
  YEAR = {2018},
  URL = {http://arxiv.org/abs/1802.05384v3},
  FILE = {1802.05384v3.pdf}
 }",,"Conditional neural field, Sampling, Data-driven",,,,,,,,,,CVPR 2018,,Yes,,"Thibault Groueix, Matthew Fisher, Vladimir G. Kim, Bryan C. Russell, Mathieu Aubry",groueix2018atlasnet,00000002,"We introduce a method for learning to generate the surface of 3D shapes. Our approach represents a 3D shape as a collection of parametric surface elements and, in contrast to methods generating voxel grids or point clouds, naturally infers a surface representation of the shape. Beyond its novelty, our new shape generation framework, AtlasNet, comes with significant advantages, such as improved precision and generalization capabilities, and the possibility to generate a shape of arbitrary resolution without memory issues. We demonstrate these benefits and compare to strong baselines on the ShapeNet benchmark for two applications: (i) auto-encoding shapes, and (ii) single-view reconstruction from a still image. We also provide results showing its potential for other applications, such as morphing, parametrization, super-resolution, matching, and co-segmentation.",6,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnueiuL51nb8dOvM7QtFI9CQ5z5pYBoBLBJPY1t3Zu0wfM_CfuefApLYoHk8G4OIOyd0
9/17/2021 22:42:54,Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations,,6/13/2018,https://www.sciencedirect.com/science/article/pii/S0021999118307125,https://maziarraissi.github.io/PINNs/,https://github.com/maziarraissi/PINNs,,,,,"@article{raissi2019physicsinformed,
  ID = {raissi2019physicsinformed},
  ENTRYTYPE = {article},
  ABSTRACT = {We introduce physics-informed neural networks-neural networks that are trained to solve supervised learning tasks while respecting any given laws of physics described by general nonlinear partial differential equations. In this work, we present our developments in the context of solving two main classes of problems: data-driven solution and data-driven discovery of partial differential equations. Depending on the nature and arrangement of the available data, we devise two distinct types of algorithms, namely continuous time and},
  AUTHOR = {Maziar Raissi and Paris Perdikaris and George E Karniadakis},
  JOURNAL = {Journal of Computational Physics},
  PAGES = {686--707},
  PUB_YEAR = {2019},
  PUBLISHER = {Elsevier},
  TITLE = {Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations},
  VENUE = {Journal of Computational Physics},
  VOLUME = {378}
 }","Beyond graphics, Fundamentals, Science and engineering, PDE",,,,,,,,,,,Journal of Computational Physics 2019,,,,"Maziar Raissi, Paris Perdikaris, George E Karniadakis",raissi2019physicsinformed,00000213,"We introduce physics-informed neural networks-neural networks that are trained to solve supervised learning tasks while respecting any given laws of physics described by general nonlinear partial differential equations. In this work, we present our developments in the context of solving two main classes of problems: data-driven solution and data-driven discovery of partial differential equations. Depending on the nature and arrangement of the available data, we devise two distinct types of algorithms, namely continuous time and",,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnudUCBrQjg0VOBD1ZBTShMPRRN57oj8R4w__1WJMHq-4ZKHI28uhdmcwpWD2HIAyDKA
6/29/2021 15:56:46,Deep Geometric Prior for Surface Reconstruction,,11/27/2018,https://arxiv.org/pdf/1811.10943.pdf,,https://github.com/fwilliams/deep-geometric-prior,,,,,"@inproceedings{williams2019deep,
  BOOKTITLE = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  ID = {williams2019deep},
  ENTRYTYPE = {inproceedings},
  AUTHOR = {Francis Williams and Teseo Schneider and Claudio Silva and Denis Zorin and Joan Bruna and Daniele Panozzo},
  TITLE = {Deep Geometric Prior for Surface Reconstruction},
  EPRINT = {1811.10943v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {The reconstruction of a discrete surface from a point cloud is a fundamental geometry processing problem that has been studied for decades, with many methods developed. We propose the use of a deep neural network as a geometric prior for surface reconstruction. Specifically, we overfit a neural network representing a local chart parameterization to part of an input point cloud using the Wasserstein distance as a measure of approximation. By jointly fitting many such networks to overlapping parts of the point cloud, while enforcing a consistency condition, we compute a manifold atlas. By sampling this atlas, we can produce a dense reconstruction of the surface approximating the input cloud. The entire procedure does not require any training data or explicit regularization, yet, we show that it is able to perform remarkably well: not introducing typical overfitting artifacts, and approximating sharp features closely at the same time. We experimentally show that this geometric prior produces good results for both man-made objects containing sharp features and smoother organic objects, as well as noisy inputs. We compare our method with a number of well-known reconstruction methods on a standard surface reconstruction benchmark.},
  YEAR = {2019},
  URL = {http://arxiv.org/abs/1811.10943v2},
  FILE = {1811.10943v2.pdf}
 }",,,None,Atlas,Per-scene,,,,,,,CVPR 2019,,Yes,,"Francis Williams, Teseo Schneider, Claudio Silva, Denis Zorin, Joan Bruna, Daniele Panozzo",williams2019deep,00000003,"The reconstruction of a discrete surface from a point cloud is a fundamental geometry processing problem that has been studied for decades, with many methods developed. We propose the use of a deep neural network as a geometric prior for surface reconstruction. Specifically, we overfit a neural network representing a local chart parameterization to part of an input point cloud using the Wasserstein distance as a measure of approximation. By jointly fitting many such networks to overlapping parts of the point cloud, while enforcing a consistency condition, we compute a manifold atlas. By sampling this atlas, we can produce a dense reconstruction of the surface approximating the input cloud. The entire procedure does not require any training data or explicit regularization, yet, we show that it is able to perform remarkably well: not introducing typical overfitting artifacts, and approximating sharp features closely at the same time. We experimentally show that this geometric prior produces good results for both man-made objects containing sharp features and smoother organic objects, as well as noisy inputs. We compare our method with a number of well-known reconstruction methods on a standard surface reconstruction benchmark.",64,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnufMO8wx0iBcNCelxUupJeAX8Vzy4HUWdSjUP0PePvb7lh81v7Vx_EqHgPj3ay3zfi4
6/29/2021 15:38:57,Learning Implicit Fields for Generative Shape Modeling,IM-NET,12/6/2018,https://arxiv.org/pdf/1812.02822.pdf,https://www.sfu.ca/~zhiqinc/imgan/Readme.html,https://github.com/czq142857/implicit-decoder,,,,,"@inproceedings{chen2019imnet,
  BOOKTITLE = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  ID = {chen2019imnet},
  ENTRYTYPE = {inproceedings},
  AUTHOR = {Zhiqin Chen and Hao Zhang},
  TITLE = {Learning Implicit Fields for Generative Shape Modeling},
  EPRINT = {1812.02822v5},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.GR},
  ABSTRACT = {We advocate the use of implicit fields for learning generative models of shapes and introduce an implicit field decoder, called IM-NET, for shape generation, aimed at improving the visual quality of the generated shapes. An implicit field assigns a value to each point in 3D space, so that a shape can be extracted as an iso-surface. IM-NET is trained to perform this assignment by means of a binary classifier. Specifically, it takes a point coordinate, along with a feature vector encoding a shape, and outputs a value which indicates whether the point is outside the shape or not. By replacing conventional decoders by our implicit decoder for representation learning (via IM-AE) and shape generation (via IM-GAN), we demonstrate superior results for tasks such as generative shape modeling, interpolation, and single-view 3D reconstruction, particularly in terms of visual quality. Code and supplementary material are available at https://github.com/czq142857/implicit-decoder.},
  YEAR = {2019},
  URL = {http://arxiv.org/abs/1812.02822v5},
  FILE = {1812.02822v5.pdf}
 }",Generalization,"Generative/adversarial formulation, Conditional neural field, Data-driven",None,Occupancy,Category-level,,,,,,,CVPR 2019,,Yes,,"Zhiqin Chen, Hao Zhang",chen2019imnet,00000004,"We advocate the use of implicit fields for learning generative models of shapes and introduce an implicit field decoder, called IM-NET, for shape generation, aimed at improving the visual quality of the generated shapes. An implicit field assigns a value to each point in 3D space, so that a shape can be extracted as an iso-surface. IM-NET is trained to perform this assignment by means of a binary classifier. Specifically, it takes a point coordinate, along with a feature vector encoding a shape, and outputs a value which indicates whether the point is outside the shape or not. By replacing conventional decoders by our implicit decoder for representation learning (via IM-AE) and shape generation (via IM-GAN), we demonstrate superior results for tasks such as generative shape modeling, interpolation, and single-view 3D reconstruction, particularly in terms of visual quality. Code and supplementary material are available at https://github.com/czq142857/implicit-decoder.",324,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnucCJ2CBz62kpppZ18WGIe-PKNL0VGowvckqJAEYGlVGln-46_mRtMAd61SRpYZGkdo
5/23/2021 19:22:38,Occupancy Networks: Learning 3D Reconstruction in Function Space,Occupancy Networks,12/10/2018,https://arxiv.org/pdf/1812.03828.pdf,https://avg.is.tuebingen.mpg.de/publications/occupancy-networks,https://github.com/autonomousvision/occupancy_networks,https://www.youtube.com/watch?v=w1Qo3bOiPaE,,,,"@inproceedings{mescheder2019occupancynetworks,
  BOOKTITLE = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  ID = {mescheder2019occupancynetworks},
  ENTRYTYPE = {inproceedings},
  AUTHOR = {Lars Mescheder and Michael Oechsle and Michael Niemeyer and Sebastian Nowozin and Andreas Geiger},
  TITLE = {Occupancy Networks: Learning 3D Reconstruction in Function Space},
  EPRINT = {1812.03828v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {With the advent of deep neural networks, learning-based approaches for 3D reconstruction have gained popularity. However, unlike for images, in 3D there is no canonical representation which is both computationally and memory efficient yet allows for representing high-resolution geometry of arbitrary topology. Many of the state-of-the-art learning-based 3D reconstruction approaches can hence only represent very coarse 3D geometry or are limited to a restricted domain. In this paper, we propose Occupancy Networks, a new representation for learning-based 3D reconstruction methods. Occupancy networks implicitly represent the 3D surface as the continuous decision boundary of a deep neural network classifier. In contrast to existing approaches, our representation encodes a description of the 3D output at infinite resolution without excessive memory footprint. We validate that our representation can efficiently encode 3D structure and can be inferred from various kinds of input. Our experiments demonstrate competitive results, both qualitatively and quantitatively, for the challenging tasks of 3D reconstruction from single images, noisy point clouds and coarse discrete voxel grids. We believe that occupancy networks will become a useful tool in a wide variety of learning-based 3D tasks.},
  YEAR = {2019},
  URL = {http://arxiv.org/abs/1812.03828v2},
  FILE = {1812.03828v2.pdf}
 }",Generalization,"Conditional neural field, Sampling",,Occupancy,,,,,,,,CVPR 2019,,Yes,,"Lars Mescheder, Michael Oechsle, Michael Niemeyer, Sebastian Nowozin, Andreas Geiger",mescheder2019occupancynetworks,00000005,"With the advent of deep neural networks, learning-based approaches for 3D reconstruction have gained popularity. However, unlike for images, in 3D there is no canonical representation which is both computationally and memory efficient yet allows for representing high-resolution geometry of arbitrary topology. Many of the state-of-the-art learning-based 3D reconstruction approaches can hence only represent very coarse 3D geometry or are limited to a restricted domain. In this paper, we propose Occupancy Networks, a new representation for learning-based 3D reconstruction methods. Occupancy networks implicitly represent the 3D surface as the continuous decision boundary of a deep neural network classifier. In contrast to existing approaches, our representation encodes a description of the 3D output at infinite resolution without excessive memory footprint. We validate that our representation can efficiently encode 3D structure and can be inferred from various kinds of input. Our experiments demonstrate competitive results, both qualitatively and quantitatively, for the challenging tasks of 3D reconstruction from single images, noisy point clouds and coarse discrete voxel grids. We believe that occupancy networks will become a useful tool in a wide variety of learning-based 3D tasks.",540,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnudfTlW6ReA6w6Q9rC38zylpXEvcdvJgheHeBPcMtCZAa-wddBVC1C71lBZSNAzXmrM
5/23/2021 19:20:13,DeepSDF: Learning Continuous Signed Distance Functions for Shape Representation,DeepSDF,1/16/2019,https://arxiv.org/pdf/1901.05103.pdf,,https://github.com/facebookresearch/DeepSDF,,,,,"@inproceedings{park2019deepsdf,
  BOOKTITLE = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  ID = {park2019deepsdf},
  ENTRYTYPE = {inproceedings},
  AUTHOR = {Jeong Joon Park and Peter Florence and Julian Straub and Richard Newcombe and Steven Lovegrove},
  TITLE = {DeepSDF: Learning Continuous Signed Distance Functions for Shape Representation},
  EPRINT = {1901.05103v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Computer graphics, 3D computer vision and robotics communities have produced multiple approaches to representing 3D geometry for rendering and reconstruction. These provide trade-offs across fidelity, efficiency and compression capabilities. In this work, we introduce DeepSDF, a learned continuous Signed Distance Function (SDF) representation of a class of shapes that enables high quality shape representation, interpolation and completion from partial and noisy 3D input data. DeepSDF, like its classical counterpart, represents a shape's surface by a continuous volumetric field: the magnitude of a point in the field represents the distance to the surface boundary and the sign indicates whether the region is inside (-) or outside (+) of the shape, hence our representation implicitly encodes a shape's boundary as the zero-level-set of the learned function while explicitly representing the classification of space as being part of the shapes interior or not. While classical SDF's both in analytical or discretized voxel form typically represent the surface of a single shape, DeepSDF can represent an entire class of shapes. Furthermore, we show state-of-the-art performance for learned 3D shape representation and completion while reducing the model size by an order of magnitude compared with previous work.},
  YEAR = {2019},
  URL = {http://arxiv.org/abs/1901.05103v1},
  FILE = {1901.05103v1.pdf}
 }",Generalization,"Generative/adversarial formulation, Conditional neural field",,SDF,,,,,,,,CVPR 2019,,Yes,,"Jeong Joon Park, Peter Florence, Julian Straub, Richard Newcombe, Steven Lovegrove",park2019deepsdf,00000006,"Computer graphics, 3D computer vision and robotics communities have produced multiple approaches to representing 3D geometry for rendering and reconstruction. These provide trade-offs across fidelity, efficiency and compression capabilities. In this work, we introduce DeepSDF, a learned continuous Signed Distance Function (SDF) representation of a class of shapes that enables high quality shape representation, interpolation and completion from partial and noisy 3D input data. DeepSDF, like its classical counterpart, represents a shape's surface by a continuous volumetric field: the magnitude of a point in the field represents the distance to the surface boundary and the sign indicates whether the region is inside (-) or outside (+) of the shape, hence our representation implicitly encodes a shape's boundary as the zero-level-set of the learned function while explicitly representing the classification of space as being part of the shapes interior or not. While classical SDF's both in analytical or discretized voxel form typically represent the surface of a single shape, DeepSDF can represent an entire class of shapes. Furthermore, we show state-of-the-art performance for learned 3D shape representation and completion while reducing the model size by an order of magnitude compared with previous work.",593,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnucQsvhhQQJI8wojBk0UOw9WwZdQB6q3wR3pgISbiYGRVyzSUaNEqcRkm422DKqbjIA
6/29/2021 16:44:22,PIFu: Pixel-Aligned Implicit Function for High-Resolution Clothed Human Digitization,PIFu,5/13/2019,https://arxiv.org/pdf/1905.05172.pdf,https://shunsukesaito.github.io/PIFu/,https://github.com/shunsukesaito/PIFu,https://www.youtube.com/watch?v=S1FpjwKqtPs,,,,"@inproceedings{saito2019pifu,
  BOOKTITLE = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  ID = {saito2019pifu},
  ENTRYTYPE = {inproceedings},
  AUTHOR = {Shunsuke Saito and Zeng Huang and Ryota Natsume and Shigeo Morishima and Angjoo Kanazawa and Hao Li},
  TITLE = {PIFu: Pixel-Aligned Implicit Function for High-Resolution Clothed Human Digitization},
  EPRINT = {1905.05172v3},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We introduce Pixel-aligned Implicit Function (PIFu), a highly effective implicit representation that locally aligns pixels of 2D images with the global context of their corresponding 3D object. Using PIFu, we propose an end-to-end deep learning method for digitizing highly detailed clothed humans that can infer both 3D surface and texture from a single image, and optionally, multiple input images. Highly intricate shapes, such as hairstyles, clothing, as well as their variations and deformations can be digitized in a unified way. Compared to existing representations used for 3D deep learning, PIFu can produce high-resolution surfaces including largely unseen regions such as the back of a person. In particular, it is memory efficient unlike the voxel representation, can handle arbitrary topology, and the resulting surface is spatially aligned with the input image. Furthermore, while previous techniques are designed to process either a single image or multiple views, PIFu extends naturally to arbitrary number of views. We demonstrate high-resolution and robust reconstructions on real world images from the DeepFashion dataset, which contains a variety of challenging clothing types. Our method achieves state-of-the-art performance on a public benchmark and outperforms the prior work for clothed human digitization from a single image.},
  YEAR = {2019},
  NOTE = {The IEEE International Conference on Computer Vision (ICCV), 2019, pp. 2304-2314},
  URL = {http://arxiv.org/abs/1905.05172v3},
  FILE = {1905.05172v3.pdf}
 }","Human body, Few-shot reconstruction, Generalization","Lifting 2D features to 3D, Image-based rendering, Data-driven",None,Occupancy,Category-level,,,,,,,CVPR 2019,,No,,"Shunsuke Saito, Zeng Huang, Ryota Natsume, Shigeo Morishima, Angjoo Kanazawa, Hao Li",saito2019pifu,00000007,"We introduce Pixel-aligned Implicit Function (PIFu), a highly effective implicit representation that locally aligns pixels of 2D images with the global context of their corresponding 3D object. Using PIFu, we propose an end-to-end deep learning method for digitizing highly detailed clothed humans that can infer both 3D surface and texture from a single image, and optionally, multiple input images. Highly intricate shapes, such as hairstyles, clothing, as well as their variations and deformations can be digitized in a unified way. Compared to existing representations used for 3D deep learning, PIFu can produce high-resolution surfaces including largely unseen regions such as the back of a person. In particular, it is memory efficient unlike the voxel representation, can handle arbitrary topology, and the resulting surface is spatially aligned with the input image. Furthermore, while previous techniques are designed to process either a single image or multiple views, PIFu extends naturally to arbitrary number of views. We demonstrate high-resolution and robust reconstructions on real world images from the DeepFashion dataset, which contains a variety of challenging clothing types. Our method achieves state-of-the-art performance on a public benchmark and outperforms the prior work for clothed human digitization from a single image.",295,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnuczDvzOo6jijfwpqZsDKHYM4_gVORdLznomp7IN1xtFDnBhVE2uRNDdzRs1Ix4EndI
8/29/2021 21:45:27,Texture Fields: Learning Texture Representations in Function Space,Texture Fields,5/17/2019,https://arxiv.org/pdf/1905.07259.pdf,https://autonomousvision.github.io/texture-fields/,https://github.com/autonomousvision/texture_fields,https://www.youtube.com/watch?v=pbfeE0qmD2E,http://www.cvlibs.net/publications/Oechsle2019ICCV_supplementary.pdf,,,"@inproceedings{oechsle2019texturefields,
  BOOKTITLE = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
  ID = {oechsle2019texturefields},
  ENTRYTYPE = {inproceedings},
  AUTHOR = {Michael Oechsle and Lars Mescheder and Michael Niemeyer and Thilo Strauss and Andreas Geiger},
  TITLE = {Texture Fields: Learning Texture Representations in Function Space},
  EPRINT = {1905.07259v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {In recent years, substantial progress has been achieved in learning-based reconstruction of 3D objects. At the same time, generative models were proposed that can generate highly realistic images. However, despite this success in these closely related tasks, texture reconstruction of 3D objects has received little attention from the research community and state-of-the-art methods are either limited to comparably low resolution or constrained experimental setups. A major reason for these limitations is that common representations of texture are inefficient or hard to interface for modern deep learning techniques. In this paper, we propose Texture Fields, a novel texture representation which is based on regressing a continuous 3D function parameterized with a neural network. Our approach circumvents limiting factors like shape discretization and parameterization, as the proposed texture representation is independent of the shape representation of the 3D object. We show that Texture Fields are able to represent high frequency texture and naturally blend with modern deep learning techniques. Experimentally, we find that Texture Fields compare favorably to state-of-the-art methods for conditional texture reconstruction of 3D objects and enable learning of probabilistic generative models for texturing unseen 3D models. We believe that Texture Fields will become an important building block for the next generation of generative 3D models.},
  YEAR = {2019},
  URL = {http://arxiv.org/abs/1905.07259v1},
  FILE = {1905.07259v1.pdf}
 }",Generalization,"Generative/adversarial formulation, Conditional neural field, Data-driven",None,,,,,,,,,ICCV 2019,,,Direct,"Michael Oechsle, Lars Mescheder, Michael Niemeyer, Thilo Strauss, Andreas Geiger",oechsle2019texturefields,00000186,"In recent years, substantial progress has been achieved in learning-based reconstruction of 3D objects. At the same time, generative models were proposed that can generate highly realistic images. However, despite this success in these closely related tasks, texture reconstruction of 3D objects has received little attention from the research community and state-of-the-art methods are either limited to comparably low resolution or constrained experimental setups. A major reason for these limitations is that common representations of texture are inefficient or hard to interface for modern deep learning techniques. In this paper, we propose Texture Fields, a novel texture representation which is based on regressing a continuous 3D function parameterized with a neural network. Our approach circumvents limiting factors like shape discretization and parameterization, as the proposed texture representation is independent of the shape representation of the 3D object. We show that Texture Fields are able to represent high frequency texture and naturally blend with modern deep learning techniques. Experimentally, we find that Texture Fields compare favorably to state-of-the-art methods for conditional texture reconstruction of 3D objects and enable learning of probabilistic generative models for texturing unseen 3D models. We believe that Texture Fields will become an important building block for the next generation of generative 3D models.",77,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnucXdMLOIdNu7tSreaD7IfcKXpYg9JOMk_XSeRkeXsGh2SEXjGv-1oSuNKi82-sgqA4
8/29/2021 17:38:51,Controlling Neural Level Sets,,5/28/2019,https://arxiv.org/pdf/1905.11911.pdf,https://github.com/matanatz/ControllingNeuralLevelsets,,,,,,"@inproceedings{atzmon2019controlling,
  PUBLISHER = {Curran Associates, Inc.},
  BOOKTITLE = {Advances in Neural Information Processing Systems (NeurIPS)},
  ID = {atzmon2019controlling},
  ENTRYTYPE = {inproceedings},
  AUTHOR = {Matan Atzmon and Niv Haim and Lior Yariv and Ofer Israelov and Haggai Maron and Yaron Lipman},
  TITLE = {Controlling Neural Level Sets},
  EPRINT = {1905.11911v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.LG},
  ABSTRACT = {The level sets of neural networks represent fundamental properties such as decision boundaries of classifiers and are used to model non-linear manifold data such as curves and surfaces. Thus, methods for controlling the neural level sets could find many applications in machine learning. In this paper we present a simple and scalable approach to directly control level sets of a deep neural network. Our method consists of two parts: (i) sampling of the neural level sets, and (ii) relating the samples' positions to the network parameters. The latter is achieved by a sample network that is constructed by adding a single fixed linear layer to the original network. In turn, the sample network can be used to incorporate the level set samples into a loss function of interest. We have tested our method on three different learning tasks: improving generalization to unseen data, training networks robust to adversarial attacks, and curve and surface reconstruction from point clouds. For surface reconstruction, we produce high fidelity surfaces directly from raw 3D point clouds. When training small to medium networks to be robust to adversarial attacks we obtain robust accuracy comparable to state-of-the-art methods.},
  YEAR = {2019},
  URL = {http://arxiv.org/abs/1905.11911v2},
  FILE = {1905.11911v2.pdf}
 }","Generalization, Fundamentals","Conditional neural field, Sampling, Data-driven",,,,,,,,,,NeurIPS 2019,http://faust.is.tue.mpg.de/,Yes,Direct,"Matan Atzmon, Niv Haim, Lior Yariv, Ofer Israelov, Haggai Maron, Yaron Lipman",atzmon2019controlling,00000173,"The level sets of neural networks represent fundamental properties such as decision boundaries of classifiers and are used to model non-linear manifold data such as curves and surfaces. Thus, methods for controlling the neural level sets could find many applications in machine learning. In this paper we present a simple and scalable approach to directly control level sets of a deep neural network. Our method consists of two parts: (i) sampling of the neural level sets, and (ii) relating the samples' positions to the network parameters. The latter is achieved by a sample network that is constructed by adding a single fixed linear layer to the original network. In turn, the sample network can be used to incorporate the level set samples into a loss function of interest. We have tested our method on three different learning tasks: improving generalization to unseen data, training networks robust to adversarial attacks, and curve and surface reconstruction from point clouds. For surface reconstruction, we produce high fidelity surfaces directly from raw 3D point clouds. When training small to medium networks to be robust to adversarial attacks we obtain robust accuracy comparable to state-of-the-art methods.",30,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnufhBXeC1lFFWdAEu25o_CWpKVV1rQ4t3HZyqh7DQn_2okZHTrtNFaT0Ps-mrG3Pdkg
5/23/2021 19:15:08,Scene Representation Networks: Continuous 3D-Structure-Aware Neural Scene Representations,SRN,6/4/2019,https://arxiv.org/pdf/1906.01618.pdf,https://vsitzmann.github.io/srns/,https://github.com/vsitzmann/scene-representation-networks,"https://www.youtube.com/watch?v=6vMEBWD8O20, https://slideslive.com/38922305/scene-representation-networks-continuous-3dstructureaware-neural-scene-representations",,,,"@inproceedings{sitzmann2020srn,
  BOOKTITLE = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  ID = {sitzmann2020srn},
  ENTRYTYPE = {inproceedings},
  AUTHOR = {Vincent Sitzmann and Michael Zollhofer and Gordon Wetzstein},
  TITLE = {Scene Representation Networks: Continuous 3D-Structure-Aware Neural Scene Representations},
  EPRINT = {1906.01618v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Unsupervised learning with generative models has the potential of discovering rich representations of 3D scenes. While geometric deep learning has explored 3D-structure-aware representations of scene geometry, these models typically require explicit 3D supervision. Emerging neural scene representations can be trained only with posed 2D images, but existing methods ignore the three-dimensional structure of scenes. We propose Scene Representation Networks (SRNs), a continuous, 3D-structure-aware scene representation that encodes both geometry and appearance. SRNs represent scenes as continuous functions that map world coordinates to a feature representation of local scene properties. By formulating the image formation as a differentiable ray-marching algorithm, SRNs can be trained end-to-end from only 2D images and their camera poses, without access to depth or shape. This formulation naturally generalizes across scenes, learning powerful geometry and appearance priors in the process. We demonstrate the potential of SRNs by evaluating them for novel view synthesis, few-shot reconstruction, joint shape and appearance interpolation, and unsupervised discovery of a non-rigid face model.},
  YEAR = {2020},
  URL = {http://arxiv.org/abs/1906.01618v2},
  FILE = {1906.01618v2.pdf}
 }",Generalization,"Conditional neural field, Hypernetwork",,,,,,,,,,CVPR 2020,https://drive.google.com/drive/folders/1OkYgeRcIcLOFu1ft5mRODWNQaPJ0ps90,No,,"Vincent Sitzmann, Michael Zollhöfer, Gordon Wetzstein",sitzmann2020srn,00000008,"Unsupervised learning with generative models has the potential of discovering rich representations of 3D scenes. While geometric deep learning has explored 3D-structure-aware representations of scene geometry, these models typically require explicit 3D supervision. Emerging neural scene representations can be trained only with posed 2D images, but existing methods ignore the three-dimensional structure of scenes. We propose Scene Representation Networks (SRNs), a continuous, 3D-structure-aware scene representation that encodes both geometry and appearance. SRNs represent scenes as continuous functions that map world coordinates to a feature representation of local scene properties. By formulating the image formation as a differentiable ray-marching algorithm, SRNs can be trained end-to-end from only 2D images and their camera poses, without access to depth or shape. This formulation naturally generalizes across scenes, learning powerful geometry and appearance priors in the process. We demonstrate the potential of SRNs by evaluating them for novel view synthesis, few-shot reconstruction, joint shape and appearance interpolation, and unsupervised discovery of a non-rigid face model.",262,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnud0ylozZ3FiZsRhcRQIYWGg0I-KmEgL0HEBLWcVP-2kwLSWsybQ9EtFp52q-3y4JIQ
5/23/2021 19:19:44,Neural Volumes: Learning Dynamic Renderable Volumes from Images,NV,6/18/2019,https://arxiv.org/pdf/1906.07751.pdf,https://stephenlombardi.github.io/projects/neuralvolumes/,https://github.com/facebookresearch/neuralvolumes,"https://youtu.be/JlyGNvbGKB8?t=5347, https://crossminds.ai/video/neural-volumes-learning-dynamic-renderable-volumes-from-images-606f94d175292b321dd0906f/",,,,"@article{lombardi2019nv,
  PUBLISHER = {Association for Computing Machinery},
  JOURNAL = {ACM Transactions on Graphics (TOG)},
  ID = {lombardi2019nv},
  ENTRYTYPE = {article},
  AUTHOR = {Stephen Lombardi and Tomas Simon and Jason Saragih and Gabriel Schwartz and Andreas Lehrmann and Yaser Sheikh},
  TITLE = {Neural Volumes: Learning Dynamic Renderable Volumes from Images},
  EPRINT = {1906.07751v1},
  DOI = {10.1145/3306346.3323020},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.GR},
  ABSTRACT = {Modeling and rendering of dynamic scenes is challenging, as natural scenes often contain complex phenomena such as thin structures, evolving topology, translucency, scattering, occlusion, and biological motion. Mesh-based reconstruction and tracking often fail in these cases, and other approaches (e.g., light field video) typically rely on constrained viewing conditions, which limit interactivity. We circumvent these difficulties by presenting a learning-based approach to representing dynamic objects inspired by the integral projection model used in tomographic imaging. The approach is supervised directly from 2D images in a multi-view capture setting and does not require explicit reconstruction or tracking of the object. Our method has two primary components: an encoder-decoder network that transforms input images into a 3D volume representation, and a differentiable ray-marching operation that enables end-to-end training. By virtue of its 3D representation, our construction extrapolates better to novel viewpoints compared to screen-space rendering techniques. The encoder-decoder architecture learns a latent representation of a dynamic scene that enables us to produce novel content sequences not seen during training. To overcome memory limitations of voxel-based representations, we learn a dynamic irregular grid structure implemented with a warp field during ray-marching. This structure greatly improves the apparent resolution and reduces grid-like artifacts and jagged motion. Finally, we demonstrate how to incorporate surface-based representations into our volumetric-learning framework for applications where the highest resolution is required, using facial performance capture as a case in point.},
  YEAR = {2019},
  NOTE = {ACM Transactions on Graphics (SIGGRAPH 2019) 38, 4, Article 65},
  URL = {http://arxiv.org/abs/1906.07751v1},
  FILE = {1906.07751v1.pdf}
 }",Dynamic,"Conditional neural field, Lifting 2D features to 3D, Representation, Warping field/Flow field",,,,compression,,,,,,SIGGRAPH 2019,,No,,"Stephen Lombardi, Tomas Simon, Jason Saragih, Gabriel Schwartz, Andreas Lehrmann, Yaser Sheikh",lombardi2019nv,00000009,"Modeling and rendering of dynamic scenes is challenging, as natural scenes often contain complex phenomena such as thin structures, evolving topology, translucency, scattering, occlusion, and biological motion. Mesh-based reconstruction and tracking often fail in these cases, and other approaches (e.g., light field video) typically rely on constrained viewing conditions, which limit interactivity. We circumvent these difficulties by presenting a learning-based approach to representing dynamic objects inspired by the integral projection model used in tomographic imaging. The approach is supervised directly from 2D images in a multi-view capture setting and does not require explicit reconstruction or tracking of the object. Our method has two primary components: an encoder-decoder network that transforms input images into a 3D volume representation, and a differentiable ray-marching operation that enables end-to-end training. By virtue of its 3D representation, our construction extrapolates better to novel viewpoints compared to screen-space rendering techniques. The encoder-decoder architecture learns a latent representation of a dynamic scene that enables us to produce novel content sequences not seen during training. To overcome memory limitations of voxel-based representations, we learn a dynamic irregular grid structure implemented with a warp field during ray-marching. This structure greatly improves the apparent resolution and reduces grid-like artifacts and jagged motion. Finally, we demonstrate how to incorporate surface-based representations into our volumetric-learning framework for applications where the highest resolution is required, using facial performance capture as a case in point.",161,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnudSV31PVn2TvxMvkS_mQG4JEOpa4HEGaodRqwTpo21CBLHQhKtIyikp_QRBIYOZvMo
7/19/2021 21:58:28,Learning elementary structures for 3D shape generation and matching,,8/13/2019,https://arxiv.org/pdf/1908.04725.pdf,,,,,,,"@inproceedings{deprelle2019learning,
  BOOKTITLE = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
  ID = {deprelle2019learning},
  ENTRYTYPE = {inproceedings},
  AUTHOR = {Theo Deprelle and Thibault Groueix and Matthew Fisher and Vladimir G. Kim and Bryan C. Russell and Mathieu Aubry},
  TITLE = {Learning elementary structures for 3D shape generation and matching},
  EPRINT = {1908.04725v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We propose to represent shapes as the deformation and combination of learnable elementary 3D structures, which are primitives resulting from training over a collection of shape. We demonstrate that the learned elementary 3D structures lead to clear improvements in 3D shape generation and matching. More precisely, we present two complementary approaches for learning elementary structures: (i) patch deformation learning and (ii) point translation learning. Both approaches can be extended to abstract structures of higher dimensions for improved results. We evaluate our method on two tasks: reconstructing ShapeNet objects and estimating dense correspondences between human scans (FAUST inter challenge). We show 16% improvement over surface deformation approaches for shape reconstruction and outperform FAUST inter challenge state of the art by 6%.},
  YEAR = {2019},
  URL = {http://arxiv.org/abs/1908.04725v2},
  FILE = {1908.04725v2.pdf}
 }",Surface reconstruction,Conditional neural field,None,Atlas,Per-scene,,,,,,,ICCV 2019,,Yes,Direct,"Theo Deprelle, Thibault Groueix, Matthew Fisher, Vladimir G. Kim, Bryan C. Russell, Mathieu Aubry",deprelle2019learning,00000010,"We propose to represent shapes as the deformation and combination of learnable elementary 3D structures, which are primitives resulting from training over a collection of shape. We demonstrate that the learned elementary 3D structures lead to clear improvements in 3D shape generation and matching. More precisely, we present two complementary approaches for learning elementary structures: (i) patch deformation learning and (ii) point translation learning. Both approaches can be extended to abstract structures of higher dimensions for improved results. We evaluate our method on two tasks: reconstructing ShapeNet objects and estimating dense correspondences between human scans (FAUST inter challenge). We show 16% improvement over surface deformation approaches for shape reconstruction and outperform FAUST inter challenge state of the art by 6%.",61,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnucWz0JbhTmqnEnC07hZk6dfXbutol328ey3mcCNjzye-IafqvSveTTQ6e2LgkCFGGg
9/19/2021 15:43:39,Reconstructing continuous distributions of 3D protein structure from cryo-EM images,cryoDRGN,9/11/2019,https://arxiv.org/pdf/1909.05215.pdf,http://cb.csail.mit.edu/cb/cryodrgn/,https://github.com/zhonge/cryodrgn,https://www.youtube.com/watch?v=zd6YcUyDhPE,,,,"@inproceedings{zhong2020cryodrgn,
  BOOKTITLE = {International Conference on Learning Representations},
  ID = {zhong2020cryodrgn},
  ENTRYTYPE = {inproceedings},
  AUTHOR = {Ellen D. Zhong and Tristan Bepler and Joseph H. Davis and Bonnie Berger},
  TITLE = {Reconstructing continuous distributions of 3D protein structure from cryo-EM images},
  EPRINT = {1909.05215v3},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {q-bio.QM},
  ABSTRACT = {Cryo-electron microscopy (cryo-EM) is a powerful technique for determining the structure of proteins and other macromolecular complexes at near-atomic resolution. In single particle cryo-EM, the central problem is to reconstruct the three-dimensional structure of a macromolecule from $10^{4-7}$ noisy and randomly oriented two-dimensional projections. However, the imaged protein complexes may exhibit structural variability, which complicates reconstruction and is typically addressed using discrete clustering approaches that fail to capture the full range of protein dynamics. Here, we introduce a novel method for cryo-EM reconstruction that extends naturally to modeling continuous generative factors of structural heterogeneity. This method encodes structures in Fourier space using coordinate-based deep neural networks, and trains these networks from unlabeled 2D cryo-EM images by combining exact inference over image orientation with variational inference for structural heterogeneity. We demonstrate that the proposed method, termed cryoDRGN, can perform ab initio reconstruction of 3D protein complexes from simulated and real 2D cryo-EM image data. To our knowledge, cryoDRGN is the first neural network-based approach for cryo-EM reconstruction and the first end-to-end method for directly reconstructing continuous ensembles of protein structures from cryo-EM images.},
  YEAR = {2020},
  NOTE = {International Conference on Learning Representations (ICLR), 2020},
  URL = {http://arxiv.org/abs/1909.05215v3},
  FILE = {1909.05215v3.pdf}
 }","Beyond graphics, Alternative imaging, Science and engineering","Conditional neural field, Lifting 2D features to 3D, Data-driven",NeRF,Electron density,,,,,,,,ICLR 2020,,,,"Ellen D. Zhong, Tristan Bepler, Joseph H. Davis, Bonnie Berger",zhong2020cryodrgn,00000218,"Cryo-electron microscopy (cryo-EM) is a powerful technique for determining the structure of proteins and other macromolecular complexes at near-atomic resolution. In single particle cryo-EM, the central problem is to reconstruct the three-dimensional structure of a macromolecule from $10^{4-7}$ noisy and randomly oriented two-dimensional projections. However, the imaged protein complexes may exhibit structural variability, which complicates reconstruction and is typically addressed using discrete clustering approaches that fail to capture the full range of protein dynamics. Here, we introduce a novel method for cryo-EM reconstruction that extends naturally to modeling continuous generative factors of structural heterogeneity. This method encodes structures in Fourier space using coordinate-based deep neural networks, and trains these networks from unlabeled 2D cryo-EM images by combining exact inference over image orientation with variational inference for structural heterogeneity. We demonstrate that the proposed method, termed cryoDRGN, can perform ab initio reconstruction of 3D protein complexes from simulated and real 2D cryo-EM image data. To our knowledge, cryoDRGN is the first neural network-based approach for cryo-EM reconstruction and the first end-to-end method for directly reconstructing continuous ensembles of protein structures from cryo-EM images.",,,No,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnufciBS-zic05ygwXkIkgvFCjSHUE_-VEfPH4s6RWWbLvOUWjaH2qCvdkptro1r41YA
8/29/2021 21:39:35,Occupancy Flow: 4D Reconstruction by Learning Particle Dynamics,Occupancy Flow,10/1/2019,https://openaccess.thecvf.com/content_ICCV_2019/papers/Niemeyer_Occupancy_Flow_4D_Reconstruction_by_Learning_Particle_Dynamics_ICCV_2019_paper.pdf,https://avg.is.tuebingen.mpg.de/publications/niemeyer2019iccv,https://github.com/autonomousvision/occupancy_flow,https://www.youtube.com/watch?v=c0yOugTgrWc,http://www.cvlibs.net/publications/Niemeyer2019ICCV_supplementary.pdf,,,"@inproceedings{niemeyer2019occupancyflow,
  ID = {niemeyer2019occupancyflow},
  ENTRYTYPE = {inproceedings},
  TITLE = {Occupancy flow: 4d reconstruction by learning particle dynamics},
  AUTHOR = {Michael Niemeyer and Lars Mescheder and Michael Oechsle and Andreas Geiger},
  BOOKTITLE = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
  PAGES = {5379--5389},
  YEAR = {2019}
 }","Dynamic, Human body",Warping field/Flow field,None,Occupancy,,,,,,,,ICCV 2019,,Yes,Direct,"Michael Niemeyer, Lars Mescheder, Michael Oechsle, Andreas Geiger",niemeyer2019occupancyflow,00000185,"Deep learning based 3D reconstruction techniques have recently achieved impressive results. However, while stateof-the-art methods are able to output complex 3D geometry, it is not clear how to extend these results to time-varying topologies. Approaches treating each time step individually lack continuity and exhibit slow inference, while traditional 4D reconstruction methods often utilize a template model or discretize the 4D space at fixed resolution. In this work, we present Occupancy Flow, a novel spatio-temporal representation of time-varying 3D geometry with implicit correspondences. Towards this goal, we learn a temporally and spatially continuous vector field which assigns a motion vector to every point in space and time. In order to perform dense 4D reconstruction from images or sparse point clouds, we combine our method with a continuous 3D representation. Implicitly, our model yields correspondences over time, thus enabling fast inference while providing a sound physical description of the temporal dynamics. We show that our method can be used for interpolation and reconstruction tasks, and demonstrate the accuracy of the learned correspondences. We believe that Occupancy Flow is a promising new 4D representation which will be useful for a variety of spatio-temporal reconstruction tasks.",65,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnucLoBtsV4XpWH8QTfo_tqTthd-FXM7fmOIlKawfZ8Kq3BBdUyuXDu4oPh8tD8K_r0s
7/19/2021 21:31:35,Learning to Infer Implicit Surfaces without 3D Supervision,,11/2/2019,https://arxiv.org/pdf/1911.00767.pdf,,,,,,,"@inproceedings{liu2019learning,
  PUBLISHER = {Curran Associates, Inc.},
  BOOKTITLE = {Advances in Neural Information Processing Systems (NeurIPS)},
  ID = {liu2019learning},
  ENTRYTYPE = {inproceedings},
  AUTHOR = {Shichen Liu and Shunsuke Saito and Weikai Chen and Hao Li},
  TITLE = {Learning to Infer Implicit Surfaces without 3D Supervision},
  EPRINT = {1911.00767v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Recent advances in 3D deep learning have shown that it is possible to train highly effective deep models for 3D shape generation, directly from 2D images. This is particularly interesting since the availability of 3D models is still limited compared to the massive amount of accessible 2D images, which is invaluable for training. The representation of 3D surfaces itself is a key factor for the quality and resolution of the 3D output. While explicit representations, such as point clouds and voxels, can span a wide range of shape variations, their resolutions are often limited. Mesh-based representations are more efficient but are limited by their ability to handle varying topologies. Implicit surfaces, however, can robustly handle complex shapes, topologies, and also provide flexible resolution control. We address the fundamental problem of learning implicit surfaces for shape inference without the need of 3D supervision. Despite their advantages, it remains nontrivial to (1) formulate a differentiable connection between implicit surfaces and their 2D renderings, which is needed for image-based supervision; and (2) ensure precise geometric properties and control, such as local smoothness. In particular, sampling implicit surfaces densely is also known to be a computationally demanding and very slow operation. To this end, we propose a novel ray-based field probing technique for efficient image-to-field supervision, as well as a general geometric regularizer for implicit surfaces, which provides natural shape priors in unconstrained regions. We demonstrate the effectiveness of our framework on the task of single-view image-based 3D shape digitization and show how we outperform state-of-the-art techniques both quantitatively and qualitatively.},
  YEAR = {2019},
  URL = {http://arxiv.org/abs/1911.00767v1},
  FILE = {1911.00767v1.pdf}
 }",Fundamentals,"Representation, Sampling",None,Occupancy,,,,,,,,NeurIPS 2019,,Yes,Direct,"Shichen Liu, Shunsuke Saito, Weikai Chen, Hao Li",liu2019learning,00000011,"Recent advances in 3D deep learning have shown that it is possible to train highly effective deep models for 3D shape generation, directly from 2D images. This is particularly interesting since the availability of 3D models is still limited compared to the massive amount of accessible 2D images, which is invaluable for training. The representation of 3D surfaces itself is a key factor for the quality and resolution of the 3D output. While explicit representations, such as point clouds and voxels, can span a wide range of shape variations, their resolutions are often limited. Mesh-based representations are more efficient but are limited by their ability to handle varying topologies. Implicit surfaces, however, can robustly handle complex shapes, topologies, and also provide flexible resolution control. We address the fundamental problem of learning implicit surfaces for shape inference without the need of 3D supervision. Despite their advantages, it remains nontrivial to (1) formulate a differentiable connection between implicit surfaces and their 2D renderings, which is needed for image-based supervision; and (2) ensure precise geometric properties and control, such as local smoothness. In particular, sampling implicit surfaces densely is also known to be a computationally demanding and very slow operation. To this end, we propose a novel ray-based field probing technique for efficient image-to-field supervision, as well as a general geometric regularizer for implicit surfaces, which provides natural shape priors in unconstrained regions. We demonstrate the effectiveness of our framework on the task of single-view image-based 3D shape digitization and show how we outperform state-of-the-art techniques both quantitatively and qualitatively.",69,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnud-Oh-fyxVSOD7ivE12s_etP1s-4t3G4Bzfqta6cXWVObfDnsCwnMC1Q3zXgMwgKdg
6/29/2021 16:19:49,SAL: Sign Agnostic Learning of Shapes from Raw Data,SAL,11/23/2019,https://arxiv.org/pdf/1911.10414.pdf,,https://github.com/matanatz/SAL,,,,,"@inproceedings{atzmon2020sal,
  BOOKTITLE = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  ID = {atzmon2020sal},
  ENTRYTYPE = {inproceedings},
  AUTHOR = {Matan Atzmon and Yaron Lipman},
  TITLE = {SAL: Sign Agnostic Learning of Shapes from Raw Data},
  EPRINT = {1911.10414v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Recently, neural networks have been used as implicit representations for surface reconstruction, modelling, learning, and generation. So far, training neural networks to be implicit representations of surfaces required training data sampled from a ground-truth signed implicit functions such as signed distance or occupancy functions, which are notoriously hard to compute. In this paper we introduce Sign Agnostic Learning (SAL), a deep learning approach for learning implicit shape representations directly from raw, unsigned geometric data, such as point clouds and triangle soups. We have tested SAL on the challenging problem of surface reconstruction from an un-oriented point cloud, as well as end-to-end human shape space learning directly from raw scans dataset, and achieved state of the art reconstructions compared to current approaches. We believe SAL opens the door to many geometric deep learning applications with real-world data, alleviating the usual painstaking, often manual pre-process.},
  YEAR = {2020},
  URL = {http://arxiv.org/abs/1911.10414v2},
  FILE = {1911.10414v2.pdf}
 }",,,,SDF,Per-scene,,,,,,,CVPR 2020,,Yes,,"Matan Atzmon, Yaron Lipman",atzmon2020sal,00000012,"Recently, neural networks have been used as implicit representations for surface reconstruction, modelling, learning, and generation. So far, training neural networks to be implicit representations of surfaces required training data sampled from a ground-truth signed implicit functions such as signed distance or occupancy functions, which are notoriously hard to compute. In this paper we introduce Sign Agnostic Learning (SAL), a deep learning approach for learning implicit shape representations directly from raw, unsigned geometric data, such as point clouds and triangle soups. We have tested SAL on the challenging problem of surface reconstruction from an un-oriented point cloud, as well as end-to-end human shape space learning directly from raw scans dataset, and achieved state of the art reconstructions compared to current approaches. We believe SAL opens the door to many geometric deep learning applications with real-world data, alleviating the usual painstaking, often manual pre-process.",68,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnufuPXmJWqL2T5e91KneRVXLaE6EgNa744eFUeYRdvS8OFRbF5kjJYxLUcgrGIVKJv0
8/29/2021 20:20:23,DIST: Rendering Deep Implicit Signed Distance Function with Differentiable Sphere Tracing,DIST,11/29/2019,https://arxiv.org/pdf/1911.13225.pdf,http://b1ueber2y.me/projects/DIST-Renderer/,https://github.com/B1ueber2y/DIST-Renderer,https://www.youtube.com/watch?v=KjfNS1mnqoM,http://b1ueber2y.me/projects/DIST-Renderer/dist-supp.pdf,,,"@inproceedings{liu2020dist,
  BOOKTITLE = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  ID = {liu2020dist},
  ENTRYTYPE = {inproceedings},
  AUTHOR = {Shaohui Liu and Yinda Zhang and Songyou Peng and Boxin Shi and Marc Pollefeys and Zhaopeng Cui},
  TITLE = {DIST: Rendering Deep Implicit Signed Distance Function with Differentiable Sphere Tracing},
  EPRINT = {1911.13225v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We propose a differentiable sphere tracing algorithm to bridge the gap between inverse graphics methods and the recently proposed deep learning based implicit signed distance function. Due to the nature of the implicit function, the rendering process requires tremendous function queries, which is particularly problematic when the function is represented as a neural network. We optimize both the forward and backward passes of our rendering layer to make it run efficiently with affordable memory consumption on a commodity graphics card. Our rendering method is fully differentiable such that losses can be directly computed on the rendered 2D observations, and the gradients can be propagated backwards to optimize the 3D geometry. We show that our rendering method can effectively reconstruct accurate 3D shapes from various inputs, such as sparse depth and multi-view images, through inverse optimization. With the geometry based reasoning, our 3D shape prediction methods show excellent generalization capability and robustness against various noises.},
  YEAR = {2020},
  URL = {http://arxiv.org/abs/1911.13225v2},
  FILE = {1911.13225v2.pdf}
 }",Fundamentals,Data-driven,None,SDF,Category-level,,,,,,,CVPR 2020,,Yes,Direct,"Shaohui Liu, Yinda Zhang, Songyou Peng, Boxin Shi, Marc Pollefeys, Zhaopeng Cui",liu2020dist,00000177,"We propose a differentiable sphere tracing algorithm to bridge the gap between inverse graphics methods and the recently proposed deep learning based implicit signed distance function. Due to the nature of the implicit function, the rendering process requires tremendous function queries, which is particularly problematic when the function is represented as a neural network. We optimize both the forward and backward passes of our rendering layer to make it run efficiently with affordable memory consumption on a commodity graphics card. Our rendering method is fully differentiable such that losses can be directly computed on the rendered 2D observations, and the gradients can be propagated backwards to optimize the 3D geometry. We show that our rendering method can effectively reconstruct accurate 3D shapes from various inputs, such as sparse depth and multi-view images, through inverse optimization. With the geometry based reasoning, our 3D shape prediction methods show excellent generalization capability and robustness against various noises.",68,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnucSv_N_1mcoMDYwuoGvgJZ-coaXPLwWIPJ97ZfIa73DvfmfZlitc_9OiJMGtx3LSJ8
9/17/2021 11:55:52,NASA: Neural Articulated Shape Approximation,NASA,12/6/2019,https://arxiv.org/pdf/1912.03207.pdf,,,,,,,"@inproceedings{deng2020nasa,
  BOOKTITLE = {Proceedings of the European Conference on Computer Vision (ECCV)},
  ID = {deng2020nasa},
  ENTRYTYPE = {inproceedings},
  AUTHOR = {Boyang Deng and JP Lewis and Timothy Jeruzalski and Gerard Pons-Moll and Geoffrey Hinton and Mohammad Norouzi and Andrea Tagliasacchi},
  TITLE = {NASA: Neural Articulated Shape Approximation},
  EPRINT = {1912.03207v4},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Efficient representation of articulated objects such as human bodies is an important problem in computer vision and graphics. To efficiently simulate deformation, existing approaches represent 3D objects using polygonal meshes and deform them using skinning techniques. This paper introduces neural articulated shape approximation (NASA), an alternative framework that enables efficient representation of articulated deformable objects using neural indicator functions that are conditioned on pose. Occupancy testing using NASA is straightforward, circumventing the complexity of meshes and the issue of water-tightness. We demonstrate the effectiveness of NASA for 3D tracking applications, and discuss other potential extensions.},
  YEAR = {2020},
  URL = {http://arxiv.org/abs/1912.03207v4},
  FILE = {1912.03207v4.pdf}
 }",Human body,Articulated,,Occupancy,Category-level,,,,,,,ECCV 2020,,Yes,Direct,"Boyang Deng, JP Lewis, Timothy Jeruzalski, Gerard Pons-Moll, Geoffrey Hinton, Mohammad Norouzi, Andrea Tagliasacchi",deng2020nasa,00000199,"Efficient representation of articulated objects such as human bodies is an important problem in computer vision and graphics. To efficiently simulate deformation, existing approaches represent 3D objects using polygonal meshes and deform them using skinning techniques. This paper introduces neural articulated shape approximation (NASA), an alternative framework that enables efficient representation of articulated deformable objects using neural indicator functions that are conditioned on pose. Occupancy testing using NASA is straightforward, circumventing the complexity of meshes and the issue of water-tightness. We demonstrate the effectiveness of NASA for 3D tracking applications, and discuss other potential extensions.",,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnucxBBsUM7zxUOUHkZfLi4G7EGr12VqOFqm2_uTwGYxYQav94kO8GbhHDfgDpPi1xJE
6/29/2021 16:55:27,Local Deep Implicit Functions for 3D Shape,LDIF,12/12/2019,https://arxiv.org/pdf/1912.06126.pdf,https://ldif.cs.princeton.edu/,https://github.com/google/ldif,https://www.youtube.com/watch?v=3RAITzNWVJs,,,,"@inproceedings{genova2020ldif,
  BOOKTITLE = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  ID = {genova2020ldif},
  ENTRYTYPE = {inproceedings},
  AUTHOR = {Kyle Genova and Forrester Cole and Avneesh Sud and Aaron Sarna and Thomas Funkhouser},
  TITLE = {Local Deep Implicit Functions for 3D Shape},
  EPRINT = {1912.06126v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {The goal of this project is to learn a 3D shape representation that enables accurate surface reconstruction, compact storage, efficient computation, consistency for similar shapes, generalization across diverse shape categories, and inference from depth camera observations. Towards this end, we introduce Local Deep Implicit Functions (LDIF), a 3D shape representation that decomposes space into a structured set of learned implicit functions. We provide networks that infer the space decomposition and local deep implicit functions from a 3D mesh or posed depth image. During experiments, we find that it provides 10.3 points higher surface reconstruction accuracy (F-Score) than the state-of-the-art (OccNet), while requiring fewer than 1 percent of the network parameters. Experiments on posed depth image completion and generalization to unseen classes show 15.8 and 17.8 point improvements over the state-of-the-art, while producing a structured 3D representation for each input with consistency across diverse shape collections.},
  YEAR = {2020},
  URL = {http://arxiv.org/abs/1912.06126v2},
  FILE = {1912.06126v2.pdf}
 }",Human body,"Conditional neural field, Representation, Data-driven",None,Occupancy,Category-level,,,,,,,CVPR 2020,,Yes,,"Kyle Genova, Forrester Cole, Avneesh Sud, Aaron Sarna, Thomas Funkhouser",genova2020ldif,00000013,"The goal of this project is to learn a 3D shape representation that enables accurate surface reconstruction, compact storage, efficient computation, consistency for similar shapes, generalization across diverse shape categories, and inference from depth camera observations. Towards this end, we introduce Local Deep Implicit Functions (LDIF), a 3D shape representation that decomposes space into a structured set of learned implicit functions. We provide networks that infer the space decomposition and local deep implicit functions from a 3D mesh or posed depth image. During experiments, we find that it provides 10.3 points higher surface reconstruction accuracy (F-Score) than the state-of-the-art (OccNet), while requiring fewer than 1 percent of the network parameters. Experiments on posed depth image completion and generalization to unseen classes show 15.8 and 17.8 point improvements over the state-of-the-art, while producing a structured 3D representation for each input with consistency across diverse shape collections.",70,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnud0NOtW3Us7A67Nz7L1V_gDGI8rCiFPD9tFO9fIct-W2f3R8N0u35RxdhggiZ6vTC4
7/7/2021 17:49:56,Differentiable Volumetric Rendering: Learning Implicit 3D Representations without 3D Supervision,DVR,12/16/2019,https://arxiv.org/pdf/1912.07372.pdf,https://www.youtube.com/watch?v=U_jIN3qWVEw,https://github.com/autonomousvision/differentiable_volumetric_rendering,https://www.youtube.com/watch?v=U_jIN3qWVEw,http://www.cvlibs.net/publications/Niemeyer2020CVPR_supplementary.pdf,https://www.youtube.com/watch?v=lcub1KH-mmk,,"@inproceedings{niemeyer2020dvr,
  BOOKTITLE = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  ID = {niemeyer2020dvr},
  ENTRYTYPE = {inproceedings},
  AUTHOR = {Michael Niemeyer and Lars Mescheder and Michael Oechsle and Andreas Geiger},
  TITLE = {Differentiable Volumetric Rendering: Learning Implicit 3D Representations without 3D Supervision},
  EPRINT = {1912.07372v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Learning-based 3D reconstruction methods have shown impressive results. However, most methods require 3D supervision which is often hard to obtain for real-world datasets. Recently, several works have proposed differentiable rendering techniques to train reconstruction models from RGB images. Unfortunately, these approaches are currently restricted to voxel- and mesh-based representations, suffering from discretization or low resolution. In this work, we propose a differentiable rendering formulation for implicit shape and texture representations. Implicit representations have recently gained popularity as they represent shape and texture continuously. Our key insight is that depth gradients can be derived analytically using the concept of implicit differentiation. This allows us to learn implicit shape and texture representations directly from RGB images. We experimentally show that our single-view reconstructions rival those learned with full 3D supervision. Moreover, we find that our method can be used for multi-view 3D reconstruction, directly resulting in watertight meshes.},
  YEAR = {2020},
  URL = {http://arxiv.org/abs/1912.07372v2},
  FILE = {1912.07372v2.pdf}
 }",,Conditional neural field,,Occupancy,,,,,,,,CVPR 2020,,No,,"Michael Niemeyer, Lars Mescheder, Michael Oechsle, Andreas Geiger",niemeyer2020dvr,00000014,"Learning-based 3D reconstruction methods have shown impressive results. However, most methods require 3D supervision which is often hard to obtain for real-world datasets. Recently, several works have proposed differentiable rendering techniques to train reconstruction models from RGB images. Unfortunately, these approaches are currently restricted to voxel- and mesh-based representations, suffering from discretization or low resolution. In this work, we propose a differentiable rendering formulation for implicit shape and texture representations. Implicit representations have recently gained popularity as they represent shape and texture continuously. Our key insight is that depth gradients can be derived analytically using the concept of implicit differentiation. This allows us to learn implicit shape and texture representations directly from RGB images. We experimentally show that our single-view reconstructions rival those learned with full 3D supervision. Moreover, we find that our method can be used for multi-view 3D reconstruction, directly resulting in watertight meshes.",142,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnueaYgL3YRw3Cfa4MN2bTCh6_mHXH5Mt9hAYUZkmYozqr36xsY1sNkUKCtl0tPReSuY
6/29/2021 16:32:13,Implicit Geometric Regularization for Learning Shapes,IGR,2/24/2020,https://arxiv.org/pdf/2002.10099.pdf,,https://github.com/amosgropp/IGR,https://www.youtube.com/watch?v=6cOvBGBQF9g,,,,"@inproceedings{gropp2020igr,
  BOOKTITLE = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  ID = {gropp2020igr},
  ENTRYTYPE = {inproceedings},
  AUTHOR = {Amos Gropp and Lior Yariv and Niv Haim and Matan Atzmon and Yaron Lipman},
  TITLE = {Implicit Geometric Regularization for Learning Shapes},
  EPRINT = {2002.10099v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.LG},
  ABSTRACT = {Representing shapes as level sets of neural networks has been recently proved to be useful for different shape analysis and reconstruction tasks. So far, such representations were computed using either: (i) pre-computed implicit shape representations; or (ii) loss functions explicitly defined over the neural level sets. In this paper we offer a new paradigm for computing high fidelity implicit neural representations directly from raw data (i.e., point clouds, with or without normal information). We observe that a rather simple loss function, encouraging the neural network to vanish on the input point cloud and to have a unit norm gradient, possesses an implicit geometric regularization property that favors smooth and natural zero level set surfaces, avoiding bad zero-loss solutions. We provide a theoretical analysis of this property for the linear case, and show that, in practice, our method leads to state of the art implicit neural representations with higher level-of-details and fidelity compared to previous methods.},
  YEAR = {2020},
  URL = {http://arxiv.org/abs/2002.10099v2},
  FILE = {2002.10099v2.pdf}
 }","Human body, Generalization, Fundamentals",,None,SDF,,,,,,,,CVPR 2020,,Yes,,"Amos Gropp, Lior Yariv, Niv Haim, Matan Atzmon, Yaron Lipman",gropp2020igr,00000015,"Representing shapes as level sets of neural networks has been recently proved to be useful for different shape analysis and reconstruction tasks. So far, such representations were computed using either: (i) pre-computed implicit shape representations; or (ii) loss functions explicitly defined over the neural level sets. In this paper we offer a new paradigm for computing high fidelity implicit neural representations directly from raw data (i.e., point clouds, with or without normal information). We observe that a rather simple loss function, encouraging the neural network to vanish on the input point cloud and to have a unit norm gradient, possesses an implicit geometric regularization property that favors smooth and natural zero level set surfaces, avoiding bad zero-loss solutions. We provide a theoretical analysis of this property for the linear case, and show that, in practice, our method leads to state of the art implicit neural representations with higher level-of-details and fidelity compared to previous methods.",65,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnucK-qvSkUpnWszOx8LNQ2oWAVukBIPL2UDQi3XsH1MjyKbSt0zpsVnxhSXzbt3B95g
7/19/2021 21:08:05,Implicit Functions in Feature Space for 3D Shape Reconstruction and Completion,IF-Net,3/3/2020,https://arxiv.org/pdf/2003.01456.pdf,https://virtualhumans.mpi-inf.mpg.de/ifnets/,https://github.com/jchibane/if-net,https://www.youtube.com/watch?v=cko07jINRZg,http://virtualhumans.mpi-inf.mpg.de/papers/chibane20ifnet/chibane20ifnet_supp.pdf,,,"@inproceedings{chibane2020ifnet,
  BOOKTITLE = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  ID = {chibane2020ifnet},
  ENTRYTYPE = {inproceedings},
  AUTHOR = {Julian Chibane and Thiemo Alldieck and Gerard Pons-Moll},
  TITLE = {Implicit Functions in Feature Space for 3D Shape Reconstruction and Completion},
  EPRINT = {2003.01456v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {While many works focus on 3D reconstruction from images, in this paper, we focus on 3D shape reconstruction and completion from a variety of 3D inputs, which are deficient in some respect: low and high resolution voxels, sparse and dense point clouds, complete or incomplete. Processing of such 3D inputs is an increasingly important problem as they are the output of 3D scanners, which are becoming more accessible, and are the intermediate output of 3D computer vision algorithms. Recently, learned implicit functions have shown great promise as they produce continuous reconstructions. However, we identified two limitations in reconstruction from 3D inputs: 1) details present in the input data are not retained, and 2) poor reconstruction of articulated humans. To solve this, we propose Implicit Feature Networks (IF-Nets), which deliver continuous outputs, can handle multiple topologies, and complete shapes for missing or sparse input data retaining the nice properties of recent learned implicit functions, but critically they can also retain detail when it is present in the input data, and can reconstruct articulated humans. Our work differs from prior work in two crucial aspects. First, instead of using a single vector to encode a 3D shape, we extract a learnable 3-dimensional multi-scale tensor of deep features, which is aligned with the original Euclidean space embedding the shape. Second, instead of classifying x-y-z point coordinates directly, we classify deep features extracted from the tensor at a continuous query point. We show that this forces our model to make decisions based on global and local shape structure, as opposed to point coordinates, which are arbitrary under Euclidean transformations. Experiments demonstrate that IF-Nets clearly outperform prior work in 3D object reconstruction in ShapeNet, and obtain significantly more accurate 3D human reconstructions.},
  YEAR = {2020},
  NOTE = {{IEEE} Conference on Computer Vision and Pattern Recognition (CVPR) 2020},
  URL = {http://arxiv.org/abs/2003.01456v2},
  FILE = {2003.01456v2.pdf}
 }",Generalization,"Feature volume, Data-driven",None,Occupancy,,,,,,,,CVPR 2020,,Yes,,"Julian Chibane, Thiemo Alldieck, Gerard Pons-Moll",chibane2020ifnet,00000016,"While many works focus on 3D reconstruction from images, in this paper, we focus on 3D shape reconstruction and completion from a variety of 3D inputs, which are deficient in some respect: low and high resolution voxels, sparse and dense point clouds, complete or incomplete. Processing of such 3D inputs is an increasingly important problem as they are the output of 3D scanners, which are becoming more accessible, and are the intermediate output of 3D computer vision algorithms. Recently, learned implicit functions have shown great promise as they produce continuous reconstructions. However, we identified two limitations in reconstruction from 3D inputs: 1) details present in the input data are not retained, and 2) poor reconstruction of articulated humans. To solve this, we propose Implicit Feature Networks (IF-Nets), which deliver continuous outputs, can handle multiple topologies, and complete shapes for missing or sparse input data retaining the nice properties of recent learned implicit functions, but critically they can also retain detail when it is present in the input data, and can reconstruct articulated humans. Our work differs from prior work in two crucial aspects. First, instead of using a single vector to encode a 3D shape, we extract a learnable 3-dimensional multi-scale tensor of deep features, which is aligned with the original Euclidean space embedding the shape. Second, instead of classifying x-y-z point coordinates directly, we classify deep features extracted from the tensor at a continuous query point. We show that this forces our model to make decisions based on global and local shape structure, as opposed to point coordinates, which are arbitrary under Euclidean transformations. Experiments demonstrate that IF-Nets clearly outperform prior work in 3D object reconstruction in ShapeNet, and obtain significantly more accurate 3D human reconstructions.",86,Yes,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnuf_ZfZLbi1YiQ1xg5UdIFo3Ss6BmrIuaSBjkiz9dJIlGfkw7nO1drbjs35YZ1HJKVo
5/23/2021 19:21:26,Convolutional Occupancy Networks,,3/10/2020,https://arxiv.org/pdf/2003.04618.pdf,https://pengsongyou.github.io/conv_onet,https://github.com/autonomousvision/convolutional_occupancy_networks,"https://www.youtube.com/watch?v=k0monzIcjUo, https://www.youtube.com/watch?v=EmauovgrDSM",http://www.cvlibs.net/publications/Peng2020ECCV_supplementary.pdf,,,"@inproceedings{peng2020convolutional,
  BOOKTITLE = {Proceedings of the European Conference on Computer Vision (ECCV)},
  ID = {peng2020convolutional},
  ENTRYTYPE = {inproceedings},
  AUTHOR = {Songyou Peng and Michael Niemeyer and Lars Mescheder and Marc Pollefeys and Andreas Geiger},
  TITLE = {Convolutional Occupancy Networks},
  EPRINT = {2003.04618v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Recently, implicit neural representations have gained popularity for learning-based 3D reconstruction. While demonstrating promising results, most implicit approaches are limited to comparably simple geometry of single objects and do not scale to more complicated or large-scale scenes. The key limiting factor of implicit methods is their simple fully-connected network architecture which does not allow for integrating local information in the observations or incorporating inductive biases such as translational equivariance. In this paper, we propose Convolutional Occupancy Networks, a more flexible implicit representation for detailed reconstruction of objects and 3D scenes. By combining convolutional encoders with implicit occupancy decoders, our model incorporates inductive biases, enabling structured reasoning in 3D space. We investigate the effectiveness of the proposed representation by reconstructing complex geometry from noisy point clouds and low-resolution voxel representations. We empirically find that our method enables the fine-grained implicit 3D reconstruction of single objects, scales to large indoor scenes, and generalizes well from synthetic to real data.},
  YEAR = {2020},
  URL = {http://arxiv.org/abs/2003.04618v2},
  FILE = {2003.04618v2.pdf}
 }",,Feature volume,,Occupancy,,,,,,,,ECCV 2020,,Yes,,"Songyou Peng, Michael Niemeyer, Lars Mescheder, Marc Pollefeys, Andreas Geiger",peng2020convolutional,00000017,"Recently, implicit neural representations have gained popularity for learning-based 3D reconstruction. While demonstrating promising results, most implicit approaches are limited to comparably simple geometry of single objects and do not scale to more complicated or large-scale scenes. The key limiting factor of implicit methods is their simple fully-connected network architecture which does not allow for integrating local information in the observations or incorporating inductive biases such as translational equivariance. In this paper, we propose Convolutional Occupancy Networks, a more flexible implicit representation for detailed reconstruction of objects and 3D scenes. By combining convolutional encoders with implicit occupancy decoders, our model incorporates inductive biases, enabling structured reasoning in 3D space. We investigate the effectiveness of the proposed representation by reconstructing complex geometry from noisy point clouds and low-resolution voxel representations. We empirically find that our method enables the fine-grained implicit 3D reconstruction of single objects, scales to large indoor scenes, and generalizes well from synthetic to real data.",99,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnudXHuBDr4Q8p_15m5RK3TztFHuKAcvQUzQbPetpBJZfk9Vu9KHDJ_DJD7ECAj47iRM
5/23/2021 19:13:45,NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis,NeRF,3/19/2020,https://arxiv.org/pdf/2003.08934.pdf,https://www.matthewtancik.com/nerf,https://github.com/bmild/nerf,https://www.youtube.com/watch?v=JuH79E8rdKc,,,,"@inproceedings{mildenhall2020nerf,
  BOOKTITLE = {Proceedings of the European Conference on Computer Vision (ECCV)},
  ID = {mildenhall2020nerf},
  ENTRYTYPE = {inproceedings},
  AUTHOR = {Ben Mildenhall and Pratul P. Srinivasan and Matthew Tancik and Jonathan T. Barron and Ravi Ramamoorthi and Ren Ng},
  TITLE = {NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis},
  EPRINT = {2003.08934v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We present a method that achieves state-of-the-art results for synthesizing novel views of complex scenes by optimizing an underlying continuous volumetric scene function using a sparse set of input views. Our algorithm represents a scene using a fully-connected (non-convolutional) deep network, whose input is a single continuous 5D coordinate (spatial location $(x,y,z)$ and viewing direction $(\theta, \phi)$) and whose output is the volume density and view-dependent emitted radiance at that spatial location. We synthesize views by querying 5D coordinates along camera rays and use classic volume rendering techniques to project the output colors and densities into an image. Because volume rendering is naturally differentiable, the only input required to optimize our representation is a set of images with known camera poses. We describe how to effectively optimize neural radiance fields to render photorealistic novel views of scenes with complicated geometry and appearance, and demonstrate results that outperform prior work on neural rendering and view synthesis. View synthesis results are best viewed as videos, so we urge readers to view our supplementary video for convincing comparisons.},
  YEAR = {2020},
  URL = {http://arxiv.org/abs/2003.08934v2},
  FILE = {2003.08934v2.pdf}
 }",,Sampling,,,,,,,,,,ECCV 2020,https://drive.google.com/drive/folders/128yBriW1IG_3NJ5Rp7APSTZsJqdJdfc1,No,,"Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, Ren Ng",mildenhall2020nerf,00000018,"We present a method that achieves state-of-the-art results for synthesizing novel views of complex scenes by optimizing an underlying continuous volumetric scene function using a sparse set of input views. Our algorithm represents a scene using a fully-connected (non-convolutional) deep network, whose input is a single continuous 5D coordinate (spatial location $(x,y,z)$ and viewing direction $(\theta, \phi)$) and whose output is the volume density and view-dependent emitted radiance at that spatial location. We synthesize views by querying 5D coordinates along camera rays and use classic volume rendering techniques to project the output colors and densities into an image. Because volume rendering is naturally differentiable, the only input required to optimize our representation is a set of images with known camera poses. We describe how to effectively optimize neural radiance fields to render photorealistic novel views of scenes with complicated geometry and appearance, and demonstrate results that outperform prior work on neural rendering and view synthesis. View synthesis results are best viewed as videos, so we urge readers to view our supplementary video for convincing comparisons.",366,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnucKxcWv5YPEuxLZqt2UJ82MumR-k667T611Wse7OMzzd73j4GkWQTdYAbMc4EB9lHw
5/23/2021 19:12:39,Multiview Neural Surface Reconstruction by Disentangling Geometry and Appearance,IDR,3/22/2020,https://arxiv.org/pdf/2003.09852.pdf,,https://github.com/lioryariv/idr,,,,,"@inproceedings{yariv2020idr,
  PUBLISHER = {Curran Associates, Inc.},
  BOOKTITLE = {Advances in Neural Information Processing Systems (NeurIPS)},
  ID = {yariv2020idr},
  ENTRYTYPE = {inproceedings},
  AUTHOR = {Lior Yariv and Yoni Kasten and Dror Moran and Meirav Galun and Matan Atzmon and Ronen Basri and Yaron Lipman},
  TITLE = {Multiview Neural Surface Reconstruction by Disentangling Geometry and Appearance},
  EPRINT = {2003.09852v3},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {In this work we address the challenging problem of multiview 3D surface reconstruction. We introduce a neural network architecture that simultaneously learns the unknown geometry, camera parameters, and a neural renderer that approximates the light reflected from the surface towards the camera. The geometry is represented as a zero level-set of a neural network, while the neural renderer, derived from the rendering equation, is capable of (implicitly) modeling a wide set of lighting conditions and materials. We trained our network on real world 2D images of objects with different material properties, lighting conditions, and noisy camera initializations from the DTU MVS dataset. We found our model to produce state of the art 3D surface reconstructions with high fidelity, resolution and detail.},
  YEAR = {2020},
  URL = {http://arxiv.org/abs/2003.09852v3},
  FILE = {2003.09852v3.pdf}
 }",Material/lighting estimation,,,SDF,,,,,,,,NeurIPS 2020 (spotlight),,No,,"Lior Yariv, Yoni Kasten, Dror Moran, Meirav Galun, Matan Atzmon, Ronen Basri, Yaron Lipman",yariv2020idr,00000019,"In this work we address the challenging problem of multiview 3D surface reconstruction. We introduce a neural network architecture that simultaneously learns the unknown geometry, camera parameters, and a neural renderer that approximates the light reflected from the surface towards the camera. The geometry is represented as a zero level-set of a neural network, while the neural renderer, derived from the rendering equation, is capable of (implicitly) modeling a wide set of lighting conditions and materials. We trained our network on real world 2D images of objects with different material properties, lighting conditions, and noisy camera initializations from the DTU MVS dataset. We found our model to produce state of the art 3D surface reconstructions with high fidelity, resolution and detail.",52,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnufODFVKNyzjLhfblkhTNH0QkXBma935Iecu45JjCKNfBPMoIpSbezUV9eVH6uytPfs
6/29/2021 16:58:44,Deep Local Shapes: Learning Local SDF Priors for Detailed 3D Reconstruction,DeepLS,3/24/2020,https://arxiv.org/pdf/2003.10983.pdf,,,,,,,"@inproceedings{chabra2020deepls,
  BOOKTITLE = {Proceedings of the European Conference on Computer Vision (ECCV)},
  ID = {chabra2020deepls},
  ENTRYTYPE = {inproceedings},
  AUTHOR = {Rohan Chabra and Jan Eric Lenssen and Eddy Ilg and Tanner Schmidt and Julian Straub and Steven Lovegrove and Richard Newcombe},
  TITLE = {Deep Local Shapes: Learning Local SDF Priors for Detailed 3D Reconstruction},
  EPRINT = {2003.10983v3},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Efficiently reconstructing complex and intricate surfaces at scale is a long-standing goal in machine perception. To address this problem we introduce Deep Local Shapes (DeepLS), a deep shape representation that enables encoding and reconstruction of high-quality 3D shapes without prohibitive memory requirements. DeepLS replaces the dense volumetric signed distance function (SDF) representation used in traditional surface reconstruction systems with a set of locally learned continuous SDFs defined by a neural network, inspired by recent work such as DeepSDF. Unlike DeepSDF, which represents an object-level SDF with a neural network and a single latent code, we store a grid of independent latent codes, each responsible for storing information about surfaces in a small local neighborhood. This decomposition of scenes into local shapes simplifies the prior distribution that the network must learn, and also enables efficient inference. We demonstrate the effectiveness and generalization power of DeepLS by showing object shape encoding and reconstructions of full scenes, where DeepLS delivers high compression, accuracy, and local shape completion.},
  YEAR = {2020},
  URL = {http://arxiv.org/abs/2003.10983v3},
  FILE = {2003.10983v3.pdf}
 }",Generalization,Voxelization,None,SDF,Category-level,,,,,,,ECCV 2020,,Yes,,"Rohan Chabra, Jan Eric Lenssen, Eddy Ilg, Tanner Schmidt, Julian Straub, Steven Lovegrove, Richard Newcombe",chabra2020deepls,00000020,"Efficiently reconstructing complex and intricate surfaces at scale is a long-standing goal in machine perception. To address this problem we introduce Deep Local Shapes (DeepLS), a deep shape representation that enables encoding and reconstruction of high-quality 3D shapes without prohibitive memory requirements. DeepLS replaces the dense volumetric signed distance function (SDF) representation used in traditional surface reconstruction systems with a set of locally learned continuous SDFs defined by a neural network, inspired by recent work such as DeepSDF. Unlike DeepSDF, which represents an object-level SDF with a neural network and a single latent code, we store a grid of independent latent codes, each responsible for storing information about surfaces in a small local neighborhood. This decomposition of scenes into local shapes simplifies the prior distribution that the network must learn, and also enables efficient inference. We demonstrate the effectiveness and generalization power of DeepLS by showing object shape encoding and reconstructions of full scenes, where DeepLS delivers high compression, accuracy, and local shape completion.",61,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnueuo3x0dYSpqD8MtAaQsUB25TUCG_zMh6a-1FZGNKC7CyzBXQIDz9RsQa5F70LU8_E
9/18/2021 9:39:17,EikoNet: Solving the Eikonal equation with Deep Neural Networks,EikoNet,3/25/2020,https://arxiv.org/pdf/2004.00361.pdf,,https://github.com/Ulvetanna/EikoNet,,,,,"@article{smith2020eikonet,
  ID = {smith2020eikonet},
  ENTRYTYPE = {article},
  AUTHOR = {Jonathan D. Smith and Kamyar Azizzadenesheli and Zachary E. Ross},
  TITLE = {EikoNet: Solving the Eikonal equation with Deep Neural Networks},
  EPRINT = {2004.00361v3},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {physics.comp-ph},
  ABSTRACT = {The recent deep learning revolution has created an enormous opportunity for accelerating compute capabilities in the context of physics-based simulations. Here, we propose EikoNet, a deep learning approach to solving the Eikonal equation, which characterizes the first-arrival-time field in heterogeneous 3D velocity structures. Our grid-free approach allows for rapid determination of the travel time between any two points within a continuous 3D domain. These travel time solutions are allowed to violate the differential equation - which casts the problem as one of optimization - with the goal of finding network parameters that minimize the degree to which the equation is violated. In doing so, the method exploits the differentiability of neural networks to calculate the spatial gradients analytically, meaning the network can be trained on its own without ever needing solutions from a finite difference algorithm. EikoNet is rigorously tested on several velocity models and sampling methods to demonstrate robustness and versatility. Training and inference are highly parallelized, making the approach well-suited for GPUs. EikoNet has low memory overhead, and further avoids the need for travel-time lookup tables. The developed approach has important applications to earthquake hypocenter inversion, ray multi-pathing, and tomographic modeling, as well as to other fields beyond seismology where ray tracing is essential.},
  YEAR = {2020},
  MONTH = {Mar},
  URL = {http://arxiv.org/abs/2004.00361v3},
  FILE = {2004.00361v3.pdf}
 }","Beyond graphics, Science and engineering, PDE",,,,,,,,,,,IEEE Transactions on Geoscience and Remote Sensing 2020,,,Direct,"Jonathan D. Smith, Kamyar Azizzadenesheli, Zachary E. Ross",smith2020eikonet,00000219,"The recent deep learning revolution has created an enormous opportunity for accelerating compute capabilities in the context of physics-based simulations. Here, we propose EikoNet, a deep learning approach to solving the Eikonal equation, which characterizes the first-arrival-time field in heterogeneous 3D velocity structures. Our grid-free approach allows for rapid determination of the travel time between any two points within a continuous 3D domain. These travel time solutions are allowed to violate the differential equation - which casts the problem as one of optimization - with the goal of finding network parameters that minimize the degree to which the equation is violated. In doing so, the method exploits the differentiability of neural networks to calculate the spatial gradients analytically, meaning the network can be trained on its own without ever needing solutions from a finite difference algorithm. EikoNet is rigorously tested on several velocity models and sampling methods to demonstrate robustness and versatility. Training and inference are highly parallelized, making the approach well-suited for GPUs. EikoNet has low memory overhead, and further avoids the need for travel-time lookup tables. The developed approach has important applications to earthquake hypocenter inversion, ray multi-pathing, and tomographic modeling, as well as to other fields beyond seismology where ray tracing is essential.",,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnucBTg5g2tBHzeBTOM1r25YI9zt5UCACnZGBmKft9X2YGCDlLM3QuyHzDs2cM7722RM
5/23/2021 19:16:06,Semantic Implicit Neural Scene Representations With Semi-Supervised Training,,3/28/2020,https://arxiv.org/pdf/2003.12673.pdf,http://www.computationalimaging.org/publications/semantic-srn/,,https://www.youtube.com/watch?v=iVubC_ymE5w,,,,"@article{kohli2020semantic,
  ID = {kohli2020semantic},
  ENTRYTYPE = {article},
  AUTHOR = {Amit Kohli and Vincent Sitzmann and Gordon Wetzstein},
  TITLE = {Semantic Implicit Neural Scene Representations With Semi-Supervised Training},
  EPRINT = {2003.12673v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {The recent success of implicit neural scene representations has presented a viable new method for how we capture and store 3D scenes. Unlike conventional 3D representations, such as point clouds, which explicitly store scene properties in discrete, localized units, these implicit representations encode a scene in the weights of a neural network which can be queried at any coordinate to produce these same scene properties. Thus far, implicit representations have primarily been optimized to estimate only the appearance and/or 3D geometry information in a scene. We take the next step and demonstrate that an existing implicit representation (SRNs) is actually multi-modal; it can be further leveraged to perform per-point semantic segmentation while retaining its ability to represent appearance and geometry. To achieve this multi-modal behavior, we utilize a semi-supervised learning strategy atop the existing pre-trained scene representation. Our method is simple, general, and only requires a few tens of labeled 2D segmentation masks in order to achieve dense 3D semantic segmentation. We explore two novel applications for this semantically aware implicit neural scene representation: 3D novel view and semantic label synthesis given only a single input RGB image or 2D label mask, as well as 3D interpolation of appearance and semantics.},
  YEAR = {2020},
  MONTH = {Mar},
  URL = {http://arxiv.org/abs/2003.12673v2},
  FILE = {2003.12673v2.pdf}
 }",,"Generative/adversarial formulation, Conditional neural field",,,,,,,,,,3DV 2020,,No,,"Amit Kohli, Vincent Sitzmann, Gordon Wetzstein",kohli2020semantic,00000021,"The recent success of implicit neural scene representations has presented a viable new method for how we capture and store 3D scenes. Unlike conventional 3D representations, such as point clouds, which explicitly store scene properties in discrete, localized units, these implicit representations encode a scene in the weights of a neural network which can be queried at any coordinate to produce these same scene properties. Thus far, implicit representations have primarily been optimized to estimate only the appearance and/or 3D geometry information in a scene. We take the next step and demonstrate that an existing implicit representation (SRNs) is actually multi-modal; it can be further leveraged to perform per-point semantic segmentation while retaining its ability to represent appearance and geometry. To achieve this multi-modal behavior, we utilize a semi-supervised learning strategy atop the existing pre-trained scene representation. Our method is simple, general, and only requires a few tens of labeled 2D segmentation masks in order to achieve dense 3D semantic segmentation. We explore two novel applications for this semantically aware implicit neural scene representation: 3D novel view and semantic label synthesis given only a single input RGB image or 2D label mask, as well as 3D interpolation of appearance and semantics.",7,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnuc4oAW62td1yAqjr5q9X7YITQVMi3qSgHOproN38kbQPeOrAUaT4FKi_Fpg_lfIfho
6/29/2021 16:42:37,PIFuHD: Multi-Level Pixel-Aligned Implicit Function for High-Resolution 3D Human Digitization,PIFuHD,4/1/2020,https://arxiv.org/pdf/2004.00452.pdf,https://shunsukesaito.github.io/PIFuHD/,https://github.com/facebookresearch/pifuhd,"https://www.youtube.com/watch?v=uEDqCxvF5yc, https://www.youtube.com/watch?v=-1XYTmm8HhE",,,,"@inproceedings{saito2020pifuhd,
  BOOKTITLE = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  ID = {saito2020pifuhd},
  ENTRYTYPE = {inproceedings},
  AUTHOR = {Shunsuke Saito and Tomas Simon and Jason Saragih and Hanbyul Joo},
  TITLE = {PIFuHD: Multi-Level Pixel-Aligned Implicit Function for High-Resolution 3D Human Digitization},
  EPRINT = {2004.00452v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Recent advances in image-based 3D human shape estimation have been driven by the significant improvement in representation power afforded by deep neural networks. Although current approaches have demonstrated the potential in real world settings, they still fail to produce reconstructions with the level of detail often present in the input images. We argue that this limitation stems primarily form two conflicting requirements; accurate predictions require large context, but precise predictions require high resolution. Due to memory limitations in current hardware, previous approaches tend to take low resolution images as input to cover large spatial context, and produce less precise (or low resolution) 3D estimates as a result. We address this limitation by formulating a multi-level architecture that is end-to-end trainable. A coarse level observes the whole image at lower resolution and focuses on holistic reasoning. This provides context to an fine level which estimates highly detailed geometry by observing higher-resolution images. We demonstrate that our approach significantly outperforms existing state-of-the-art techniques on single image human shape reconstruction by fully leveraging 1k-resolution input images.},
  YEAR = {2020},
  NOTE = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2020},
  URL = {http://arxiv.org/abs/2004.00452v1},
  FILE = {2004.00452v1.pdf}
 }","Human body, Few-shot reconstruction, Generalization","Lifting 2D features to 3D, Image-based rendering, Coarse-to-fine, Data-driven",None,Occupancy,Category-level,,,,,,,CVPR 2020 (Oral),,No,,"Shunsuke Saito, Tomas Simon, Jason Saragih, Hanbyul Joo",saito2020pifuhd,00000022,"Recent advances in image-based 3D human shape estimation have been driven by the significant improvement in representation power afforded by deep neural networks. Although current approaches have demonstrated the potential in real world settings, they still fail to produce reconstructions with the level of detail often present in the input images. We argue that this limitation stems primarily form two conflicting requirements; accurate predictions require large context, but precise predictions require high resolution. Due to memory limitations in current hardware, previous approaches tend to take low resolution images as input to cover large spatial context, and produce less precise (or low resolution) 3D estimates as a result. We address this limitation by formulating a multi-level architecture that is end-to-end trainable. A coarse level observes the whole image at lower resolution and focuses on holistic reasoning. This provides context to an fine level which estimates highly detailed geometry by observing higher-resolution images. We demonstrate that our approach significantly outperforms existing state-of-the-art techniques on single image human shape reconstruction by fully leveraging 1k-resolution input images.",115,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnufsdL1aTwRvEhjdWWzIHXTDlfJn2_3Ou5qfqS6keEs3uMKWFAT_B-A1cDUtxHwie_s
8/29/2021 16:20:20,DualSDF: Semantic Shape Manipulation using a Two-Level Representation,DualSDF,4/6/2020,https://arxiv.org/pdf/2004.02869.pdf,https://www.cs.cornell.edu/~hadarelor/dualsdf/,https://github.com/zekunhao1995/DualSDF,https://www.youtube.com/watch?v=pAszEMLd5Xk,,https://www.youtube.com/watch?v=u40ZwDINz0A,,"@inproceedings{hao2020dualsdf,
  BOOKTITLE = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  ID = {hao2020dualsdf},
  ENTRYTYPE = {inproceedings},
  AUTHOR = {Zekun Hao and Hadar Averbuch-Elor and Noah Snavely and Serge Belongie},
  TITLE = {DualSDF: Semantic Shape Manipulation using a Two-Level Representation},
  EPRINT = {2004.02869v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We are seeing a Cambrian explosion of 3D shape representations for use in machine learning. Some representations seek high expressive power in capturing high-resolution detail. Other approaches seek to represent shapes as compositions of simple parts, which are intuitive for people to understand and easy to edit and manipulate. However, it is difficult to achieve both fidelity and interpretability in the same representation. We propose DualSDF, a representation expressing shapes at two levels of granularity, one capturing fine details and the other representing an abstracted proxy shape using simple and semantically consistent shape primitives. To achieve a tight coupling between the two representations, we use a variational objective over a shared latent space. Our two-level model gives rise to a new shape manipulation technique in which a user can interactively manipulate the coarse proxy shape and see the changes instantly mirrored in the high-resolution shape. Moreover, our model actively augments and guides the manipulation towards producing semantically meaningful shapes, making complex manipulations possible with minimal user input.},
  YEAR = {2020},
  URL = {http://arxiv.org/abs/2004.02869v1},
  FILE = {2004.02869v1.pdf}
 }",Editable,"Conditional neural field, Representation, Data-driven",,SDF,Category-level,,,,,,,CVPR 2020,,Yes,Direct,"Zekun Hao, Hadar Averbuch-Elor, Noah Snavely, Serge Belongie",hao2020dualsdf,00000163,"We are seeing a Cambrian explosion of 3D shape representations for use in machine learning. Some representations seek high expressive power in capturing high-resolution detail. Other approaches seek to represent shapes as compositions of simple parts, which are intuitive for people to understand and easy to edit and manipulate. However, it is difficult to achieve both fidelity and interpretability in the same representation. We propose DualSDF, a representation expressing shapes at two levels of granularity, one capturing fine details and the other representing an abstracted proxy shape using simple and semantically consistent shape primitives. To achieve a tight coupling between the two representations, we use a variational objective over a shared latent space. Our two-level model gives rise to a new shape manipulation technique in which a user can interactively manipulate the coarse proxy shape and see the changes instantly mirrored in the high-resolution shape. Moreover, our model actively augments and guides the manipulation towards producing semantically meaningful shapes, making complex manipulations possible with minimal user input.",21,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnue-HObu5J7VlGN99qsg14uLDkQG_UMtC_ZweSQOtxABI8ilDdlcWizYVSJk3Ni2a5M
9/17/2021 13:51:22,Robust 3D Self-portraits in Seconds,PIFusion,4/6/2020,https://arxiv.org/pdf/2004.02460.pdf,http://www.liuyebin.com/portrait/portrait.html,,https://www.youtube.com/watch?v=tayZT0exfVA,,http://www.liuyebin.com/portrait/assets/portrait.mp4,,"@inproceedings{li2020pifusion,
  BOOKTITLE = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  ID = {li2020pifusion},
  ENTRYTYPE = {inproceedings},
  AUTHOR = {Zhe Li and Tao Yu and Chuanyu Pan and Zerong Zheng and Yebin Liu},
  TITLE = {Robust 3D Self-portraits in Seconds},
  EPRINT = {2004.02460v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {In this paper, we propose an efficient method for robust 3D self-portraits using a single RGBD camera. Benefiting from the proposed PIFusion and lightweight bundle adjustment algorithm, our method can generate detailed 3D self-portraits in seconds and shows the ability to handle subjects wearing extremely loose clothes. To achieve highly efficient and robust reconstruction, we propose PIFusion, which combines learning-based 3D recovery with volumetric non-rigid fusion to generate accurate sparse partial scans of the subject. Moreover, a non-rigid volumetric deformation method is proposed to continuously refine the learned shape prior. Finally, a lightweight bundle adjustment algorithm is proposed to guarantee that all the partial scans can not only ""loop"" with each other but also remain consistent with the selected live key observations. The results and experiments show that the proposed method achieves more robust and efficient 3D self-portraits compared with state-of-the-art methods.},
  YEAR = {2020},
  URL = {http://arxiv.org/abs/2004.02460v1},
  FILE = {2004.02460v1.pdf}
 }",Human body,"Lifting 2D features to 3D, Feature volume",,,Category-level,,,,,,,CVPR 2020,,No,,"Zhe Li, Tao Yu, Chuanyu Pan, Zerong Zheng, Yebin Liu",li2020pifusion,00000203,"In this paper, we propose an efficient method for robust 3D self-portraits using a single RGBD camera. Benefiting from the proposed PIFusion and lightweight bundle adjustment algorithm, our method can generate detailed 3D self-portraits in seconds and shows the ability to handle subjects wearing extremely loose clothes. To achieve highly efficient and robust reconstruction, we propose PIFusion, which combines learning-based 3D recovery with volumetric non-rigid fusion to generate accurate sparse partial scans of the subject. Moreover, a non-rigid volumetric deformation method is proposed to continuously refine the learned shape prior. Finally, a lightweight bundle adjustment algorithm is proposed to guarantee that all the partial scans can not only ""loop"" with each other but also remain consistent with the selected live key observations. The results and experiments show that the proposed method achieves more robust and efficient 3D self-portraits compared with state-of-the-art methods.",,Yes,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnueII465_YWFznXlunBgk3ynB25lDaAhpFgEmUC_QRxcR9_Mk7GrxnlcP6j88g_QO8A
9/17/2021 14:27:00,ARCH: Animatable Reconstruction of Clothed Humans,ARCH,4/8/2020,https://arxiv.org/pdf/2004.04572.pdf,https://vgl.ict.usc.edu/Research/ARCH/,,https://www.youtube.com/watch?v=DG3QNMcmTvo,,,,"@inproceedings{huang2020arch,
  BOOKTITLE = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  ID = {huang2020arch},
  ENTRYTYPE = {inproceedings},
  AUTHOR = {Zeng Huang and Yuanlu Xu and Christoph Lassner and Hao Li and Tony Tung},
  TITLE = {ARCH: Animatable Reconstruction of Clothed Humans},
  EPRINT = {2004.04572v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.GR},
  ABSTRACT = {In this paper, we propose ARCH (Animatable Reconstruction of Clothed Humans), a novel end-to-end framework for accurate reconstruction of animation-ready 3D clothed humans from a monocular image. Existing approaches to digitize 3D humans struggle to handle pose variations and recover details. Also, they do not produce models that are animation ready. In contrast, ARCH is a learned pose-aware model that produces detailed 3D rigged full-body human avatars from a single unconstrained RGB image. A Semantic Space and a Semantic Deformation Field are created using a parametric 3D body estimator. They allow the transformation of 2D/3D clothed humans into a canonical space, reducing ambiguities in geometry caused by pose variations and occlusions in training data. Detailed surface geometry and appearance are learned using an implicit function representation with spatial local features. Furthermore, we propose additional per-pixel supervision on the 3D reconstruction using opacity-aware differentiable rendering. Our experiments indicate that ARCH increases the fidelity of the reconstructed humans. We obtain more than 50% lower reconstruction errors for standard metrics compared to state-of-the-art methods on public datasets. We also show numerous qualitative examples of animated, high-quality reconstructed avatars unseen in the literature so far.},
  YEAR = {2020},
  URL = {http://arxiv.org/abs/2004.04572v2},
  FILE = {2004.04572v2.pdf}
 }",Human body,"Conditional neural field, Lifting 2D features to 3D, Voxelization, Feature volume, Warping field/Flow field, Data-driven",None,Occupancy,Category-level,,,,,,,CVPR 2020,,No,Direct,"Zeng Huang, Yuanlu Xu, Christoph Lassner, Hao Li, Tony Tung",huang2020arch,00000208,"In this paper, we propose ARCH (Animatable Reconstruction of Clothed Humans), a novel end-to-end framework for accurate reconstruction of animation-ready 3D clothed humans from a monocular image. Existing approaches to digitize 3D humans struggle to handle pose variations and recover details. Also, they do not produce models that are animation ready. In contrast, ARCH is a learned pose-aware model that produces detailed 3D rigged full-body human avatars from a single unconstrained RGB image. A Semantic Space and a Semantic Deformation Field are created using a parametric 3D body estimator. They allow the transformation of 2D/3D clothed humans into a canonical space, reducing ambiguities in geometry caused by pose variations and occlusions in training data. Detailed surface geometry and appearance are learned using an implicit function representation with spatial local features. Furthermore, we propose additional per-pixel supervision on the 3D reconstruction using opacity-aware differentiable rendering. Our experiments indicate that ARCH increases the fidelity of the reconstructed humans. We obtain more than 50% lower reconstruction errors for standard metrics compared to state-of-the-art methods on public datasets. We also show numerous qualitative examples of animated, high-quality reconstructed avatars unseen in the literature so far.",,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnucXLVLDkbvnTkIi4-2ibJ3hstWdwvJFYTorAlfJd_69xrTw1OXy4sFPNHFTMV14Yqs
8/29/2021 20:52:57,MeshfreeFlowNet: A Physics-Constrained Deep Continuous Space-Time Super-Resolution Framework,MeshfreeFlowNet,5/1/2020,https://arxiv.org/pdf/2005.01463.pdf,http://www.maxjiang.ml/proj/meshfreeflownet,https://github.com/maxjiang93/space_time_pde,"https://www.youtube.com/watch?v=mjqwPch9gDo, https://www.youtube.com/watch?v=anZ_gLrvnYs&t=538s",,,,"@article{jiang2020meshfreeflownet,
  ID = {jiang2020meshfreeflownet},
  ENTRYTYPE = {article},
  AUTHOR = {Chiyu Max Jiang and Soheil Esmaeilzadeh and Kamyar Azizzadenesheli and Karthik Kashinath and Mustafa Mustafa and Hamdi A. Tchelepi and Philip Marcus and Prabhat and Anima Anandkumar},
  TITLE = {MeshfreeFlowNet: A Physics-Constrained Deep Continuous Space-Time Super-Resolution Framework},
  EPRINT = {2005.01463v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.LG},
  ABSTRACT = {We propose MeshfreeFlowNet, a novel deep learning-based super-resolution framework to generate continuous (grid-free) spatio-temporal solutions from the low-resolution inputs. While being computationally efficient, MeshfreeFlowNet accurately recovers the fine-scale quantities of interest. MeshfreeFlowNet allows for: (i) the output to be sampled at all spatio-temporal resolutions, (ii) a set of Partial Differential Equation (PDE) constraints to be imposed, and (iii) training on fixed-size inputs on arbitrarily sized spatio-temporal domains owing to its fully convolutional encoder. We empirically study the performance of MeshfreeFlowNet on the task of super-resolution of turbulent flows in the Rayleigh-Benard convection problem. Across a diverse set of evaluation metrics, we show that MeshfreeFlowNet significantly outperforms existing baselines. Furthermore, we provide a large scale implementation of MeshfreeFlowNet and show that it efficiently scales across large clusters, achieving 96.80% scaling efficiency on up to 128 GPUs and a training time of less than 4 minutes.},
  YEAR = {2020},
  MONTH = {May},
  URL = {http://arxiv.org/abs/2005.01463v2},
  FILE = {2005.01463v2.pdf}
 }","Beyond graphics, Science and engineering",PDE,,,,,,,,,,"SC20: International Conference for High Performance Computing, Networking, Storage and Analysis 2020",,,,"Chiyu Max Jiang, Soheil Esmaeilzadeh, Kamyar Azizzadenesheli, Karthik Kashinath, Mustafa Mustafa, Hamdi A. Tchelepi, Philip Marcus, Prabhat, Anima Anandkumar",jiang2020meshfreeflownet,00000182,"We propose MeshfreeFlowNet, a novel deep learning-based super-resolution framework to generate continuous (grid-free) spatio-temporal solutions from the low-resolution inputs. While being computationally efficient, MeshfreeFlowNet accurately recovers the fine-scale quantities of interest. MeshfreeFlowNet allows for: (i) the output to be sampled at all spatio-temporal resolutions, (ii) a set of Partial Differential Equation (PDE) constraints to be imposed, and (iii) training on fixed-size inputs on arbitrarily sized spatio-temporal domains owing to its fully convolutional encoder. We empirically study the performance of MeshfreeFlowNet on the task of super-resolution of turbulent flows in the Rayleigh-Benard convection problem. Across a diverse set of evaluation metrics, we show that MeshfreeFlowNet significantly outperforms existing baselines. Furthermore, we provide a large scale implementation of MeshfreeFlowNet and show that it efficiently scales across large clusters, achieving 96.80% scaling efficiency on up to 128 GPUs and a training time of less than 4 minutes.",11,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnucgMNXLa8_plRY-082Ge4OOBbA0HY5tnFMXhtYCvClgxojkgIypyQhgys8L7ivXfHQ
9/17/2021 14:03:22,Geo-PIFu: Geometry and Pixel Aligned Implicit Functions for Single-view Human Reconstruction,Geo-PIFu,6/15/2020,https://arxiv.org/pdf/2006.08072.pdf,,https://github.com/simpleig/Geo-PIFu,,,,,"@inproceedings{he2020geopifu,
  PUBLISHER = {Curran Associates, Inc.},
  BOOKTITLE = {Advances in Neural Information Processing Systems (NeurIPS)},
  ID = {he2020geopifu},
  ENTRYTYPE = {inproceedings},
  AUTHOR = {Tong He and John Collomosse and Hailin Jin and Stefano Soatto},
  TITLE = {Geo-PIFu: Geometry and Pixel Aligned Implicit Functions for Single-view Human Reconstruction},
  EPRINT = {2006.08072v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We propose Geo-PIFu, a method to recover a 3D mesh from a monocular color image of a clothed person. Our method is based on a deep implicit function-based representation to learn latent voxel features using a structure-aware 3D U-Net, to constrain the model in two ways: first, to resolve feature ambiguities in query point encoding, second, to serve as a coarse human shape proxy to regularize the high-resolution mesh and encourage global shape regularity. We show that, by both encoding query points and constraining global shape using latent voxel features, the reconstruction we obtain for clothed human meshes exhibits less shape distortion and improved surface details compared to competing methods. We evaluate Geo-PIFu on a recent human mesh public dataset that is $10 \times$ larger than the private commercial dataset used in PIFu and previous derivative work. On average, we exceed the state of the art by $42.7\%$ reduction in Chamfer and Point-to-Surface Distances, and $19.4\%$ reduction in normal estimation errors.},
  YEAR = {2020},
  URL = {http://arxiv.org/abs/2006.08072v2},
  FILE = {2006.08072v2.pdf}
 }",Human body,"Conditional neural field, Lifting 2D features to 3D, Voxelization, Feature volume",,Occupancy,Category-level,,,,,,,NeurIPS 2020,,Yes,,"Tong He, John Collomosse, Hailin Jin, Stefano Soatto",he2020geopifu,00000205,"We propose Geo-PIFu, a method to recover a 3D mesh from a monocular color image of a clothed person. Our method is based on a deep implicit function-based representation to learn latent voxel features using a structure-aware 3D U-Net, to constrain the model in two ways: first, to resolve feature ambiguities in query point encoding, second, to serve as a coarse human shape proxy to regularize the high-resolution mesh and encourage global shape regularity. We show that, by both encoding query points and constraining global shape using latent voxel features, the reconstruction we obtain for clothed human meshes exhibits less shape distortion and improved surface details compared to competing methods. We evaluate Geo-PIFu on a recent human mesh public dataset that is $10 \times$ larger than the private commercial dataset used in PIFu and previous derivative work. On average, we exceed the state of the art by $42.7\%$ reduction in Chamfer and Point-to-Surface Distances, and $19.4\%$ reduction in normal estimation errors.",,Yes,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnudUOifJELEJTZsyXt9MOLfEej-C6vpG0K6CoXDZJeg-nsJARGtR-2XzXC6ZuYbYEcM
6/23/2021 13:23:51,Implicit Neural Representations with Periodic Activation Functions,SIREN,6/17/2020,https://arxiv.org/pdf/2006.09661.pdf,https://vsitzmann.github.io/siren/,https://github.com/vsitzmann/siren,,,,,"@inproceedings{sitzmann2020siren,
  PUBLISHER = {Curran Associates, Inc.},
  BOOKTITLE = {Advances in Neural Information Processing Systems (NeurIPS)},
  ID = {sitzmann2020siren},
  ENTRYTYPE = {inproceedings},
  AUTHOR = {Vincent Sitzmann and Julien N. P. Martel and Alexander W. Bergman and David B. Lindell and Gordon Wetzstein},
  TITLE = {Implicit Neural Representations with Periodic Activation Functions},
  EPRINT = {2006.09661v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Implicitly defined, continuous, differentiable signal representations parameterized by neural networks have emerged as a powerful paradigm, offering many possible benefits over conventional representations. However, current network architectures for such implicit neural representations are incapable of modeling signals with fine detail, and fail to represent a signal's spatial and temporal derivatives, despite the fact that these are essential to many physical signals defined implicitly as the solution to partial differential equations. We propose to leverage periodic activation functions for implicit neural representations and demonstrate that these networks, dubbed sinusoidal representation networks or Sirens, are ideally suited for representing complex natural signals and their derivatives. We analyze Siren activation statistics to propose a principled initialization scheme and demonstrate the representation of images, wavefields, video, sound, and their derivatives. Further, we show how Sirens can be leveraged to solve challenging boundary value problems, such as particular Eikonal equations (yielding signed distance functions), the Poisson equation, and the Helmholtz and wave equations. Lastly, we combine Sirens with hypernetworks to learn priors over the space of Siren functions.},
  YEAR = {2020},
  URL = {http://arxiv.org/abs/2006.09661v1},
  FILE = {2006.09661v1.pdf}
 }","Generalization, Beyond graphics, Fundamentals, Audio","Hypernetwork, PDE",SIREN,,,,,,,,,NeurIPS 2020,https://drive.google.com/drive/u/1/folders/1_iq__37-hw7FJOEUK1tX7mdp8SKB368K,,,"Vincent Sitzmann, Julien N. P. Martel, Alexander W. Bergman, David B. Lindell, Gordon Wetzstein",sitzmann2020siren,00000023,"Implicitly defined, continuous, differentiable signal representations parameterized by neural networks have emerged as a powerful paradigm, offering many possible benefits over conventional representations. However, current network architectures for such implicit neural representations are incapable of modeling signals with fine detail, and fail to represent a signal's spatial and temporal derivatives, despite the fact that these are essential to many physical signals defined implicitly as the solution to partial differential equations. We propose to leverage periodic activation functions for implicit neural representations and demonstrate that these networks, dubbed sinusoidal representation networks or Sirens, are ideally suited for representing complex natural signals and their derivatives. We analyze Siren activation statistics to propose a principled initialization scheme and demonstrate the representation of images, wavefields, video, sound, and their derivatives. Further, we show how Sirens can be leveraged to solve challenging boundary value problems, such as particular Eikonal equations (yielding signed distance functions), the Poisson equation, and the Helmholtz and wave equations. Lastly, we combine Sirens with hypernetworks to learn priors over the space of Siren functions.",185,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnuchy2Ku09XRMFAZaI4OkCuQG_njfVCkZw_RifvIWqzEav4GGxG5bFeciybd7ml-riQ
7/19/2021 17:57:41,MetaSDF: Meta-learning Signed Distance Functions,MetaSDF,6/17/2020,https://arxiv.org/pdf/2006.09662.pdf,https://vsitzmann.github.io/metasdf/,https://github.com/vsitzmann/metasdf,,,,,"@inproceedings{sitzmann2020metasdf,
  PUBLISHER = {Curran Associates, Inc.},
  BOOKTITLE = {Advances in Neural Information Processing Systems (NeurIPS)},
  ID = {sitzmann2020metasdf},
  ENTRYTYPE = {inproceedings},
  AUTHOR = {Vincent Sitzmann and Eric R. Chan and Richard Tucker and Noah Snavely and Gordon Wetzstein},
  TITLE = {MetaSDF: Meta-learning Signed Distance Functions},
  EPRINT = {2006.09662v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Neural implicit shape representations are an emerging paradigm that offers many potential benefits over conventional discrete representations, including memory efficiency at a high spatial resolution. Generalizing across shapes with such neural implicit representations amounts to learning priors over the respective function space and enables geometry reconstruction from partial or noisy observations. Existing generalization methods rely on conditioning a neural network on a low-dimensional latent code that is either regressed by an encoder or jointly optimized in the auto-decoder framework. Here, we formalize learning of a shape space as a meta-learning problem and leverage gradient-based meta-learning algorithms to solve this task. We demonstrate that this approach performs on par with auto-decoder based approaches while being an order of magnitude faster at test-time inference. We further demonstrate that the proposed gradient-based method outperforms encoder-decoder based methods that leverage pooling-based set encoders.},
  YEAR = {2020},
  URL = {http://arxiv.org/abs/2006.09662v1},
  FILE = {2006.09662v1.pdf}
 }",Generalization,"Hypernetwork, Data-driven",,,,,,,,,,NeurIPS 2020,,,,"Vincent Sitzmann, Eric R. Chan, Richard Tucker, Noah Snavely, Gordon Wetzstein",sitzmann2020metasdf,00000024,"Neural implicit shape representations are an emerging paradigm that offers many potential benefits over conventional discrete representations, including memory efficiency at a high spatial resolution. Generalizing across shapes with such neural implicit representations amounts to learning priors over the respective function space and enables geometry reconstruction from partial or noisy observations. Existing generalization methods rely on conditioning a neural network on a low-dimensional latent code that is either regressed by an encoder or jointly optimized in the auto-decoder framework. Here, we formalize learning of a shape space as a meta-learning problem and leverage gradient-based meta-learning algorithms to solve this task. We demonstrate that this approach performs on par with auto-decoder based approaches while being an order of magnitude faster at test-time inference. We further demonstrate that the proposed gradient-based method outperforms encoder-decoder based methods that leverage pooling-based set encoders.",25,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnudSE7uYyxUnuqCU7qaGrjjLno8KNztX7xgGiJ1y9HCo9ie52MGhpEf1TwqayYYOBOU
6/23/2021 13:23:16,Fourier Features Let Networks Learn High Frequency Functions in Low Dimensional Domains,FFN,6/18/2020,https://arxiv.org/pdf/2006.10739.pdf,https://bmild.github.io/fourfeat/,https://github.com/tancik/fourier-feature-networks,"https://www.youtube.com/watch?v=iKyIJ_EtSkw, https://www.youtube.com/watch?v=h0SXP6lJxak",,,,"@inproceedings{tancik2020ffn,
  PUBLISHER = {Curran Associates, Inc.},
  BOOKTITLE = {Advances in Neural Information Processing Systems (NeurIPS)},
  ID = {tancik2020ffn},
  ENTRYTYPE = {inproceedings},
  AUTHOR = {Matthew Tancik and Pratul P. Srinivasan and Ben Mildenhall and Sara Fridovich-Keil and Nithin Raghavan and Utkarsh Singhal and Ravi Ramamoorthi and Jonathan T. Barron and Ren Ng},
  TITLE = {Fourier Features Let Networks Learn High Frequency Functions in Low Dimensional Domains},
  EPRINT = {2006.10739v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We show that passing input points through a simple Fourier feature mapping enables a multilayer perceptron (MLP) to learn high-frequency functions in low-dimensional problem domains. These results shed light on recent advances in computer vision and graphics that achieve state-of-the-art results by using MLPs to represent complex 3D objects and scenes. Using tools from the neural tangent kernel (NTK) literature, we show that a standard MLP fails to learn high frequencies both in theory and in practice. To overcome this spectral bias, we use a Fourier feature mapping to transform the effective NTK into a stationary kernel with a tunable bandwidth. We suggest an approach for selecting problem-specific Fourier features that greatly improves the performance of MLPs for low-dimensional regression tasks relevant to the computer vision and graphics communities.},
  YEAR = {2020},
  URL = {http://arxiv.org/abs/2006.10739v1},
  FILE = {2006.10739v1.pdf}
 }",Fundamentals,,NeRF,,,,,,,,,NeurIPS 2020,,,,"Matthew Tancik, Pratul P. Srinivasan, Ben Mildenhall, Sara Fridovich-Keil, Nithin Raghavan, Utkarsh Singhal, Ravi Ramamoorthi, Jonathan T. Barron, Ren Ng",tancik2020ffn,00000025,"We show that passing input points through a simple Fourier feature mapping enables a multilayer perceptron (MLP) to learn high-frequency functions in low-dimensional problem domains. These results shed light on recent advances in computer vision and graphics that achieve state-of-the-art results by using MLPs to represent complex 3D objects and scenes. Using tools from the neural tangent kernel (NTK) literature, we show that a standard MLP fails to learn high frequencies both in theory and in practice. To overcome this spectral bias, we use a Fourier feature mapping to transform the effective NTK into a stationary kernel with a tunable bandwidth. We suggest an approach for selecting problem-specific Fourier features that greatly improves the performance of MLPs for low-dimensional regression tasks relevant to the computer vision and graphics communities.",135,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnufZqolm0dE5lZ7ju0TWx9uVGSHBZ9xcthSFoWlJXTW_yOA7EtKm596UAhW28Sf52Us
5/23/2021 19:11:04,Deep Reflectance Volumes,,6/20/2020,https://arxiv.org/pdf/2007.09892.pdf,,,https://drive.google.com/file/d/1JEbeIrIttznaowJJcBGZD56KR22j635q/view,https://drive.google.com/file/d/12IAg73kWtGtvKp2RNeaJ8WrmlsTehV4U/view,,,"@inproceedings{bi2020deep,
  BOOKTITLE = {Proceedings of the European Conference on Computer Vision (ECCV)},
  ID = {bi2020deep},
  ENTRYTYPE = {inproceedings},
  AUTHOR = {Sai Bi and Zexiang Xu and Kalyan Sunkavalli and Milos Hasan and Yannick Hold-Geoffroy and David Kriegman and Ravi Ramamoorthi},
  TITLE = {Deep Reflectance Volumes: Relightable Reconstructions from Multi-View Photometric Images},
  EPRINT = {2007.09892v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We present a deep learning approach to reconstruct scene appearance from unstructured images captured under collocated point lighting. At the heart of Deep Reflectance Volumes is a novel volumetric scene representation consisting of opacity, surface normal and reflectance voxel grids. We present a novel physically-based differentiable volume ray marching framework to render these scene volumes under arbitrary viewpoint and lighting. This allows us to optimize the scene volumes to minimize the error between their rendered images and the captured images. Our method is able to reconstruct real scenes with challenging non-Lambertian reflectance and complex geometry with occlusions and shadowing. Moreover, it accurately generalizes to novel viewpoints and lighting, including non-collocated lighting, rendering photorealistic images that are significantly better than state-of-the-art mesh-based methods. We also show that our learned reflectance volumes are editable, allowing for modifying the materials of the captured scenes.},
  YEAR = {2020},
  URL = {http://arxiv.org/abs/2007.09892v1},
  FILE = {2007.09892v1.pdf}
 }",Material/lighting estimation,,,,,,,,,,,ECCV 2020,,,,"Sai Bi, Zexiang Xu, Kalyan Sunkavalli, Miloš Hašan, Yannick Hold-Geoffroy, David Kriegman, Ravi Ramamoorthi",bi2020deep,00000026,"We present a deep learning approach to reconstruct scene appearance from unstructured images captured under collocated point lighting. At the heart of Deep Reflectance Volumes is a novel volumetric scene representation consisting of opacity, surface normal and reflectance voxel grids. We present a novel physically-based differentiable volume ray marching framework to render these scene volumes under arbitrary viewpoint and lighting. This allows us to optimize the scene volumes to minimize the error between their rendered images and the captured images. Our method is able to reconstruct real scenes with challenging non-Lambertian reflectance and complex geometry with occlusions and shadowing. Moreover, it accurately generalizes to novel viewpoints and lighting, including non-collocated lighting, rendering photorealistic images that are significantly better than state-of-the-art mesh-based methods. We also show that our learned reflectance volumes are editable, allowing for modifying the materials of the captured scenes.",18,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnudq_D_2uOWJeFi3fSOv_tV6wJafd0uqUd2oZqqpY9Kk4xDRpqRe9tTUDTLaKBabduY
6/29/2021 16:22:47,Neural Splines: Fitting 3D Surfaces with Infinitely-Wide Neural Networks,Neural Splines,6/24/2020,https://arxiv.org/pdf/2006.13782.pdf,,https://github.com/fwilliams/neural-splines,,,,,"@inproceedings{williams2021neuralsplines,
  BOOKTITLE = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  ID = {williams2021neuralsplines},
  ENTRYTYPE = {inproceedings},
  AUTHOR = {Francis Williams and Matthew Trager and Joan Bruna and Denis Zorin},
  TITLE = {Neural Splines: Fitting 3D Surfaces with Infinitely-Wide Neural Networks},
  EPRINT = {2006.13782v3},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We present Neural Splines, a technique for 3D surface reconstruction that is based on random feature kernels arising from infinitely-wide shallow ReLU networks. Our method achieves state-of-the-art results, outperforming recent neural network-based techniques and widely used Poisson Surface Reconstruction (which, as we demonstrate, can also be viewed as a type of kernel method). Because our approach is based on a simple kernel formulation, it is easy to analyze and can be accelerated by general techniques designed for kernel-based learning. We provide explicit analytical expressions for our kernel and argue that our formulation can be seen as a generalization of cubic spline interpolation to higher dimensions. In particular, the RKHS norm associated with Neural Splines biases toward smooth interpolants.},
  YEAR = {2021},
  URL = {http://arxiv.org/abs/2006.13782v3},
  FILE = {2006.13782v3.pdf}
 }",Fundamentals,,None,SDF,Per-scene,,,,,,,CVPR 2021,,Yes,,"Francis Williams, Matthew Trager, Joan Bruna, Denis Zorin",williams2021neuralsplines,00000027,"We present Neural Splines, a technique for 3D surface reconstruction that is based on random feature kernels arising from infinitely-wide shallow ReLU networks. Our method achieves state-of-the-art results, outperforming recent neural network-based techniques and widely used Poisson Surface Reconstruction (which, as we demonstrate, can also be viewed as a type of kernel method). Because our approach is based on a simple kernel formulation, it is easy to analyze and can be accelerated by general techniques designed for kernel-based learning. We provide explicit analytical expressions for our kernel and argue that our formulation can be seen as a generalization of cubic spline interpolation to higher dimensions. In particular, the RKHS norm associated with Neural Splines biases toward smooth interpolants.",2,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnueUgFLaCKrHEZwzadFVEpe1VV1Fp0Cb6EvUy1S1NQpJPWH9j0LBGZlQ-5RqNFaqIN4
5/23/2021 19:12:08,GRAF: Generative Radiance Fields for 3D-Aware Image Synthesis,GRAF,7/5/2020,https://arxiv.org/pdf/2007.02442.pdf,,https://github.com/autonomousvision/graf,https://www.youtube.com/watch?v=akQf7WaCOHo,,,,"@inproceedings{schwarz2020graf,
  PUBLISHER = {Curran Associates, Inc.},
  BOOKTITLE = {Advances in Neural Information Processing Systems (NeurIPS)},
  ID = {schwarz2020graf},
  ENTRYTYPE = {inproceedings},
  AUTHOR = {Katja Schwarz and Yiyi Liao and Michael Niemeyer and Andreas Geiger},
  TITLE = {GRAF: Generative Radiance Fields for 3D-Aware Image Synthesis},
  EPRINT = {2007.02442v4},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {While 2D generative adversarial networks have enabled high-resolution image synthesis, they largely lack an understanding of the 3D world and the image formation process. Thus, they do not provide precise control over camera viewpoint or object pose. To address this problem, several recent approaches leverage intermediate voxel-based representations in combination with differentiable rendering. However, existing methods either produce low image resolution or fall short in disentangling camera and scene properties, e.g., the object identity may vary with the viewpoint. In this paper, we propose a generative model for radiance fields which have recently proven successful for novel view synthesis of a single scene. In contrast to voxel-based representations, radiance fields are not confined to a coarse discretization of the 3D space, yet allow for disentangling camera and scene properties while degrading gracefully in the presence of reconstruction ambiguity. By introducing a multi-scale patch-based discriminator, we demonstrate synthesis of high-resolution images while training our model from unposed 2D images alone. We systematically analyze our approach on several challenging synthetic and real-world datasets. Our experiments reveal that radiance fields are a powerful representation for generative image synthesis, leading to 3D consistent models that render with high fidelity.},
  YEAR = {2020},
  NOTE = {Advances in Neural Information Processing Systems, NeurIPS 2020},
  URL = {http://arxiv.org/abs/2007.02442v4},
  FILE = {2007.02442v4.pdf}
 }",Generalization,"Generative/adversarial formulation, Conditional neural field, Lifting 2D features to 3D, Data-driven",,,,,,,,,,NeurIPS 2020,,,,"Katja Schwarz, Yiyi Liao, Michael Niemeyer, Andreas Geiger",schwarz2020graf,00000028,"While 2D generative adversarial networks have enabled high-resolution image synthesis, they largely lack an understanding of the 3D world and the image formation process. Thus, they do not provide precise control over camera viewpoint or object pose. To address this problem, several recent approaches leverage intermediate voxel-based representations in combination with differentiable rendering. However, existing methods either produce low image resolution or fall short in disentangling camera and scene properties, e.g., the object identity may vary with the viewpoint. In this paper, we propose a generative model for radiance fields which have recently proven successful for novel view synthesis of a single scene. In contrast to voxel-based representations, radiance fields are not confined to a coarse discretization of the 3D space, yet allow for disentangling camera and scene properties while degrading gracefully in the presence of reconstruction ambiguity. By introducing a multi-scale patch-based discriminator, we demonstrate synthesis of high-resolution images while training our model from unposed 2D images alone. We systematically analyze our approach on several challenging synthetic and real-world datasets. Our experiments reveal that radiance fields are a powerful representation for generative image synthesis, leading to 3D consistent models that render with high fidelity.",61,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnud6yJHfbEWHbmVUAkwoT-pQkO8O0Mog5CgI1e6fSFZ6AT-zgHlplVObL6CAYl51aAg
10/2/2021 20:25:06,Coupling Explicit and Implicit Surface Representations for Generative 3D Modeling,HybridNet,7/20/2020,https://arxiv.org/pdf/2007.10294.pdf,https://omidpoursaeed.github.io/publication/hybrid/,,https://drive.google.com/file/d/1wwnp6HlDdfYw19__ESxWdTQfmc8Bf--v/view,https://omidpoursaeed.github.io/pdf/HybridNet_Supp.pdf,,,"@inproceedings{poursaeed2020hybridnet,
  BOOKTITLE = {Proceedings of the European Conference on Computer Vision (ECCV)},
  ID = {poursaeed2020hybridnet},
  ENTRYTYPE = {inproceedings},
  AUTHOR = {Omid Poursaeed and Matthew Fisher and Noam Aigerman and Vladimir G. Kim},
  TITLE = {Coupling Explicit and Implicit Surface Representations for Generative 3D Modeling},
  EPRINT = {2007.10294v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We propose a novel neural architecture for representing 3D surfaces, which harnesses two complementary shape representations: (i) an explicit representation via an atlas, i.e., embeddings of 2D domains into 3D; (ii) an implicit-function representation, i.e., a scalar function over the 3D volume, with its levels denoting surfaces. We make these two representations synergistic by introducing novel consistency losses that ensure that the surface created from the atlas aligns with the level-set of the implicit function. Our hybrid architecture outputs results which are superior to the output of the two equivalent single-representation networks, yielding smoother explicit surfaces with more accurate normals, and a more accurate implicit occupancy function. Additionally, our surface reconstruction step can directly leverage the explicit atlas-based representation. This process is computationally efficient, and can be directly used by differentiable rasterizers, enabling training our hybrid representation with image-based losses.},
  YEAR = {2020},
  URL = {http://arxiv.org/abs/2007.10294v2},
  FILE = {2007.10294v2.pdf}
 }",Surface reconstruction,"Conditional neural field, Representation",,"Occupancy, Atlas",Category-level,,,,,,,ECCV 2020,,No,Direct,"Omid Poursaeed, Matthew Fisher, Noam Aigerman, Vladimir G. Kim",poursaeed2020hybridnet,00000230,"We propose a novel neural architecture for representing 3D surfaces, which harnesses two complementary shape representations: (i) an explicit representation via an atlas, i.e., embeddings of 2D domains into 3D; (ii) an implicit-function representation, i.e., a scalar function over the 3D volume, with its levels denoting surfaces. We make these two representations synergistic by introducing novel consistency losses that ensure that the surface created from the atlas aligns with the level-set of the implicit function. Our hybrid architecture outputs results which are superior to the output of the two equivalent single-representation networks, yielding smoother explicit surfaces with more accurate normals, and a more accurate implicit occupancy function. Additionally, our surface reconstruction step can directly leverage the explicit atlas-based representation. This process is computationally efficient, and can be directly used by differentiable rasterizers, enabling training our hybrid representation with image-based losses.",,,No,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnucu-ebyHdU7AuNtr3TeAQpnlCaANoPGwsNHcIE9gbO5r4UM0eE2dUieyllhcP1btmo
5/23/2021 19:11:37,Neural Sparse Voxel Fields,NSVF,7/22/2020,https://arxiv.org/pdf/2007.11571.pdf,https://lingjie0206.github.io/papers/NSVF/,https://github.com/facebookresearch/NSVF,https://www.youtube.com/watch?v=RFqPwH7QFEI,,,,"@inproceedings{liu2020nsvf,
  BOOKTITLE = {Proceedings of the European Conference on Computer Vision (ECCV)},
  ID = {liu2020nsvf},
  ENTRYTYPE = {inproceedings},
  AUTHOR = {Lingjie Liu and Jiatao Gu and Kyaw Zaw Lin and Tat-Seng Chua and Christian Theobalt},
  TITLE = {Neural Sparse Voxel Fields},
  EPRINT = {2007.11571v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Photo-realistic free-viewpoint rendering of real-world scenes using classical computer graphics techniques is challenging, because it requires the difficult step of capturing detailed appearance and geometry models. Recent studies have demonstrated promising results by learning scene representations that implicitly encode both geometry and appearance without 3D supervision. However, existing approaches in practice often show blurry renderings caused by the limited network capacity or the difficulty in finding accurate intersections of camera rays with the scene geometry. Synthesizing high-resolution imagery from these representations often requires time-consuming optical ray marching. In this work, we introduce Neural Sparse Voxel Fields (NSVF), a new neural scene representation for fast and high-quality free-viewpoint rendering. NSVF defines a set of voxel-bounded implicit fields organized in a sparse voxel octree to model local properties in each cell. We progressively learn the underlying voxel structures with a differentiable ray-marching operation from only a set of posed RGB images. With the sparse voxel octree structure, rendering novel views can be accelerated by skipping the voxels containing no relevant scene content. Our method is typically over 10 times faster than the state-of-the-art (namely, NeRF(Mildenhall et al., 2020)) at inference time while achieving higher quality results. Furthermore, by utilizing an explicit sparse voxel representation, our method can easily be applied to scene editing and scene composition. We also demonstrate several challenging tasks, including multi-scene learning, free-viewpoint rendering of a moving human, and large-scale scene rendering. Code and data are available at our website: https://github.com/facebookresearch/NSVF.},
  YEAR = {2020},
  URL = {http://arxiv.org/abs/2007.11571v2},
  FILE = {2007.11571v2.pdf}
 }","Performance (training), Performance (rendering)","Coarse-to-fine, Sampling, Voxelization, Feature volume, Representation",,,,,,,,,,ECCV 2020,,,,"Lingjie Liu, Jiatao Gu, Kyaw Zaw Lin, Tat-Seng Chua, Christian Theobalt",liu2020nsvf,00000029,"Photo-realistic free-viewpoint rendering of real-world scenes using classical computer graphics techniques is challenging, because it requires the difficult step of capturing detailed appearance and geometry models. Recent studies have demonstrated promising results by learning scene representations that implicitly encode both geometry and appearance without 3D supervision. However, existing approaches in practice often show blurry renderings caused by the limited network capacity or the difficulty in finding accurate intersections of camera rays with the scene geometry. Synthesizing high-resolution imagery from these representations often requires time-consuming optical ray marching. In this work, we introduce Neural Sparse Voxel Fields (NSVF), a new neural scene representation for fast and high-quality free-viewpoint rendering. NSVF defines a set of voxel-bounded implicit fields organized in a sparse voxel octree to model local properties in each cell. We progressively learn the underlying voxel structures with a differentiable ray-marching operation from only a set of posed RGB images. With the sparse voxel octree structure, rendering novel views can be accelerated by skipping the voxels containing no relevant scene content. Our method is typically over 10 times faster than the state-of-the-art (namely, NeRF(Mildenhall et al., 2020)) at inference time while achieving higher quality results. Furthermore, by utilizing an explicit sparse voxel representation, our method can easily be applied to scene editing and scene composition. We also demonstrate several challenging tasks, including multi-scene learning, free-viewpoint rendering of a moving human, and large-scale scene rendering. Code and data are available at our website: https://github.com/facebookresearch/NSVF.",90,Yes,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnudKuKRt2X526ZlgyJ0Yi_pH7BxGziEZYfP1f65YWnTcRpmddePjVCgC3PhXgEdOHeo
9/17/2021 13:11:20,Combining Implicit Function Learning and Parametric Models for 3D Human Reconstruction,IP-Net,7/22/2020,https://arxiv.org/pdf/2007.11432.pdf,https://virtualhumans.mpi-inf.mpg.de/ipnet/,https://github.com/bharat-b7/IPNet,https://virtualhumans.mpi-inf.mpg.de/ipnet/ECCV_short.mp4,,,,"@article{bhatnagarOralipnet,
  ID = {bhatnagarOralipnet},
  ENTRYTYPE = {article},
  AUTHOR = {Bharat Lal Bhatnagar and Cristian Sminchisescu and Christian Theobalt and Gerard Pons-Moll},
  TITLE = {Combining Implicit Function Learning and Parametric Models for 3D Human Reconstruction},
  EPRINT = {2007.11432v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Implicit functions represented as deep learning approximations are powerful for reconstructing 3D surfaces. However, they can only produce static surfaces that are not controllable, which provides limited ability to modify the resulting model by editing its pose or shape parameters. Nevertheless, such features are essential in building flexible models for both computer graphics and computer vision. In this work, we present methodology that combines detail-rich implicit functions and parametric representations in order to reconstruct 3D models of people that remain controllable and accurate even in the presence of clothing. Given sparse 3D point clouds sampled on the surface of a dressed person, we use an Implicit Part Network (IP-Net)to jointly predict the outer 3D surface of the dressed person, the and inner body surface, and the semantic correspondences to a parametric body model. We subsequently use correspondences to fit the body model to our inner surface and then non-rigidly deform it (under a parametric body + displacement model) to the outer surface in order to capture garment, face and hair detail. In quantitative and qualitative experiments with both full body data and hand scans we show that the proposed methodology generalizes, and is effective even given incomplete point clouds collected from single-view depth images. Our models and code can be downloaded from http://virtualhumans.mpi-inf.mpg.de/ipnet.},
  YEAR = {2020},
  MONTH = {Jul},
  URL = {http://arxiv.org/abs/2007.11432v1},
  FILE = {2007.11432v1.pdf}
 }",Human body,"Voxelization, Feature volume, Representation",,Occupancy,,,,,,,,ECCV 2020 Oral,,Yes,Direct,"Bharat Lal Bhatnagar, Cristian Sminchisescu, Christian Theobalt, Gerard Pons-Moll",bhatnagaroralipnet,00000201,"Implicit functions represented as deep learning approximations are powerful for reconstructing 3D surfaces. However, they can only produce static surfaces that are not controllable, which provides limited ability to modify the resulting model by editing its pose or shape parameters. Nevertheless, such features are essential in building flexible models for both computer graphics and computer vision. In this work, we present methodology that combines detail-rich implicit functions and parametric representations in order to reconstruct 3D models of people that remain controllable and accurate even in the presence of clothing. Given sparse 3D point clouds sampled on the surface of a dressed person, we use an Implicit Part Network (IP-Net)to jointly predict the outer 3D surface of the dressed person, the and inner body surface, and the semantic correspondences to a parametric body model. We subsequently use correspondences to fit the body model to our inner surface and then non-rigidly deform it (under a parametric body + displacement model) to the outer surface in order to capture garment, face and hair detail. In quantitative and qualitative experiments with both full body data and hand scans we show that the proposed methodology generalizes, and is effective even given incomplete point clouds collected from single-view depth images. Our models and code can be downloaded from http://virtualhumans.mpi-inf.mpg.de/ipnet.",,Yes,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnucvPhphJ-gdaWaQHOTB7tsu16sT0TJjKz8I2y5ETXAAasFdbtCSJRLHu5FjPUC9CNw
8/29/2021 16:29:22,Ladybird: Quasi-Monte Carlo Sampling for Deep Implicit Field Based 3D Reconstruction with Symmetry,Ladybird,7/27/2020,https://arxiv.org/pdf/2007.13393.pdf,,https://github.com/FuxiCV/Ladybird,,,,,"@article{xuOralladybird,
  ID = {xuOralladybird},
  ENTRYTYPE = {article},
  AUTHOR = {Yifan Xu and Tianqi Fan and Yi Yuan and Gurprit Singh},
  TITLE = {Ladybird: Quasi-Monte Carlo Sampling for Deep Implicit Field Based 3D Reconstruction with Symmetry},
  EPRINT = {2007.13393v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Deep implicit field regression methods are effective for 3D reconstruction from single-view images. However, the impact of different sampling patterns on the reconstruction quality is not well-understood. In this work, we first study the effect of point set discrepancy on the network training. Based on Farthest Point Sampling algorithm, we propose a sampling scheme that theoretically encourages better generalization performance, and results in fast convergence for SGD-based optimization algorithms. Secondly, based on the reflective symmetry of an object, we propose a feature fusion method that alleviates issues due to self-occlusions which makes it difficult to utilize local image features. Our proposed system Ladybird is able to create high quality 3D object reconstructions from a single input image. We evaluate Ladybird on a large scale 3D dataset (ShapeNet) demonstrating highly competitive results in terms of Chamfer distance, Earth Mover's distance and Intersection Over Union (IoU).},
  YEAR = {2020},
  MONTH = {Jul},
  URL = {http://arxiv.org/abs/2007.13393v1},
  FILE = {2007.13393v1.pdf}
 }","Few-shot reconstruction, Fundamentals","Sampling, Symmetry",,,,,,,,,,ECCV 2020 Oral,,,,"Yifan Xu, Tianqi Fan, Yi Yuan, Gurprit Singh",xuoralladybird,00000165,"Deep implicit field regression methods are effective for 3D reconstruction from single-view images. However, the impact of different sampling patterns on the reconstruction quality is not well-understood. In this work, we first study the effect of point set discrepancy on the network training. Based on Farthest Point Sampling algorithm, we propose a sampling scheme that theoretically encourages better generalization performance, and results in fast convergence for SGD-based optimization algorithms. Secondly, based on the reflective symmetry of an object, we propose a feature fusion method that alleviates issues due to self-occlusions which makes it difficult to utilize local image features. Our proposed system Ladybird is able to create high quality 3D object reconstructions from a single input image. We evaluate Ladybird on a large scale 3D dataset (ShapeNet) demonstrating highly competitive results in terms of Chamfer distance, Earth Mover's distance and Intersection Over Union (IoU).",6,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnufK_x2Z_Xd0qqU17yj-9ilOiq2eYj9AnRYbTNg1xMJO3ObAzrZkK0L_V2v1voq0m6E
9/17/2021 14:11:39,Monocular Real-Time Volumetric Performance Capture,Monoport,7/28/2020,https://arxiv.org/pdf/2007.13988.pdf,https://project-splinter.github.io/monoport/,https://github.com/Project-Splinter/MonoPort,https://github.com/Project-Splinter/MonoPort,,,,"@inproceedings{li2020monoport,
  BOOKTITLE = {Proceedings of the European Conference on Computer Vision (ECCV)},
  ID = {li2020monoport},
  ENTRYTYPE = {inproceedings},
  AUTHOR = {Ruilong Li and Yuliang Xiu and Shunsuke Saito and Zeng Huang and Kyle Olszewski and Hao Li},
  TITLE = {Monocular Real-Time Volumetric Performance Capture},
  EPRINT = {2007.13988v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We present the first approach to volumetric performance capture and novel-view rendering at real-time speed from monocular video, eliminating the need for expensive multi-view systems or cumbersome pre-acquisition of a personalized template model. Our system reconstructs a fully textured 3D human from each frame by leveraging Pixel-Aligned Implicit Function (PIFu). While PIFu achieves high-resolution reconstruction in a memory-efficient manner, its computationally expensive inference prevents us from deploying such a system for real-time applications. To this end, we propose a novel hierarchical surface localization algorithm and a direct rendering method without explicitly extracting surface meshes. By culling unnecessary regions for evaluation in a coarse-to-fine manner, we successfully accelerate the reconstruction by two orders of magnitude from the baseline without compromising the quality. Furthermore, we introduce an Online Hard Example Mining (OHEM) technique that effectively suppresses failure modes due to the rare occurrence of challenging examples. We adaptively update the sampling probability of the training data based on the current reconstruction accuracy, which effectively alleviates reconstruction artifacts. Our experiments and evaluations demonstrate the robustness of our system to various challenging angles, illuminations, poses, and clothing styles. We also show that our approach compares favorably with the state-of-the-art monocular performance capture. Our proposed approach removes the need for multi-view studio settings and enables a consumer-accessible solution for volumetric capture.},
  YEAR = {2020},
  URL = {http://arxiv.org/abs/2007.13988v1},
  FILE = {2007.13988v1.pdf}
 }",Human body,Lifting 2D features to 3D,,,Category-level,,,,,,,ECCV 2020,,Yes,,"Ruilong Li, Yuliang Xiu, Shunsuke Saito, Zeng Huang, Kyle Olszewski, Hao Li",li2020monoport,00000206,"We present the first approach to volumetric performance capture and novel-view rendering at real-time speed from monocular video, eliminating the need for expensive multi-view systems or cumbersome pre-acquisition of a personalized template model. Our system reconstructs a fully textured 3D human from each frame by leveraging Pixel-Aligned Implicit Function (PIFu). While PIFu achieves high-resolution reconstruction in a memory-efficient manner, its computationally expensive inference prevents us from deploying such a system for real-time applications. To this end, we propose a novel hierarchical surface localization algorithm and a direct rendering method without explicitly extracting surface meshes. By culling unnecessary regions for evaluation in a coarse-to-fine manner, we successfully accelerate the reconstruction by two orders of magnitude from the baseline without compromising the quality. Furthermore, we introduce an Online Hard Example Mining (OHEM) technique that effectively suppresses failure modes due to the rare occurrence of challenging examples. We adaptively update the sampling probability of the training data based on the current reconstruction accuracy, which effectively alleviates reconstruction artifacts. Our experiments and evaluations demonstrate the robustness of our system to various challenging angles, illuminations, poses, and clothing styles. We also show that our approach compares favorably with the state-of-the-art monocular performance capture. Our proposed approach removes the need for multi-view studio settings and enables a consumer-accessible solution for volumetric capture.",,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnufQ-gEi5aEMeh13NEXaszFR27WELg2bhtg_cqLGrjJDFpf_RAIGOrFllkf0yTz9XTg
7/19/2021 21:28:27,Continuous Object Representation Networks: Novel View Synthesis without Target View Supervision,CORN,7/30/2020,https://arxiv.org/pdf/2007.15627.pdf,,,,,,,"@inproceedings{hani2020corn,
  PUBLISHER = {Curran Associates, Inc.},
  BOOKTITLE = {Advances in Neural Information Processing Systems (NeurIPS)},
  ID = {hani2020corn},
  ENTRYTYPE = {inproceedings},
  AUTHOR = {Nicolai Hani and Selim Engin and Jun-Jee Chao and Volkan Isler},
  TITLE = {Continuous Object Representation Networks: Novel View Synthesis without Target View Supervision},
  EPRINT = {2007.15627v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Novel View Synthesis (NVS) is concerned with synthesizing views under camera viewpoint transformations from one or multiple input images. NVS requires explicit reasoning about 3D object structure and unseen parts of the scene to synthesize convincing results. As a result, current approaches typically rely on supervised training with either ground truth 3D models or multiple target images. We propose Continuous Object Representation Networks (CORN), a conditional architecture that encodes an input image's geometry and appearance that map to a 3D consistent scene representation. We can train CORN with only two source images per object by combining our model with a neural renderer. A key feature of CORN is that it requires no ground truth 3D models or target view supervision. Regardless, CORN performs well on challenging tasks such as novel view synthesis and single-view 3D reconstruction and achieves performance comparable to state-of-the-art approaches that use direct supervision. For up-to-date information, data, and code, please see our project page: https://nicolaihaeni.github.io/corn/.},
  YEAR = {2020},
  URL = {http://arxiv.org/abs/2007.15627v2},
  FILE = {2007.15627v2.pdf}
 }","Few-shot reconstruction, Generalization","Conditional neural field, Lifting 2D features to 3D, Image-based rendering, Data-driven",,,,,,,,,,NeurIPS 2020,,,,"Nicolai Häni, Selim Engin, Jun-Jee Chao, Volkan Isler",hani2020corn,00000030,"Novel View Synthesis (NVS) is concerned with synthesizing views under camera viewpoint transformations from one or multiple input images. NVS requires explicit reasoning about 3D object structure and unseen parts of the scene to synthesize convincing results. As a result, current approaches typically rely on supervised training with either ground truth 3D models or multiple target images. We propose Continuous Object Representation Networks (CORN), a conditional architecture that encodes an input image's geometry and appearance that map to a 3D consistent scene representation. We can train CORN with only two source images per object by combining our model with a neural renderer. A key feature of CORN is that it requires no ground truth 3D models or target view supervision. Regardless, CORN performs well on challenging tasks such as novel view synthesis and single-view 3D reconstruction and achieves performance comparable to state-of-the-art approaches that use direct supervision. For up-to-date information, data, and code, please see our project page: https://nicolaihaeni.github.io/corn/.",5,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnuftJ9c3qSAjCWifaKzzfZXaq86dYBG3gy9jo51d55nlWDvs-UOKGVR-dZHCRwPuHbw
8/29/2021 21:13:48,PatchNets: Patch-Based Generalizable Deep Implicit 3D Shape Representations,PatchNets,8/4/2020,https://arxiv.org/pdf/2008.01639.pdf,http://gvv.mpi-inf.mpg.de/projects/PatchNets/,https://github.com/edgar-tr/patchnets,"http://gvv.mpi-inf.mpg.de/projects/PatchNets/data/patchnets_short.mp4, http://gvv.mpi-inf.mpg.de/projects/PatchNets/data/patchnets_supplemental.mp4, http://gvv.mpi-inf.mpg.de/projects/PatchNets/data/patchnets_talk.mp4",http://gvv.mpi-inf.mpg.de/projects/PatchNets/data/patchnets_supplemental.pdf,,,"@inproceedings{tretschk2020patchnets,
  BOOKTITLE = {Proceedings of the European Conference on Computer Vision (ECCV)},
  ID = {tretschk2020patchnets},
  ENTRYTYPE = {inproceedings},
  AUTHOR = {Edgar Tretschk and Ayush Tewari and Vladislav Golyanik and Michael Zollhofer and Carsten Stoll and Christian Theobalt},
  TITLE = {PatchNets: Patch-Based Generalizable Deep Implicit 3D Shape Representations},
  EPRINT = {2008.01639v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Implicit surface representations, such as signed-distance functions, combined with deep learning have led to impressive models which can represent detailed shapes of objects with arbitrary topology. Since a continuous function is learned, the reconstructions can also be extracted at any arbitrary resolution. However, large datasets such as ShapeNet are required to train such models. In this paper, we present a new mid-level patch-based surface representation. At the level of patches, objects across different categories share similarities, which leads to more generalizable models. We then introduce a novel method to learn this patch-based representation in a canonical space, such that it is as object-agnostic as possible. We show that our representation trained on one category of objects from ShapeNet can also well represent detailed shapes from any other category. In addition, it can be trained using much fewer shapes, compared to existing approaches. We show several applications of our new representation, including shape interpolation and partial point cloud completion. Due to explicit control over positions, orientations and scales of patches, our representation is also more controllable compared to object-level representations, which enables us to deform encoded shapes non-rigidly.},
  YEAR = {2020},
  URL = {http://arxiv.org/abs/2008.01639v2},
  FILE = {2008.01639v2.pdf}
 }","Human body, Generalization","Conditional neural field, Volume partitioning, Data-driven",,SDF,Universal,,,,,,,ECCV 2020,,Yes,Direct,"Edgar Tretschk, Ayush Tewari, Vladislav Golyanik, Michael Zollhöfer, Carsten Stoll, Christian Theobalt",tretschk2020patchnets,00000184,"Implicit surface representations, such as signed-distance functions, combined with deep learning have led to impressive models which can represent detailed shapes of objects with arbitrary topology. Since a continuous function is learned, the reconstructions can also be extracted at any arbitrary resolution. However, large datasets such as ShapeNet are required to train such models. In this paper, we present a new mid-level patch-based surface representation. At the level of patches, objects across different categories share similarities, which leads to more generalizable models. We then introduce a novel method to learn this patch-based representation in a canonical space, such that it is as object-agnostic as possible. We show that our representation trained on one category of objects from ShapeNet can also well represent detailed shapes from any other category. In addition, it can be trained using much fewer shapes, compared to existing approaches. We show several applications of our new representation, including shape interpolation and partial point cloud completion. Due to explicit control over positions, orientations and scales of patches, our representation is also more controllable compared to object-level representations, which enables us to deform encoded shapes non-rigidly.",16,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnufOhNCcU_6_tYUecQo7kJc6KXhF7IhIvsUy2NRVH4wWhjTVpZwo2t34enCiJC93gDw
5/23/2021 19:07:32,NeRF in the Wild: Neural Radiance Fields for Unconstrained Photo Collections,NeRF-W,8/5/2020,https://arxiv.org/pdf/2008.02268.pdf,https://nerf-w.github.io/,,,,,,"@inproceedings{martin-brualla2021nerfw,
  BOOKTITLE = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  ID = {martin-brualla2021nerfw},
  ENTRYTYPE = {inproceedings},
  AUTHOR = {Ricardo Martin-Brualla and Noha Radwan and Mehdi S. M. Sajjadi and Jonathan T. Barron and Alexey Dosovitskiy and Daniel Duckworth},
  TITLE = {NeRF in the Wild: Neural Radiance Fields for Unconstrained Photo Collections},
  EPRINT = {2008.02268v3},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We present a learning-based method for synthesizing novel views of complex scenes using only unstructured collections of in-the-wild photographs. We build on Neural Radiance Fields (NeRF), which uses the weights of a multilayer perceptron to model the density and color of a scene as a function of 3D coordinates. While NeRF works well on images of static subjects captured under controlled settings, it is incapable of modeling many ubiquitous, real-world phenomena in uncontrolled images, such as variable illumination or transient occluders. We introduce a series of extensions to NeRF to address these issues, thereby enabling accurate reconstructions from unstructured image collections taken from the internet. We apply our system, dubbed NeRF-W, to internet photo collections of famous landmarks, and demonstrate temporally consistent novel view renderings that are significantly closer to photorealism than the prior state of the art.},
  YEAR = {2021},
  URL = {http://arxiv.org/abs/2008.02268v3},
  FILE = {2008.02268v3.pdf}
 }","Dynamic, Segmentation/composition, Material/lighting estimation",Volume partitioning,,,,,,,,,,CVPR 2021 (Oral),,,,"Ricardo Martin-Brualla, Noha Radwan, Mehdi S. M. Sajjadi, Jonathan T. Barron, Alexey Dosovitskiy, Daniel Duckworth",martin-brualla2021nerfw,00000031,"We present a learning-based method for synthesizing novel views of complex scenes using only unstructured collections of in-the-wild photographs. We build on Neural Radiance Fields (NeRF), which uses the weights of a multilayer perceptron to model the density and color of a scene as a function of 3D coordinates. While NeRF works well on images of static subjects captured under controlled settings, it is incapable of modeling many ubiquitous, real-world phenomena in uncontrolled images, such as variable illumination or transient occluders. We introduce a series of extensions to NeRF to address these issues, thereby enabling accurate reconstructions from unstructured image collections taken from the internet. We apply our system, dubbed NeRF-W, to internet photo collections of famous landmarks, and demonstrate temporally consistent novel view renderings that are significantly closer to photorealism than the prior state of the art.",78,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnudJbhp-mitDgz-cxQCv7HRgbz58nsTTyYkD-QsDKH9KPAkxLJK5tIlPfnNSqGKn8cg
5/23/2021 19:10:06,Neural Reflectance Fields for Appearance Acquisition,,8/9/2020,https://arxiv.org/pdf/2008.03824.pdf,,,https://www.youtube.com/watch?v=tQZk5OoFgsc,,,,"@article{bi2020neural,
  JOURNAL = {arXiv preprint arXiv:2008.03824},
  BOOKTITLE = {ArXiv Pre-print},
  ID = {bi2020neural},
  ENTRYTYPE = {article},
  AUTHOR = {Sai Bi and Zexiang Xu and Pratul Srinivasan and Ben Mildenhall and Kalyan Sunkavalli and Milos Hasan and Yannick Hold-Geoffroy and David Kriegman and Ravi Ramamoorthi},
  TITLE = {Neural Reflectance Fields for Appearance Acquisition},
  EPRINT = {2008.03824v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We present Neural Reflectance Fields, a novel deep scene representation that encodes volume density, normal and reflectance properties at any 3D point in a scene using a fully-connected neural network. We combine this representation with a physically-based differentiable ray marching framework that can render images from a neural reflectance field under any viewpoint and light. We demonstrate that neural reflectance fields can be estimated from images captured with a simple collocated camera-light setup, and accurately model the appearance of real-world scenes with complex geometry and reflectance. Once estimated, they can be used to render photo-realistic images under novel viewpoint and (non-collocated) lighting conditions and accurately reproduce challenging effects like specularities, shadows and occlusions. This allows us to perform high-quality view synthesis and relighting that is significantly better than previous methods. We also demonstrate that we can compose the estimated neural reflectance field of a real scene with traditional scene models and render them using standard Monte Carlo rendering engines. Our work thus enables a complete pipeline from high-quality and practical appearance acquisition to 3D scene composition and rendering.},
  YEAR = {2020},
  URL = {http://arxiv.org/abs/2008.03824v2},
  FILE = {2008.03824v2.pdf}
 }",Material/lighting estimation,,,,,,,,,,,ARXIV 2020,,,,"Sai Bi, Zexiang Xu, Pratul Srinivasan, Ben Mildenhall, Kalyan Sunkavalli, Miloš Hašan, Yannick Hold-Geoffroy, David Kriegman, Ravi Ramamoorthi",bi2020neural,00000032,"We present Neural Reflectance Fields, a novel deep scene representation that encodes volume density, normal and reflectance properties at any 3D point in a scene using a fully-connected neural network. We combine this representation with a physically-based differentiable ray marching framework that can render images from a neural reflectance field under any viewpoint and light. We demonstrate that neural reflectance fields can be estimated from images captured with a simple collocated camera-light setup, and accurately model the appearance of real-world scenes with complex geometry and reflectance. Once estimated, they can be used to render photo-realistic images under novel viewpoint and (non-collocated) lighting conditions and accurately reproduce challenging effects like specularities, shadows and occlusions. This allows us to perform high-quality view synthesis and relighting that is significantly better than previous methods. We also demonstrate that we can compose the estimated neural reflectance field of a real scene with traditional scene models and render them using standard Monte Carlo rendering engines. Our work thus enables a complete pipeline from high-quality and practical appearance acquisition to 3D scene composition and rendering.",21,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnufVMe_pxDvMSWMA0vTHYGNLHJPzFsgZg3saq62pxKFePzAFXydwpW9JDVDeeFkDMAY
9/17/2021 11:38:32,Grasping Field: Learning Implicit Representations for Human Grasps,Grasping Field,8/10/2020,https://arxiv.org/pdf/2008.04451.pdf,https://mano.is.tue.mpg.de/,https://github.com/korrawe/grasping_field,https://www.youtube.com/watch?v=_1o21xc3TD0,,,,"@article{karunratanakul2020graspingfield,
  ID = {karunratanakul2020graspingfield},
  ENTRYTYPE = {article},
  AUTHOR = {Korrawe Karunratanakul and Jinlong Yang and Yan Zhang and Michael Black and Krikamol Muandet and Siyu Tang},
  TITLE = {Grasping Field: Learning Implicit Representations for Human Grasps},
  EPRINT = {2008.04451v3},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Robotic grasping of house-hold objects has made remarkable progress in recent years. Yet, human grasps are still difficult to synthesize realistically. There are several key reasons: (1) the human hand has many degrees of freedom (more than robotic manipulators); (2) the synthesized hand should conform to the surface of the object; and (3) it should interact with the object in a semantically and physically plausible manner. To make progress in this direction, we draw inspiration from the recent progress on learning-based implicit representations for 3D object reconstruction. Specifically, we propose an expressive representation for human grasp modelling that is efficient and easy to integrate with deep neural networks. Our insight is that every point in a three-dimensional space can be characterized by the signed distances to the surface of the hand and the object, respectively. Consequently, the hand, the object, and the contact area can be represented by implicit surfaces in a common space, in which the proximity between the hand and the object can be modelled explicitly. We name this 3D to 2D mapping as Grasping Field, parameterize it with a deep neural network, and learn it from data. We demonstrate that the proposed grasping field is an effective and expressive representation for human grasp generation. Specifically, our generative model is able to synthesize high-quality human grasps, given only on a 3D object point cloud. The extensive experiments demonstrate that our generative model compares favorably with a strong baseline and approaches the level of natural human grasps. Our method improves the physical plausibility of the hand-object contact reconstruction and achieves comparable performance for 3D hand reconstruction compared to state-of-the-art methods.},
  YEAR = {2020},
  MONTH = {Aug},
  URL = {http://arxiv.org/abs/2008.04451v3},
  FILE = {2008.04451v3.pdf}
 }","Beyond graphics, Robotics",,,SDF,Category-level,,,,,,,3DV 2020 (Best Paper),,Yes,"Direct, Indirect","Korrawe Karunratanakul, Jinlong Yang, Yan Zhang, Michael Black, Krikamol Muandet, Siyu Tang",karunratanakul2020graspingfield,00000195,"Robotic grasping of house-hold objects has made remarkable progress in recent years. Yet, human grasps are still difficult to synthesize realistically. There are several key reasons: (1) the human hand has many degrees of freedom (more than robotic manipulators); (2) the synthesized hand should conform to the surface of the object; and (3) it should interact with the object in a semantically and physically plausible manner. To make progress in this direction, we draw inspiration from the recent progress on learning-based implicit representations for 3D object reconstruction. Specifically, we propose an expressive representation for human grasp modelling that is efficient and easy to integrate with deep neural networks. Our insight is that every point in a three-dimensional space can be characterized by the signed distances to the surface of the hand and the object, respectively. Consequently, the hand, the object, and the contact area can be represented by implicit surfaces in a common space, in which the proximity between the hand and the object can be modelled explicitly. We name this 3D to 2D mapping as Grasping Field, parameterize it with a deep neural network, and learn it from data. We demonstrate that the proposed grasping field is an effective and expressive representation for human grasp generation. Specifically, our generative model is able to synthesize high-quality human grasps, given only on a 3D object point cloud. The extensive experiments demonstrate that our generative model compares favorably with a strong baseline and approaches the level of natural human grasps. Our method improves the physical plausibility of the hand-object contact reconstruction and achieves comparable performance for 3D hand reconstruction compared to state-of-the-art methods.",,,No,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnufJYxl7u98LVNosbGp7GORnkfzV5dnmTWYC-PWmnsSX1E2BN34qB2SEJQ7fTqviN6M
9/18/2021 10:22:36,Pix2Surf: Learning Parametric 3D Surface Models of Objects from Images,Pix2Surf,8/18/2020,https://arxiv.org/pdf/2008.07760.pdf,https://geometry.stanford.edu/projects/pix2surf/,https://github.com/JiahuiLei/Pix2Surf,https://www.youtube.com/watch?v=jaxB0VSuvms,https://geometry.stanford.edu/projects/pix2surf/pub/pix2surf_supp.pdf,,,"@inproceedings{lei2020pix2surf,
  BOOKTITLE = {Proceedings of the European Conference on Computer Vision (ECCV)},
  ID = {lei2020pix2surf},
  ENTRYTYPE = {inproceedings},
  AUTHOR = {Jiahui Lei and Srinath Sridhar and Paul Guerrero and Minhyuk Sung and Niloy Mitra and Leonidas J. Guibas},
  TITLE = {Pix2Surf: Learning Parametric 3D Surface Models of Objects from Images},
  EPRINT = {2008.07760v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We investigate the problem of learning to generate 3D parametric surface representations for novel object instances, as seen from one or more views. Previous work on learning shape reconstruction from multiple views uses discrete representations such as point clouds or voxels, while continuous surface generation approaches lack multi-view consistency. We address these issues by designing neural networks capable of generating high-quality parametric 3D surfaces which are also consistent between views. Furthermore, the generated 3D surfaces preserve accurate image pixel to 3D surface point correspondences, allowing us to lift texture information to reconstruct shapes with rich geometry and appearance. Our method is supervised and trained on a public dataset of shapes from common object categories. Quantitative results indicate that our method significantly outperforms previous work, while qualitative results demonstrate the high quality of our reconstructions.},
  YEAR = {2020},
  URL = {http://arxiv.org/abs/2008.07760v1},
  FILE = {2008.07760v1.pdf}
 }","Few-shot reconstruction, Generalization","Generative/adversarial formulation, Conditional neural field, Lifting 2D features to 3D",None,"Atlas, Explicit",Category-level,,,,,,,ECCV 2020,,Yes,Direct,"Jiahui Lei, Srinath Sridhar, Paul Guerrero, Minhyuk Sung, Niloy Mitra, Leonidas J. Guibas",lei2020pix2surf,00000224,"We investigate the problem of learning to generate 3D parametric surface representations for novel object instances, as seen from one or more views. Previous work on learning shape reconstruction from multiple views uses discrete representations such as point clouds or voxels, while continuous surface generation approaches lack multi-view consistency. We address these issues by designing neural networks capable of generating high-quality parametric 3D surfaces which are also consistent between views. Furthermore, the generated 3D surfaces preserve accurate image pixel to 3D surface point correspondences, allowing us to lift texture information to reconstruct shapes with rich geometry and appearance. Our method is supervised and trained on a public dataset of shapes from common object categories. Quantitative results indicate that our method significantly outperforms previous work, while qualitative results demonstrate the high quality of our reconstructions.",,,No,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnueyhdc2bh2JyPf2GXuqOx7mfx7eFhehYVfrxhOzrME4tSznktL0FcXXRDvL8QZydXU
6/29/2021 15:53:44,On the Effectiveness of Weight-Encoded Neural Implicit 3D Shapes,,9/17/2020,https://arxiv.org/pdf/2009.09808.pdf,,https://github.com/u2ni/ICML2021,,,,,"@inproceedings{davies2021on,
  PUBLISHER = {PMLR},
  BOOKTITLE = {International Conference on Machine Learning (ICML)},
  ID = {davies2021on},
  ENTRYTYPE = {inproceedings},
  AUTHOR = {Thomas Davies and Derek Nowrouzezahrai and Alec Jacobson},
  TITLE = {On the Effectiveness of Weight-Encoded Neural Implicit 3D Shapes},
  EPRINT = {2009.09808v3},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.GR},
  ABSTRACT = {A neural implicit outputs a number indicating whether the given query point in space is inside, outside, or on a surface. Many prior works have focused on _latent-encoded_ neural implicits, where a latent vector encoding of a specific shape is also fed as input. While affording latent-space interpolation, this comes at the cost of reconstruction accuracy for any _single_ shape. Training a specific network for each 3D shape, a _weight-encoded_ neural implicit may forgo the latent vector and focus reconstruction accuracy on the details of a single shape. While previously considered as an intermediary representation for 3D scanning tasks or as a toy-problem leading up to latent-encoding tasks, weight-encoded neural implicits have not yet been taken seriously as a 3D shape representation. In this paper, we establish that weight-encoded neural implicits meet the criteria of a first-class 3D shape representation. We introduce a suite of technical contributions to improve reconstruction accuracy, convergence, and robustness when learning the signed distance field induced by a polygonal mesh -- the _de facto_ standard representation. Viewed as a lossy compression, our conversion outperforms standard techniques from geometry processing. Compared to previous latent- and weight-encoded neural implicits we demonstrate superior robustness, scalability, and performance.},
  YEAR = {2021},
  URL = {http://arxiv.org/abs/2009.09808v3},
  FILE = {2009.09808v3.pdf}
 }","Performance (training), Performance (rendering), Compression, Fundamentals",Sampling,,,,,,,,,,ICML 2021,https://ten-thousand-models.appspot.com/,Yes,,"Thomas Davies, Derek Nowrouzezahrai, Alec Jacobson",davies2021on,00000033,"A neural implicit outputs a number indicating whether the given query point in space is inside, outside, or on a surface. Many prior works have focused on _latent-encoded_ neural implicits, where a latent vector encoding of a specific shape is also fed as input. While affording latent-space interpolation, this comes at the cost of reconstruction accuracy for any _single_ shape. Training a specific network for each 3D shape, a _weight-encoded_ neural implicit may forgo the latent vector and focus reconstruction accuracy on the details of a single shape. While previously considered as an intermediary representation for 3D scanning tasks or as a toy-problem leading up to latent-encoding tasks, weight-encoded neural implicits have not yet been taken seriously as a 3D shape representation. In this paper, we establish that weight-encoded neural implicits meet the criteria of a first-class 3D shape representation. We introduce a suite of technical contributions to improve reconstruction accuracy, convergence, and robustness when learning the signed distance field induced by a polygonal mesh -- the _de facto_ standard representation. Viewed as a lossy compression, our conversion outperforms standard techniques from geometry processing. Compared to previous latent- and weight-encoded neural implicits we demonstrate superior robustness, scalability, and performance.",5,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnuc9DlJPzGFkuLraXBw0XEwdEX4RvwduOeIOK1B_Wd_AKGKiqXB_9weIClm5hg4udto
7/19/2021 21:09:05,Implicit Feature Networks for Texture Completion from Partial 3D Data,IF-Net-Texture,9/20/2020,https://arxiv.org/pdf/2009.09458.pdf,https://virtualhumans.mpi-inf.mpg.de/ifnets/,https://github.com/jchibane/if-net_texture,,,,,"@inproceedings{chibane2020ifnettexture,
  BOOKTITLE = {Proceedings of the European Conference on Computer Vision (ECCV)},
  ID = {chibane2020ifnettexture},
  ENTRYTYPE = {inproceedings},
  AUTHOR = {Julian Chibane and Gerard Pons-Moll},
  TITLE = {Implicit Feature Networks for Texture Completion from Partial 3D Data},
  EPRINT = {2009.09458v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Prior work to infer 3D texture use either texture atlases, which require uv-mappings and hence have discontinuities, or colored voxels, which are memory inefficient and limited in resolution. Recent work, predicts RGB color at every XYZ coordinate forming a texture field, but focus on completing texture given a single 2D image. Instead, we focus on 3D texture and geometry completion from partial and incomplete 3D scans. IF-Nets have recently achieved state-of-the-art results on 3D geometry completion using a multi-scale deep feature encoding, but the outputs lack texture. In this work, we generalize IF-Nets to texture completion from partial textured scans of humans and arbitrary objects. Our key insight is that 3D texture completion benefits from incorporating local and global deep features extracted from both the 3D partial texture and completed geometry. Specifically, given the partial 3D texture and the 3D geometry completed with IF-Nets, our model successfully in-paints the missing texture parts in consistence with the completed geometry. Our model won the SHARP ECCV'20 challenge, achieving highest performance on all challenges.},
  YEAR = {2020},
  NOTE = {SHARP Workshop, European Conference on Computer Vision (ECCV), 2020},
  URL = {http://arxiv.org/abs/2009.09458v1},
  FILE = {2009.09458v1.pdf}
 }",,"Feature volume, Data-driven",None,Occupancy,,,,,,,,ECCV 2020,,No,,"Julian Chibane, Gerard Pons-Moll",chibane2020ifnettexture,00000034,"Prior work to infer 3D texture use either texture atlases, which require uv-mappings and hence have discontinuities, or colored voxels, which are memory inefficient and limited in resolution. Recent work, predicts RGB color at every XYZ coordinate forming a texture field, but focus on completing texture given a single 2D image. Instead, we focus on 3D texture and geometry completion from partial and incomplete 3D scans. IF-Nets have recently achieved state-of-the-art results on 3D geometry completion using a multi-scale deep feature encoding, but the outputs lack texture. In this work, we generalize IF-Nets to texture completion from partial textured scans of humans and arbitrary objects. Our key insight is that 3D texture completion benefits from incorporating local and global deep features extracted from both the 3D partial texture and completed geometry. Specifically, given the partial 3D texture and the 3D geometry completed with IF-Nets, our model successfully in-paints the missing texture parts in consistence with the completed geometry. Our model won the SHARP ECCV'20 challenge, achieving highest performance on all challenges.",6,Yes,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnuf64KwMbm20qLtd5hKjlFfm7gu_XjAyq29NohRI3t_uDpGdqPoxIf-SxIlWSeERpDU
9/1/2021 14:36:37,"ContactNets: Learning Discontinuous Contact Dynamics with Smooth, Implicit Representations",ContactNets,9/23/2020,https://arxiv.org/pdf/2009.11193.pdf,,https://github.com/DAIRLab/contact-nets,https://www.youtube.com/watch?v=I6p8JrIp1Es,,,,"@inproceedings{pfrommer2020contactnets,
  BOOKTITLE = {Proceedings of the Conference on Robot Learning (CoRL)},
  ID = {pfrommer2020contactnets},
  ENTRYTYPE = {inproceedings},
  AUTHOR = {Samuel Pfrommer and Mathew Halm and Michael Posa},
  TITLE = {ContactNets: Learning Discontinuous Contact Dynamics with Smooth, Implicit Representations},
  EPRINT = {2009.11193v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.RO},
  ABSTRACT = {Common methods for learning robot dynamics assume motion is continuous, causing unrealistic model predictions for systems undergoing discontinuous impact and stiction behavior. In this work, we resolve this conflict with a smooth, implicit encoding of the structure inherent to contact-induced discontinuities. Our method, ContactNets, learns parameterizations of inter-body signed distance and contact-frame Jacobians, a representation that is compatible with many simulation, control, and planning environments for robotics. We furthermore circumvent the need to differentiate through stiff or non-smooth dynamics with a novel loss function inspired by the principles of complementarity and maximum dissipation. Our method can predict realistic impact, non-penetration, and stiction when trained on 60 seconds of real-world data.},
  YEAR = {2020},
  URL = {http://arxiv.org/abs/2009.11193v2},
  FILE = {2009.11193v2.pdf}
 }","Beyond graphics, Science and engineering, Robotics","Representation, Data-driven",,,,,,,,,,CoRL 2020,,,Direct,"Samuel Pfrommer, Mathew Halm, Michael Posa",pfrommer2020contactnets,00000087,"Common methods for learning robot dynamics assume motion is continuous, causing unrealistic model predictions for systems undergoing discontinuous impact and stiction behavior. In this work, we resolve this conflict with a smooth, implicit encoding of the structure inherent to contact-induced discontinuities. Our method, ContactNets, learns parameterizations of inter-body signed distance and contact-frame Jacobians, a representation that is compatible with many simulation, control, and planning environments for robotics. We furthermore circumvent the need to differentiate through stiff or non-smooth dynamics with a novel loss function inspired by the principles of complementarity and maximum dissipation. Our method can predict realistic impact, non-penetration, and stiction when trained on 60 seconds of real-world data.",0,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnufAgQ3J210fFgaT8bsS7yDLaiFrCrCOK9QphFOoc0GyayyAPKTWcTW_1xULBFtQkBU
9/30/2021 11:06:25,Learning Equality Constraints for Motion Planning on Manifolds,,9/24/2020,https://arxiv.org/pdf/2009.11852.pdf,,https://github.com/gsutanto/smp_manifold_learning,https://www.youtube.com/watch?v=WoC7nqp4XNk,,,,"@inproceedings{sutanto2020learning,
  BOOKTITLE = {Proceedings of the Conference on Robot Learning (CoRL)},
  ID = {sutanto2020learning},
  ENTRYTYPE = {inproceedings},
  AUTHOR = {Giovanni Sutanto and Isabel M. Rayas Fernandez and Peter Englert and Ragesh K. Ramachandran and Gaurav S. Sukhatme},
  TITLE = {Learning Equality Constraints for Motion Planning on Manifolds},
  EPRINT = {2009.11852v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.RO},
  ABSTRACT = {Constrained robot motion planning is a widely used technique to solve complex robot tasks. We consider the problem of learning representations of constraints from demonstrations with a deep neural network, which we call Equality Constraint Manifold Neural Network (ECoMaNN). The key idea is to learn a level-set function of the constraint suitable for integration into a constrained sampling-based motion planner. Learning proceeds by aligning subspaces in the network with subspaces of the data. We combine both learned constraints and analytically described constraints into the planner and use a projection-based strategy to find valid points. We evaluate ECoMaNN on its representation capabilities of constraint manifolds, the impact of its individual loss terms, and the motions produced when incorporated into a planner.},
  YEAR = {2020},
  URL = {http://arxiv.org/abs/2009.11852v1},
  FILE = {2009.11852v1.pdf}
 }","Fundamentals, Science and engineering, Robotics",,,,,,,,,,,CoRL 2020,,,,"Giovanni Sutanto, Isabel M. Rayas Fernández, Peter Englert, Ragesh K. Ramachandran, Gaurav S. Sukhatme",sutanto2020learning,00000226,"Constrained robot motion planning is a widely used technique to solve complex robot tasks. We consider the problem of learning representations of constraints from demonstrations with a deep neural network, which we call Equality Constraint Manifold Neural Network (ECoMaNN). The key idea is to learn a level-set function of the constraint suitable for integration into a constrained sampling-based motion planner. Learning proceeds by aligning subspaces in the network with subspaces of the data. We combine both learned constraints and analytically described constraints into the planner and use a projection-based strategy to find valid points. We evaluate ECoMaNN on its representation capabilities of constraint manifolds, the impact of its individual loss terms, and the motions produced when incorporated into a planner.",,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnucV2_GEzHh-I-zWv4MhASSIeelfnm1CgXQcaYdwoM6w6H5BmKoBIItMDnkDWFsCoDg
8/29/2021 21:03:41,"X-Fields: Implicit Neural View-, Light- and Time-Image Interpolation",X-Fields,10/1/2020,https://arxiv.org/pdf/2010.00450.pdf,https://xfields.mpi-inf.mpg.de/,https://github.com/m-bemana/xfields,https://www.youtube.com/watch?v=0tsw7yJGfFI,,,,"@article{bemana2020xfields,
  PUBLISHER = {Association for Computing Machinery},
  JOURNAL = {ACM Transactions on Graphics (TOG)},
  ID = {bemana2020xfields},
  ENTRYTYPE = {article},
  AUTHOR = {Mojtaba Bemana and Karol Myszkowski and Hans-Peter Seidel and Tobias Ritschel},
  TITLE = {X-Fields: Implicit Neural View-, Light- and Time-Image Interpolation},
  EPRINT = {2010.00450v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We suggest to represent an X-Field -a set of 2D images taken across different view, time or illumination conditions, i.e., video, light field, reflectance fields or combinations thereof-by learning a neural network (NN) to map their view, time or light coordinates to 2D images. Executing this NN at new coordinates results in joint view, time or light interpolation. The key idea to make this workable is a NN that already knows the ""basic tricks"" of graphics (lighting, 3D projection, occlusion) in a hard-coded and differentiable form. The NN represents the input to that rendering as an implicit map, that for any view, time, or light coordinate and for any pixel can quantify how it will move if view, time or light coordinates change (Jacobian of pixel position with respect to view, time, illumination, etc.). Our X-Field representation is trained for one scene within minutes, leading to a compact set of trainable parameters and hence real-time navigation in view, time and illumination.},
  YEAR = {2020},
  URL = {http://arxiv.org/abs/2010.00450v1},
  FILE = {2010.00450v1.pdf}
 }","Dynamic, Image, Editable, Material/lighting estimation",Warping field/Flow field,,,,,,,,,,SIGGRAPH 2020,https://xfields.mpi-inf.mpg.de/dataset/view_light_time.zip,,"Direct, Indirect","Mojtaba Bemana, Karol Myszkowski, Hans-Peter Seidel, Tobias Ritschel",bemana2020xfields,00000183,"We suggest to represent an X-Field -a set of 2D images taken across different view, time or illumination conditions, i.e., video, light field, reflectance fields or combinations thereof-by learning a neural network (NN) to map their view, time or light coordinates to 2D images. Executing this NN at new coordinates results in joint view, time or light interpolation. The key idea to make this workable is a NN that already knows the ""basic tricks"" of graphics (lighting, 3D projection, occlusion) in a hard-coded and differentiable form. The NN represents the input to that rendering as an implicit map, that for any view, time, or light coordinate and for any pixel can quantify how it will move if view, time or light coordinates change (Jacobian of pixel position with respect to view, time, illumination, etc.). Our X-Field representation is trained for one scene within minutes, leading to a compact set of trainable parameters and hence real-time navigation in view, time and illumination.",17,,Yes,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnuccPU3g-5A5dCmF3yCNLlEchZVbyGZBBA6rYu9cgxDtzrztMAOO3aYJyayFdS3lXA8
7/20/2021 10:58:11,General Radiance Field,GRF,10/9/2020,https://arxiv.org/pdf/2010.04595.pdf,,https://github.com/alextrevithick/GRF,,,https://drive.google.com/file/d/1H2FNeAsKoQqCsO0n7PiA1HcT1ingnwJd/view,,"@inproceedings{trevithick2021grf,
  BOOKTITLE = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
  ID = {trevithick2021grf},
  ENTRYTYPE = {inproceedings},
  AUTHOR = {Alex Trevithick and Bo Yang},
  TITLE = {GRF: Learning a General Radiance Field for 3D Representation and Rendering},
  EPRINT = {2010.04595v3},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We present a simple yet powerful neural network that implicitly represents and renders 3D objects and scenes only from 2D observations. The network models 3D geometries as a general radiance field, which takes a set of 2D images with camera poses and intrinsics as input, constructs an internal representation for each point of the 3D space, and then renders the corresponding appearance and geometry of that point viewed from an arbitrary position. The key to our approach is to learn local features for each pixel in 2D images and to then project these features to 3D points, thus yielding general and rich point representations. We additionally integrate an attention mechanism to aggregate pixel features from multiple 2D views, such that visual occlusions are implicitly taken into account. Extensive experiments demonstrate that our method can generate high-quality and realistic novel views for novel objects, unseen categories and challenging real-world scenes.},
  YEAR = {2021},
  URL = {http://arxiv.org/abs/2010.04595v3},
  FILE = {2010.04595v3.pdf}
 }","Performance (training), Generalization","Lifting 2D features to 3D, Image-based rendering",,,,,,,,,,ICCV 2021,,,,"Alex Trevithick, Bo Yang",trevithick2021grf,00000035,"We present a simple yet powerful neural network that implicitly represents and renders 3D objects and scenes only from 2D observations. The network models 3D geometries as a general radiance field, which takes a set of 2D images with camera poses and intrinsics as input, constructs an internal representation for each point of the 3D space, and then renders the corresponding appearance and geometry of that point viewed from an arbitrary position. The key to our approach is to learn local features for each pixel in 2D images and to then project these features to 3D points, thus yielding general and rich point representations. We additionally integrate an attention mechanism to aggregate pixel features from multiple 2D views, such that visual occlusions are implicitly taken into account. Extensive experiments demonstrate that our method can generate high-quality and realistic novel views for novel objects, unseen categories and challenging real-world scenes.",25,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnucOa_bFYMQTE_EOvrVDpHH3FKntR_ptD7laR8Px-6vUwC04waQ2h92JoFyymeuEqIE
5/23/2021 19:04:29,NeRF++: Analyzing and Improving Neural Radiance Fields,NeRF++,10/15/2020,https://arxiv.org/pdf/2010.07492.pdf,https://github.com/Kai-46/nerfplusplus,,https://www.youtube.com/watch?v=Rd0nBO6--bM&feature=youtu.be&t=1992,,,,"@article{zhang2020nerf++,
  JOURNAL = {arXiv preprint arXiv:2010.07492},
  BOOKTITLE = {ArXiv Pre-print},
  ID = {zhang2020nerf++},
  ENTRYTYPE = {article},
  AUTHOR = {Kai Zhang and Gernot Riegler and Noah Snavely and Vladlen Koltun},
  TITLE = {NeRF++: Analyzing and Improving Neural Radiance Fields},
  EPRINT = {2010.07492v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Neural Radiance Fields (NeRF) achieve impressive view synthesis results for a variety of capture settings, including 360 capture of bounded scenes and forward-facing capture of bounded and unbounded scenes. NeRF fits multi-layer perceptrons (MLPs) representing view-invariant opacity and view-dependent color volumes to a set of training images, and samples novel views based on volume rendering techniques. In this technical report, we first remark on radiance fields and their potential ambiguities, namely the shape-radiance ambiguity, and analyze NeRF's success in avoiding such ambiguities. Second, we address a parametrization issue involved in applying NeRF to 360 captures of objects within large-scale, unbounded 3D scenes. Our method improves view synthesis fidelity in this challenging scenario. Code is available at https://github.com/Kai-46/nerfplusplus.},
  YEAR = {2020},
  URL = {http://arxiv.org/abs/2010.07492v2},
  FILE = {2010.07492v2.pdf}
 }",Fundamentals,"Sampling, Volume partitioning",,,,,,,,,,ARXIV 2020,,,,"Kai Zhang, Gernot Riegler, Noah Snavely, Vladlen Koltun",zhang2020nerf++,00000036,"Neural Radiance Fields (NeRF) achieve impressive view synthesis results for a variety of capture settings, including 360 capture of bounded scenes and forward-facing capture of bounded and unbounded scenes. NeRF fits multi-layer perceptrons (MLPs) representing view-invariant opacity and view-dependent color volumes to a set of training images, and samples novel views based on volume rendering techniques. In this technical report, we first remark on radiance fields and their potential ambiguities, namely the shape-radiance ambiguity, and analyze NeRF's success in avoiding such ambiguities. Second, we address a parametrization issue involved in applying NeRF to 360 captures of objects within large-scale, unbounded 3D scenes. Our method improves view synthesis fidelity in this challenging scenario. Code is available at https://github.com/Kai-46/nerfplusplus.",48,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnueRPBNGU8eOxAjdcB5s9K0xGntKzK2_iLVjuKyLpYOYSzmcSYpdJG7N3S4WgsgPO4o
7/19/2021 21:13:04,SDF-SRN: Learning Signed Distance 3D Object Reconstruction from Static Images,SDF-SRN,10/20/2020,https://arxiv.org/pdf/2010.10505.pdf,https://chenhsuanlin.bitbucket.io/signed-distance-SRN/,https://github.com/chenhsuanlin/signed-distance-SRN,"https://chenhsuanlin.bitbucket.io/signed-distance-SRN/video.mp4, https://chenhsuanlin.bitbucket.io/signed-distance-SRN/shorttalk.mp4",https://chenhsuanlin.bitbucket.io/signed-distance-SRN/supplementary.pdf,,,"@inproceedings{lin2020sdfsrn,
  PUBLISHER = {Curran Associates, Inc.},
  BOOKTITLE = {Advances in Neural Information Processing Systems (NeurIPS)},
  ID = {lin2020sdfsrn},
  ENTRYTYPE = {inproceedings},
  AUTHOR = {Chen-Hsuan Lin and Chaoyang Wang and Simon Lucey},
  TITLE = {SDF-SRN: Learning Signed Distance 3D Object Reconstruction from Static Images},
  EPRINT = {2010.10505v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Dense 3D object reconstruction from a single image has recently witnessed remarkable advances, but supervising neural networks with ground-truth 3D shapes is impractical due to the laborious process of creating paired image-shape datasets. Recent efforts have turned to learning 3D reconstruction without 3D supervision from RGB images with annotated 2D silhouettes, dramatically reducing the cost and effort of annotation. These techniques, however, remain impractical as they still require multi-view annotations of the same object instance during training. As a result, most experimental efforts to date have been limited to synthetic datasets. In this paper, we address this issue and propose SDF-SRN, an approach that requires only a single view of objects at training time, offering greater utility for real-world scenarios. SDF-SRN learns implicit 3D shape representations to handle arbitrary shape topologies that may exist in the datasets. To this end, we derive a novel differentiable rendering formulation for learning signed distance functions (SDF) from 2D silhouettes. Our method outperforms the state of the art under challenging single-view supervision settings on both synthetic and real-world datasets.},
  YEAR = {2020},
  URL = {http://arxiv.org/abs/2010.10505v1},
  FILE = {2010.10505v1.pdf}
 }",Generalization,"Hypernetwork, Data-driven",,,,,,,,,,NeurIPS 2020,,Yes,,"Chen-Hsuan Lin, Chaoyang Wang, Simon Lucey",lin2020sdfsrn,00000037,"Dense 3D object reconstruction from a single image has recently witnessed remarkable advances, but supervising neural networks with ground-truth 3D shapes is impractical due to the laborious process of creating paired image-shape datasets. Recent efforts have turned to learning 3D reconstruction without 3D supervision from RGB images with annotated 2D silhouettes, dramatically reducing the cost and effort of annotation. These techniques, however, remain impractical as they still require multi-view annotations of the same object instance during training. As a result, most experimental efforts to date have been limited to synthetic datasets. In this paper, we address this issue and propose SDF-SRN, an approach that requires only a single view of objects at training time, offering greater utility for real-world scenarios. SDF-SRN learns implicit 3D shape representations to handle arbitrary shape topologies that may exist in the datasets. To this end, we derive a novel differentiable rendering formulation for learning signed distance functions (SDF) from 2D silhouettes. Our method outperforms the state of the art under challenging single-view supervision settings on both synthetic and real-world datasets.",7,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnucIwSHgYKgEpr6gj0-2S1hvpldW_7eRVisXXF8etsuv4Pp_-o_-ytyz-YOX_LEHfZM
9/17/2021 13:19:41,"LoopReg: Self-supervised Learning of Implicit Surface Correspondences, Pose and Shape for 3D Human Mesh Registration",LoopReg,10/23/2020,https://arxiv.org/pdf/2010.12447.pdf,,https://github.com/bharat-b7/LoopReg,https://www.youtube.com/watch?v=fIhm_tWG_X8,,,,"@inproceedings{bhatnagar2020loopreg,
  PUBLISHER = {Curran Associates, Inc.},
  BOOKTITLE = {Advances in Neural Information Processing Systems (NeurIPS)},
  ID = {bhatnagar2020loopreg},
  ENTRYTYPE = {inproceedings},
  AUTHOR = {Bharat Lal Bhatnagar and Cristian Sminchisescu and Christian Theobalt and Gerard Pons-Moll},
  TITLE = {LoopReg: Self-supervised Learning of Implicit Surface Correspondences, Pose and Shape for 3D Human Mesh Registration},
  EPRINT = {2010.12447v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We address the problem of fitting 3D human models to 3D scans of dressed humans. Classical methods optimize both the data-to-model correspondences and the human model parameters (pose and shape), but are reliable only when initialized close to the solution. Some methods initialize the optimization based on fully supervised correspondence predictors, which is not differentiable end-to-end, and can only process a single scan at a time. Our main contribution is LoopReg, an end-to-end learning framework to register a corpus of scans to a common 3D human model. The key idea is to create a self-supervised loop. A backward map, parameterized by a Neural Network, predicts the correspondence from every scan point to the surface of the human model. A forward map, parameterized by a human model, transforms the corresponding points back to the scan based on the model parameters (pose and shape), thus closing the loop. Formulating this closed loop is not straightforward because it is not trivial to force the output of the NN to be on the surface of the human model - outside this surface the human model is not even defined. To this end, we propose two key innovations. First, we define the canonical surface implicitly as the zero level set of a distance field in R3, which in contrast to morecommon UV parameterizations, does not require cutting the surface, does not have discontinuities, and does not induce distortion. Second, we diffuse the human model to the 3D domain R3. This allows to map the NN predictions forward,even when they slightly deviate from the zero level set. Results demonstrate that we can train LoopRegmainly self-supervised - following a supervised warm-start, the model becomes increasingly more accurate as additional unlabelled raw scans are processed. Our code and pre-trained models can be downloaded for research.},
  YEAR = {2020},
  NOTE = {NeurIPS 2020},
  URL = {http://arxiv.org/abs/2010.12447v1},
  FILE = {2010.12447v1.pdf}
 }",Human body,Warping field/Flow field,,SDF,Category-level,,,,,,,NeurIPS 2020,,Yes,Direct,"Bharat Lal Bhatnagar, Cristian Sminchisescu, Christian Theobalt, Gerard Pons-Moll",bhatnagar2020loopreg,00000202,"We address the problem of fitting 3D human models to 3D scans of dressed humans. Classical methods optimize both the data-to-model correspondences and the human model parameters (pose and shape), but are reliable only when initialized close to the solution. Some methods initialize the optimization based on fully supervised correspondence predictors, which is not differentiable end-to-end, and can only process a single scan at a time. Our main contribution is LoopReg, an end-to-end learning framework to register a corpus of scans to a common 3D human model. The key idea is to create a self-supervised loop. A backward map, parameterized by a Neural Network, predicts the correspondence from every scan point to the surface of the human model. A forward map, parameterized by a human model, transforms the corresponding points back to the scan based on the model parameters (pose and shape), thus closing the loop. Formulating this closed loop is not straightforward because it is not trivial to force the output of the NN to be on the surface of the human model - outside this surface the human model is not even defined. To this end, we propose two key innovations. First, we define the canonical surface implicitly as the zero level set of a distance field in R3, which in contrast to morecommon UV parameterizations, does not require cutting the surface, does not have discontinuities, and does not induce distortion. Second, we diffuse the human model to the 3D domain R3. This allows to map the NN predictions forward,even when they slightly deviate from the zero level set. Results demonstrate that we can train LoopRegmainly self-supervised - following a supervised warm-start, the model becomes increasingly more accurate as additional unlabelled raw scans are processed. Our code and pre-trained models can be downloaded for research.",,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnufVrGAQg1RxGyxzHnga5jbNq4Sj_lodxkJiDDRjSvHd7wePUtD0L5pyZiqmG2fHmqo
6/29/2021 15:14:51,Neural Unsigned Distance Fields for Implicit Function Learning,NDF,10/26/2020,https://arxiv.org/pdf/2010.13938.pdf,http://virtualhumans.mpi-inf.mpg.de/ndf/,https://github.com/jchibane/ndf/,https://www.youtube.com/watch?v=_xsLdVzX8DY,http://virtualhumans.mpi-inf.mpg.de/papers/chibane2020ndf/chibane2020ndf-supp.pdf,,,"@inproceedings{chibane2020ndf,
  PUBLISHER = {Curran Associates, Inc.},
  BOOKTITLE = {Advances in Neural Information Processing Systems (NeurIPS)},
  ID = {chibane2020ndf},
  ENTRYTYPE = {inproceedings},
  AUTHOR = {Julian Chibane and Aymen Mir and Gerard Pons-Moll},
  TITLE = {Neural Unsigned Distance Fields for Implicit Function Learning},
  EPRINT = {2010.13938v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {In this work we target a learnable output representation that allows continuous, high resolution outputs of arbitrary shape. Recent works represent 3D surfaces implicitly with a Neural Network, thereby breaking previous barriers in resolution, and ability to represent diverse topologies. However, neural implicit representations are limited to closed surfaces, which divide the space into inside and outside. Many real world objects such as walls of a scene scanned by a sensor, clothing, or a car with inner structures are not closed. This constitutes a significant barrier, in terms of data pre-processing (objects need to be artificially closed creating artifacts), and the ability to output open surfaces. In this work, we propose Neural Distance Fields (NDF), a neural network based model which predicts the unsigned distance field for arbitrary 3D shapes given sparse point clouds. NDF represent surfaces at high resolutions as prior implicit models, but do not require closed surface data, and significantly broaden the class of representable shapes in the output. NDF allow to extract the surface as very dense point clouds and as meshes. We also show that NDF allow for surface normal calculation and can be rendered using a slight modification of sphere tracing. We find NDF can be used for multi-target regression (multiple outputs for one input) with techniques that have been exclusively used for rendering in graphics. Experiments on ShapeNet show that NDF, while simple, is the state-of-the art, and allows to reconstruct shapes with inner structures, such as the chairs inside a bus. Notably, we show that NDF are not restricted to 3D shapes, and can approximate more general open surfaces such as curves, manifolds, and functions. Code is available for research at https://virtualhumans.mpi-inf.mpg.de/ndf/.},
  YEAR = {2020},
  NOTE = {Neural Information Processing Systems (NeurIPS) 2020},
  URL = {http://arxiv.org/abs/2010.13938v1},
  FILE = {2010.13938v1.pdf}
 }",Generalization,Data-driven,,UDF,Category-level,,,,,,,NeurIPS 2020,,Yes,,"Julian Chibane, Aymen Mir, Gerard Pons-Moll",chibane2020ndf,00000038,"In this work we target a learnable output representation that allows continuous, high resolution outputs of arbitrary shape. Recent works represent 3D surfaces implicitly with a Neural Network, thereby breaking previous barriers in resolution, and ability to represent diverse topologies. However, neural implicit representations are limited to closed surfaces, which divide the space into inside and outside. Many real world objects such as walls of a scene scanned by a sensor, clothing, or a car with inner structures are not closed. This constitutes a significant barrier, in terms of data pre-processing (objects need to be artificially closed creating artifacts), and the ability to output open surfaces. In this work, we propose Neural Distance Fields (NDF), a neural network based model which predicts the unsigned distance field for arbitrary 3D shapes given sparse point clouds. NDF represent surfaces at high resolutions as prior implicit models, but do not require closed surface data, and significantly broaden the class of representable shapes in the output. NDF allow to extract the surface as very dense point clouds and as meshes. We also show that NDF allow for surface normal calculation and can be rendered using a slight modification of sphere tracing. We find NDF can be used for multi-target regression (multiple outputs for one input) with techniques that have been exclusively used for rendering in graphics. Experiments on ShapeNet show that NDF, while simple, is the state-of-the art, and allows to reconstruct shapes with inner structures, such as the chairs inside a bus. Notably, we show that NDF are not restricted to 3D shapes, and can approximate more general open surfaces such as curves, manifolds, and functions. Code is available for research at https://virtualhumans.mpi-inf.mpg.de/ndf/.",17,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnucGgxZRd_xuEyVO3AkPDoIVCq_sdhhnkuHyiqozLmwJGCBg6I28noFTff07alAiF98
9/17/2021 11:30:00,Neural Light Field 3D Printing,,11/1/2020,https://dl.acm.org/doi/pdf/10.1145/3414685.3417879,https://quan-zheng.github.io/publication/neuralLF3Dprinting20/,Coming soon,,https://quan-zheng.github.io/publication/NeuralLightField3DPrinting-supp.pdf,,,"@article{zheng2020neural,
  YEAR = {2020},
  ID = {zheng2020neural},
  ENTRYTYPE = {article},
  ABSTRACT = {Modern 3D printers are capable of printing large-size light-field displays at high-resolutions. However, optimizing such displays in full 3D volume for a given light-field imagery is still a challenging task. Existing light field displays optimize over relatively small resolutions using},
  AUTHOR = {Quan Zheng and Vahid Babaei and Gordon Wetzstein and Hans-Peter Seidel and Matthias Zwicker and Gurprit Singh},
  JOURNAL = {ACM Transactions on Graphics (TOG)},
  NUMBER = {6},
  PAGES = {1--12},
  PUB_YEAR = {2020},
  PUBLISHER = {Association for Computing Machinery},
  TITLE = {Neural light field 3D printing},
  VENUE = {ACM Transactions on ...},
  VOLUME = {39}
 }","Beyond graphics, Alternative imaging, Science and engineering",,NeRF,,,,,,,,,SIGGRAPH 2020,https://drive.google.com/uc?id=1EGwwQsAlw4C1IS6jgK_4P9uP0Rf4ICug&export=download,,Direct,"Quan Zheng, Vahid Babaei, Gordon Wetzstein, Hans-Peter Seidel, Matthias Zwicker, Gurprit Singh",zheng2020neural,00000193,"Modern 3D printers are capable of printing large-size light-field displays at high-resolutions. However, optimizing such displays in full 3D volume for a given light-field imagery is still a challenging task. Existing light field displays optimize over relatively small resolutions using",,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnufwmW8XMLbhSzPw_3RyvwVEKgnUV6qSz3zhBiM5SIBd60ZkRzzT7J82ycJwGY5H4b0
9/30/2021 8:45:45,Learning Barrier Functions with Memory for Robust Safe Navigation,,11/3/2020,https://arxiv.org/pdf/2011.01899.pdf,,,,,,,"@article{long2021learning,
  ID = {long2021learning},
  ENTRYTYPE = {article},
  AUTHOR = {Kehan Long and Cheng Qian and Jorge Cortes and Nikolay Atanasov},
  TITLE = {Learning Barrier Functions with Memory for Robust Safe Navigation},
  EPRINT = {2011.01899v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.RO},
  ABSTRACT = {Control barrier functions are widely used to enforce safety properties in robot motion planning and control. However, the problem of constructing barrier functions online and synthesizing safe controllers that can deal with the associated uncertainty has received little attention. This paper investigates safe navigation in unknown environments, using onboard range sensing to construct control barrier functions online. To represent different objects in the environment, we use the distance measurements to train neural network approximations of the signed distance functions incrementally with replay memory. This allows us to formulate a novel robust control barrier safety constraint which takes into account the error in the estimated distance fields and its gradient. Our formulation leads to a second-order cone program, enabling safe and stable control synthesis in a priori unknown environments.},
  YEAR = {2020},
  MONTH = {Nov},
  URL = {http://arxiv.org/abs/2011.01899v2},
  FILE = {2011.01899v2.pdf}
 }","Robotics, Multi-task/Continual/Transfer learning",,,SDF,,,,,,,,RAL 2021,,,,"Kehan Long, Cheng Qian, Jorge Cortés, Nikolay Atanasov",long2021learning,00000224,"Control barrier functions are widely used to enforce safety properties in robot motion planning and control. However, the problem of constructing barrier functions online and synthesizing safe controllers that can deal with the associated uncertainty has received little attention. This paper investigates safe navigation in unknown environments, using onboard range sensing to construct control barrier functions online. To represent different objects in the environment, we use the distance measurements to train neural network approximations of the signed distance functions incrementally with replay memory. This allows us to formulate a novel robust control barrier safety constraint which takes into account the error in the estimated distance fields and its gradient. Our formulation leads to a second-order cone program, enabling safe and stable control synthesis in a priori unknown environments.",,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnudr9tmP6zXIDvPuHamrBJ7ZfJt8V6suVSmW9nr9_feBgisSGW0do0aZz3ywYQXZcI8
6/21/2021 16:40:25,Neural Scene Graphs for Dynamic Scenes,,11/20/2020,https://arxiv.org/pdf/2011.10379.pdf,https://light.princeton.edu/publication/neural-scene-graphs/,,https://www.youtube.com/watch?v=ea4Y6P0Hk3o,https://light.cs.princeton.edu/wp-content/uploads/2021/02/NeuralSceneGraphs_Supplement.pdf,,,"@inproceedings{ost2021neural,
  BOOKTITLE = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  ID = {ost2021neural},
  ENTRYTYPE = {inproceedings},
  AUTHOR = {Julian Ost and Fahim Mannan and Nils Thuerey and Julian Knodt and Felix Heide},
  TITLE = {Neural Scene Graphs for Dynamic Scenes},
  EPRINT = {2011.10379v3},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Recent implicit neural rendering methods have demonstrated that it is possible to learn accurate view synthesis for complex scenes by predicting their volumetric density and color supervised solely by a set of RGB images. However, existing methods are restricted to learning efficient representations of static scenes that encode all scene objects into a single neural network, and lack the ability to represent dynamic scenes and decompositions into individual scene objects. In this work, we present the first neural rendering method that decomposes dynamic scenes into scene graphs. We propose a learned scene graph representation, which encodes object transformation and radiance, to efficiently render novel arrangements and views of the scene. To this end, we learn implicitly encoded scenes, combined with a jointly learned latent representation to describe objects with a single implicit function. We assess the proposed method on synthetic and real automotive data, validating that our approach learns dynamic scenes -- only by observing a video of this scene -- and allows for rendering novel photo-realistic views of novel scene compositions with unseen sets of objects at unseen poses.},
  YEAR = {2021},
  URL = {http://arxiv.org/abs/2011.10379v3},
  FILE = {2011.10379v3.pdf}
 }","Dynamic, Segmentation/composition, Beyond graphics","Conditional neural field, Object-centric representation",,,,,,,,,,CVPR 2021 (Oral),,,,"Julian Ost, Fahim Mannan, Nils Thuerey, Julian Knodt, Felix Heide",ost2021neural,00000039,"Recent implicit neural rendering methods have demonstrated that it is possible to learn accurate view synthesis for complex scenes by predicting their volumetric density and color supervised solely by a set of RGB images. However, existing methods are restricted to learning efficient representations of static scenes that encode all scene objects into a single neural network, and lack the ability to represent dynamic scenes and decompositions into individual scene objects. In this work, we present the first neural rendering method that decomposes dynamic scenes into scene graphs. We propose a learned scene graph representation, which encodes object transformation and radiance, to efficiently render novel arrangements and views of the scene. To this end, we learn implicitly encoded scenes, combined with a jointly learned latent representation to describe objects with a single implicit function. We assess the proposed method on synthetic and real automotive data, validating that our approach learns dynamic scenes -- only by observing a video of this scene -- and allows for rendering novel photo-realistic views of novel scene compositions with unseen sets of objects at unseen poses.",12,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnueDY6CdA1obXGcDsBFUpFnvkNSn2p5iSxY-l-aLPcKDgYGRMw0km20_bxneOI_dx6M
5/23/2021 19:03:39,GIRAFFE: Representing Scenes as Compositional Generative Neural Feature Fields,GIRAFFE,11/24/2020,https://arxiv.org/pdf/2011.12100.pdf,,,,,,,"@inproceedings{niemeyer2021giraffe,
  BOOKTITLE = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  ID = {niemeyer2021giraffe},
  ENTRYTYPE = {inproceedings},
  AUTHOR = {Michael Niemeyer and Andreas Geiger},
  TITLE = {GIRAFFE: Representing Scenes as Compositional Generative Neural Feature Fields},
  EPRINT = {2011.12100v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Deep generative models allow for photorealistic image synthesis at high resolutions. But for many applications, this is not enough: content creation also needs to be controllable. While several recent works investigate how to disentangle underlying factors of variation in the data, most of them operate in 2D and hence ignore that our world is three-dimensional. Further, only few works consider the compositional nature of scenes. Our key hypothesis is that incorporating a compositional 3D scene representation into the generative model leads to more controllable image synthesis. Representing scenes as compositional generative neural feature fields allows us to disentangle one or multiple objects from the background as well as individual objects' shapes and appearances while learning from unstructured and unposed image collections without any additional supervision. Combining this scene representation with a neural rendering pipeline yields a fast and realistic image synthesis model. As evidenced by our experiments, our model is able to disentangle individual objects and allows for translating and rotating them in the scene as well as changing the camera pose.},
  YEAR = {2021},
  URL = {http://arxiv.org/abs/2011.12100v2},
  FILE = {2011.12100v2.pdf}
 }",Segmentation/composition,"Generative/adversarial formulation, Conditional neural field",,,,,,,,,,"CVPR 2021 (Oral, Best Paper Award)",,,,"Michael Niemeyer, Andreas Geiger",niemeyer2021giraffe,00000040,"Deep generative models allow for photorealistic image synthesis at high resolutions. But for many applications, this is not enough: content creation also needs to be controllable. While several recent works investigate how to disentangle underlying factors of variation in the data, most of them operate in 2D and hence ignore that our world is three-dimensional. Further, only few works consider the compositional nature of scenes. Our key hypothesis is that incorporating a compositional 3D scene representation into the generative model leads to more controllable image synthesis. Representing scenes as compositional generative neural feature fields allows us to disentangle one or multiple objects from the background as well as individual objects' shapes and appearances while learning from unstructured and unposed image collections without any additional supervision. Combining this scene representation with a neural rendering pipeline yields a fast and realistic image synthesis model. As evidenced by our experiments, our model is able to disentangle individual objects and allows for translating and rotating them in the scene as well as changing the camera pose.",21,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnueLIEBO3FB_EEaJMtTpVBpDxhilnA8FYjDmF_0sna1bMhb_4HoqnWTikgNd_O9UeXw
8/29/2021 20:43:12,Adversarial Generation of Continuous Images,INR-GAN,11/24/2020,https://arxiv.org/pdf/2011.12026.pdf,https://universome.github.io/inr-gan,https://github.com/universome/inr-gan,,,,,"@inproceedings{skorokhodov2021inrgan,
  BOOKTITLE = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  ID = {skorokhodov2021inrgan},
  ENTRYTYPE = {inproceedings},
  AUTHOR = {Ivan Skorokhodov and Savva Ignatyev and Mohamed Elhoseiny},
  TITLE = {Adversarial Generation of Continuous Images},
  EPRINT = {2011.12026v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {In most existing learning systems, images are typically viewed as 2D pixel arrays. However, in another paradigm gaining popularity, a 2D image is represented as an implicit neural representation (INR) - an MLP that predicts an RGB pixel value given its (x,y) coordinate. In this paper, we propose two novel architectural techniques for building INR-based image decoders: factorized multiplicative modulation and multi-scale INRs, and use them to build a state-of-the-art continuous image GAN. Previous attempts to adapt INRs for image generation were limited to MNIST-like datasets and do not scale to complex real-world data. Our proposed INR-GAN architecture improves the performance of continuous image generators by several times, greatly reducing the gap between continuous image GANs and pixel-based ones. Apart from that, we explore several exciting properties of the INR-based decoders, like out-of-the-box superresolution, meaningful image-space interpolation, accelerated inference of low-resolution images, an ability to extrapolate outside of image boundaries, and strong geometric prior. The project page is located at https://universome.github.io/inr-gan.},
  YEAR = {2021},
  URL = {http://arxiv.org/abs/2011.12026v2},
  FILE = {2011.12026v2.pdf}
 }","Generalization, Image","Generative/adversarial formulation, Conditional neural field, Hypernetwork",,,Category-level,,,,,,,CVPR 2021,,,Direct,"Ivan Skorokhodov, Savva Ignatyev, Mohamed Elhoseiny",skorokhodov2021inrgan,00000180,"In most existing learning systems, images are typically viewed as 2D pixel arrays. However, in another paradigm gaining popularity, a 2D image is represented as an implicit neural representation (INR) - an MLP that predicts an RGB pixel value given its (x,y) coordinate. In this paper, we propose two novel architectural techniques for building INR-based image decoders: factorized multiplicative modulation and multi-scale INRs, and use them to build a state-of-the-art continuous image GAN. Previous attempts to adapt INRs for image generation were limited to MNIST-like datasets and do not scale to complex real-world data. Our proposed INR-GAN architecture improves the performance of continuous image generators by several times, greatly reducing the gap between continuous image GANs and pixel-based ones. Apart from that, we explore several exciting properties of the INR-based decoders, like out-of-the-box superresolution, meaningful image-space interpolation, accelerated inference of low-resolution images, an ability to extrapolate outside of image boundaries, and strong geometric prior. The project page is located at https://universome.github.io/inr-gan.",7,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnuf2_8WKa5v6-WXchVkn0Zj0mRAQTae6nc2psudf6j6EquIh89vcF5ejQmS3RImNG-k
5/23/2021 19:01:49,D-NeRF: Deformable Neural Radiance Fields,"D-NeRF, Nerfies",11/25/2020,https://arxiv.org/pdf/2011.12948.pdf,https://nerfies.github.io/,,https://www.youtube.com/watch?v=MrKrnHhk8IA,,,,"@inproceedings{park2021dnerf,
  BOOKTITLE = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
  ID = {park2021dnerf},
  ENTRYTYPE = {inproceedings},
  AUTHOR = {Keunhong Park and Utkarsh Sinha and Jonathan T. Barron and Sofien Bouaziz and Dan B Goldman and Steven M. Seitz and Ricardo Martin-Brualla},
  TITLE = {Nerfies: Deformable Neural Radiance Fields},
  EPRINT = {2011.12948v5},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We present the first method capable of photorealistically reconstructing deformable scenes using photos/videos captured casually from mobile phones. Our approach augments neural radiance fields (NeRF) by optimizing an additional continuous volumetric deformation field that warps each observed point into a canonical 5D NeRF. We observe that these NeRF-like deformation fields are prone to local minima, and propose a coarse-to-fine optimization method for coordinate-based models that allows for more robust optimization. By adapting principles from geometry processing and physical simulation to NeRF-like models, we propose an elastic regularization of the deformation field that further improves robustness. We show that our method can turn casually captured selfie photos/videos into deformable NeRF models that allow for photorealistic renderings of the subject from arbitrary viewpoints, which we dub ""nerfies."" We evaluate our method by collecting time-synchronized data using a rig with two mobile phones, yielding train/validation images of the same pose at different viewpoints. We show that our method faithfully reconstructs non-rigidly deforming scenes and reproduces unseen views with high fidelity.},
  YEAR = {2021},
  URL = {http://arxiv.org/abs/2011.12948v5},
  FILE = {2011.12948v5.pdf}
 }",Dynamic,"Conditional neural field, Coarse-to-fine, Warping field/Flow field",,,,,,,,,,ICCV 2021,,,,"Keunhong Park, Utkarsh Sinha, Jonathan T. Barron, Sofien Bouaziz, Dan B Goldman, Steven M. Seitz, Ricardo Martin-Brualla",park2021dnerf,00000041,"We present the first method capable of photorealistically reconstructing deformable scenes using photos/videos captured casually from mobile phones. Our approach augments neural radiance fields (NeRF) by optimizing an additional continuous volumetric deformation field that warps each observed point into a canonical 5D NeRF. We observe that these NeRF-like deformation fields are prone to local minima, and propose a coarse-to-fine optimization method for coordinate-based models that allows for more robust optimization. By adapting principles from geometry processing and physical simulation to NeRF-like models, we propose an elastic regularization of the deformation field that further improves robustness. We show that our method can turn casually captured selfie photos/videos into deformable NeRF models that allow for photorealistic renderings of the subject from arbitrary viewpoints, which we dub ""nerfies."" We evaluate our method by collecting time-synchronized data using a rig with two mobile phones, yielding train/validation images of the same pose at different viewpoints. We show that our method faithfully reconstructs non-rigidly deforming scenes and reproduces unseen views with high fidelity.",54,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnueO-6r__SJZlq6de28PIi-QXdZ_pGQjs2KrXCswr04O3TB7NJXPXEIBWE8oMy7SyA4
5/23/2021 19:01:57,DeRF: Decomposed Radiance Fields,DeRF,11/25/2020,https://arxiv.org/pdf/2011.12490.pdf,,,,,,,"@inproceedings{rebain2021derf,
  BOOKTITLE = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  ID = {rebain2021derf},
  ENTRYTYPE = {inproceedings},
  AUTHOR = {Daniel Rebain and Wei Jiang and Soroosh Yazdani and Ke Li and Kwang Moo Yi and Andrea Tagliasacchi},
  TITLE = {DeRF: Decomposed Radiance Fields},
  EPRINT = {2011.12490v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {With the advent of Neural Radiance Fields (NeRF), neural networks can now render novel views of a 3D scene with quality that fools the human eye. Yet, generating these images is very computationally intensive, limiting their applicability in practical scenarios. In this paper, we propose a technique based on spatial decomposition capable of mitigating this issue. Our key observation is that there are diminishing returns in employing larger (deeper and/or wider) networks. Hence, we propose to spatially decompose a scene and dedicate smaller networks for each decomposed part. When working together, these networks can render the whole scene. This allows us near-constant inference time regardless of the number of decomposed parts. Moreover, we show that a Voronoi spatial decomposition is preferable for this purpose, as it is provably compatible with the Painter's Algorithm for efficient and GPU-friendly rendering. Our experiments show that for real-world scenes, our method provides up to 3x more efficient inference than NeRF (with the same rendering quality), or an improvement of up to 1.0~dB in PSNR (for the same inference cost).},
  YEAR = {2021},
  URL = {http://arxiv.org/abs/2011.12490v1},
  FILE = {2011.12490v1.pdf}
 }","Performance (training), Performance (rendering)",Volume partitioning,,,,,,,,,,CVPR 2021,,,,"Daniel Rebain, Wei Jiang, Soroosh Yazdani, Ke Li, Kwang Moo Yi, Andrea Tagliasacchi",rebain2021derf,00000042,"With the advent of Neural Radiance Fields (NeRF), neural networks can now render novel views of a 3D scene with quality that fools the human eye. Yet, generating these images is very computationally intensive, limiting their applicability in practical scenarios. In this paper, we propose a technique based on spatial decomposition capable of mitigating this issue. Our key observation is that there are diminishing returns in employing larger (deeper and/or wider) networks. Hence, we propose to spatially decompose a scene and dedicate smaller networks for each decomposed part. When working together, these networks can render the whole scene. This allows us near-constant inference time regardless of the number of decomposed parts. Moreover, we show that a Voronoi spatial decomposition is preferable for this purpose, as it is provably compatible with the Painter's Algorithm for efficient and GPU-friendly rendering. Our experiments show that for real-world scenes, our method provides up to 3x more efficient inference than NeRF (with the same rendering quality), or an improvement of up to 1.0~dB in PSNR (for the same inference cost).",17,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnuc3o4WN1ezG9eCuYjPCyGWLQPweh423-2doTR3rfpw4S8Zsh7lK9y1HlEj2qjfhyEE
5/23/2021 19:02:40,Space-time Neural Irradiance Fields for Free-Viewpoint Video,,11/25/2020,https://arxiv.org/pdf/2011.12950.pdf,https://video-nerf.github.io/,,https://www.youtube.com/watch?v=2tN8ghNu2sI,,,,"@inproceedings{xian2021spacetime,
  BOOKTITLE = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  ID = {xian2021spacetime},
  ENTRYTYPE = {inproceedings},
  AUTHOR = {Wenqi Xian and Jia-Bin Huang and Johannes Kopf and Changil Kim},
  TITLE = {Space-time Neural Irradiance Fields for Free-Viewpoint Video},
  EPRINT = {2011.12950v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We present a method that learns a spatiotemporal neural irradiance field for dynamic scenes from a single video. Our learned representation enables free-viewpoint rendering of the input video. Our method builds upon recent advances in implicit representations. Learning a spatiotemporal irradiance field from a single video poses significant challenges because the video contains only one observation of the scene at any point in time. The 3D geometry of a scene can be legitimately represented in numerous ways since varying geometry (motion) can be explained with varying appearance and vice versa. We address this ambiguity by constraining the time-varying geometry of our dynamic scene representation using the scene depth estimated from video depth estimation methods, aggregating contents from individual frames into a single global representation. We provide an extensive quantitative evaluation and demonstrate compelling free-viewpoint rendering results.},
  YEAR = {2021},
  URL = {http://arxiv.org/abs/2011.12950v2},
  FILE = {2011.12950v2.pdf}
 }",Dynamic,,,,,,,,,,,CVPR 2021,,,,"Wenqi Xian, Jia-Bin Huang, Johannes Kopf, Changil Kim",xian2021spacetime,00000043,"We present a method that learns a spatiotemporal neural irradiance field for dynamic scenes from a single video. Our learned representation enables free-viewpoint rendering of the input video. Our method builds upon recent advances in implicit representations. Learning a spatiotemporal irradiance field from a single video poses significant challenges because the video contains only one observation of the scene at any point in time. The 3D geometry of a scene can be legitimately represented in numerous ways since varying geometry (motion) can be explained with varying appearance and vice versa. We address this ambiguity by constraining the time-varying geometry of our dynamic scene representation using the scene depth estimated from video depth estimation methods, aggregating contents from individual frames into a single global representation. We provide an extensive quantitative evaluation and demonstrate compelling free-viewpoint rendering results.",28,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnudmMg_ZIx_zjsnRPyrMPTzRgLTyz7O4bT04jFdpoxu0b-bXbm4sl8sW39187Oyk5A8
5/23/2021 19:03:19,Neural Scene Flow Fields for Space-Time View Synthesis of Dynamic Scenes,NSFF,11/26/2020,https://arxiv.org/pdf/2011.13084.pdf,http://www.cs.cornell.edu/~zl548/NSFF/,https://github.com/zhengqili/Neural-Scene-Flow-Fields,,https://www.cs.cornell.edu/~zl548/NSFF/NSFF_supp.pdf,,,"@inproceedings{li2021nsff,
  BOOKTITLE = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  ID = {li2021nsff},
  ENTRYTYPE = {inproceedings},
  AUTHOR = {Zhengqi Li and Simon Niklaus and Noah Snavely and Oliver Wang},
  TITLE = {Neural Scene Flow Fields for Space-Time View Synthesis of Dynamic Scenes},
  EPRINT = {2011.13084v3},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We present a method to perform novel view and time synthesis of dynamic scenes, requiring only a monocular video with known camera poses as input. To do this, we introduce Neural Scene Flow Fields, a new representation that models the dynamic scene as a time-variant continuous function of appearance, geometry, and 3D scene motion. Our representation is optimized through a neural network to fit the observed input views. We show that our representation can be used for complex dynamic scenes, including thin structures, view-dependent effects, and natural degrees of motion. We conduct a number of experiments that demonstrate our approach significantly outperforms recent monocular view synthesis methods, and show qualitative results of space-time view synthesis on a variety of real-world videos.},
  YEAR = {2021},
  URL = {http://arxiv.org/abs/2011.13084v3},
  FILE = {2011.13084v3.pdf}
 }","Dynamic, Segmentation/composition",Warping field/Flow field,,,,,,,,,,CVPR 2021,,,,"Zhengqi Li, Simon Niklaus, Noah Snavely, Oliver Wang",li2021nsff,00000044,"We present a method to perform novel view and time synthesis of dynamic scenes, requiring only a monocular video with known camera poses as input. To do this, we introduce Neural Scene Flow Fields, a new representation that models the dynamic scene as a time-variant continuous function of appearance, geometry, and 3D scene motion. Our representation is optimized through a neural network to fit the observed input views. We show that our representation can be used for complex dynamic scenes, including thin structures, view-dependent effects, and natural degrees of motion. We conduct a number of experiments that demonstrate our approach significantly outperforms recent monocular view synthesis methods, and show qualitative results of space-time view synthesis on a variety of real-world videos.",36,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnufqLswBohA95lyfpF4fsbT6UR-RDvHjxDiaWwlnv27_kE9OmY5GFnDysubK2CBZllA
5/23/2021 18:16:12,D-NeRF: Neural Radiance Fields for Dynamic Scenes,D-NeRF,11/27/2020,https://arxiv.org/pdf/2011.13961.pdf,https://www.albertpumarola.com/research/D-NeRF/index.html,https://github.com/albertpumarola/D-NeRF,https://www.youtube.com/watch?v=lSgzmgi2JPw,,,,"@inproceedings{pumarola2021dnerf,
  BOOKTITLE = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  ID = {pumarola2021dnerf},
  ENTRYTYPE = {inproceedings},
  AUTHOR = {Albert Pumarola and Enric Corona and Gerard Pons-Moll and Francesc Moreno-Noguer},
  TITLE = {D-NeRF: Neural Radiance Fields for Dynamic Scenes},
  EPRINT = {2011.13961v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Neural rendering techniques combining machine learning with geometric reasoning have arisen as one of the most promising approaches for synthesizing novel views of a scene from a sparse set of images. Among these, stands out the Neural radiance fields (NeRF), which trains a deep network to map 5D input coordinates (representing spatial location and viewing direction) into a volume density and view-dependent emitted radiance. However, despite achieving an unprecedented level of photorealism on the generated images, NeRF is only applicable to static scenes, where the same spatial location can be queried from different images. In this paper we introduce D-NeRF, a method that extends neural radiance fields to a dynamic domain, allowing to reconstruct and render novel images of objects under rigid and non-rigid motions from a \emph{single} camera moving around the scene. For this purpose we consider time as an additional input to the system, and split the learning process in two main stages: one that encodes the scene into a canonical space and another that maps this canonical representation into the deformed scene at a particular time. Both mappings are simultaneously learned using fully-connected networks. Once the networks are trained, D-NeRF can render novel images, controlling both the camera view and the time variable, and thus, the object movement. We demonstrate the effectiveness of our approach on scenes with objects under rigid, articulated and non-rigid motions. Code, model weights and the dynamic scenes dataset will be released.},
  YEAR = {2021},
  URL = {http://arxiv.org/abs/2011.13961v1},
  FILE = {2011.13961v1.pdf}
 }",Dynamic,Warping field/Flow field,,,,,,,,,,CVPR 2021,https://www.dropbox.com/s/0bf6fl0ye2vz3vr/data.zip?dl=0,,,"Albert Pumarola, Enric Corona, Gerard Pons-Moll, Francesc Moreno-Noguer",pumarola2021dnerf,00000045,"Neural rendering techniques combining machine learning with geometric reasoning have arisen as one of the most promising approaches for synthesizing novel views of a scene from a sparse set of images. Among these, stands out the Neural radiance fields (NeRF), which trains a deep network to map 5D input coordinates (representing spatial location and viewing direction) into a volume density and view-dependent emitted radiance. However, despite achieving an unprecedented level of photorealism on the generated images, NeRF is only applicable to static scenes, where the same spatial location can be queried from different images. In this paper we introduce D-NeRF, a method that extends neural radiance fields to a dynamic domain, allowing to reconstruct and render novel images of objects under rigid and non-rigid motions from a \emph{single} camera moving around the scene. For this purpose we consider time as an additional input to the system, and split the learning process in two main stages: one that encodes the scene into a canonical space and another that maps this canonical representation into the deformed scene at a particular time. Both mappings are simultaneously learned using fully-connected networks. Once the networks are trained, D-NeRF can render novel images, controlling both the camera view and the time variable, and thus, the object movement. We demonstrate the effectiveness of our approach on scenes with objects under rigid, articulated and non-rigid motions. Code, model weights and the dynamic scenes dataset will be released.",41,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnufYI3q44iMdRLOHegTcugFMZqjR11Kom80IGPMi2QchtidKU3aQDVh1Q72yYO2MqBc
8/29/2021 20:48:22,Image Generators with Conditionally-Independent Pixel Synthesis,CIPS,11/27/2020,https://arxiv.org/pdf/2011.13775.pdf,,https://github.com/saic-mdal/CIPS,,,,,"@inproceedings{anokhin2021cips,
  BOOKTITLE = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  ID = {anokhin2021cips},
  ENTRYTYPE = {inproceedings},
  AUTHOR = {Ivan Anokhin and Kirill Demochkin and Taras Khakhulin and Gleb Sterkin and Victor Lempitsky and Denis Korzhenkov},
  TITLE = {Image Generators with Conditionally-Independent Pixel Synthesis},
  EPRINT = {2011.13775v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Existing image generator networks rely heavily on spatial convolutions and, optionally, self-attention blocks in order to gradually synthesize images in a coarse-to-fine manner. Here, we present a new architecture for image generators, where the color value at each pixel is computed independently given the value of a random latent vector and the coordinate of that pixel. No spatial convolutions or similar operations that propagate information across pixels are involved during the synthesis. We analyze the modeling capabilities of such generators when trained in an adversarial fashion, and observe the new generators to achieve similar generation quality to state-of-the-art convolutional generators. We also investigate several interesting properties unique to the new architecture.},
  YEAR = {2021},
  URL = {http://arxiv.org/abs/2011.13775v1},
  FILE = {2011.13775v1.pdf}
 }",Image,,,,,,,,,,,CVPR 2021,,,,"Ivan Anokhin, Kirill Demochkin, Taras Khakhulin, Gleb Sterkin, Victor Lempitsky, Denis Korzhenkov",anokhin2021cips,00000181,"Existing image generator networks rely heavily on spatial convolutions and, optionally, self-attention blocks in order to gradually synthesize images in a coarse-to-fine manner. Here, we present a new architecture for image generators, where the color value at each pixel is computed independently given the value of a random latent vector and the coordinate of that pixel. No spatial convolutions or similar operations that propagate information across pixels are involved during the synthesis. We analyze the modeling capabilities of such generators when trained in an adversarial fashion, and observe the new generators to achieve similar generation quality to state-of-the-art convolutional generators. We also investigate several interesting properties unique to the new architecture.",11,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnudFI6_Lm4DjzDGJotDsCvFC_BSWZ0a9MTimrNRNckUQ6YhC4OVKnU3KdTS34OXLnTM
7/19/2021 21:22:17,i3DMM: Deep Implicit 3D Morphable Model of Human Heads,i3DMM,11/28/2020,https://arxiv.org/pdf/2011.14143.pdf,,Coming soon,https://www.youtube.com/watch?v=4pYzV3ButPY,,,,"@inproceedings{yenamandra2021i3dmm,
  BOOKTITLE = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  ID = {yenamandra2021i3dmm},
  ENTRYTYPE = {inproceedings},
  AUTHOR = {Tarun Yenamandra and Ayush Tewari and Florian Bernard and Hans-Peter Seidel and Mohamed Elgharib and Daniel Cremers and Christian Theobalt},
  TITLE = {i3DMM: Deep Implicit 3D Morphable Model of Human Heads},
  EPRINT = {2011.14143v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We present the first deep implicit 3D morphable model (i3DMM) of full heads. Unlike earlier morphable face models it not only captures identity-specific geometry, texture, and expressions of the frontal face, but also models the entire head, including hair. We collect a new dataset consisting of 64 people with different expressions and hairstyles to train i3DMM. Our approach has the following favorable properties: (i) It is the first full head morphable model that includes hair. (ii) In contrast to mesh-based models it can be trained on merely rigidly aligned scans, without requiring difficult non-rigid registration. (iii) We design a novel architecture to decouple the shape model into an implicit reference shape and a deformation of this reference shape. With that, dense correspondences between shapes can be learned implicitly. (iv) This architecture allows us to semantically disentangle the geometry and color components, as color is learned in the reference space. Geometry is further disentangled as identity, expressions, and hairstyle, while color is disentangled as identity and hairstyle components. We show the merits of i3DMM using ablation studies, comparisons to state-of-the-art models, and applications such as semantic head editing and texture transfer. We will make our model publicly available.},
  YEAR = {2021},
  URL = {http://arxiv.org/abs/2011.14143v1},
  FILE = {2011.14143v1.pdf}
 }","Dynamic, Human head, Editable","Conditional neural field, Warping field/Flow field",,SDF,,,,,,,,CVPR 2021,,,,"Tarun Yenamandra, Ayush Tewari, Florian Bernard, Hans-Peter Seidel, Mohamed Elgharib, Daniel Cremers, Christian Theobalt",yenamandra2021i3dmm,00000046,"We present the first deep implicit 3D morphable model (i3DMM) of full heads. Unlike earlier morphable face models it not only captures identity-specific geometry, texture, and expressions of the frontal face, but also models the entire head, including hair. We collect a new dataset consisting of 64 people with different expressions and hairstyles to train i3DMM. Our approach has the following favorable properties: (i) It is the first full head morphable model that includes hair. (ii) In contrast to mesh-based models it can be trained on merely rigidly aligned scans, without requiring difficult non-rigid registration. (iii) We design a novel architecture to decouple the shape model into an implicit reference shape and a deformation of this reference shape. With that, dense correspondences between shapes can be learned implicitly. (iv) This architecture allows us to semantically disentangle the geometry and color components, as color is learned in the reference space. Geometry is further disentangled as identity, expressions, and hairstyle, while color is disentangled as identity and hairstyle components. We show the merits of i3DMM using ablation studies, comparisons to state-of-the-art models, and applications such as semantic head editing and texture transfer. We will make our model publicly available.",3,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnue_Ty5nq9CoDkFmExpeuQlv_RUbnaZlIUEl-CuvEtHHR-QLB-OYCHYOiFMFjxEbwrA
7/19/2021 22:05:43,Neural Deformation Graphs for Globally-consistent Non-rigid Reconstruction,Neural Deformation Graphs,12/2/2020,https://arxiv.org/pdf/2012.01451.pdf,,,,,,,"@inproceedings{bozic2021neuraldeformationgraphs,
  BOOKTITLE = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  ID = {bozic2021neuraldeformationgraphs},
  ENTRYTYPE = {inproceedings},
  AUTHOR = {Aljaz Bozic and Pablo Palafox and Michael Zollhofer and Justus Thies and Angela Dai and Matthias Niessner},
  TITLE = {Neural Deformation Graphs for Globally-consistent Non-rigid Reconstruction},
  EPRINT = {2012.01451v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We introduce Neural Deformation Graphs for globally-consistent deformation tracking and 3D reconstruction of non-rigid objects. Specifically, we implicitly model a deformation graph via a deep neural network. This neural deformation graph does not rely on any object-specific structure and, thus, can be applied to general non-rigid deformation tracking. Our method globally optimizes this neural graph on a given sequence of depth camera observations of a non-rigidly moving object. Based on explicit viewpoint consistency as well as inter-frame graph and surface consistency constraints, the underlying network is trained in a self-supervised fashion. We additionally optimize for the geometry of the object with an implicit deformable multi-MLP shape representation. Our approach does not assume sequential input data, thus enabling robust tracking of fast motions or even temporally disconnected recordings. Our experiments demonstrate that our Neural Deformation Graphs outperform state-of-the-art non-rigid reconstruction approaches both qualitatively and quantitatively, with 64% improved reconstruction and 62% improved deformation tracking performance.},
  YEAR = {2021},
  URL = {http://arxiv.org/abs/2012.01451v1},
  FILE = {2012.01451v1.pdf}
 }","Dynamic, Human body, Beyond graphics","Volume partitioning, Warping field/Flow field",,,,,,,,,,CVPR 2021,,,,"Aljaž Božič, Pablo Palafox, Michael Zollhöfer, Justus Thies, Angela Dai, Matthias Nießner",bozic2021neuraldeformationgraphs,00000047,"We introduce Neural Deformation Graphs for globally-consistent deformation tracking and 3D reconstruction of non-rigid objects. Specifically, we implicitly model a deformation graph via a deep neural network. This neural deformation graph does not rely on any object-specific structure and, thus, can be applied to general non-rigid deformation tracking. Our method globally optimizes this neural graph on a given sequence of depth camera observations of a non-rigidly moving object. Based on explicit viewpoint consistency as well as inter-frame graph and surface consistency constraints, the underlying network is trained in a self-supervised fashion. We additionally optimize for the geometry of the object with an implicit deformable multi-MLP shape representation. Our approach does not assume sequential input data, thus enabling robust tracking of fast motions or even temporally disconnected recordings. Our experiments demonstrate that our Neural Deformation Graphs outperform state-of-the-art non-rigid reconstruction approaches both qualitatively and quantitatively, with 64% improved reconstruction and 62% improved deformation tracking performance.",1,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnufiPk4A-CGDfPmHevzPgzhLtLiMRC4g1XokZcNMIl7z6n_-Omv1Wy6LlRreDEkALtk
5/23/2021 18:50:19,pixelNeRF: Neural Radiance Fields from One or Few Images,pixelNeRF,12/3/2020,https://arxiv.org/pdf/2012.02190.pdf,https://alexyu.net/pixelnerf/,https://github.com/sxyu/pixel-nerf,https://www.youtube.com/watch?v=voebZx7f32g,,,,"@inproceedings{yu2021pixelnerf,
  BOOKTITLE = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  ID = {yu2021pixelnerf},
  ENTRYTYPE = {inproceedings},
  AUTHOR = {Alex Yu and Vickie Ye and Matthew Tancik and Angjoo Kanazawa},
  TITLE = {pixelNeRF: Neural Radiance Fields from One or Few Images},
  EPRINT = {2012.02190v3},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We propose pixelNeRF, a learning framework that predicts a continuous neural scene representation conditioned on one or few input images. The existing approach for constructing neural radiance fields involves optimizing the representation to every scene independently, requiring many calibrated views and significant compute time. We take a step towards resolving these shortcomings by introducing an architecture that conditions a NeRF on image inputs in a fully convolutional manner. This allows the network to be trained across multiple scenes to learn a scene prior, enabling it to perform novel view synthesis in a feed-forward manner from a sparse set of views (as few as one). Leveraging the volume rendering approach of NeRF, our model can be trained directly from images with no explicit 3D supervision. We conduct extensive experiments on ShapeNet benchmarks for single image novel view synthesis tasks with held-out objects as well as entire unseen categories. We further demonstrate the flexibility of pixelNeRF by demonstrating it on multi-object ShapeNet scenes and real scenes from the DTU dataset. In all cases, pixelNeRF outperforms current state-of-the-art baselines for novel view synthesis and single image 3D reconstruction. For the video and code, please visit the project website: https://alexyu.net/pixelnerf},
  YEAR = {2021},
  URL = {http://arxiv.org/abs/2012.02190v3},
  FILE = {2012.02190v3.pdf}
 }","Few-shot reconstruction, Generalization","Lifting 2D features to 3D, Data-driven",,,,,,,,,,CVPR 2021,https://drive.google.com/drive/folders/1PsT3uKwqHHD2bEEHkIXB99AlIjtmrEiR,,,"Alex Yu, Vickie Ye, Matthew Tancik, Angjoo Kanazawa",yu2021pixelnerf,00000048,"We propose pixelNeRF, a learning framework that predicts a continuous neural scene representation conditioned on one or few input images. The existing approach for constructing neural radiance fields involves optimizing the representation to every scene independently, requiring many calibrated views and significant compute time. We take a step towards resolving these shortcomings by introducing an architecture that conditions a NeRF on image inputs in a fully convolutional manner. This allows the network to be trained across multiple scenes to learn a scene prior, enabling it to perform novel view synthesis in a feed-forward manner from a sparse set of views (as few as one). Leveraging the volume rendering approach of NeRF, our model can be trained directly from images with no explicit 3D supervision. We conduct extensive experiments on ShapeNet benchmarks for single image novel view synthesis tasks with held-out objects as well as entire unseen categories. We further demonstrate the flexibility of pixelNeRF by demonstrating it on multi-object ShapeNet scenes and real scenes from the DTU dataset. In all cases, pixelNeRF outperforms current state-of-the-art baselines for novel view synthesis and single image 3D reconstruction. For the video and code, please visit the project website: https://alexyu.net/pixelnerf",37,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnudFp3lsu5-HcXDDnsV-5NmuOTQhfUSFraiTfKXctt_UiFb06pZKOk2rypLo1Cwkle0
5/23/2021 18:53:10,Learned Initializations for Optimizing Coordinate-Based Neural Representations,,12/3/2020,https://arxiv.org/pdf/2012.02189.pdf,https://www.matthewtancik.com/learnit,,,,,,"@inproceedings{tancik2021learned,
  BOOKTITLE = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  ID = {tancik2021learned},
  ENTRYTYPE = {inproceedings},
  AUTHOR = {Matthew Tancik and Ben Mildenhall and Terrance Wang and Divi Schmidt and Pratul P. Srinivasan and Jonathan T. Barron and Ren Ng},
  TITLE = {Learned Initializations for Optimizing Coordinate-Based Neural Representations},
  EPRINT = {2012.02189v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Coordinate-based neural representations have shown significant promise as an alternative to discrete, array-based representations for complex low dimensional signals. However, optimizing a coordinate-based network from randomly initialized weights for each new signal is inefficient. We propose applying standard meta-learning algorithms to learn the initial weight parameters for these fully-connected networks based on the underlying class of signals being represented (e.g., images of faces or 3D models of chairs). Despite requiring only a minor change in implementation, using these learned initial weights enables faster convergence during optimization and can serve as a strong prior over the signal class being modeled, resulting in better generalization when only partial observations of a given signal are available. We explore these benefits across a variety of tasks, including representing 2D images, reconstructing CT scans, and recovering 3D shapes and scenes from 2D image observations.},
  YEAR = {2021},
  URL = {http://arxiv.org/abs/2012.02189v2},
  FILE = {2012.02189v2.pdf}
 }","Generalization, Fundamentals","Hypernetwork, Data-driven",,,,,,,,,,CVPR 2021,,,,"Matthew Tancik, Ben Mildenhall, Terrance Wang, Divi Schmidt, Pratul P. Srinivasan, Jonathan T. Barron, Ren Ng",tancik2021learned,00000049,"Coordinate-based neural representations have shown significant promise as an alternative to discrete, array-based representations for complex low dimensional signals. However, optimizing a coordinate-based network from randomly initialized weights for each new signal is inefficient. We propose applying standard meta-learning algorithms to learn the initial weight parameters for these fully-connected networks based on the underlying class of signals being represented (e.g., images of faces or 3D models of chairs). Despite requiring only a minor change in implementation, using these learned initial weights enables faster convergence during optimization and can serve as a strong prior over the signal class being modeled, resulting in better generalization when only partial observations of a given signal are available. We explore these benefits across a variety of tasks, including representing 2D images, reconstructing CT scans, and recovering 3D shapes and scenes from 2D image observations.",12,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnuesHR6EXqA8uk5Az6mNuTEOpm0NaEuNgSA1qXfgY-Nw5Lds9es7jAF12K2xOvXIAVo
5/23/2021 18:59:09,AutoInt: Automatic Integration for Fast Neural Volume Rendering,AutoInt,12/3/2020,https://arxiv.org/pdf/2012.01714.pdf,http://www.computationalimaging.org/publications/automatic-integration/,,,,,,"@inproceedings{lindell2021autoint,
  BOOKTITLE = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  ID = {lindell2021autoint},
  ENTRYTYPE = {inproceedings},
  AUTHOR = {David B. Lindell and Julien N. P. Martel and Gordon Wetzstein},
  TITLE = {AutoInt: Automatic Integration for Fast Neural Volume Rendering},
  EPRINT = {2012.01714v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Numerical integration is a foundational technique in scientific computing and is at the core of many computer vision applications. Among these applications, neural volume rendering has recently been proposed as a new paradigm for view synthesis, achieving photorealistic image quality. However, a fundamental obstacle to making these methods practical is the extreme computational and memory requirements caused by the required volume integrations along the rendered rays during training and inference. Millions of rays, each requiring hundreds of forward passes through a neural network are needed to approximate those integrations with Monte Carlo sampling. Here, we propose automatic integration, a new framework for learning efficient, closed-form solutions to integrals using coordinate-based neural networks. For training, we instantiate the computational graph corresponding to the derivative of the network. The graph is fitted to the signal to integrate. After optimization, we reassemble the graph to obtain a network that represents the antiderivative. By the fundamental theorem of calculus, this enables the calculation of any definite integral in two evaluations of the network. Applying this approach to neural rendering, we improve a tradeoff between rendering speed and image quality: improving render times by greater than 10 times with a tradeoff of slightly reduced image quality.},
  YEAR = {2021},
  URL = {http://arxiv.org/abs/2012.01714v2},
  FILE = {2012.01714v2.pdf}
 }","Performance (rendering), Beyond graphics, Fundamentals",Sampling,,,,,,,,,,CVPR 2021,,,,"David B. Lindell, Julien N. P. Martel, Gordon Wetzstein",lindell2021autoint,00000050,"Numerical integration is a foundational technique in scientific computing and is at the core of many computer vision applications. Among these applications, neural volume rendering has recently been proposed as a new paradigm for view synthesis, achieving photorealistic image quality. However, a fundamental obstacle to making these methods practical is the extreme computational and memory requirements caused by the required volume integrations along the rendered rays during training and inference. Millions of rays, each requiring hundreds of forward passes through a neural network are needed to approximate those integrations with Monte Carlo sampling. Here, we propose automatic integration, a new framework for learning efficient, closed-form solutions to integrals using coordinate-based neural networks. For training, we instantiate the computational graph corresponding to the derivative of the network. The graph is fitted to the signal to integrate. After optimization, we reassemble the graph to obtain a network that represents the antiderivative. By the fundamental theorem of calculus, this enables the calculation of any definite integral in two evaluations of the network. Applying this approach to neural rendering, we improve a tradeoff between rendering speed and image quality: improving render times by greater than 10 times with a tradeoff of slightly reduced image quality.",16,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnue3brdCUEavbCF7LfKefxnq5cjyBfOw-Y-g24gKZn56xCeNwXyuERyWHZ18s38Efp8
7/19/2021 21:30:04,Dynamic Neural Radiance Fields for Monocular 4D Facial Avatar Reconstruction,NerFACE,12/5/2020,https://arxiv.org/pdf/2012.03065.pdf,https://gafniguy.github.io/4D-Facial-Avatars/,https://github.com/gafniguy/4D-Facial-Avatars,"https://www.youtube.com/watch?v=XihxC65tmyA, https://www.youtube.com/watch?v=m7oROLdQnjk",,,,"@inproceedings{gafni2021nerface,
  BOOKTITLE = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  ID = {gafni2021nerface},
  ENTRYTYPE = {inproceedings},
  AUTHOR = {Guy Gafni and Justus Thies and Michael Zollhofer and Matthias Niessner},
  TITLE = {Dynamic Neural Radiance Fields for Monocular 4D Facial Avatar Reconstruction},
  EPRINT = {2012.03065v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We present dynamic neural radiance fields for modeling the appearance and dynamics of a human face. Digitally modeling and reconstructing a talking human is a key building-block for a variety of applications. Especially, for telepresence applications in AR or VR, a faithful reproduction of the appearance including novel viewpoints or head-poses is required. In contrast to state-of-the-art approaches that model the geometry and material properties explicitly, or are purely image-based, we introduce an implicit representation of the head based on scene representation networks. To handle the dynamics of the face, we combine our scene representation network with a low-dimensional morphable model which provides explicit control over pose and expressions. We use volumetric rendering to generate images from this hybrid representation and demonstrate that such a dynamic neural scene representation can be learned from monocular input data only, without the need of a specialized capture setup. In our experiments, we show that this learned volumetric representation allows for photo-realistic image generation that surpasses the quality of state-of-the-art video-based reenactment methods.},
  YEAR = {2021},
  URL = {http://arxiv.org/abs/2012.03065v1},
  FILE = {2012.03065v1.pdf}
 }",Human head,"Conditional neural field, Data-driven",NeRF,Density,,,,,,,,CVPR 2021,Available upon request,No,Direct,"Guy Gafni, Justus Thies, Michael Zollhöfer, Matthias Nießner",gafni2021nerface,00000051,"We present dynamic neural radiance fields for modeling the appearance and dynamics of a human face. Digitally modeling and reconstructing a talking human is a key building-block for a variety of applications. Especially, for telepresence applications in AR or VR, a faithful reproduction of the appearance including novel viewpoints or head-poses is required. In contrast to state-of-the-art approaches that model the geometry and material properties explicitly, or are purely image-based, we introduce an implicit representation of the head based on scene representation networks. To handle the dynamics of the face, we combine our scene representation network with a low-dimensional morphable model which provides explicit control over pose and expressions. We use volumetric rendering to generate images from this hybrid representation and demonstrate that such a dynamic neural scene representation can be learned from monocular input data only, without the need of a specialized capture setup. In our experiments, we show that this learned volumetric representation allows for photo-realistic image generation that surpasses the quality of state-of-the-art video-based reenactment methods.",18,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnuc2LC9Tf_qnqaRxI3pKVk1xqj1mYmIkJMSk1cJwPY0pBzs_reCa_A_chUg6jf_IbXk
8/29/2021 20:39:19,Spatially-Adaptive Pixelwise Networks for Fast Image Translation,ASAPNet,12/5/2020,https://arxiv.org/pdf/2012.02992.pdf,https://tamarott.github.io/ASAPNet_web/,https://github.com/tamarott/ASAPNet,https://www.youtube.com/watch?v=6-OfZ32CoBE,,,,"@inproceedings{shaham2021asapnet,
  BOOKTITLE = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  ID = {shaham2021asapnet},
  ENTRYTYPE = {inproceedings},
  AUTHOR = {Tamar Rott Shaham and Michael Gharbi and Richard Zhang and Eli Shechtman and Tomer Michaeli},
  TITLE = {Spatially-Adaptive Pixelwise Networks for Fast Image Translation},
  EPRINT = {2012.02992v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We introduce a new generator architecture, aimed at fast and efficient high-resolution image-to-image translation. We design the generator to be an extremely lightweight function of the full-resolution image. In fact, we use pixel-wise networks; that is, each pixel is processed independently of others, through a composition of simple affine transformations and nonlinearities. We take three important steps to equip such a seemingly simple function with adequate expressivity. First, the parameters of the pixel-wise networks are spatially varying so they can represent a broader function class than simple 1x1 convolutions. Second, these parameters are predicted by a fast convolutional network that processes an aggressively low-resolution representation of the input; Third, we augment the input image with a sinusoidal encoding of spatial coordinates, which provides an effective inductive bias for generating realistic novel high-frequency image content. As a result, our model is up to 18x faster than state-of-the-art baselines. We achieve this speedup while generating comparable visual quality across different image resolutions and translation domains.},
  YEAR = {2021},
  URL = {http://arxiv.org/abs/2012.02992v1},
  FILE = {2012.02992v1.pdf}
 }","Performance (rendering), Image",Data-driven,,,,,,,,,,CVPR 2021,,,"Direct, Indirect","Tamar Rott Shaham, Michael Gharbi, Richard Zhang, Eli Shechtman, Tomer Michaeli",shaham2021asapnet,00000179,"We introduce a new generator architecture, aimed at fast and efficient high-resolution image-to-image translation. We design the generator to be an extremely lightweight function of the full-resolution image. In fact, we use pixel-wise networks; that is, each pixel is processed independently of others, through a composition of simple affine transformations and nonlinearities. We take three important steps to equip such a seemingly simple function with adequate expressivity. First, the parameters of the pixel-wise networks are spatially varying so they can represent a broader function class than simple 1x1 convolutions. Second, these parameters are predicted by a fast convolutional network that processes an aggressively low-resolution representation of the input; Third, we augment the input image with a sinusoidal encoding of spatial coordinates, which provides an effective inductive bias for generating realistic novel high-frequency image content. As a result, our model is up to 18x faster than state-of-the-art baselines. We achieve this speedup while generating comparable visual quality across different image resolutions and translation domains.",2,,Yes,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnue9mtVcJz4QYSzYvoF_7hxN8bSV8HrGhO0R8cMKm6DuL_0XttuQVDsln7S3RpkcUU8
5/23/2021 19:07:06,NeRV: Neural Reflectance and Visibility Fields for Relighting and View Synthesis,NeRV,12/7/2020,https://arxiv.org/pdf/2012.03927.pdf,https://people.eecs.berkeley.edu/~pratul/nerv/?s=09,Coming soon,https://www.youtube.com/watch?v=4XyDdvhhjVo,,,,"@inproceedings{srinivasan2021nerv,
  BOOKTITLE = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  ID = {srinivasan2021nerv},
  ENTRYTYPE = {inproceedings},
  AUTHOR = {Pratul P. Srinivasan and Boyang Deng and Xiuming Zhang and Matthew Tancik and Ben Mildenhall and Jonathan T. Barron},
  TITLE = {NeRV: Neural Reflectance and Visibility Fields for Relighting and View Synthesis},
  EPRINT = {2012.03927v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We present a method that takes as input a set of images of a scene illuminated by unconstrained known lighting, and produces as output a 3D representation that can be rendered from novel viewpoints under arbitrary lighting conditions. Our method represents the scene as a continuous volumetric function parameterized as MLPs whose inputs are a 3D location and whose outputs are the following scene properties at that input location: volume density, surface normal, material parameters, distance to the first surface intersection in any direction, and visibility of the external environment in any direction. Together, these allow us to render novel views of the object under arbitrary lighting, including indirect illumination effects. The predicted visibility and surface intersection fields are critical to our model's ability to simulate direct and indirect illumination during training, because the brute-force techniques used by prior work are intractable for lighting conditions outside of controlled setups with a single light. Our method outperforms alternative approaches for recovering relightable 3D scene representations, and performs well in complex lighting settings that have posed a significant challenge to prior work.},
  YEAR = {2021},
  URL = {http://arxiv.org/abs/2012.03927v1},
  FILE = {2012.03927v1.pdf}
 }",Material/lighting estimation,,,,,,,,,,,CVPR 2021,,,,"Pratul P. Srinivasan, Boyang Deng, Xiuming Zhang, Matthew Tancik, Ben Mildenhall, Jonathan T. Barron",srinivasan2021nerv,00000052,"We present a method that takes as input a set of images of a scene illuminated by unconstrained known lighting, and produces as output a 3D representation that can be rendered from novel viewpoints under arbitrary lighting conditions. Our method represents the scene as a continuous volumetric function parameterized as MLPs whose inputs are a 3D location and whose outputs are the following scene properties at that input location: volume density, surface normal, material parameters, distance to the first surface intersection in any direction, and visibility of the external environment in any direction. Together, these allow us to render novel views of the object under arbitrary lighting, including indirect illumination effects. The predicted visibility and surface intersection fields are critical to our model's ability to simulate direct and indirect illumination during training, because the brute-force techniques used by prior work are intractable for lighting conditions outside of controlled setups with a single light. Our method outperforms alternative approaches for recovering relightable 3D scene representations, and performs well in complex lighting settings that have posed a significant challenge to prior work.",21,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnufoqNiJL5AJ5sD0BJPXxu1sBvFYbUC3W5CQfC5CwAUxIdTT7qyCc9-5ETEiWmRCyyk
5/23/2021 19:01:05,NeRD: Neural Reflectance Decomposition from Image Collections,NeRD,12/7/2020,https://arxiv.org/pdf/2012.03918.pdf,https://markboss.me/publication/2021-nerd/?s=09,https://github.com/cgtuebingen/NeRD-Neural-Reflectance-Decomposition,"https://www.youtube.com/watch?v=JL-qMTXw9VU, https://www.youtube.com/watch?v=IM9OgMwHNTI",,,,"@inproceedings{boss2021nerd,
  BOOKTITLE = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
  ID = {boss2021nerd},
  ENTRYTYPE = {inproceedings},
  AUTHOR = {Mark Boss and Raphael Braun and Varun Jampani and Jonathan T. Barron and Ce Liu and Hendrik P. A. Lensch},
  TITLE = {NeRD: Neural Reflectance Decomposition from Image Collections},
  EPRINT = {2012.03918v4},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Decomposing a scene into its shape, reflectance, and illumination is a challenging but important problem in computer vision and graphics. This problem is inherently more challenging when the illumination is not a single light source under laboratory conditions but is instead an unconstrained environmental illumination. Though recent work has shown that implicit representations can be used to model the radiance field of an object, most of these techniques only enable view synthesis and not relighting. Additionally, evaluating these radiance fields is resource and time-intensive. We propose a neural reflectance decomposition (NeRD) technique that uses physically-based rendering to decompose the scene into spatially varying BRDF material properties. In contrast to existing techniques, our input images can be captured under different illumination conditions. In addition, we also propose techniques to convert the learned reflectance volume into a relightable textured mesh enabling fast real-time rendering with novel illuminations. We demonstrate the potential of the proposed approach with experiments on both synthetic and real datasets, where we are able to obtain high-quality relightable 3D assets from image collections. The datasets and code is available on the project page: https://markboss.me/publication/2021-nerd/},
  YEAR = {2021},
  URL = {http://arxiv.org/abs/2012.03918v4},
  FILE = {2012.03918v4.pdf}
 }",Material/lighting estimation,Sampling,,,,,,,,,,ICCV 2021,https://github.com/cgtuebingen/NeRD-Neural-Reflectance-Decomposition/blob/master/download_datasets.py,,,"Mark Boss, Raphael Braun, Varun Jampani, Jonathan T. Barron, Ce Liu, Hendrik P. A. Lensch",boss2021nerd,00000053,"Decomposing a scene into its shape, reflectance, and illumination is a challenging but essential problem in computer vision and graphics. This problem is inherently more challenging when the illumination is not a single light source under laboratory conditions but is instead an unconstrained environmental illumination. Though recent work has shown that implicit representations can be used to model the radiance field of an object, these techniques only enable view synthesis and not relighting. Additionally, evaluating these radiance fields is resource and time-intensive. By decomposing a scene into explicit representations, any rendering framework can be leveraged to generate novel views under any illumination in real-time. NeRD is a method that achieves this decomposition by introducing physically-based rendering to neural radiance fields. Even challenging non-Lambertian reflectances, complex geometry, and unknown illumination can be decomposed into high-quality models. The datasets and code is available on the project page: https://markboss.me/publication/2021-nerd/",3,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnuePlOSX_I1aYBlHKcMFZZyIjMxXxMin1bKenitrc5xeANX8ipLrCixWwbVb1r69aro
5/23/2021 18:55:43,iNeRF: Inverting Neural Radiance Fields for Pose Estimation,iNeRF,12/10/2020,https://arxiv.org/pdf/2012.05877.pdf,https://yenchenlin.me/inerf/,Coming soon,https://www.youtube.com/watch?v=eQuCZaQN0tI,,,,"@article{yen-chen2021inerf,
  ID = {yen-chen2021inerf},
  ENTRYTYPE = {article},
  AUTHOR = {Lin Yen-Chen and Pete Florence and Jonathan T. Barron and Alberto Rodriguez and Phillip Isola and Tsung-Yi Lin},
  TITLE = {INeRF: Inverting Neural Radiance Fields for Pose Estimation},
  EPRINT = {2012.05877v3},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We present iNeRF, a framework that performs mesh-free pose estimation by ""inverting"" a Neural RadianceField (NeRF). NeRFs have been shown to be remarkably effective for the task of view synthesis - synthesizing photorealistic novel views of real-world scenes or objects. In this work, we investigate whether we can apply analysis-by-synthesis via NeRF for mesh-free, RGB-only 6DoF pose estimation - given an image, find the translation and rotation of a camera relative to a 3D object or scene. Our method assumes that no object mesh models are available during either training or test time. Starting from an initial pose estimate, we use gradient descent to minimize the residual between pixels rendered from a NeRF and pixels in an observed image. In our experiments, we first study 1) how to sample rays during pose refinement for iNeRF to collect informative gradients and 2) how different batch sizes of rays affect iNeRF on a synthetic dataset. We then show that for complex real-world scenes from the LLFF dataset, iNeRF can improve NeRF by estimating the camera poses of novel images and using these images as additional training data for NeRF. Finally, we show iNeRF can perform category-level object pose estimation, including object instances not seen during training, with RGB images by inverting a NeRF model inferred from a single view.},
  YEAR = {2020},
  MONTH = {Dec},
  URL = {http://arxiv.org/abs/2012.05877v3},
  FILE = {2012.05877v3.pdf}
 }",Camera parameter estimation,Sampling,,,,,,,,,,IROS 2021,,,,"Lin Yen-Chen, Pete Florence, Jonathan T. Barron, Alberto Rodriguez, Phillip Isola, Tsung-Yi Lin",yen-chen2021inerf,00000054,"We present iNeRF, a framework that performs mesh-free pose estimation by ""inverting"" a Neural RadianceField (NeRF). NeRFs have been shown to be remarkably effective for the task of view synthesis - synthesizing photorealistic novel views of real-world scenes or objects. In this work, we investigate whether we can apply analysis-by-synthesis via NeRF for mesh-free, RGB-only 6DoF pose estimation - given an image, find the translation and rotation of a camera relative to a 3D object or scene. Our method assumes that no object mesh models are available during either training or test time. Starting from an initial pose estimate, we use gradient descent to minimize the residual between pixels rendered from a NeRF and pixels in an observed image. In our experiments, we first study 1) how to sample rays during pose refinement for iNeRF to collect informative gradients and 2) how different batch sizes of rays affect iNeRF on a synthetic dataset. We then show that for complex real-world scenes from the LLFF dataset, iNeRF can improve NeRF by estimating the camera poses of novel images and using these images as additional training data for NeRF. Finally, we show iNeRF can perform category-level object pose estimation, including object instances not seen during training, with RGB images by inverting a NeRF model inferred from a single view.",9,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnufbKmsXpug0aE3wyjj5ehz5dWE_8FTn5FNo87VfE1hsruAjJeXo9z7NLOsTubjcORw
7/19/2021 21:25:34,Portrait Neural Radiance Fields from a Single Image,PortraitNeRF,12/10/2020,https://arxiv.org/pdf/2012.05903.pdf,https://portrait-nerf.github.io/,Coming soon,,,,,"@article{gao2020portraitnerf,
  JOURNAL = {arXiv preprint arXiv:2012.05903},
  BOOKTITLE = {ArXiv Pre-print},
  ID = {gao2020portraitnerf},
  ENTRYTYPE = {article},
  AUTHOR = {Chen Gao and Yichang Shih and Wei-Sheng Lai and Chia-Kai Liang and Jia-Bin Huang},
  TITLE = {Portrait Neural Radiance Fields from a Single Image},
  EPRINT = {2012.05903v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We present a method for estimating Neural Radiance Fields (NeRF) from a single headshot portrait. While NeRF has demonstrated high-quality view synthesis, it requires multiple images of static scenes and thus impractical for casual captures and moving subjects. In this work, we propose to pretrain the weights of a multilayer perceptron (MLP), which implicitly models the volumetric density and colors, with a meta-learning framework using a light stage portrait dataset. To improve the generalization to unseen faces, we train the MLP in the canonical coordinate space approximated by 3D face morphable models. We quantitatively evaluate the method using controlled captures and demonstrate the generalization to real portrait images, showing favorable results against state-of-the-arts.},
  YEAR = {2020},
  URL = {http://arxiv.org/abs/2012.05903v2},
  FILE = {2012.05903v2.pdf}
 }","Human head, Few-shot reconstruction, Generalization","Per-instance fine-tuning, Data-driven",NeRF,Density,Category-level,,,,,,,ARXIV 2020,https://drive.google.com/drive/folders/1wbXc8rMHjRKj6cynKQePnEXMvyPb2HAn,No,Direct,"Chen Gao, Yichang Shih, Wei-Sheng Lai, Chia-Kai Liang, Jia-Bin Huang",gao2020portraitnerf,00000055,"We present a method for estimating Neural Radiance Fields (NeRF) from a single headshot portrait. While NeRF has demonstrated high-quality view synthesis, it requires multiple images of static scenes and thus impractical for casual captures and moving subjects. In this work, we propose to pretrain the weights of a multilayer perceptron (MLP), which implicitly models the volumetric density and colors, with a meta-learning framework using a light stage portrait dataset. To improve the generalization to unseen faces, we train the MLP in the canonical coordinate space approximated by 3D face morphable models. We quantitatively evaluate the method using controlled captures and demonstrate the generalization to real portrait images, showing favorable results against state-of-the-arts.",6,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnudfZHqfEaKeWUZAoouLvxQRNvH_v5QXbR-rnaNqQMePCNv0zIHG4W-DBM_wKxQvdUQ
5/23/2021 18:59:28,Iso-Points: Optimizing Neural Implicit Surfaces with Hybrid Representations,Iso-Points,12/11/2020,https://arxiv.org/pdf/2012.06434.pdf,,,,,,,"@inproceedings{yifan2021isopoints,
  BOOKTITLE = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  ID = {yifan2021isopoints},
  ENTRYTYPE = {inproceedings},
  AUTHOR = {Wang Yifan and Shihao Wu and Cengiz Oztireli and Olga Sorkine-Hornung},
  TITLE = {Iso-Points: Optimizing Neural Implicit Surfaces with Hybrid Representations},
  EPRINT = {2012.06434v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Neural implicit functions have emerged as a powerful representation for surfaces in 3D. Such a function can encode a high quality surface with intricate details into the parameters of a deep neural network. However, optimizing for the parameters for accurate and robust reconstructions remains a challenge, especially when the input data is noisy or incomplete. In this work, we develop a hybrid neural surface representation that allows us to impose geometry-aware sampling and regularization, which significantly improves the fidelity of reconstructions. We propose to use \emph{iso-points} as an explicit representation for a neural implicit function. These points are computed and updated on-the-fly during training to capture important geometric features and impose geometric constraints on the optimization. We demonstrate that our method can be adopted to improve state-of-the-art techniques for reconstructing neural implicit surfaces from multi-view images or point clouds. Quantitative and qualitative evaluations show that, compared with existing sampling and optimization methods, our approach allows faster convergence, better generalization, and accurate recovery of details and topology.},
  YEAR = {2021},
  URL = {http://arxiv.org/abs/2012.06434v2},
  FILE = {2012.06434v2.pdf}
 }",,"Sampling, Representation",,,,,,,,,,CVPR 2021,,,,"Wang Yifan, Shihao Wu, Cengiz Oztireli, Olga Sorkine-Hornung",yifan2021isopoints,00000056,"Neural implicit functions have emerged as a powerful representation for surfaces in 3D. Such a function can encode a high quality surface with intricate details into the parameters of a deep neural network. However, optimizing for the parameters for accurate and robust reconstructions remains a challenge, especially when the input data is noisy or incomplete. In this work, we develop a hybrid neural surface representation that allows us to impose geometry-aware sampling and regularization, which significantly improves the fidelity of reconstructions. We propose to use \emph{iso-points} as an explicit representation for a neural implicit function. These points are computed and updated on-the-fly during training to capture important geometric features and impose geometric constraints on the optimization. We demonstrate that our method can be adopted to improve state-of-the-art techniques for reconstructing neural implicit surfaces from multi-view images or point clouds. Quantitative and qualitative evaluations show that, compared with existing sampling and optimization methods, our approach allows faster convergence, better generalization, and accurate recovery of details and topology.",1,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnudxRsGezq5JndkCRGjsCgcvxlEDPWKGnvM45q0rvU8oBgAjF-anRu7Y6n45LI6lxcA
7/19/2021 21:48:01,Deep Optimized Priors for 3D Shape Modeling and Reconstruction,,12/14/2020,https://arxiv.org/pdf/2012.07241.pdf,,,,,,,"@inproceedings{yang2021deep,
  BOOKTITLE = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  ID = {yang2021deep},
  ENTRYTYPE = {inproceedings},
  AUTHOR = {Mingyue Yang and Yuxin Wen and Weikai Chen and Yongwei Chen and Kui Jia},
  TITLE = {Deep Optimized Priors for 3D Shape Modeling and Reconstruction},
  EPRINT = {2012.07241v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Many learning-based approaches have difficulty scaling to unseen data, as the generality of its learned prior is limited to the scale and variations of the training samples. This holds particularly true with 3D learning tasks, given the sparsity of 3D datasets available. We introduce a new learning framework for 3D modeling and reconstruction that greatly improves the generalization ability of a deep generator. Our approach strives to connect the good ends of both learning-based and optimization-based methods. In particular, unlike the common practice that fixes the pre-trained priors at test time, we propose to further optimize the learned prior and latent code according to the input physical measurements after the training. We show that the proposed strategy effectively breaks the barriers constrained by the pre-trained priors and could lead to high-quality adaptation to unseen data. We realize our framework using the implicit surface representation and validate the efficacy of our approach in a variety of challenging tasks that take highly sparse or collapsed observations as input. Experimental results show that our approach compares favorably with the state-of-the-art methods in terms of both generality and accuracy.},
  YEAR = {2021},
  URL = {http://arxiv.org/abs/2012.07241v1},
  FILE = {2012.07241v1.pdf}
 }",Generalization,"Conditional neural field, Per-instance fine-tuning, Data-driven",,,,,,,,,,CVPR 2021,,,,"Mingyue Yang, Yuxin Wen, Weikai Chen, Yongwei Chen, Kui Jia",yang2021deep,00000057,"Many learning-based approaches have difficulty scaling to unseen data, as the generality of its learned prior is limited to the scale and variations of the training samples. This holds particularly true with 3D learning tasks, given the sparsity of 3D datasets available. We introduce a new learning framework for 3D modeling and reconstruction that greatly improves the generalization ability of a deep generator. Our approach strives to connect the good ends of both learning-based and optimization-based methods. In particular, unlike the common practice that fixes the pre-trained priors at test time, we propose to further optimize the learned prior and latent code according to the input physical measurements after the training. We show that the proposed strategy effectively breaks the barriers constrained by the pre-trained priors and could lead to high-quality adaptation to unseen data. We realize our framework using the implicit surface representation and validate the efficacy of our approach in a variety of challenging tasks that take highly sparse or collapsed observations as input. Experimental results show that our approach compares favorably with the state-of-the-art methods in terms of both generality and accuracy.",3,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnueKUmtABjJmC8fsGXg18r_i_nLmXLQKRZ1uPWl8McYLh4fLCsPBmtUwoIQX-KTLBdc
5/23/2021 18:37:55,Object-Centric Neural Scene Rendering,OSFs,12/15/2020,https://arxiv.org/pdf/2012.08503.pdf,https://www.shellguo.com/osf/,,https://www.youtube.com/watch?v=NtR7xgxSL1U,,,,"@article{guo2020osfs,
  JOURNAL = {arXiv preprint arXiv:2012.08503},
  BOOKTITLE = {ArXiv Pre-print},
  ID = {guo2020osfs},
  ENTRYTYPE = {article},
  AUTHOR = {Michelle Guo and Alireza Fathi and Jiajun Wu and Thomas Funkhouser},
  TITLE = {Object-Centric Neural Scene Rendering},
  EPRINT = {2012.08503v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We present a method for composing photorealistic scenes from captured images of objects. Our work builds upon neural radiance fields (NeRFs), which implicitly model the volumetric density and directionally-emitted radiance of a scene. While NeRFs synthesize realistic pictures, they only model static scenes and are closely tied to specific imaging conditions. This property makes NeRFs hard to generalize to new scenarios, including new lighting or new arrangements of objects. Instead of learning a scene radiance field as a NeRF does, we propose to learn object-centric neural scattering functions (OSFs), a representation that models per-object light transport implicitly using a lighting- and view-dependent neural network. This enables rendering scenes even when objects or lights move, without retraining. Combined with a volumetric path tracing procedure, our framework is capable of rendering both intra- and inter-object light transport effects including occlusions, specularities, shadows, and indirect illumination. We evaluate our approach on scene composition and show that it generalizes to novel illumination conditions, producing photorealistic, physically accurate renderings of multi-object scenes.},
  YEAR = {2020},
  URL = {http://arxiv.org/abs/2012.08503v1},
  FILE = {2012.08503v1.pdf}
 }","Editable, Segmentation/composition, Material/lighting estimation",Object-centric representation,,,,,,,,,,ARXIV 2020,,,,"Michelle Guo, Alireza Fathi, Jiajun Wu, Thomas Funkhouser",guo2020osfs,00000058,"We present a method for composing photorealistic scenes from captured images of objects. Our work builds upon neural radiance fields (NeRFs), which implicitly model the volumetric density and directionally-emitted radiance of a scene. While NeRFs synthesize realistic pictures, they only model static scenes and are closely tied to specific imaging conditions. This property makes NeRFs hard to generalize to new scenarios, including new lighting or new arrangements of objects. Instead of learning a scene radiance field as a NeRF does, we propose to learn object-centric neural scattering functions (OSFs), a representation that models per-object light transport implicitly using a lighting- and view-dependent neural network. This enables rendering scenes even when objects or lights move, without retraining. Combined with a volumetric path tracing procedure, our framework is capable of rendering both intra- and inter-object light transport effects including occlusions, specularities, shadows, and indirect illumination. We evaluate our approach on scene composition and show that it generalizes to novel illumination conditions, producing photorealistic, physically accurate renderings of multi-object scenes.",6,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnufH9MtS9fNvgtaEojxtpna6RZmkgKF5rvaJl9--cH9NnH55lB2OcEmdbCEZWHAYA2I
8/29/2021 17:03:11,Learning Continuous Image Representation with Local Implicit Image Function,LIIF,12/16/2020,https://arxiv.org/pdf/2012.09161.pdf,https://yinboc.github.io/liif/,https://github.com/yinboc/liif,https://www.youtube.com/watch?v=6f2roieSY_8,,,,"@article{chenOralliif,
  ID = {chenOralliif},
  ENTRYTYPE = {article},
  AUTHOR = {Yinbo Chen and Sifei Liu and Xiaolong Wang},
  TITLE = {Learning Continuous Image Representation with Local Implicit Image Function},
  EPRINT = {2012.09161v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {How to represent an image? While the visual world is presented in a continuous manner, machines store and see the images in a discrete way with 2D arrays of pixels. In this paper, we seek to learn a continuous representation for images. Inspired by the recent progress in 3D reconstruction with implicit neural representation, we propose Local Implicit Image Function (LIIF), which takes an image coordinate and the 2D deep features around the coordinate as inputs, predicts the RGB value at a given coordinate as an output. Since the coordinates are continuous, LIIF can be presented in arbitrary resolution. To generate the continuous representation for images, we train an encoder with LIIF representation via a self-supervised task with super-resolution. The learned continuous representation can be presented in arbitrary resolution even extrapolate to x30 higher resolution, where the training tasks are not provided. We further show that LIIF representation builds a bridge between discrete and continuous representation in 2D, it naturally supports the learning tasks with size-varied image ground-truths and significantly outperforms the method with resizing the ground-truths.},
  YEAR = {2020},
  MONTH = {Dec},
  URL = {http://arxiv.org/abs/2012.09161v2},
  FILE = {2012.09161v2.pdf}
 }","Image, Fundamentals","Conditional neural field, Representation, Data-driven",SIREN,,,,,,,,,CVPR 2021 Oral,,,Direct,"Yinbo Chen, Sifei Liu, Xiaolong Wang",chenoralliif,00000172,"How to represent an image? While the visual world is presented in a continuous manner, machines store and see the images in a discrete way with 2D arrays of pixels. In this paper, we seek to learn a continuous representation for images. Inspired by the recent progress in 3D reconstruction with implicit neural representation, we propose Local Implicit Image Function (LIIF), which takes an image coordinate and the 2D deep features around the coordinate as inputs, predicts the RGB value at a given coordinate as an output. Since the coordinates are continuous, LIIF can be presented in arbitrary resolution. To generate the continuous representation for images, we train an encoder with LIIF representation via a self-supervised task with super-resolution. The learned continuous representation can be presented in arbitrary resolution even extrapolate to x30 higher resolution, where the training tasks are not provided. We further show that LIIF representation builds a bridge between discrete and continuous representation in 2D, it naturally supports the learning tasks with size-varied image ground-truths and significantly outperforms the method with resizing the ground-truths.",10,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnuc0cf8OovL38qIfGO9rvcoCyQpgTBBSlJraNlg3LLrcGxSKZ0yZJFwefrJbFUo1GT4
5/23/2021 18:14:09,Neural Radiance Flow for 4D View Synthesis and Video Processing,NeRFlow,12/17/2020,https://arxiv.org/pdf/2012.09790.pdf,https://yilundu.github.io/nerflow/,Coming soon,,,,,"@inproceedings{du2021nerflow,
  BOOKTITLE = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
  ID = {du2021nerflow},
  ENTRYTYPE = {inproceedings},
  AUTHOR = {Yilun Du and Yinan Zhang and Hong-Xing Yu and Joshua B. Tenenbaum and Jiajun Wu},
  TITLE = {Neural Radiance Flow for 4D View Synthesis and Video Processing},
  EPRINT = {2012.09790v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We present a method, Neural Radiance Flow (NeRFlow),to learn a 4D spatial-temporal representation of a dynamic scene from a set of RGB images. Key to our approach is the use of a neural implicit representation that learns to capture the 3D occupancy, radiance, and dynamics of the scene. By enforcing consistency across different modalities, our representation enables multi-view rendering in diverse dynamic scenes, including water pouring, robotic interaction, and real images, outperforming state-of-the-art methods for spatial-temporal view synthesis. Our approach works even when inputs images are captured with only one camera. We further demonstrate that the learned representation can serve as an implicit scene prior, enabling video processing tasks such as image super-resolution and de-noising without any additional supervision.},
  YEAR = {2021},
  URL = {http://arxiv.org/abs/2012.09790v2},
  FILE = {2012.09790v2.pdf}
 }",Dynamic,,,,,,,,,,,ICCV 2021,,,,"Yilun Du, Yinan Zhang, Hong-Xing Yu, Joshua B. Tenenbaum, Jiajun Wu",du2021nerflow,00000059,"We present a method, Neural Radiance Flow (NeRFlow),to learn a 4D spatial-temporal representation of a dynamic scene from a set of RGB images. Key to our approach is the use of a neural implicit representation that learns to capture the 3D occupancy, radiance, and dynamics of the scene. By enforcing consistency across different modalities, our representation enables multi-view rendering in diverse dynamic scenes, including water pouring, robotic interaction, and real images, outperforming state-of-the-art methods for spatial-temporal view synthesis. Our approach works even when inputs images are captured with only one camera. We further demonstrate that the learned representation can serve as an implicit scene prior, enabling video processing tasks such as image super-resolution and de-noising without any additional supervision.",5,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnuehe9a2imk4g82Lx6dDBF4x92biMR6nSMEOy0RPcApn0N5wYw7-vRIzKOGP6s1yYvo
7/19/2021 16:53:32,Learning Compositional Radiance Fields of Dynamic Human Heads,HybridNeRF,12/17/2020,https://arxiv.org/pdf/2012.09955.pdf,https://ziyanw1.github.io/hybrid_nerf/,,,https://openaccess.thecvf.com/content/CVPR2021/supplemental/Wang_Learning_Compositional_Radiance_CVPR_2021_supplemental.pdf,,,"@article{wangOralhybridnerf,
  ID = {wangOralhybridnerf},
  ENTRYTYPE = {article},
  AUTHOR = {Ziyan Wang and Timur Bagautdinov and Stephen Lombardi and Tomas Simon and Jason Saragih and Jessica Hodgins and Michael Zollhofer},
  TITLE = {Learning Compositional Radiance Fields of Dynamic Human Heads},
  EPRINT = {2012.09955v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Photorealistic rendering of dynamic humans is an important ability for telepresence systems, virtual shopping, synthetic data generation, and more. Recently, neural rendering methods, which combine techniques from computer graphics and machine learning, have created high-fidelity models of humans and objects. Some of these methods do not produce results with high-enough fidelity for driveable human models (Neural Volumes) whereas others have extremely long rendering times (NeRF). We propose a novel compositional 3D representation that combines the best of previous methods to produce both higher-resolution and faster results. Our representation bridges the gap between discrete and continuous volumetric representations by combining a coarse 3D-structure-aware grid of animation codes with a continuous learned scene function that maps every position and its corresponding local animation code to its view-dependent emitted radiance and local volume density. Differentiable volume rendering is employed to compute photo-realistic novel views of the human head and upper body as well as to train our novel representation end-to-end using only 2D supervision. In addition, we show that the learned dynamic radiance field can be used to synthesize novel unseen expressions based on a global animation code. Our approach achieves state-of-the-art results for synthesizing novel views of dynamic human heads and the upper body.},
  YEAR = {2020},
  MONTH = {Dec},
  URL = {http://arxiv.org/abs/2012.09955v1},
  FILE = {2012.09955v1.pdf}
 }","Dynamic, Human head, Generalization","Voxelization, Feature volume, Sampling",,,,,,,,,,CVPR 2021 Oral,,No,,"Ziyan Wang, Timur Bagautdinov, Stephen Lombardi, Tomas Simon, Jason Saragih, Jessica Hodgins, Michael Zollhöfer",wangoralhybridnerf,00000060,"Photorealistic rendering of dynamic humans is an important ability for telepresence systems, virtual shopping, synthetic data generation, and more. Recently, neural rendering methods, which combine techniques from computer graphics and machine learning, have created high-fidelity models of humans and objects. Some of these methods do not produce results with high-enough fidelity for driveable human models (Neural Volumes) whereas others have extremely long rendering times (NeRF). We propose a novel compositional 3D representation that combines the best of previous methods to produce both higher-resolution and faster results. Our representation bridges the gap between discrete and continuous volumetric representations by combining a coarse 3D-structure-aware grid of animation codes with a continuous learned scene function that maps every position and its corresponding local animation code to its view-dependent emitted radiance and local volume density. Differentiable volume rendering is employed to compute photo-realistic novel views of the human head and upper body as well as to train our novel representation end-to-end using only 2D supervision. In addition, we show that the learned dynamic radiance field can be used to synthesize novel unseen expressions based on a global animation code. Our approach achieves state-of-the-art results for synthesizing novel views of dynamic human heads and the upper body.",5,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnuf_2Fn46b_7o8rVeg2QI2Tvzi2RnkxOzxaVsBHfM66wyf7W8X2B5XB993LeCmpULag
8/29/2021 16:42:59,HyperSeg: Patch-wise Hypernetwork for Real-time Semantic Segmentation,HyperSeg,12/21/2020,https://arxiv.org/pdf/2012.11582.pdf,,https://github.com/YuvalNirkin/hyperseg,,,,,"@inproceedings{nirkin2021hyperseg,
  BOOKTITLE = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  ID = {nirkin2021hyperseg},
  ENTRYTYPE = {inproceedings},
  AUTHOR = {Yuval Nirkin and Lior Wolf and Tal Hassner},
  TITLE = {HyperSeg: Patch-wise Hypernetwork for Real-time Semantic Segmentation},
  EPRINT = {2012.11582v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We present a novel, real-time, semantic segmentation network in which the encoder both encodes and generates the parameters (weights) of the decoder. Furthermore, to allow maximal adaptivity, the weights at each decoder block vary spatially. For this purpose, we design a new type of hypernetwork, composed of a nested U-Net for drawing higher level context features, a multi-headed weight generating module which generates the weights of each block in the decoder immediately before they are consumed, for efficient memory utilization, and a primary network that is composed of novel dynamic patch-wise convolutions. Despite the usage of less-conventional blocks, our architecture obtains real-time performance. In terms of the runtime vs. accuracy trade-off, we surpass state of the art (SotA) results on popular semantic segmentation benchmarks: PASCAL VOC 2012 (val. set) and real-time semantic segmentation on Cityscapes, and CamVid. The code is available: https://nirkin.com/hyperseg.},
  YEAR = {2021},
  URL = {http://arxiv.org/abs/2012.11582v2},
  FILE = {2012.11582v2.pdf}
 }",Image,"Hypernetwork, Data-driven, Coordinate CNN",,,,,,,,,,CVPR 2021,https://github.com/YuvalNirkin/hyperseg,,"Direct, Indirect","Yuval Nirkin, Lior Wolf, Tal Hassner",nirkin2021hyperseg,00000169,"We present a novel, real-time, semantic segmentation network in which the encoder both encodes and generates the parameters (weights) of the decoder. Furthermore, to allow maximal adaptivity, the weights at each decoder block vary spatially. For this purpose, we design a new type of hypernetwork, composed of a nested U-Net for drawing higher level context features, a multi-headed weight generating module which generates the weights of each block in the decoder immediately before they are consumed, for efficient memory utilization, and a primary network that is composed of novel dynamic patch-wise convolutions. Despite the usage of less-conventional blocks, our architecture obtains real-time performance. In terms of the runtime vs. accuracy trade-off, we surpass state of the art (SotA) results on popular semantic segmentation benchmarks: PASCAL VOC 2012 (val. set) and real-time semantic segmentation on Cityscapes, and CamVid. The code is available: https://nirkin.com/hyperseg.",2,,Yes,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnuf4oc89_cMKdVY39Ua695_vSJN6rY5hXToSNm-S18arnOJzLq_ORnZLcKy8GlPVGbI
5/23/2021 18:16:35,Non-Rigid Neural Radiance Fields: Reconstruction and Novel View Synthesis of a Dynamic Scene From Monocular Video,NR-NeRF,12/22/2020,https://arxiv.org/pdf/2012.12247.pdf,,,,,,,"@inproceedings{tretschk2021nrnerf,
  BOOKTITLE = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
  ID = {tretschk2021nrnerf},
  ENTRYTYPE = {inproceedings},
  AUTHOR = {Edgar Tretschk and Ayush Tewari and Vladislav Golyanik and Michael Zollhofer and Christoph Lassner and Christian Theobalt},
  TITLE = {Non-Rigid Neural Radiance Fields: Reconstruction and Novel View Synthesis of a Dynamic Scene From Monocular Video},
  EPRINT = {2012.12247v4},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We present Non-Rigid Neural Radiance Fields (NR-NeRF), a reconstruction and novel view synthesis approach for general non-rigid dynamic scenes. Our approach takes RGB images of a dynamic scene as input (e.g., from a monocular video recording), and creates a high-quality space-time geometry and appearance representation. We show that a single handheld consumer-grade camera is sufficient to synthesize sophisticated renderings of a dynamic scene from novel virtual camera views, e.g. a `bullet-time' video effect. NR-NeRF disentangles the dynamic scene into a canonical volume and its deformation. Scene deformation is implemented as ray bending, where straight rays are deformed non-rigidly. We also propose a novel rigidity network to better constrain rigid regions of the scene, leading to more stable results. The ray bending and rigidity network are trained without explicit supervision. Our formulation enables dense correspondence estimation across views and time, and compelling video editing applications such as motion exaggeration. Our code will be open sourced.},
  YEAR = {2021},
  URL = {http://arxiv.org/abs/2012.12247v4},
  FILE = {2012.12247v4.pdf}
 }",Dynamic,"Conditional neural field, Warping field/Flow field",,,,,,,,,,ICCV 2021,,,,"Edgar Tretschk, Ayush Tewari, Vladislav Golyanik, Michael Zollhöfer, Christoph Lassner, Christian Theobalt",tretschk2021nrnerf,00000061,"We present Non-Rigid Neural Radiance Fields (NR-NeRF), a reconstruction and novel view synthesis approach for general non-rigid dynamic scenes. Our approach takes RGB images of a dynamic scene as input, e.g., from a monocular video recording, and creates a high-quality space-time geometry and appearance representation. In particular, we show that even a single handheld consumer-grade camera is sufficient to synthesize sophisticated renderings of a dynamic scene from novel virtual camera views, for example a `bullet-time' video effect. Our method disentangles the dynamic scene into a canonical volume and its deformation. Scene deformation is implemented as ray bending, where straight rays are deformed non-rigidly to represent scene motion. We also propose a novel rigidity regression network that enables us to better constrain rigid regions of the scene, which leads to more stable results. The ray bending and rigidity network are trained without any explicit supervision. In addition to novel view synthesis, our formulation enables dense correspondence estimation across views and time, as well as compelling video editing applications such as motion exaggeration. We demonstrate the effectiveness of our method using extensive evaluations, including ablation studies and comparisons to the state of the art. We urge the reader to watch the supplemental video for qualitative results. Our code will be open sourced.",18,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnuepoKfRycIWWX2v1Nh-2qyJNQBq5CiAZV4q4C8F6KwXCJ2G5iGyTZURFI5jjXGMeH4
5/23/2021 18:56:31,STaR: Self-supervised Tracking and Reconstruction of Rigid Objects in Motion with Neural Rendering,STaR,12/22/2020,https://arxiv.org/pdf/2101.01602.pdf,https://wentaoyuan.github.io/star/,Coming soon,,,,,"@inproceedings{yuan2021star,
  BOOKTITLE = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  ID = {yuan2021star},
  ENTRYTYPE = {inproceedings},
  AUTHOR = {Wentao Yuan and Zhaoyang Lv and Tanner Schmidt and Steven Lovegrove},
  TITLE = {STaR: Self-supervised Tracking and Reconstruction of Rigid Objects in Motion with Neural Rendering},
  EPRINT = {2101.01602v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We present STaR, a novel method that performs Self-supervised Tracking and Reconstruction of dynamic scenes with rigid motion from multi-view RGB videos without any manual annotation. Recent work has shown that neural networks are surprisingly effective at the task of compressing many views of a scene into a learned function which maps from a viewing ray to an observed radiance value via volume rendering. Unfortunately, these methods lose all their predictive power once any object in the scene has moved. In this work, we explicitly model rigid motion of objects in the context of neural representations of radiance fields. We show that without any additional human specified supervision, we can reconstruct a dynamic scene with a single rigid object in motion by simultaneously decomposing it into its two constituent parts and encoding each with its own neural representation. We achieve this by jointly optimizing the parameters of two neural radiance fields and a set of rigid poses which align the two fields at each frame. On both synthetic and real world datasets, we demonstrate that our method can render photorealistic novel views, where novelty is measured on both spatial and temporal axes. Our factored representation furthermore enables animation of unseen object motion.},
  YEAR = {2021},
  URL = {http://arxiv.org/abs/2101.01602v1},
  FILE = {2101.01602v1.pdf}
 }","Dynamic, Segmentation/composition",,,,,,,,,,,CVPR 2021,,,,"Wentao Yuan, Zhaoyang Lv, Tanner Schmidt, Steven Lovegrove",yuan2021star,00000062,"We present STaR, a novel method that performs Self-supervised Tracking and Reconstruction of dynamic scenes with rigid motion from multi-view RGB videos without any manual annotation. Recent work has shown that neural networks are surprisingly effective at the task of compressing many views of a scene into a learned function which maps from a viewing ray to an observed radiance value via volume rendering. Unfortunately, these methods lose all their predictive power once any object in the scene has moved. In this work, we explicitly model rigid motion of objects in the context of neural representations of radiance fields. We show that without any additional human specified supervision, we can reconstruct a dynamic scene with a single rigid object in motion by simultaneously decomposing it into its two constituent parts and encoding each with its own neural representation. We achieve this by jointly optimizing the parameters of two neural radiance fields and a set of rigid poses which align the two fields at each frame. On both synthetic and real world datasets, we demonstrate that our method can render photorealistic novel views, where novelty is measured on both spatial and temporal axes. Our factored representation furthermore enables animation of unseen object motion.",3,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnucgVFNN6uRAwtLeryspP9KnKLcaUwkyBIi4B3ajrmJ2G2Sx-B1s-sEwrVPGtpiP_Wc
5/23/2021 18:58:16,Neural Body: Implicit Neural Representations with Structured Latent Codes for Novel View Synthesis of Dynamic Humans,Neural Body,12/31/2020,https://arxiv.org/pdf/2012.15838.pdf,https://zju3dv.github.io/neuralbody/,https://github.com/zju3dv/neuralbody,,https://github.com/zju3dv/neuralbody/blob/master/supplementary_material.md,,,"@inproceedings{peng2021neuralbody,
  BOOKTITLE = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  ID = {peng2021neuralbody},
  ENTRYTYPE = {inproceedings},
  AUTHOR = {Sida Peng and Yuanqing Zhang and Yinghao Xu and Qianqian Wang and Qing Shuai and Hujun Bao and Xiaowei Zhou},
  TITLE = {Neural Body: Implicit Neural Representations with Structured Latent Codes for Novel View Synthesis of Dynamic Humans},
  EPRINT = {2012.15838v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {This paper addresses the challenge of novel view synthesis for a human performer from a very sparse set of camera views. Some recent works have shown that learning implicit neural representations of 3D scenes achieves remarkable view synthesis quality given dense input views. However, the representation learning will be ill-posed if the views are highly sparse. To solve this ill-posed problem, our key idea is to integrate observations over video frames. To this end, we propose Neural Body, a new human body representation which assumes that the learned neural representations at different frames share the same set of latent codes anchored to a deformable mesh, so that the observations across frames can be naturally integrated. The deformable mesh also provides geometric guidance for the network to learn 3D representations more efficiently. To evaluate our approach, we create a multi-view dataset named ZJU-MoCap that captures performers with complex motions. Experiments on ZJU-MoCap show that our approach outperforms prior works by a large margin in terms of novel view synthesis quality. We also demonstrate the capability of our approach to reconstruct a moving person from a monocular video on the People-Snapshot dataset. The code and dataset are available at https://zju3dv.github.io/neuralbody/.},
  YEAR = {2021},
  URL = {http://arxiv.org/abs/2012.15838v2},
  FILE = {2012.15838v2.pdf}
 }","Dynamic, Human body","Feature volume, Articulated",,,,,,,,,,CVPR 2021,https://zjueducn-my.sharepoint.com/:f:/g/personal/pengsida_zju_edu_cn/Eo9zn4x_xcZKmYHZNjzel7gBdWf_d4m-pISHhPWB-GZBYw?e=Hf4mz7,,,"Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, Xiaowei Zhou",peng2021neuralbody,00000063,"This paper addresses the challenge of novel view synthesis for a human performer from a very sparse set of camera views. Some recent works have shown that learning implicit neural representations of 3D scenes achieves remarkable view synthesis quality given dense input views. However, the representation learning will be ill-posed if the views are highly sparse. To solve this ill-posed problem, our key idea is to integrate observations over video frames. To this end, we propose Neural Body, a new human body representation which assumes that the learned neural representations at different frames share the same set of latent codes anchored to a deformable mesh, so that the observations across frames can be naturally integrated. The deformable mesh also provides geometric guidance for the network to learn 3D representations more efficiently. To evaluate our approach, we create a multi-view dataset named ZJU-MoCap that captures performers with complex motions. Experiments on ZJU-MoCap show that our approach outperforms prior works by a large margin in terms of novel view synthesis quality. We also demonstrate the capability of our approach to reconstruct a moving person from a monocular video on the People-Snapshot dataset. The code and dataset are available at https://zju3dv.github.io/neuralbody/.",23,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnucx5idL5k8tHfwXv2DcaqfoSwYkjHldWcJtCoTULiLtPaPk19Ofu_FIzCDkYb_BQxs
5/23/2021 18:43:36,Non-line-of-Sight Imaging via Neural Transient Fields,,1/2/2021,https://arxiv.org/pdf/2101.00373.pdf,https://sci2020.github.io/paper/2021/01/05/Non-line-of-Sight-Imaging-via-Neural-Transient-Fields.html,https://github.com/zeromakerplus/NeTF_public,,,,,"@article{shen2021nonlineofsight,
  JOURNAL = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  ID = {shen2021nonlineofsight},
  ENTRYTYPE = {article},
  AUTHOR = {Siyuan Shen and Zi Wang and Ping Liu and Zhengqing Pan and Ruiqian Li and Tian Gao and Shiying Li and Jingyi Yu},
  TITLE = {Non-line-of-Sight Imaging via Neural Transient Fields},
  EPRINT = {2101.00373v3},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {eess.IV},
  ABSTRACT = {We present a neural modeling framework for Non-Line-of-Sight (NLOS) imaging. Previous solutions have sought to explicitly recover the 3D geometry (e.g., as point clouds) or voxel density (e.g., within a pre-defined volume) of the hidden scene. In contrast, inspired by the recent Neural Radiance Field (NeRF) approach, we use a multi-layer perceptron (MLP) to represent the neural transient field or NeTF. However, NeTF measures the transient over spherical wavefronts rather than the radiance along lines. We therefore formulate a spherical volume NeTF reconstruction pipeline, applicable to both confocal and non-confocal setups. Compared with NeRF, NeTF samples a much sparser set of viewpoints (scanning spots) and the sampling is highly uneven. We thus introduce a Monte Carlo technique to improve the robustness in the reconstruction. Comprehensive experiments on synthetic and real datasets demonstrate NeTF provides higher quality reconstruction and preserves fine details largely missing in the state-of-the-art.},
  YEAR = {2021},
  URL = {http://arxiv.org/abs/2101.00373v3},
  FILE = {2101.00373v3.pdf}
 }",Beyond graphics,Learning residual,,,,,,,,,,TPAMI 2021,,,,"Siyuan Shen, Zi Wang, Ping Liu, Zhengqing Pan, Ruiqian Li, Tian Gao, Shiying Li, Jingyi Yu",shen2021nonlineofsight,00000064,"We present a neural modeling framework for Non-Line-of-Sight (NLOS) imaging. Previous solutions have sought to explicitly recover the 3D geometry (e.g., as point clouds) or voxel density (e.g., within a pre-defined volume) of the hidden scene. In contrast, inspired by the recent Neural Radiance Field (NeRF) approach, we use a multi-layer perceptron (MLP) to represent the neural transient field or NeTF. However, NeTF measures the transient over spherical wavefronts rather than the radiance along lines. We therefore formulate a spherical volume NeTF reconstruction pipeline, applicable to both confocal and non-confocal setups. Compared with NeRF, NeTF samples a much sparser set of viewpoints (scanning spots) and the sampling is highly uneven. We thus introduce a Monte Carlo technique to improve the robustness in the reconstruction. Comprehensive experiments on synthetic and real datasets demonstrate NeTF provides higher quality reconstruction and preserves fine details largely missing in the state-of-the-art.",0,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnueJLwl3tW273LENiNn3UuQI-aCZhVuA1fJFdxkN4_lTPvXk5II1dBAorqNn2Ez8yCI
5/23/2021 18:56:46,Pixel-aligned Volumetric Avatars,PVA,1/7/2021,https://arxiv.org/pdf/2101.02697.pdf,,,,,,,"@inproceedings{raj2021pva,
  BOOKTITLE = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  ID = {raj2021pva},
  ENTRYTYPE = {inproceedings},
  AUTHOR = {Amit Raj and Michael Zollhoefer and Tomas Simon and Jason Saragih and Shunsuke Saito and James Hays and Stephen Lombardi},
  TITLE = {PVA: Pixel-aligned Volumetric Avatars},
  EPRINT = {2101.02697v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Acquisition and rendering of photo-realistic human heads is a highly challenging research problem of particular importance for virtual telepresence. Currently, the highest quality is achieved by volumetric approaches trained in a person specific manner on multi-view data. These models better represent fine structure, such as hair, compared to simpler mesh-based models. Volumetric models typically employ a global code to represent facial expressions, such that they can be driven by a small set of animation parameters. While such architectures achieve impressive rendering quality, they can not easily be extended to the multi-identity setting. In this paper, we devise a novel approach for predicting volumetric avatars of the human head given just a small number of inputs. We enable generalization across identities by a novel parameterization that combines neural radiance fields with local, pixel-aligned features extracted directly from the inputs, thus sidestepping the need for very deep or complex networks. Our approach is trained in an end-to-end manner solely based on a photometric re-rendering loss without requiring explicit 3D supervision.We demonstrate that our approach outperforms the existing state of the art in terms of quality and is able to generate faithful facial expressions in a multi-identity setting.},
  YEAR = {2021},
  URL = {http://arxiv.org/abs/2101.02697v1},
  FILE = {2101.02697v1.pdf}
 }","Few-shot reconstruction, Generalization","Lifting 2D features to 3D, Image-based rendering, Data-driven",,,,,,,,,,CVPR 2021,,,,"Amit Raj, Michael Zollhoefer, Tomas Simon, Jason Saragih, Shunsuke Saito, James Hays, Stephen Lombardi",raj2021pva,00000065,"Acquisition and rendering of photo-realistic human heads is a highly challenging research problem of particular importance for virtual telepresence. Currently, the highest quality is achieved by volumetric approaches trained in a person specific manner on multi-view data. These models better represent fine structure, such as hair, compared to simpler mesh-based models. Volumetric models typically employ a global code to represent facial expressions, such that they can be driven by a small set of animation parameters. While such architectures achieve impressive rendering quality, they can not easily be extended to the multi-identity setting. In this paper, we devise a novel approach for predicting volumetric avatars of the human head given just a small number of inputs. We enable generalization across identities by a novel parameterization that combines neural radiance fields with local, pixel-aligned features extracted directly from the inputs, thus sidestepping the need for very deep or complex networks. Our approach is trained in an end-to-end manner solely based on a photometric re-rendering loss without requiring explicit 3D supervision.We demonstrate that our approach outperforms the existing state of the art in terms of quality and is able to generate faithful facial expressions in a multi-identity setting.",6,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnudxATwMvzHUQ_x1-LfF-uKxFrPxw4nR3vWT_jmtJr7NjgiU-PfHDyhnBnN3-r-bsS8
9/17/2021 11:46:52,"S3: Neural Shape, Skeleton, and Skinning Fields for 3D Human Modeling",S3,1/17/2021,https://arxiv.org/pdf/2101.06571.pdf,,,,,,,"@inproceedings{yang2021s3,
  BOOKTITLE = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  ID = {yang2021s3},
  ENTRYTYPE = {inproceedings},
  AUTHOR = {Ze Yang and Shenlong Wang and Sivabalan Manivasagam and Zeng Huang and Wei-Chiu Ma and Xinchen Yan and Ersin Yumer and Raquel Urtasun},
  TITLE = {S3: Neural Shape, Skeleton, and Skinning Fields for 3D Human Modeling},
  EPRINT = {2101.06571v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Constructing and animating humans is an important component for building virtual worlds in a wide variety of applications such as virtual reality or robotics testing in simulation. As there are exponentially many variations of humans with different shape, pose and clothing, it is critical to develop methods that can automatically reconstruct and animate humans at scale from real world data. Towards this goal, we represent the pedestrian's shape, pose and skinning weights as neural implicit functions that are directly learned from data. This representation enables us to handle a wide variety of different pedestrian shapes and poses without explicitly fitting a human parametric body model, allowing us to handle a wider range of human geometries and topologies. We demonstrate the effectiveness of our approach on various datasets and show that our reconstructions outperform existing state-of-the-art methods. Furthermore, our re-animation experiments show that we can generate 3D human animations at scale from a single RGB image (and/or an optional LiDAR sweep) as input.},
  YEAR = {2021},
  URL = {http://arxiv.org/abs/2101.06571v1},
  FILE = {2101.06571v1.pdf}
 }","Human body, Editable","Conditional neural field, Voxelization, Feature volume",,Occupancy,Category-level,,,,,,,CVPR 2021,,Yes,Direct,"Ze Yang, Shenlong Wang, Sivabalan Manivasagam, Zeng Huang, Wei-Chiu Ma, Xinchen Yan, Ersin Yumer, Raquel Urtasun",yang2021s3,00000197,"Constructing and animating humans is an important component for building virtual worlds in a wide variety of applications such as virtual reality or robotics testing in simulation. As there are exponentially many variations of humans with different shape, pose and clothing, it is critical to develop methods that can automatically reconstruct and animate humans at scale from real world data. Towards this goal, we represent the pedestrian's shape, pose and skinning weights as neural implicit functions that are directly learned from data. This representation enables us to handle a wide variety of different pedestrian shapes and poses without explicitly fitting a human parametric body model, allowing us to handle a wider range of human geometries and topologies. We demonstrate the effectiveness of our approach on various datasets and show that our reconstructions outperform existing state-of-the-art methods. Furthermore, our re-animation experiments show that we can generate 3D human animations at scale from a single RGB image (and/or an optional LiDAR sweep) as input.",,No,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnueRZAOQxTf4Z-onuhtpnYt4cFX8RELKjQmgzy23U_t_HpLu2Jg4Y64OSMxjIj8lpeI
6/29/2021 15:40:58,Neural Geometric Level of Detail: Real-time Rendering with Implicit 3D Shapes,,1/26/2021,https://arxiv.org/pdf/2101.10994.pdf,nv-tlabs.github.io/nglod,https://github.com/nv-tlabs/nglod,https://www.youtube.com/watch?v=Pi7W6XrFtMs,,,,"@inproceedings{takikawa2021neural,
  BOOKTITLE = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  ID = {takikawa2021neural},
  ENTRYTYPE = {inproceedings},
  AUTHOR = {Towaki Takikawa and Joey Litalien and Kangxue Yin and Karsten Kreis and Charles Loop and Derek Nowrouzezahrai and Alec Jacobson and Morgan McGuire and Sanja Fidler},
  TITLE = {Neural Geometric Level of Detail: Real-time Rendering with Implicit 3D Shapes},
  EPRINT = {2101.10994v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Neural signed distance functions (SDFs) are emerging as an effective representation for 3D shapes. State-of-the-art methods typically encode the SDF with a large, fixed-size neural network to approximate complex shapes with implicit surfaces. Rendering with these large networks is, however, computationally expensive since it requires many forward passes through the network for every pixel, making these representations impractical for real-time graphics. We introduce an efficient neural representation that, for the first time, enables real-time rendering of high-fidelity neural SDFs, while achieving state-of-the-art geometry reconstruction quality. We represent implicit surfaces using an octree-based feature volume which adaptively fits shapes with multiple discrete levels of detail (LODs), and enables continuous LOD with SDF interpolation. We further develop an efficient algorithm to directly render our novel neural SDF representation in real-time by querying only the necessary LODs with sparse octree traversal. We show that our representation is 2-3 orders of magnitude more efficient in terms of rendering speed compared to previous works. Furthermore, it produces state-of-the-art reconstruction quality for complex shapes under both 3D geometric and 2D image-space metrics.},
  YEAR = {2021},
  URL = {http://arxiv.org/abs/2101.10994v1},
  FILE = {2101.10994v1.pdf}
 }","Performance (training), Performance (rendering), Generalization","Conditional neural field, Coarse-to-fine, Sampling, Voxelization, Representation, Volume partitioning",None,SDF,Category-level,,,,,,,CVPR 2021,,Yes,,"Towaki Takikawa, Joey Litalien, Kangxue Yin, Karsten Kreis, Charles Loop, Derek Nowrouzezahrai, Alec Jacobson, Morgan McGuire, Sanja Fidler",takikawa2021neural,00000066,"Neural signed distance functions (SDFs) are emerging as an effective representation for 3D shapes. State-of-the-art methods typically encode the SDF with a large, fixed-size neural network to approximate complex shapes with implicit surfaces. Rendering with these large networks is, however, computationally expensive since it requires many forward passes through the network for every pixel, making these representations impractical for real-time graphics. We introduce an efficient neural representation that, for the first time, enables real-time rendering of high-fidelity neural SDFs, while achieving state-of-the-art geometry reconstruction quality. We represent implicit surfaces using an octree-based feature volume which adaptively fits shapes with multiple discrete levels of detail (LODs), and enables continuous LOD with SDF interpolation. We further develop an efficient algorithm to directly render our novel neural SDF representation in real-time by querying only the necessary LODs with sparse octree traversal. We show that our representation is 2-3 orders of magnitude more efficient in terms of rendering speed compared to previous works. Furthermore, it produces state-of-the-art reconstruction quality for complex shapes under both 3D geometric and 2D image-space metrics.",12,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnueMudgb4UH3QQkqNjYL2InqZx4ptfgRL2Vow20OwPXIbqi5cN3o_AmSa6iqS7xFkYo
5/23/2021 18:53:44,Towards Generalising Neural Implicit Representations,,1/29/2021,https://arxiv.org/pdf/2101.12690.pdf,,,,,,,"@article{costain2021towards,
  JOURNAL = {arXiv preprint arXiv:2101.12690},
  BOOKTITLE = {ArXiv Pre-print},
  ID = {costain2021towards},
  ENTRYTYPE = {article},
  AUTHOR = {Theo W. Costain and Victor Adrian Prisacariu},
  TITLE = {Towards Generalising Neural Implicit Representations},
  EPRINT = {2101.12690v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Neural implicit representations have shown substantial improvements in efficiently storing 3D data, when compared to conventional formats. However, the focus of existing work has mainly been on storage and subsequent reconstruction. In this work, we show that training neural representations for reconstruction tasks alongside conventional tasks can produce more general encodings that admit equal quality reconstructions to single task training, whilst improving results on conventional tasks when compared to single task encodings. We reformulate the semantic segmentation task, creating a more representative task for implicit representation contexts, and through multi-task experiments on reconstruction, classification, and segmentation, show our approach learns feature rich encodings that admit equal performance for each task.},
  YEAR = {2021},
  URL = {http://arxiv.org/abs/2101.12690v2},
  FILE = {2101.12690v2.pdf}
 }","Generalization, Beyond graphics",Conditional neural field,,,,,,,,,,ARXIV 2021,,,,"Theo W. Costain, Victor Adrian Prisacariu",costain2021towards,00000067,"Neural implicit representations have shown substantial improvements in efficiently storing 3D data, when compared to conventional formats. However, the focus of existing work has mainly been on storage and subsequent reconstruction. In this work, we show that training neural representations for reconstruction tasks alongside conventional tasks can produce more general encodings that admit equal quality reconstructions to single task training, whilst improving results on conventional tasks when compared to single task encodings. We reformulate the semantic segmentation task, creating a more representative task for implicit representation contexts, and through multi-task experiments on reconstruction, classification, and segmentation, show our approach learns feature rich encodings that admit equal performance for each task.",0,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnudviRcPoMX8F_mpW8FZ2e_5_Z77ixQJ1d5VyfcGqIt0_Iwf6L-8jK9u-0nAtv_DTYM
7/19/2021 21:41:13,CoIL: Coordinate-based Internal Learning for Imaging Inverse Problems,CoIL,2/9/2021,https://arxiv.org/pdf/2102.05181.pdf,,,,,,,"@article{sun2021coil,
  JOURNAL = {arXiv preprint arXiv:2102.05181},
  BOOKTITLE = {ArXiv Pre-print},
  ID = {sun2021coil},
  ENTRYTYPE = {article},
  AUTHOR = {Yu Sun and Jiaming Liu and Mingyang Xie and Brendt Wohlberg and Ulugbek S. Kamilov},
  TITLE = {CoIL: Coordinate-based Internal Learning for Imaging Inverse Problems},
  EPRINT = {2102.05181v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {eess.IV},
  ABSTRACT = {We propose Coordinate-based Internal Learning (CoIL) as a new deep-learning (DL) methodology for the continuous representation of measurements. Unlike traditional DL methods that learn a mapping from the measurements to the desired image, CoIL trains a multilayer perceptron (MLP) to encode the complete measurement field by mapping the coordinates of the measurements to their responses. CoIL is a self-supervised method that requires no training examples besides the measurements of the test object itself. Once the MLP is trained, CoIL generates new measurements that can be used within a majority of image reconstruction methods. We validate CoIL on sparse-view computed tomography using several widely-used reconstruction methods, including purely model-based methods and those based on DL. Our results demonstrate the ability of CoIL to consistently improve the performance of all the considered methods by providing high-fidelity measurement fields.},
  YEAR = {2021},
  URL = {http://arxiv.org/abs/2102.05181v1},
  FILE = {2102.05181v1.pdf}
 }","Beyond graphics, Science and engineering, Alternative imaging",,,,,,,,,,,ARXIV 2021,,,,"Yu Sun, Jiaming Liu, Mingyang Xie, Brendt Wohlberg, Ulugbek S. Kamilov",sun2021coil,00000068,"We propose Coordinate-based Internal Learning (CoIL) as a new deep-learning (DL) methodology for the continuous representation of measurements. Unlike traditional DL methods that learn a mapping from the measurements to the desired image, CoIL trains a multilayer perceptron (MLP) to encode the complete measurement field by mapping the coordinates of the measurements to their responses. CoIL is a self-supervised method that requires no training examples besides the measurements of the test object itself. Once the MLP is trained, CoIL generates new measurements that can be used within a majority of image reconstruction methods. We validate CoIL on sparse-view computed tomography using several widely-used reconstruction methods, including purely model-based methods and those based on DL. Our results demonstrate the ability of CoIL to consistently improve the performance of all the considered methods by providing high-fidelity measurement fields.",3,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnufRGXOo_tCttv0hOUodh8t2BIERGuhqR7HL0OvApyrzzYNDELFLNmpgSRxQU5GcGzk
5/23/2021 18:54:28,A-NeRF: Surface-free Human 3D Pose Refinement via Neural Rendering,A-NeRF,2/11/2021,https://arxiv.org/pdf/2102.06199.pdf,,,,,,,"@article{su2021anerf,
  JOURNAL = {arXiv preprint arXiv:2102.06199},
  BOOKTITLE = {ArXiv Pre-print},
  ID = {su2021anerf},
  ENTRYTYPE = {article},
  AUTHOR = {Shih-Yang Su and Frank Yu and Michael Zollhoefer and Helge Rhodin},
  TITLE = {A-NeRF: Surface-free Human 3D Pose Refinement via Neural Rendering},
  EPRINT = {2102.06199v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {While deep learning has reshaped the classical motion capture pipeline, generative, analysis-by-synthesis elements are still in use to recover fine details if a high-quality 3D model of the user is available. Unfortunately, obtaining such a model for every user a priori is challenging, time-consuming, and limits the application scenarios. We propose a novel test-time optimization approach for monocular motion capture that learns a volumetric body model of the user in a self-supervised manner. To this end, our approach combines the advantages of neural radiance fields with an articulated skeleton representation. Our proposed skeleton embedding serves as a common reference that links constraints across time, thereby reducing the number of required camera views from traditionally dozens of calibrated cameras, down to a single uncalibrated one. As a starting point, we employ the output of an off-the-shelf model that predicts the 3D skeleton pose. The volumetric body shape and appearance is then learned from scratch, while jointly refining the initial pose estimate. Our approach is self-supervised and does not require any additional ground truth labels for appearance, pose, or 3D shape. We demonstrate that our novel combination of a discriminative pose estimation technique with surface-free analysis-by-synthesis outperforms purely discriminative monocular pose estimation approaches and generalizes well to multiple views.},
  YEAR = {2021},
  URL = {http://arxiv.org/abs/2102.06199v1},
  FILE = {2102.06199v1.pdf}
 }","Dynamic, Human body",Articulated,,,,,,,,,,ARXIV 2021,,,,"Shih-Yang Su, Frank Yu, Michael Zollhoefer, Helge Rhodin",su2021anerf,00000069,"While deep learning has reshaped the classical motion capture pipeline, generative, analysis-by-synthesis elements are still in use to recover fine details if a high-quality 3D model of the user is available. Unfortunately, obtaining such a model for every user a priori is challenging, time-consuming, and limits the application scenarios. We propose a novel test-time optimization approach for monocular motion capture that learns a volumetric body model of the user in a self-supervised manner. To this end, our approach combines the advantages of neural radiance fields with an articulated skeleton representation. Our proposed skeleton embedding serves as a common reference that links constraints across time, thereby reducing the number of required camera views from traditionally dozens of calibrated cameras, down to a single uncalibrated one. As a starting point, we employ the output of an off-the-shelf model that predicts the 3D skeleton pose. The volumetric body shape and appearance is then learned from scratch, while jointly refining the initial pose estimate. Our approach is self-supervised and does not require any additional ground truth labels for appearance, pose, or 3D shape. We demonstrate that our novel combination of a discriminative pose estimation technique with surface-free analysis-by-synthesis outperforms purely discriminative monocular pose estimation approaches and generalizes well to multiple views.",1,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnue2QAfOqOzGmCZRB05-JuDWhaIhFyuQbAoiGf5OWj-lqI3yu32RzCbuGnvucAgHkSc
5/23/2021 18:54:55,NeRF−−: Neural Radiance Fields Without Known Camera Parameters,NeRF−−,2/14/2021,https://arxiv.org/pdf/2102.07064.pdf,,https://github.com/ActiveVisionLab/nerfmm,,,,,"@article{wang2021nerf--,
  JOURNAL = {arXiv preprint arXiv:2102.07064},
  BOOKTITLE = {ArXiv Pre-print},
  ID = {wang2021nerf--},
  ENTRYTYPE = {article},
  AUTHOR = {Zirui Wang and Shangzhe Wu and Weidi Xie and Min Chen and Victor Adrian Prisacariu},
  TITLE = {NeRF--: Neural Radiance Fields Without Known Camera Parameters},
  EPRINT = {2102.07064v3},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {This paper tackles the problem of novel view synthesis (NVS) from 2D images without known camera poses and intrinsics. Among various NVS techniques, Neural Radiance Field (NeRF) has recently gained popularity due to its remarkable synthesis quality. Existing NeRF-based approaches assume that the camera parameters associated with each input image are either directly accessible at training, or can be accurately estimated with conventional techniques based on correspondences, such as Structure-from-Motion. In this work, we propose an end-to-end framework, termed NeRF--, for training NeRF models given only RGB images, without pre-computed camera parameters. Specifically, we show that the camera parameters, including both intrinsics and extrinsics, can be automatically discovered via joint optimisation during the training of the NeRF model. On the standard LLFF benchmark, our model achieves comparable novel view synthesis results compared to the baseline trained with COLMAP pre-computed camera parameters. We also conduct extensive analyses to understand the model behaviour under different camera trajectories, and show that in scenarios where COLMAP fails, our model still produces robust results.},
  YEAR = {2021},
  URL = {http://arxiv.org/abs/2102.07064v3},
  FILE = {2102.07064v3.pdf}
 }",Camera parameter estimation,,,,,,,,,,,ARXIV 2021,,,,"Zirui Wang, Shangzhe Wu, Weidi Xie, Min Chen, Victor Adrian Prisacariu",wang2021nerf--,00000070,"This paper tackles the problem of novel view synthesis (NVS) from 2D images without known camera poses and intrinsics. Among various NVS techniques, Neural Radiance Field (NeRF) has recently gained popularity due to its remarkable synthesis quality. Existing NeRF-based approaches assume that the camera parameters associated with each input image are either directly accessible at training, or can be accurately estimated with conventional techniques based on correspondences, such as Structure-from-Motion. In this work, we propose an end-to-end framework, termed NeRF--, for training NeRF models given only RGB images, without pre-computed camera parameters. Specifically, we show that the camera parameters, including both intrinsics and extrinsics, can be automatically discovered via joint optimisation during the training of the NeRF model. On the standard LLFF benchmark, our model achieves comparable novel view synthesis results compared to the baseline trained with COLMAP pre-computed camera parameters. We also conduct extensive analyses to understand the model behaviour under different camera trajectories, and show that in scenarios where COLMAP fails, our model still produces robust results.",9,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnud19BQxPwYpBBqetX2TWbfhybhQisCXu1gpdk5sObNDBQIkEEVGyWxEKUBc8NPGhTc
7/26/2021 14:37:27,ShaRF: Shape-conditioned Radiance Fields from a Single View,ShaRF,2/17/2021,https://arxiv.org/pdf/2102.08860.pdf,http://www.krematas.com/sharf/,,,,,,"@inproceedings{rematas2021sharf,
  PUBLISHER = {PMLR},
  BOOKTITLE = {International Conference on Machine Learning (ICML)},
  ID = {rematas2021sharf},
  ENTRYTYPE = {inproceedings},
  AUTHOR = {Konstantinos Rematas and Ricardo Martin-Brualla and Vittorio Ferrari},
  TITLE = {ShaRF: Shape-conditioned Radiance Fields from a Single View},
  EPRINT = {2102.08860v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We present a method for estimating neural scenes representations of objects given only a single image. The core of our method is the estimation of a geometric scaffold for the object and its use as a guide for the reconstruction of the underlying radiance field. Our formulation is based on a generative process that first maps a latent code to a voxelized shape, and then renders it to an image, with the object appearance being controlled by a second latent code. During inference, we optimize both the latent codes and the networks to fit a test image of a new object. The explicit disentanglement of shape and appearance allows our model to be fine-tuned given a single image. We can then render new views in a geometrically consistent manner and they represent faithfully the input object. Additionally, our method is able to generalize to images outside of the training domain (more realistic renderings and even real photographs). Finally, the inferred geometric scaffold is itself an accurate estimate of the object's 3D shape. We demonstrate in several experiments the effectiveness of our approach in both synthetic and real images.},
  YEAR = {2021},
  URL = {http://arxiv.org/abs/2102.08860v2},
  FILE = {2102.08860v2.pdf}
 }","Few-shot reconstruction, Generalization","Generative/adversarial formulation, Sampling, Voxelization, Data-driven",NeRF,Density,,,,,,,,ICML 2021,,No,Direct,"Konstantinos Rematas, Ricardo Martin-Brualla, Vittorio Ferrari",rematas2021sharf,00000071,"We present a method for estimating neural scenes representations of objects given only a single image. The core of our method is the estimation of a geometric scaffold for the object and its use as a guide for the reconstruction of the underlying radiance field. Our formulation is based on a generative process that first maps a latent code to a voxelized shape, and then renders it to an image, with the object appearance being controlled by a second latent code. During inference, we optimize both the latent codes and the networks to fit a test image of a new object. The explicit disentanglement of shape and appearance allows our model to be fine-tuned given a single image. We can then render new views in a geometrically consistent manner and they represent faithfully the input object. Additionally, our method is able to generalize to images outside of the training domain (more realistic renderings and even real photographs). Finally, the inferred geometric scaffold is itself an accurate estimate of the object's 3D shape. We demonstrate in several experiments the effectiveness of our approach in both synthetic and real images.",5,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnuc1F6UOVFlPZsZCFPID2cuNZC5V991gu33cql9rOWB7PY2G0EiXNSXNsbYYHQYvPjk
5/23/2021 18:53:27,NTopo: Mesh-free Topolibogy Optimization using Implicit Neural Representations,NTopo,2/22/2021,https://arxiv.org/pdf/2102.10782.pdf,,,,,,,"@article{zehnder2021ntopo,
  JOURNAL = {arXiv preprint arXiv:2102.10782},
  BOOKTITLE = {ArXiv Pre-print},
  ID = {zehnder2021ntopo},
  ENTRYTYPE = {article},
  AUTHOR = {Jonas Zehnder and Yue Li and Stelian Coros and Bernhard Thomaszewski},
  TITLE = {NTopo: Mesh-free Topology Optimization using Implicit Neural Representations},
  EPRINT = {2102.10782v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.LG},
  ABSTRACT = {Recent advances in implicit neural representations show great promise when it comes to generating numerical solutions to partial differential equations (PDEs). Compared to conventional alternatives, such representations employ parameterized neural networks to define, in a mesh-free manner, signals that are highly-detailed, continuous, and fully differentiable. Most prior works aim to exploit these benefits in order to solve PDE-governed forward problems, or associated inverse problems that are defined by a small number of parameters. In this work, we present a novel machine learning approach to tackle topology optimization (TO) problems. Topology optimization refers to an important class of inverse problems that typically feature very high-dimensional parameter spaces and objective landscapes which are highly non-linear. To effectively leverage neural representations in the context of TO problems, we use multilayer perceptrons (MLPs) to parameterize both density and displacement fields. Using sensitivity analysis with a moving mean squared error, we show that our formulation can be used to efficiently minimize traditional structural compliance objectives. As we show through our experiments, a major benefit of our approach is that it enables self-supervised learning of continuous solution spaces to topology optimization problems.},
  YEAR = {2021},
  URL = {http://arxiv.org/abs/2102.10782v1},
  FILE = {2102.10782v1.pdf}
 }","Beyond graphics, Science and engineering",PDE,,,,,,,,,,ARXIV 2021,,,,"Jonas Zehnder, Yue Li, Stelian Coros, Bernhard Thomaszewski",zehnder2021ntopo,00000072,"Recent advances in implicit neural representations show great promise when it comes to generating numerical solutions to partial differential equations (PDEs). Compared to conventional alternatives, such representations employ parameterized neural networks to define, in a mesh-free manner, signals that are highly-detailed, continuous, and fully differentiable. Most prior works aim to exploit these benefits in order to solve PDE-governed forward problems, or associated inverse problems that are defined by a small number of parameters. In this work, we present a novel machine learning approach to tackle topology optimization (TO) problems. Topology optimization refers to an important class of inverse problems that typically feature very high-dimensional parameter spaces and objective landscapes which are highly non-linear. To effectively leverage neural representations in the context of TO problems, we use multilayer perceptrons (MLPs) to parameterize both density and displacement fields. Using sensitivity analysis with a moving mean squared error, we show that our formulation can be used to efficiently minimize traditional structural compliance objectives. As we show through our experiments, a major benefit of our approach is that it enables self-supervised learning of continuous solution spaces to topology optimization problems.",0,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnue6ZRNB8D8xtWOCq6361tYPdLzVN9on395pvt_2cyLvBbj5ZzVxWTMZ09V9iZfNSXI
5/23/2021 18:52:47,IBRNet: Learning Multi-View Image-Based Rendering,IBRNet,2/25/2021,https://arxiv.org/pdf/2102.13090.pdf,https://ibrnet.github.io/,,,,,,"@inproceedings{wang2021ibrnet,
  BOOKTITLE = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  ID = {wang2021ibrnet},
  ENTRYTYPE = {inproceedings},
  AUTHOR = {Qianqian Wang and Zhicheng Wang and Kyle Genova and Pratul Srinivasan and Howard Zhou and Jonathan T. Barron and Ricardo Martin-Brualla and Noah Snavely and Thomas Funkhouser},
  TITLE = {IBRNet: Learning Multi-View Image-Based Rendering},
  EPRINT = {2102.13090v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We present a method that synthesizes novel views of complex scenes by interpolating a sparse set of nearby views. The core of our method is a network architecture that includes a multilayer perceptron and a ray transformer that estimates radiance and volume density at continuous 5D locations (3D spatial locations and 2D viewing directions), drawing appearance information on the fly from multiple source views. By drawing on source views at render time, our method hearkens back to classic work on image-based rendering (IBR), and allows us to render high-resolution imagery. Unlike neural scene representation work that optimizes per-scene functions for rendering, we learn a generic view interpolation function that generalizes to novel scenes. We render images using classic volume rendering, which is fully differentiable and allows us to train using only multi-view posed images as supervision. Experiments show that our method outperforms recent novel view synthesis methods that also seek to generalize to novel scenes. Further, if fine-tuned on each scene, our method is competitive with state-of-the-art single-scene neural rendering methods. Project page: https://ibrnet.github.io/},
  YEAR = {2021},
  URL = {http://arxiv.org/abs/2102.13090v2},
  FILE = {2102.13090v2.pdf}
 }","Performance (training), Generalization","Lifting 2D features to 3D, Image-based rendering, Data-driven",,,,,,,,,,CVPR 2021,,,,"Qianqian Wang, Zhicheng Wang, Kyle Genova, Pratul Srinivasan, Howard Zhou, Jonathan T. Barron, Ricardo Martin-Brualla, Noah Snavely, Thomas Funkhouser",wang2021ibrnet,00000073,"We present a method that synthesizes novel views of complex scenes by interpolating a sparse set of nearby views. The core of our method is a network architecture that includes a multilayer perceptron and a ray transformer that estimates radiance and volume density at continuous 5D locations (3D spatial locations and 2D viewing directions), drawing appearance information on the fly from multiple source views. By drawing on source views at render time, our method hearkens back to classic work on image-based rendering (IBR), and allows us to render high-resolution imagery. Unlike neural scene representation work that optimizes per-scene functions for rendering, we learn a generic view interpolation function that generalizes to novel scenes. We render images using classic volume rendering, which is fully differentiable and allows us to train using only multi-view posed images as supervision. Experiments show that our method outperforms recent novel view synthesis methods that also seek to generalize to novel scenes. Further, if fine-tuned on each scene, our method is competitive with state-of-the-art single-scene neural rendering methods. Project page: https://ibrnet.github.io/",20,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnudyc0pp1X8xSHsKO-WcO3Jve7_6up7iAW3hOPjICdEK3vZ81Md8oDtLnhrHa4HMP9Q
5/23/2021 18:56:02,NeuTex: Neural Texture Mapping for Volumetric Neural Rendering,NeuTex,3/1/2021,https://arxiv.org/pdf/2103.00762.pdf,,,,,,,"@inproceedings{xiang2021neutex,
  BOOKTITLE = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  ID = {xiang2021neutex},
  ENTRYTYPE = {inproceedings},
  AUTHOR = {Fanbo Xiang and Zexiang Xu and Milos Hasan and Yannick Hold-Geoffroy and Kalyan Sunkavalli and Hao Su},
  TITLE = {NeuTex: Neural Texture Mapping for Volumetric Neural Rendering},
  EPRINT = {2103.00762v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Recent work has demonstrated that volumetric scene representations combined with differentiable volume rendering can enable photo-realistic rendering for challenging scenes that mesh reconstruction fails on. However, these methods entangle geometry and appearance in a ""black-box"" volume that cannot be edited. Instead, we present an approach that explicitly disentangles geometry--represented as a continuous 3D volume--from appearance--represented as a continuous 2D texture map. We achieve this by introducing a 3D-to-2D texture mapping (or surface parameterization) network into volumetric representations. We constrain this texture mapping network using an additional 2D-to-3D inverse mapping network and a novel cycle consistency loss to make 3D surface points map to 2D texture points that map back to the original 3D points. We demonstrate that this representation can be reconstructed using only multi-view image supervision and generates high-quality rendering results. More importantly, by separating geometry and texture, we allow users to edit appearance by simply editing 2D texture maps.},
  YEAR = {2021},
  URL = {http://arxiv.org/abs/2103.00762v1},
  FILE = {2103.00762v1.pdf}
 }",Fundamentals,,,,,,,,,,,CVPR 2021,,,,"Fanbo Xiang, Zexiang Xu, Miloš Hašan, Yannick Hold-Geoffroy, Kalyan Sunkavalli, Hao Su",xiang2021neutex,00000074,"Recent work has demonstrated that volumetric scene representations combined with differentiable volume rendering can enable photo-realistic rendering for challenging scenes that mesh reconstruction fails on. However, these methods entangle geometry and appearance in a ""black-box"" volume that cannot be edited. Instead, we present an approach that explicitly disentangles geometry--represented as a continuous 3D volume--from appearance--represented as a continuous 2D texture map. We achieve this by introducing a 3D-to-2D texture mapping (or surface parameterization) network into volumetric representations. We constrain this texture mapping network using an additional 2D-to-3D inverse mapping network and a novel cycle consistency loss to make 3D surface points map to 2D texture points that map back to the original 3D points. We demonstrate that this representation can be reconstructed using only multi-view image supervision and generates high-quality rendering results. More importantly, by separating geometry and texture, we allow users to edit appearance by simply editing 2D texture maps.",2,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnueo72MgZHt-K4t-Il8Q8RICbf7NNhJVI8NA9KE2qKFjWfzqt5iwI6v2XObRdIj1gZA
5/23/2021 18:52:14,Mixture of Volumetric Primitives for Efficient Neural Rendering,MVP,3/2/2021,https://arxiv.org/pdf/2103.01954.pdf,,,,,,,"@article{lombardi2021mvp,
  PUBLISHER = {Association for Computing Machinery},
  JOURNAL = {ACM Transactions on Graphics (TOG)},
  ID = {lombardi2021mvp},
  ENTRYTYPE = {article},
  AUTHOR = {Stephen Lombardi and Tomas Simon and Gabriel Schwartz and Michael Zollhoefer and Yaser Sheikh and Jason Saragih},
  TITLE = {Mixture of Volumetric Primitives for Efficient Neural Rendering},
  EPRINT = {2103.01954v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.GR},
  ABSTRACT = {Real-time rendering and animation of humans is a core function in games, movies, and telepresence applications. Existing methods have a number of drawbacks we aim to address with our work. Triangle meshes have difficulty modeling thin structures like hair, volumetric representations like Neural Volumes are too low-resolution given a reasonable memory budget, and high-resolution implicit representations like Neural Radiance Fields are too slow for use in real-time applications. We present Mixture of Volumetric Primitives (MVP), a representation for rendering dynamic 3D content that combines the completeness of volumetric representations with the efficiency of primitive-based rendering, e.g., point-based or mesh-based methods. Our approach achieves this by leveraging spatially shared computation with a deconvolutional architecture and by minimizing computation in empty regions of space with volumetric primitives that can move to cover only occupied regions. Our parameterization supports the integration of correspondence and tracking constraints, while being robust to areas where classical tracking fails, such as around thin or translucent structures and areas with large topological variability. MVP is a hybrid that generalizes both volumetric and primitive-based representations. Through a series of extensive experiments we demonstrate that it inherits the strengths of each, while avoiding many of their limitations. We also compare our approach to several state-of-the-art methods and demonstrate that MVP produces superior results in terms of quality and runtime performance.},
  YEAR = {2021},
  URL = {http://arxiv.org/abs/2103.01954v2},
  FILE = {2103.01954v2.pdf}
 }","Performance (training), Performance (rendering)","Sampling, Voxelization, Feature volume, Representation",,,,,,,,,,SIGGRAPH 2021,,,Indirect,"Stephen Lombardi, Tomas Simon, Gabriel Schwartz, Michael Zollhoefer, Yaser Sheikh, Jason Saragih",lombardi2021mvp,00000075,"Real-time rendering and animation of humans is a core function in games, movies, and telepresence applications. Existing methods have a number of drawbacks we aim to address with our work. Triangle meshes have difficulty modeling thin structures like hair, volumetric representations like Neural Volumes are too low-resolution given a reasonable memory budget, and high-resolution implicit representations like Neural Radiance Fields are too slow for use in real-time applications. We present Mixture of Volumetric Primitives (MVP), a representation for rendering dynamic 3D content that combines the completeness of volumetric representations with the efficiency of primitive-based rendering, e.g., point-based or mesh-based methods. Our approach achieves this by leveraging spatially shared computation with a deconvolutional architecture and by minimizing computation in empty regions of space with volumetric primitives that can move to cover only occupied regions. Our parameterization supports the integration of correspondence and tracking constraints, while being robust to areas where classical tracking fails, such as around thin or translucent structures and areas with large topological variability. MVP is a hybrid that generalizes both volumetric and primitive-based representations. Through a series of extensive experiments we demonstrate that it inherits the strengths of each, while avoiding many of their limitations. We also compare our approach to several state-of-the-art methods and demonstrate that MVP produces superior results in terms of quality and runtime performance.",6,,No,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnuerrAjFLHW_oQgGR-ABmAvPNBNXUJFxttUW4SPx8phEtTfamWwm-NLrM6Wr9KYktmo
5/23/2021 18:50:30,COIN: COmpression with Implicit Neural representations,COIN,3/3/2021,https://arxiv.org/pdf/2103.03123.pdf,,,https://www.youtube.com/watch?v=FjPurtmqgmw,,,,"@inproceedings{dupont2021coin,
  BOOKTITLE = {International Conference on Learning Representations},
  ID = {dupont2021coin},
  ENTRYTYPE = {inproceedings},
  AUTHOR = {Emilien Dupont and Adam Golinski and Milad Alizadeh and Yee Whye Teh and Arnaud Doucet},
  TITLE = {COIN: COmpression with Implicit Neural representations},
  EPRINT = {2103.03123v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {eess.IV},
  ABSTRACT = {We propose a new simple approach for image compression: instead of storing the RGB values for each pixel of an image, we store the weights of a neural network overfitted to the image. Specifically, to encode an image, we fit it with an MLP which maps pixel locations to RGB values. We then quantize and store the weights of this MLP as a code for the image. To decode the image, we simply evaluate the MLP at every pixel location. We found that this simple approach outperforms JPEG at low bit-rates, even without entropy coding or learning a distribution over weights. While our framework is not yet competitive with state of the art compression methods, we show that it has various attractive properties which could make it a viable alternative to other neural data compression approaches.},
  YEAR = {2021},
  URL = {http://arxiv.org/abs/2103.03123v2},
  FILE = {2103.03123v2.pdf}
 }",Compression,,SIREN,,,,,,,,,ICLR 2021,,,,"Emilien Dupont, Adam Goliński, Milad Alizadeh, Yee Whye Teh, Arnaud Doucet",dupont2021coin,00000076,"We propose a new simple approach for image compression: instead of storing the RGB values for each pixel of an image, we store the weights of a neural network overfitted to the image. Specifically, to encode an image, we fit it with an MLP which maps pixel locations to RGB values. We then quantize and store the weights of this MLP as a code for the image. To decode the image, we simply evaluate the MLP at every pixel location. We found that this simple approach outperforms JPEG at low bit-rates, even without entropy coding or learning a distribution over weights. While our framework is not yet competitive with state of the art compression methods, we show that it has various attractive properties which could make it a viable alternative to other neural data compression approaches.",0,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnufoFFLu34JYheofFkNuKaCmsKCnU-Gbr3AKbW19FnVSxM8zIKDSM-1mQnsPEMp8Q3Q
5/23/2021 18:50:45,Neural 3D Video Synthesis,DyNeRF,3/3/2021,https://arxiv.org/pdf/2103.02597.pdf,https://neural-3d-video.github.io/,,https://neural-3d-video.github.io/resources/video.mp4,,,,"@article{li2021dynerf,
  JOURNAL = {arXiv preprint arXiv:2103.02597},
  BOOKTITLE = {ArXiv Pre-print},
  ID = {li2021dynerf},
  ENTRYTYPE = {article},
  AUTHOR = {Tianye Li and Mira Slavcheva and Michael Zollhoefer and Simon Green and Christoph Lassner and Changil Kim and Tanner Schmidt and Steven Lovegrove and Michael Goesele and Zhaoyang Lv},
  TITLE = {Neural 3D Video Synthesis},
  EPRINT = {2103.02597v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We propose a novel approach for 3D video synthesis that is able to represent multi-view video recordings of a dynamic real-world scene in a compact, yet expressive representation that enables high-quality view synthesis and motion interpolation. Our approach takes the high quality and compactness of static neural radiance fields in a new direction: to a model-free, dynamic setting. At the core of our approach is a novel time-conditioned neural radiance fields that represents scene dynamics using a set of compact latent codes. To exploit the fact that changes between adjacent frames of a video are typically small and locally consistent, we propose two novel strategies for efficient training of our neural network: 1) An efficient hierarchical training scheme, and 2) an importance sampling strategy that selects the next rays for training based on the temporal variation of the input videos. In combination, these two strategies significantly boost the training speed, lead to fast convergence of the training process, and enable high quality results. Our learned representation is highly compact and able to represent a 10 second 30 FPS multi-view video recording by 18 cameras with a model size of just 28MB. We demonstrate that our method can render high-fidelity wide-angle novel views at over 1K resolution, even for highly complex and dynamic scenes. We perform an extensive qualitative and quantitative evaluation that shows that our approach outperforms the current state of the art. We include additional video and information at: https://neural-3d-video.github.io/},
  YEAR = {2021},
  URL = {http://arxiv.org/abs/2103.02597v1},
  FILE = {2103.02597v1.pdf}
 }",Dynamic,Conditional neural field,,,,,,,,,,ARXIV 2021,Coming soon,,,"Tianye Li, Mira Slavcheva, Michael Zollhoefer, Simon Green, Christoph Lassner, Changil Kim, Tanner Schmidt, Steven Lovegrove, Michael Goesele, Zhaoyang Lv",li2021dynerf,00000077,"We propose a novel approach for 3D video synthesis that is able to represent multi-view video recordings of a dynamic real-world scene in a compact, yet expressive representation that enables high-quality view synthesis and motion interpolation. Our approach takes the high quality and compactness of static neural radiance fields in a new direction: to a model-free, dynamic setting. At the core of our approach is a novel time-conditioned neural radiance fields that represents scene dynamics using a set of compact latent codes. To exploit the fact that changes between adjacent frames of a video are typically small and locally consistent, we propose two novel strategies for efficient training of our neural network: 1) An efficient hierarchical training scheme, and 2) an importance sampling strategy that selects the next rays for training based on the temporal variation of the input videos. In combination, these two strategies significantly boost the training speed, lead to fast convergence of the training process, and enable high quality results. Our learned representation is highly compact and able to represent a 10 second 30 FPS multi-view video recording by 18 cameras with a model size of just 28MB. We demonstrate that our method can render high-fidelity wide-angle novel views at over 1K resolution, even for highly complex and dynamic scenes. We perform an extensive qualitative and quantitative evaluation that shows that our approach outperforms the current state of the art. We include additional video and information at: https://neural-3d-video.github.io/",3,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnuepnK4G93pm4xmZg3OgoMxPEjUThHW8aaTBjvSPT4WFja4cZ-tZhzuDlggNajiRwdo
5/25/2021 0:11:31,DONeRF: Towards Real-Time Rendering of Neural Radiance Fields using Depth Oracle Networks,DONeRF,3/4/2021,https://arxiv.org/pdf/2103.03231.pdf,,,,,,,"@article{neff2021donerf,
  PUBLISHER = {The Eurographics Association and John Wiley & Sons Ltd.},
  JOURNAL = {Computer Graphics Forum},
  ID = {neff2021donerf},
  ENTRYTYPE = {article},
  AUTHOR = {Thomas Neff and Pascal Stadlbauer and Mathias Parger and Andreas Kurz and Joerg H. Mueller and Chakravarty R. Alla Chaitanya and Anton Kaplanyan and Markus Steinberger},
  TITLE = {DONeRF: Towards Real-Time Rendering of Compact Neural Radiance Fields using Depth Oracle Networks},
  EPRINT = {2103.03231v4},
  DOI = {10.1111/cgf.14340},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {The recent research explosion around implicit neural representations, such as NeRF, shows that there is immense potential for implicitly storing high-quality scene and lighting information in compact neural networks. However, one major limitation preventing the use of NeRF in real-time rendering applications is the prohibitive computational cost of excessive network evaluations along each view ray, requiring dozens of petaFLOPS. In this work, we bring compact neural representations closer to practical rendering of synthetic content in real-time applications, such as games and virtual reality. We show that the number of samples required for each view ray can be significantly reduced when samples are placed around surfaces in the scene without compromising image quality. To this end, we propose a depth oracle network that predicts ray sample locations for each view ray with a single network evaluation. We show that using a classification network around logarithmically discretized and spherically warped depth values is essential to encode surface locations rather than directly estimating depth. The combination of these techniques leads to DONeRF, our compact dual network design with a depth oracle network as its first step and a locally sampled shading network for ray accumulation. With DONeRF, we reduce the inference costs by up to 48x compared to NeRF when conditioning on available ground truth depth information. Compared to concurrent acceleration methods for raymarching-based neural representations, DONeRF does not require additional memory for explicit caching or acceleration structures, and can render interactively (20 frames per second) on a single GPU.},
  YEAR = {2021},
  NOTE = {Computer Graphics Forum Volume 40, Issue 4, 2021},
  URL = {http://arxiv.org/abs/2103.03231v4},
  FILE = {2103.03231v4.pdf}
 }",Performance (rendering),"Sampling, Data-driven",,,,,,,,,,EGSR 2021,,,,"Thomas Neff, Pascal Stadlbauer, Mathias Parger, Andreas Kurz, Joerg H. Mueller, Chakravarty R. Alla Chaitanya, Anton Kaplanyan, Markus Steinberger",neff2021donerf,00000078,"The recent research explosion around implicit neural representations, such as NeRF, shows that there is immense potential for implicitly storing high-quality scene and lighting information in compact neural networks. However, one major limitation preventing the use of NeRF in real-time rendering applications is the prohibitive computational cost of excessive network evaluations along each view ray, requiring dozens of petaFLOPS. In this work, we bring compact neural representations closer to practical rendering of synthetic content in real-time applications, such as games and virtual reality. We show that the number of samples required for each view ray can be significantly reduced when samples are placed around surfaces in the scene without compromising image quality. To this end, we propose a depth oracle network that predicts ray sample locations for each view ray with a single network evaluation. We show that using a classification network around logarithmically discretized and spherically warped depth values is essential to encode surface locations rather than directly estimating depth. The combination of these techniques leads to DONeRF, our compact dual network design with a depth oracle network as its first step and a locally sampled shading network for ray accumulation. With DONeRF, we reduce the inference costs by up to 48x compared to NeRF when conditioning on available ground truth depth information. Compared to concurrent acceleration methods for raymarching-based neural representations, DONeRF does not require additional memory for explicit caching or acceleration structures, and can render interactively (20 frames per second) on a single GPU.",6,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnuf5FOkh6DPToODe-phxD5ZMhEPG8LPiOE7vWJ3u9gIE1DiNtSm47d_KnJ-UZKnVCBY
5/23/2021 18:48:47,NeX: Real-time View Synthesis with Neural Basis Expansion,NeX,3/9/2021,https://arxiv.org/pdf/2103.05606.pdf,https://nex-mpi.github.io/,https://github.com/nex-mpi/nex-code/,,,,,"@inproceedings{wizadwongsa2021nex,
  BOOKTITLE = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  ID = {wizadwongsa2021nex},
  ENTRYTYPE = {inproceedings},
  AUTHOR = {Suttisak Wizadwongsa and Pakkapon Phongthawee and Jiraphon Yenphraphai and Supasorn Suwajanakorn},
  TITLE = {NeX: Real-time View Synthesis with Neural Basis Expansion},
  EPRINT = {2103.05606v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We present NeX, a new approach to novel view synthesis based on enhancements of multiplane image (MPI) that can reproduce next-level view-dependent effects -- in real time. Unlike traditional MPI that uses a set of simple RGB$\alpha$ planes, our technique models view-dependent effects by instead parameterizing each pixel as a linear combination of basis functions learned from a neural network. Moreover, we propose a hybrid implicit-explicit modeling strategy that improves upon fine detail and produces state-of-the-art results. Our method is evaluated on benchmark forward-facing datasets as well as our newly-introduced dataset designed to test the limit of view-dependent modeling with significantly more challenging effects such as rainbow reflections on a CD. Our method achieves the best overall scores across all major metrics on these datasets with more than 1000$\times$ faster rendering time than the state of the art. For real-time demos, visit https://nex-mpi.github.io/},
  YEAR = {2021},
  URL = {http://arxiv.org/abs/2103.05606v2},
  FILE = {2103.05606v2.pdf}
 }",Performance (rendering),Representation,,,,,,,,,,CVPR 2021,https://vistec-my.sharepoint.com/personal/pakkapon_p_s19_vistec_ac_th/_layouts/15/onedrive.aspx?id=%2Fpersonal%2Fpakkapon%5Fp%5Fs19%5Fvistec%5Fac%5Fth%2FDocuments%2Fpublic%2FVLL%2FNeX%2Fshiny%5Fdatasets&originalPath=aHR0cHM6Ly92aXN0ZWMtbXkuc2hhcmVwb2ludC5jb20vOmY6L2cvcGVyc29uYWwvcGFra2Fwb25fcF9zMTlfdmlzdGVjX2FjX3RoL0VuSVVoc1JWSk9kTnNaXzRzbWRoeWUwQjh6MFZseHFPUjM1SVIzYnAwdUd1cFE%5FcnRpbWU9c1hYTTNEd2UyVWc,,,"Suttisak Wizadwongsa, Pakkapon Phongthawee, Jiraphon Yenphraphai, Supasorn Suwajanakorn",wizadwongsa2021nex,00000079,"We present NeX, a new approach to novel view synthesis based on enhancements of multiplane image (MPI) that can reproduce next-level view-dependent effects -- in real time. Unlike traditional MPI that uses a set of simple RGB$\alpha$ planes, our technique models view-dependent effects by instead parameterizing each pixel as a linear combination of basis functions learned from a neural network. Moreover, we propose a hybrid implicit-explicit modeling strategy that improves upon fine detail and produces state-of-the-art results. Our method is evaluated on benchmark forward-facing datasets as well as our newly-introduced dataset designed to test the limit of view-dependent modeling with significantly more challenging effects such as rainbow reflections on a CD. Our method achieves the best overall scores across all major metrics on these datasets with more than 1000$\times$ faster rendering time than the state of the art. For real-time demos, visit https://nex-mpi.github.io/",9,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnudoO15pLeUGNIV1WiNk-tAfI0TC3WHeZxd3fRVkNlYkrhXP4gsdSd-8xxlShEr4yO4
9/17/2021 11:42:58,SMPLicit: Topology-aware Generative Model for Clothed People,SMPLicit,3/11/2021,https://arxiv.org/pdf/2103.06871.pdf,http://www.iri.upc.edu/people/ecorona/smplicit/,https://github.com/enriccorona/SMPLicit,,,,,"@inproceedings{corona2021smplicit,
  BOOKTITLE = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  ID = {corona2021smplicit},
  ENTRYTYPE = {inproceedings},
  AUTHOR = {Enric Corona and Albert Pumarola and Guillem Alenya and Gerard Pons-Moll and Francesc Moreno-Noguer},
  TITLE = {SMPLicit: Topology-aware Generative Model for Clothed People},
  EPRINT = {2103.06871v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {In this paper we introduce SMPLicit, a novel generative model to jointly represent body pose, shape and clothing geometry. In contrast to existing learning-based approaches that require training specific models for each type of garment, SMPLicit can represent in a unified manner different garment topologies (e.g. from sleeveless tops to hoodies and to open jackets), while controlling other properties like the garment size or tightness/looseness. We show our model to be applicable to a large variety of garments including T-shirts, hoodies, jackets, shorts, pants, skirts, shoes and even hair. The representation flexibility of SMPLicit builds upon an implicit model conditioned with the SMPL human body parameters and a learnable latent space which is semantically interpretable and aligned with the clothing attributes. The proposed model is fully differentiable, allowing for its use into larger end-to-end trainable systems. In the experimental section, we demonstrate SMPLicit can be readily used for fitting 3D scans and for 3D reconstruction in images of dressed people. In both cases we are able to go beyond state of the art, by retrieving complex garment geometries, handling situations with multiple clothing layers and providing a tool for easy outfit editing. To stimulate further research in this direction, we will make our code and model publicly available at http://www.iri.upc.edu/people/ecorona/smplicit/.},
  YEAR = {2021},
  URL = {http://arxiv.org/abs/2103.06871v2},
  FILE = {2103.06871v2.pdf}
 }","Human body, Editable","Generative/adversarial formulation, Conditional neural field",NeRF,UDF,Category-level,,,,,,,CVPR 2021,,No,Direct,"Enric Corona, Albert Pumarola, Guillem Alenyà, Gerard Pons-Moll, Francesc Moreno-Noguer",corona2021smplicit,00000196,"In this paper we introduce SMPLicit, a novel generative model to jointly represent body pose, shape and clothing geometry. In contrast to existing learning-based approaches that require training specific models for each type of garment, SMPLicit can represent in a unified manner different garment topologies (e.g. from sleeveless tops to hoodies and to open jackets), while controlling other properties like the garment size or tightness/looseness. We show our model to be applicable to a large variety of garments including T-shirts, hoodies, jackets, shorts, pants, skirts, shoes and even hair. The representation flexibility of SMPLicit builds upon an implicit model conditioned with the SMPL human body parameters and a learnable latent space which is semantically interpretable and aligned with the clothing attributes. The proposed model is fully differentiable, allowing for its use into larger end-to-end trainable systems. In the experimental section, we demonstrate SMPLicit can be readily used for fitting 3D scans and for 3D reconstruction in images of dressed people. In both cases we are able to go beyond state of the art, by retrieving complex garment geometries, handling situations with multiple clothing layers and providing a tool for easy outfit editing. To stimulate further research in this direction, we will make our code and model publicly available at http://www.iri.upc.edu/people/ecorona/smplicit/.",,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnueo8XHlBKQaPa9xK1wF8neP6jEtXfv4IArMaiLQaaNazdZaXh5yCujzHCJzyR5U59w
5/23/2021 18:44:44,AD-NeRF: Audio Driven Neural Radiance Fields for Talking Head Synthesis,AD-NeRF,3/20/2021,https://arxiv.org/pdf/2103.11078.pdf,,,https://www.youtube.com/watch?v=TQO2EBYXLyU,,,,"@inproceedings{guo2021adnerf,
  BOOKTITLE = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
  ID = {guo2021adnerf},
  ENTRYTYPE = {inproceedings},
  AUTHOR = {Yudong Guo and Keyu Chen and Sen Liang and Yong-Jin Liu and Hujun Bao and Juyong Zhang},
  TITLE = {AD-NeRF: Audio Driven Neural Radiance Fields for Talking Head Synthesis},
  EPRINT = {2103.11078v3},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Generating high-fidelity talking head video by fitting with the input audio sequence is a challenging problem that receives considerable attentions recently. In this paper, we address this problem with the aid of neural scene representation networks. Our method is completely different from existing methods that rely on intermediate representations like 2D landmarks or 3D face models to bridge the gap between audio input and video output. Specifically, the feature of input audio signal is directly fed into a conditional implicit function to generate a dynamic neural radiance field, from which a high-fidelity talking-head video corresponding to the audio signal is synthesized using volume rendering. Another advantage of our framework is that not only the head (with hair) region is synthesized as previous methods did, but also the upper body is generated via two individual neural radiance fields. Experimental results demonstrate that our novel framework can (1) produce high-fidelity and natural results, and (2) support free adjustment of audio signals, viewing directions, and background images. Code is available at https://github.com/YudongGuo/AD-NeRF.},
  YEAR = {2021},
  URL = {http://arxiv.org/abs/2103.11078v3},
  FILE = {2103.11078v3.pdf}
 }","Dynamic, Beyond graphics, Audio","Conditional neural field, Volume partitioning",,,,,,,,,,ICCV 2021,,,,"Yudong Guo, Keyu Chen, Sen Liang, Yongjin Liu, Hujun Bao, Juyong Zhang",guo2021adnerf,00000080,"Generating high-fidelity talking head video by fitting with the input audio sequence is a challenging problem that receives considerable attentions recently. In this paper, we address this problem with the aid of neural scene representation networks. Our method is completely different from existing methods that rely on intermediate representations like 2D landmarks or 3D face models to bridge the gap between audio input and video output. Specifically, the feature of input audio signal is directly fed into a conditional implicit function to generate a dynamic neural radiance field, from which a high-fidelity talking-head video corresponding to the audio signal is synthesized using volume rendering. Another advantage of our framework is that not only the head (with hair) region is synthesized as previous methods did, but also the upper body is generated via two individual neural radiance fields. Experimental results demonstrate that our novel framework can (1) produce high-fidelity and natural results, and (2) support free adjustment of audio signals, viewing directions, and background images. Code is available at https://github.com/YudongGuo/AD-NeRF.",0,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnucGOj9SLLKX-4HAv60eBR8vFlSPQr0VenEoP4xsVwgvHtn0cRAJRfog6ffnnDSuTMw
6/21/2021 16:43:15,Neural Lumigraph Rendering,NLR,3/22/2021,https://arxiv.org/pdf/2103.11571.pdf,http://www.computationalimaging.org/publications/nlr/,,https://www.youtube.com/watch?v=maVF-7x9644,https://openaccess.thecvf.com/content/CVPR2021/supplemental/Kellnhofer_Neural_Lumigraph_Rendering_CVPR_2021_supplemental.pdf,,,"@inproceedings{kellnhofer2021nlr,
  BOOKTITLE = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  ID = {kellnhofer2021nlr},
  ENTRYTYPE = {inproceedings},
  AUTHOR = {Petr Kellnhofer and Lars Jebe and Andrew Jones and Ryan Spicer and Kari Pulli and Gordon Wetzstein},
  TITLE = {Neural Lumigraph Rendering},
  EPRINT = {2103.11571v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Novel view synthesis is a challenging and ill-posed inverse rendering problem. Neural rendering techniques have recently achieved photorealistic image quality for this task. State-of-the-art (SOTA) neural volume rendering approaches, however, are slow to train and require minutes of inference (i.e., rendering) time for high image resolutions. We adopt high-capacity neural scene representations with periodic activations for jointly optimizing an implicit surface and a radiance field of a scene supervised exclusively with posed 2D images. Our neural rendering pipeline accelerates SOTA neural volume rendering by about two orders of magnitude and our implicit surface representation is unique in allowing us to export a mesh with view-dependent texture information. Thus, like other implicit surface representations, ours is compatible with traditional graphics pipelines, enabling real-time rendering rates, while achieving unprecedented image quality compared to other surface methods. We assess the quality of our approach using existing datasets as well as high-quality 3D face data captured with a custom multi-camera rig.},
  YEAR = {2021},
  URL = {http://arxiv.org/abs/2103.11571v1},
  FILE = {2103.11571v1.pdf}
 }",Performance (rendering),Image-based rendering,,SDF,,,,,,,,CVPR 2021,,,,"Petr Kellnhofer, Lars Jebe, Andrew Jones, Ryan Spicer, Kari Pulli, Gordon Wetzstein",kellnhofer2021nlr,00000081,"Novel view synthesis is a challenging and ill-posed inverse rendering problem. Neural rendering techniques have recently achieved photorealistic image quality for this task. State-of-the-art (SOTA) neural volume rendering approaches, however, are slow to train and require minutes of inference (i.e., rendering) time for high image resolutions. We adopt high-capacity neural scene representations with periodic activations for jointly optimizing an implicit surface and a radiance field of a scene supervised exclusively with posed 2D images. Our neural rendering pipeline accelerates SOTA neural volume rendering by about two orders of magnitude and our implicit surface representation is unique in allowing us to export a mesh with view-dependent texture information. Thus, like other implicit surface representations, ours is compatible with traditional graphics pipelines, enabling real-time rendering rates, while achieving unprecedented image quality compared to other surface methods. We assess the quality of our approach using existing datasets as well as high-quality 3D face data captured with a custom multi-camera rig.",8,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnud7J3eL9ZOsP874PT0PGHo1ysVzSkvq8fnh9DcrnVDpjPLQ_TlaarOaMKAQocZ-W9g
5/23/2021 18:47:09,iMAP: Implicit Mapping and Positioning in Real-Time,iMAP,3/23/2021,https://arxiv.org/pdf/2103.12352.pdf,https://edgarsucar.github.io/iMAP/,,https://www.youtube.com/watch?v=c-zkKGArl5Y,,,,"@inproceedings{sucar2021imap,
  BOOKTITLE = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
  ID = {sucar2021imap},
  ENTRYTYPE = {inproceedings},
  AUTHOR = {Edgar Sucar and Shikun Liu and Joseph Ortiz and Andrew J. Davison},
  TITLE = {iMAP: Implicit Mapping and Positioning in Real-Time},
  EPRINT = {2103.12352v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We show for the first time that a multilayer perceptron (MLP) can serve as the only scene representation in a real-time SLAM system for a handheld RGB-D camera. Our network is trained in live operation without prior data, building a dense, scene-specific implicit 3D model of occupancy and colour which is also immediately used for tracking. Achieving real-time SLAM via continual training of a neural network against a live image stream requires significant innovation. Our iMAP algorithm uses a keyframe structure and multi-processing computation flow, with dynamic information-guided pixel sampling for speed, with tracking at 10 Hz and global map updating at 2 Hz. The advantages of an implicit MLP over standard dense SLAM techniques include efficient geometry representation with automatic detail control and smooth, plausible filling-in of unobserved regions such as the back surfaces of objects.},
  YEAR = {2021},
  URL = {http://arxiv.org/abs/2103.12352v2},
  FILE = {2103.12352v2.pdf}
 }","Camera parameter estimation, Robotics, Multi-task/Continual/Transfer learning",Sampling,,,,,,,,,"RGB, Depth",ICCV 2021,,,,"Edgar Sucar, Shikun Liu, Joseph Ortiz, Andrew J. Davison",sucar2021imap,00000082,"We show for the first time that a multilayer perceptron (MLP) can serve as the only scene representation in a real-time SLAM system for a handheld RGB-D camera. Our network is trained in live operation without prior data, building a dense, scene-specific implicit 3D model of occupancy and colour which is also immediately used for tracking. Achieving real-time SLAM via continual training of a neural network against a live image stream requires significant innovation. Our iMAP algorithm uses a keyframe structure and multi-processing computation flow, with dynamic information-guided pixel sampling for speed, with tracking at 10 Hz and global map updating at 2 Hz. The advantages of an implicit MLP over standard dense SLAM techniques include efficient geometry representation with automatic detail control and smooth, plausible filling-in of unobserved regions such as the back surfaces of objects.",3,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnucyWE5VuEa_0hkOiebVTGHbEl-d-MEXmRqT9hForDl4JOY1tbpCXSTAZmSLaH8DuPs
5/23/2021 18:46:10,Mip-NeRF: A Multiscale Representation for Anti-Aliasing Neural Radiance Fields,Mip-NeRF,3/24/2021,https://arxiv.org/pdf/2103.13415.pdf,https://jonbarron.info/mipnerf/,,https://www.youtube.com/watch?v=EpH175PY1A0,,,,"@inproceedings{barron2021mipnerf,
  BOOKTITLE = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
  ID = {barron2021mipnerf},
  ENTRYTYPE = {inproceedings},
  AUTHOR = {Jonathan T. Barron and Ben Mildenhall and Matthew Tancik and Peter Hedman and Ricardo Martin-Brualla and Pratul P. Srinivasan},
  TITLE = {Mip-NeRF: A Multiscale Representation for Anti-Aliasing Neural Radiance Fields},
  EPRINT = {2103.13415v3},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {The rendering procedure used by neural radiance fields (NeRF) samples a scene with a single ray per pixel and may therefore produce renderings that are excessively blurred or aliased when training or testing images observe scene content at different resolutions. The straightforward solution of supersampling by rendering with multiple rays per pixel is impractical for NeRF, because rendering each ray requires querying a multilayer perceptron hundreds of times. Our solution, which we call ""mip-NeRF"" (a la ""mipmap""), extends NeRF to represent the scene at a continuously-valued scale. By efficiently rendering anti-aliased conical frustums instead of rays, mip-NeRF reduces objectionable aliasing artifacts and significantly improves NeRF's ability to represent fine details, while also being 7% faster than NeRF and half the size. Compared to NeRF, mip-NeRF reduces average error rates by 17% on the dataset presented with NeRF and by 60% on a challenging multiscale variant of that dataset that we present. Mip-NeRF is also able to match the accuracy of a brute-force supersampled NeRF on our multiscale dataset while being 22x faster.},
  YEAR = {2021},
  URL = {http://arxiv.org/abs/2103.13415v3},
  FILE = {2103.13415v3.pdf}
 }",Fundamentals,Sampling,,,,,,,,,,ICCV 2021 (Oral),,,,"Jonathan T. Barron, Ben Mildenhall, Matthew Tancik, Peter Hedman, Ricardo Martin-Brualla, Pratul P. Srinivasan",barron2021mipnerf,00000083,"The rendering procedure used by neural radiance fields (NeRF) samples a scene with a single ray per pixel and may therefore produce renderings that are excessively blurred or aliased when training or testing images observe scene content at different resolutions. The straightforward solution of supersampling by rendering with multiple rays per pixel is impractical for NeRF, because rendering each ray requires querying a multilayer perceptron hundreds of times. Our solution, which we call ""mip-NeRF"" (a la ""mipmap""), extends NeRF to represent the scene at a continuously-valued scale. By efficiently rendering anti-aliased conical frustums instead of rays, mip-NeRF reduces objectionable aliasing artifacts and significantly improves NeRF's ability to represent fine details, while also being 7% faster than NeRF and half the size. Compared to NeRF, mip-NeRF reduces average error rates by 17% on the dataset presented with NeRF and by 60% on a challenging multiscale variant of that dataset that we present. Mip-NeRF is also able to match the accuracy of a brute-force supersampled NeRF on our multiscale dataset while being 22x faster.",3,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnudXtC9ePS9BhYtPjcLkX3Yap-3zxJNNU4U3h-rfYeaN7FMmX2lysTp3A-HHyrM9TSI
5/25/2021 15:13:46,PlenOctrees for Real-time Rendering of Neural Radiance Fields,"NeRF-SH, PlenOctrees",3/25/2021,https://arxiv.org/pdf/2103.14024.pdf,https://alexyu.net/plenoctrees/,"https://github.com/sxyu/plenoctree, https://github.com/sxyu/volrend",,,,,"@inproceedings{yu2021nerfsh,
  BOOKTITLE = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
  ID = {yu2021nerfsh},
  ENTRYTYPE = {inproceedings},
  AUTHOR = {Alex Yu and Ruilong Li and Matthew Tancik and Hao Li and Ren Ng and Angjoo Kanazawa},
  TITLE = {PlenOctrees for Real-time Rendering of Neural Radiance Fields},
  EPRINT = {2103.14024v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We introduce a method to render Neural Radiance Fields (NeRFs) in real time using PlenOctrees, an octree-based 3D representation which supports view-dependent effects. Our method can render 800x800 images at more than 150 FPS, which is over 3000 times faster than conventional NeRFs. We do so without sacrificing quality while preserving the ability of NeRFs to perform free-viewpoint rendering of scenes with arbitrary geometry and view-dependent effects. Real-time performance is achieved by pre-tabulating the NeRF into a PlenOctree. In order to preserve view-dependent effects such as specularities, we factorize the appearance via closed-form spherical basis functions. Specifically, we show that it is possible to train NeRFs to predict a spherical harmonic representation of radiance, removing the viewing direction as an input to the neural network. Furthermore, we show that PlenOctrees can be directly optimized to further minimize the reconstruction loss, which leads to equal or better quality compared to competing methods. Moreover, this octree optimization step can be used to reduce the training time, as we no longer need to wait for the NeRF training to converge fully. Our real-time neural rendering approach may potentially enable new applications such as 6-DOF industrial and product visualizations, as well as next generation AR/VR systems. PlenOctrees are amenable to in-browser rendering as well; please visit the project page for the interactive online demo, as well as video and code: https://alexyu.net/plenoctrees},
  YEAR = {2021},
  URL = {http://arxiv.org/abs/2103.14024v2},
  FILE = {2103.14024v2.pdf}
 }","Performance (rendering), Material/lighting estimation","Per-instance fine-tuning, Sampling, Representation, Caching",,,,,,,,,,ICCV 2021,,,,"Alex Yu, Ruilong Li, Matthew Tancik, Hao Li, Ren Ng, Angjoo Kanazawa",yu2021nerfsh,00000084,"We introduce a method to render Neural Radiance Fields (NeRFs) in real time using PlenOctrees, an octree-based 3D representation which supports view-dependent effects. Our method can render 800x800 images at more than 150 FPS, which is over 3000 times faster than conventional NeRFs. We do so without sacrificing quality while preserving the ability of NeRFs to perform free-viewpoint rendering of scenes with arbitrary geometry and view-dependent effects. Real-time performance is achieved by pre-tabulating the NeRF into a PlenOctree. In order to preserve view-dependent effects such as specularities, we factorize the appearance via closed-form spherical basis functions. Specifically, we show that it is possible to train NeRFs to predict a spherical harmonic representation of radiance, removing the viewing direction as an input to the neural network. Furthermore, we show that PlenOctrees can be directly optimized to further minimize the reconstruction loss, which leads to equal or better quality compared to competing methods. Moreover, this octree optimization step can be used to reduce the training time, as we no longer need to wait for the NeRF training to converge fully. Our real-time neural rendering approach may potentially enable new applications such as 6-DOF industrial and product visualizations, as well as next generation AR/VR systems. PlenOctrees are amenable to in-browser rendering as well; please visit the project page for the interactive online demo, as well as video and code: https://alexyu.net/plenoctrees",12,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnufjgWqdP_M0Q7t9BWRuCIIOF46-gbMml5Hc3Xwi41v_4wzVvektELUZCSYglm0_vps
8/5/2021 15:51:57,KiloNeRF: Speeding up Neural Radiance Fields with Thousands of Tiny MLPs,KiloNeRF,3/25/2021,https://arxiv.org/pdf/2103.13744.pdf,,,,,,,"@inproceedings{reiser2021kilonerf,
  BOOKTITLE = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
  ID = {reiser2021kilonerf},
  ENTRYTYPE = {inproceedings},
  AUTHOR = {Christian Reiser and Songyou Peng and Yiyi Liao and Andreas Geiger},
  TITLE = {KiloNeRF: Speeding up Neural Radiance Fields with Thousands of Tiny MLPs},
  EPRINT = {2103.13744v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {NeRF synthesizes novel views of a scene with unprecedented quality by fitting a neural radiance field to RGB images. However, NeRF requires querying a deep Multi-Layer Perceptron (MLP) millions of times, leading to slow rendering times, even on modern GPUs. In this paper, we demonstrate that real-time rendering is possible by utilizing thousands of tiny MLPs instead of one single large MLP. In our setting, each individual MLP only needs to represent parts of the scene, thus smaller and faster-to-evaluate MLPs can be used. By combining this divide-and-conquer strategy with further optimizations, rendering is accelerated by three orders of magnitude compared to the original NeRF model without incurring high storage costs. Further, using teacher-student distillation for training, we show that this speed-up can be achieved without sacrificing visual quality.},
  YEAR = {2021},
  URL = {http://arxiv.org/abs/2103.13744v2},
  FILE = {2103.13744v2.pdf}
 }",Performance (rendering),"Voxelization, Representation, Sampling, Volume partitioning",NeRF,Density,Per-scene,,,,,,,ICCV 2021,,No,Direct,"Christian Reiser, Songyou Peng, Yiyi Liao, Andreas Geiger",reiser2021kilonerf,00000085,"NeRF synthesizes novel views of a scene with unprecedented quality by fitting a neural radiance field to RGB images. However, NeRF requires querying a deep Multi-Layer Perceptron (MLP) millions of times, leading to slow rendering times, even on modern GPUs. In this paper, we demonstrate that real-time rendering is possible by utilizing thousands of tiny MLPs instead of one single large MLP. In our setting, each individual MLP only needs to represent parts of the scene, thus smaller and faster-to-evaluate MLPs can be used. By combining this divide-and-conquer strategy with further optimizations, rendering is accelerated by three orders of magnitude compared to the original NeRF model without incurring high storage costs. Further, using teacher-student distillation for training, we show that this speed-up can be achieved without sacrificing visual quality.",6,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnucsfhsEjvHSt0CzA-m0ZlisvMKH3Po8rDhYRAQ30QQuzVAPqWuMOsNDhsHwxUWud_4
5/23/2021 18:46:37,Baking Neural Radiance Fields for Real-Time View Synthesis,SNeRG,3/26/2021,https://arxiv.org/pdf/2103.14645.pdf,https://phog.github.io/snerg/,,https://www.youtube.com/watch?v=5jKry8n5YO8,,,,"@article{hedman2021snerg,
  JOURNAL = {arXiv preprint arXiv:2103.14645},
  BOOKTITLE = {ArXiv Pre-print},
  ID = {hedman2021snerg},
  ENTRYTYPE = {article},
  AUTHOR = {Peter Hedman and Pratul P. Srinivasan and Ben Mildenhall and Jonathan T. Barron and Paul Debevec},
  TITLE = {Baking Neural Radiance Fields for Real-Time View Synthesis},
  EPRINT = {2103.14645v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Neural volumetric representations such as Neural Radiance Fields (NeRF) have emerged as a compelling technique for learning to represent 3D scenes from images with the goal of rendering photorealistic images of the scene from unobserved viewpoints. However, NeRF's computational requirements are prohibitive for real-time applications: rendering views from a trained NeRF requires querying a multilayer perceptron (MLP) hundreds of times per ray. We present a method to train a NeRF, then precompute and store (i.e. ""bake"") it as a novel representation called a Sparse Neural Radiance Grid (SNeRG) that enables real-time rendering on commodity hardware. To achieve this, we introduce 1) a reformulation of NeRF's architecture, and 2) a sparse voxel grid representation with learned feature vectors. The resulting scene representation retains NeRF's ability to render fine geometric details and view-dependent appearance, is compact (averaging less than 90 MB per scene), and can be rendered in real-time (higher than 30 frames per second on a laptop GPU). Actual screen captures are shown in our video.},
  YEAR = {2021},
  URL = {http://arxiv.org/abs/2103.14645v1},
  FILE = {2103.14645v1.pdf}
 }","Performance (rendering), Compression","Per-instance fine-tuning, Voxelization, Feature volume, Caching",,,,,,,,,,ARXIV 2021,,,,"Peter Hedman, Pratul P. Srinivasan, Ben Mildenhall, Jonathan T. Barron, Paul Debevec",hedman2021snerg,00000086,"Neural volumetric representations such as Neural Radiance Fields (NeRF) have emerged as a compelling technique for learning to represent 3D scenes from images with the goal of rendering photorealistic images of the scene from unobserved viewpoints. However, NeRF's computational requirements are prohibitive for real-time applications: rendering views from a trained NeRF requires querying a multilayer perceptron (MLP) hundreds of times per ray. We present a method to train a NeRF, then precompute and store (i.e. ""bake"") it as a novel representation called a Sparse Neural Radiance Grid (SNeRG) that enables real-time rendering on commodity hardware. To achieve this, we introduce 1) a reformulation of NeRF's architecture, and 2) a sparse voxel grid representation with learned feature vectors. The resulting scene representation retains NeRF's ability to render fine geometric details and view-dependent appearance, is compact (averaging less than 90 MB per scene), and can be rendered in real-time (higher than 30 frames per second on a laptop GPU). Actual screen captures are shown in our video.",2,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnuek1GX_3croFGl-9mYeOHDvvgtSd3MB4Xv30yMBabTev16d_Y-Nj2Jb1coPSir2xD4
8/31/2021 16:19:57,MINE: Towards Continuous Depth MPI with NeRF for Novel View Synthesis,MINE,3/27/2021,https://arxiv.org/pdf/2103.14910.pdf,https://vincentfung13.github.io/projects/mine/,https://github.com/vincentfung13/MINE,,,,,"@inproceedings{li2021mine,
  BOOKTITLE = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
  ID = {li2021mine},
  ENTRYTYPE = {inproceedings},
  AUTHOR = {Jiaxin Li and Zijian Feng and Qi She and Henghui Ding and Changhu Wang and Gim Hee Lee},
  TITLE = {MINE: Towards Continuous Depth MPI with NeRF for Novel View Synthesis},
  EPRINT = {2103.14910v3},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {In this paper, we propose MINE to perform novel view synthesis and depth estimation via dense 3D reconstruction from a single image. Our approach is a continuous depth generalization of the Multiplane Images (MPI) by introducing the NEural radiance fields (NeRF). Given a single image as input, MINE predicts a 4-channel image (RGB and volume density) at arbitrary depth values to jointly reconstruct the camera frustum and fill in occluded contents. The reconstructed and inpainted frustum can then be easily rendered into novel RGB or depth views using differentiable rendering. Extensive experiments on RealEstate10K, KITTI and Flowers Light Fields show that our MINE outperforms state-of-the-art by a large margin in novel view synthesis. We also achieve competitive results in depth estimation on iBims-1 and NYU-v2 without annotated depth supervision. Our source code is available at https://github.com/vincentfung13/MINE},
  YEAR = {2021},
  URL = {http://arxiv.org/abs/2103.14910v3},
  FILE = {2103.14910v3.pdf}
 }","Few-shot reconstruction, Generalization","Image-based rendering, Representation, Data-driven, Coordinate CNN",NeRF,MPI,,,,,,,,ICCV 2021,,,"Direct, Indirect","Jiaxin Li, Zijian Feng, Qi She, Henghui Ding, Changhu Wang, Gim Hee Lee",li2021mine,00000167,"In this paper, we propose MINE to perform novel view synthesis and depth estimation via dense 3D reconstruction from a single image. Our approach is a continuous depth generalization of the Multiplane Images (MPI) by introducing the NEural radiance fields (NeRF). Given a single image as input, MINE predicts a 4-channel image (RGB and volume density) at arbitrary depth values to jointly reconstruct the camera frustum and fill in occluded contents. The reconstructed and inpainted frustum can then be easily rendered into novel RGB or depth views using differentiable rendering. Extensive experiments on RealEstate10K, KITTI and Flowers Light Fields show that our MINE outperforms state-of-the-art by a large margin in novel view synthesis. We also achieve competitive results in depth estimation on iBims-1 and NYU-v2 without annotated depth supervision. Our source code is available at https://github.com/vincentfung13/MINE",0,,No,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnuf-8w9eB3ooWKj4HPEHiO4XwgLDsxQe3aHo68AX1XTZ_ENfzStaYOZTyuQSqxNMNuE
6/15/2021 16:07:55,In-Place Scene Labelling and Understanding with Implicit Scene Representation,Semantic-NeRF,3/29/2021,https://arxiv.org/pdf/2103.15875.pdf,,,,,,,"@inproceedings{zhi2021semanticnerf,
  BOOKTITLE = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
  ID = {zhi2021semanticnerf},
  ENTRYTYPE = {inproceedings},
  AUTHOR = {Shuaifeng Zhi and Tristan Laidlow and Stefan Leutenegger and Andrew J. Davison},
  TITLE = {In-Place Scene Labelling and Understanding with Implicit Scene Representation},
  EPRINT = {2103.15875v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Semantic labelling is highly correlated with geometry and radiance reconstruction, as scene entities with similar shape and appearance are more likely to come from similar classes. Recent implicit neural reconstruction techniques are appealing as they do not require prior training data, but the same fully self-supervised approach is not possible for semantics because labels are human-defined properties. We extend neural radiance fields (NeRF) to jointly encode semantics with appearance and geometry, so that complete and accurate 2D semantic labels can be achieved using a small amount of in-place annotations specific to the scene. The intrinsic multi-view consistency and smoothness of NeRF benefit semantics by enabling sparse labels to efficiently propagate. We show the benefit of this approach when labels are either sparse or very noisy in room-scale scenes. We demonstrate its advantageous properties in various interesting applications such as an efficient scene labelling tool, novel semantic view synthesis, label denoising, super-resolution, label interpolation and multi-view semantic label fusion in visual semantic mapping systems.},
  YEAR = {2021},
  URL = {http://arxiv.org/abs/2103.15875v2},
  FILE = {2103.15875v2.pdf}
 }","Segmentation/composition, Beyond graphics",,,,,,,,,,,ICCV 2021,,,,"Shuaifeng Zhi, Tristan Laidlow, Stefan Leutenegger, Andrew J. Davison",zhi2021semanticnerf,00000088,"Semantic labelling is highly correlated with geometry and radiance reconstruction, as scene entities with similar shape and appearance are more likely to come from similar classes. Recent implicit neural reconstruction techniques are appealing as they do not require prior training data, but the same fully self-supervised approach is not possible for semantics because labels are human-defined properties. We extend neural radiance fields (NeRF) to jointly encode semantics with appearance and geometry, so that complete and accurate 2D semantic labels can be achieved using a small amount of in-place annotations specific to the scene. The intrinsic multi-view consistency and smoothness of NeRF benefit semantics by enabling sparse labels to efficiently propagate. We show the benefit of this approach when labels are either sparse or very noisy in room-scale scenes. We demonstrate its advantageous properties in various interesting applications such as an efficient scene labelling tool, novel semantic view synthesis, label denoising, super-resolution, label interpolation and multi-view semantic label fusion in visual semantic mapping systems.",1,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnud-1tEEydj5TUe_upzxjfX-bagtLrKh5AnFrjK8j7K3vi1PNKqM6JG2Ym2LNhoEpV4
5/23/2021 18:42:35,MVSNeRF: Fast Generalizable Radiance Field Reconstruction from Multi-View Stereo,NVSNeRF,3/29/2021,https://arxiv.org/pdf/2103.15595.pdf,https://apchenstu.github.io/mvsnerf/,https://github.com/apchenstu/mvsnerf,https://www.youtube.com/watch?v=68N21TacPxw,,,,"@inproceedings{chen2021nvsnerf,
  BOOKTITLE = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
  ID = {chen2021nvsnerf},
  ENTRYTYPE = {inproceedings},
  AUTHOR = {Anpei Chen and Zexiang Xu and Fuqiang Zhao and Xiaoshuai Zhang and Fanbo Xiang and Jingyi Yu and Hao Su},
  TITLE = {MVSNeRF: Fast Generalizable Radiance Field Reconstruction from Multi-View Stereo},
  EPRINT = {2103.15595v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We present MVSNeRF, a novel neural rendering approach that can efficiently reconstruct neural radiance fields for view synthesis. Unlike prior works on neural radiance fields that consider per-scene optimization on densely captured images, we propose a generic deep neural network that can reconstruct radiance fields from only three nearby input views via fast network inference. Our approach leverages plane-swept cost volumes (widely used in multi-view stereo) for geometry-aware scene reasoning, and combines this with physically based volume rendering for neural radiance field reconstruction. We train our network on real objects in the DTU dataset, and test it on three different datasets to evaluate its effectiveness and generalizability. Our approach can generalize across scenes (even indoor scenes, completely different from our training scenes of objects) and generate realistic view synthesis results using only three input images, significantly outperforming concurrent works on generalizable radiance field reconstruction. Moreover, if dense images are captured, our estimated radiance field representation can be easily fine-tuned; this leads to fast per-scene reconstruction with higher rendering quality and substantially less optimization time than NeRF.},
  YEAR = {2021},
  URL = {http://arxiv.org/abs/2103.15595v2},
  FILE = {2103.15595v2.pdf}
 }","Performance (training), Few-shot reconstruction, Generalization","Per-instance fine-tuning, Lifting 2D features to 3D, Data-driven",,,,,,,,,,ICCV 2021,,,,"Anpei Chen, Zexiang Xu, Fuqiang Zhao, Xiaoshuai Zhang, Fanbo Xiang, Jingyi Yu, Hao Su",chen2021nvsnerf,00000089,"We present MVSNeRF, a novel neural rendering approach that can efficiently reconstruct neural radiance fields for view synthesis. Unlike prior works on neural radiance fields that consider per-scene optimization on densely captured images, we propose a generic deep neural network that can reconstruct radiance fields from only three nearby input views via fast network inference. Our approach leverages plane-swept cost volumes (widely used in multi-view stereo) for geometry-aware scene reasoning, and combines this with physically based volume rendering for neural radiance field reconstruction. We train our network on real objects in the DTU dataset, and test it on three different datasets to evaluate its effectiveness and generalizability. Our approach can generalize across scenes (even indoor scenes, completely different from our training scenes of objects) and generate realistic view synthesis results using only three input images, significantly outperforming concurrent works on generalizable radiance field reconstruction. Moreover, if dense images are captured, our estimated radiance field representation can be easily fine-tuned; this leads to fast per-scene reconstruction with higher rendering quality and substantially less optimization time than NeRF.",4,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnufCRGfRnJXH1x8ppIEODnJZ2SkUhSoaFPrUvWxSYWkSYx9q-KETXEM__ghI77RuyEs
5/23/2021 18:43:52,GNeRF: GAN-based Neural Radiance Field without Posed Camera,GNeRF,3/29/2021,https://arxiv.org/pdf/2103.15606.pdf,,https://github.com/MQ66/gnerf,,,,,"@article{meng2021gnerf,
  JOURNAL = {arXiv preprint arXiv:2103.15606},
  BOOKTITLE = {ArXiv Pre-print},
  ID = {meng2021gnerf},
  ENTRYTYPE = {article},
  AUTHOR = {Quan Meng and Anpei Chen and Haimin Luo and Minye Wu and Hao Su and Lan Xu and Xuming He and Jingyi Yu},
  TITLE = {GNeRF: GAN-based Neural Radiance Field without Posed Camera},
  EPRINT = {2103.15606v3},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We introduce GNeRF, a framework to marry Generative Adversarial Networks (GAN) with Neural Radiance Field (NeRF) reconstruction for the complex scenarios with unknown and even randomly initialized camera poses. Recent NeRF-based advances have gained popularity for remarkable realistic novel view synthesis. However, most of them heavily rely on accurate camera poses estimation, while few recent methods can only optimize the unknown camera poses in roughly forward-facing scenes with relatively short camera trajectories and require rough camera poses initialization. Differently, our GNeRF only utilizes randomly initialized poses for complex outside-in scenarios. We propose a novel two-phases end-to-end framework. The first phase takes the use of GANs into the new realm for optimizing coarse camera poses and radiance fields jointly, while the second phase refines them with additional photometric loss. We overcome local minima using a hybrid and iterative optimization scheme. Extensive experiments on a variety of synthetic and natural scenes demonstrate the effectiveness of GNeRF. More impressively, our approach outperforms the baselines favorably in those scenes with repeated patterns or even low textures that are regarded as extremely challenging before.},
  YEAR = {2021},
  URL = {http://arxiv.org/abs/2103.15606v3},
  FILE = {2103.15606v3.pdf}
 }",Camera parameter estimation,Generative/adversarial formulation,,,,,,,,,,ARXIV 2021,,,,"Quan Meng, Anpei Chen, Haimin Luo, Minye Wu, Hao Su, Lan Xu, Xuming He, Jingyi Yu",meng2021gnerf,00000090,"We introduce GNeRF, a framework to marry Generative Adversarial Networks (GAN) with Neural Radiance Field (NeRF) reconstruction for the complex scenarios with unknown and even randomly initialized camera poses. Recent NeRF-based advances have gained popularity for remarkable realistic novel view synthesis. However, most of them heavily rely on accurate camera poses estimation, while few recent methods can only optimize the unknown camera poses in roughly forward-facing scenes with relatively short camera trajectories and require rough camera poses initialization. Differently, our GNeRF only utilizes randomly initialized poses for complex outside-in scenarios. We propose a novel two-phases end-to-end framework. The first phase takes the use of GANs into the new realm for optimizing coarse camera poses and radiance fields jointly, while the second phase refines them with additional photometric loss. We overcome local minima using a hybrid and iterative optimization scheme. Extensive experiments on a variety of synthetic and natural scenes demonstrate the effectiveness of GNeRF. More impressively, our approach outperforms the baselines favorably in those scenes with repeated patterns or even low textures that are regarded as extremely challenging before.",1,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnudBRMM7qaYw7IxQSYrJgS-QRbT8NBjelU5_gbRUh398AbTUbfhIPwjDlbKFxp30A2I
5/23/2021 18:41:27,Unsupervised Learning of 3D Object Categories from Videos in the Wild,,3/30/2021,https://arxiv.org/pdf/2103.16552.pdf,https://henzler.github.io/publication/unsupervised_videos/,,,,,,"@inproceedings{henzler2021unsupervised,
  BOOKTITLE = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  ID = {henzler2021unsupervised},
  ENTRYTYPE = {inproceedings},
  AUTHOR = {Philipp Henzler and Jeremy Reizenstein and Patrick Labatut and Roman Shapovalov and Tobias Ritschel and Andrea Vedaldi and David Novotny},
  TITLE = {Unsupervised Learning of 3D Object Categories from Videos in the Wild},
  EPRINT = {2103.16552v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Our goal is to learn a deep network that, given a small number of images of an object of a given category, reconstructs it in 3D. While several recent works have obtained analogous results using synthetic data or assuming the availability of 2D primitives such as keypoints, we are interested in working with challenging real data and with no manual annotations. We thus focus on learning a model from multiple views of a large collection of object instances. We contribute with a new large dataset of object centric videos suitable for training and benchmarking this class of models. We show that existing techniques leveraging meshes, voxels, or implicit surfaces, which work well for reconstructing isolated objects, fail on this challenging data. Finally, we propose a new neural network design, called warp-conditioned ray embedding (WCR), which significantly improves reconstruction while obtaining a detailed implicit representation of the object surface and texture, also compensating for the noise in the initial SfM reconstruction that bootstrapped the learning process. Our evaluation demonstrates performance improvements over several deep monocular reconstruction baselines on existing benchmarks and on our novel dataset.},
  YEAR = {2021},
  URL = {http://arxiv.org/abs/2103.16552v1},
  FILE = {2103.16552v1.pdf}
 }",Generalization,"Lifting 2D features to 3D, Data-driven",,,,,,,,,,CVPR 2021,,,,"Philipp Henzler, Jeremy Reizenstein, Patrick Labatut, Roman Shapovalov, Tobias Ritschel, Andrea Vedaldi, David Novotny",henzler2021unsupervised,00000091,"Our goal is to learn a deep network that, given a small number of images of an object of a given category, reconstructs it in 3D. While several recent works have obtained analogous results using synthetic data or assuming the availability of 2D primitives such as keypoints, we are interested in working with challenging real data and with no manual annotations. We thus focus on learning a model from multiple views of a large collection of object instances. We contribute with a new large dataset of object centric videos suitable for training and benchmarking this class of models. We show that existing techniques leveraging meshes, voxels, or implicit surfaces, which work well for reconstructing isolated objects, fail on this challenging data. Finally, we propose a new neural network design, called warp-conditioned ray embedding (WCR), which significantly improves reconstruction while obtaining a detailed implicit representation of the object surface and texture, also compensating for the noise in the initial SfM reconstruction that bootstrapped the learning process. Our evaluation demonstrates performance improvements over several deep monocular reconstruction baselines on existing benchmarks and on our novel dataset.",2,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnueQI_iKPOE-mNo9gmmudgMtVr1za2DeWetBPI2KTxV5KZDn-HRIJ6htoJYZHhGXttk
5/23/2021 18:42:52,Foveated Neural Radiance Fields for Real-Time and Egocentric Virtual Reality,,3/30/2021,https://arxiv.org/pdf/2103.16365.pdf,,,,,,,"@article{deng2021foveated,
  JOURNAL = {arXiv preprint arXiv:2103.16365},
  BOOKTITLE = {ArXiv Pre-print},
  ID = {deng2021foveated},
  ENTRYTYPE = {article},
  AUTHOR = {Nianchen Deng and Zhenyi He and Jiannan Ye and Praneeth Chakravarthula and Xubo Yang and Qi Sun},
  TITLE = {Foveated Neural Radiance Fields for Real-Time and Egocentric Virtual Reality},
  EPRINT = {2103.16365v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.GR},
  ABSTRACT = {Traditional high-quality 3D graphics requires large volumes of fine-detailed scene data for rendering. This demand compromises computational efficiency and local storage resources. Specifically, it becomes more concerning for future wearable and portable virtual and augmented reality (VR/AR) displays. Recent approaches to combat this problem include remote rendering/streaming and neural representations of 3D assets. These approaches have redefined the traditional local storage-rendering pipeline by distributed computing or compression of large data. However, these methods typically suffer from high latency or low quality for practical visualization of large immersive virtual scenes, notably with extra high resolution and refresh rate requirements for VR applications such as gaming and design. Tailored for the future portable, low-storage, and energy-efficient VR platforms, we present the first gaze-contingent 3D neural representation and view synthesis method. We incorporate the human psychophysics of visual- and stereo-acuity into an egocentric neural representation of 3D scenery. Furthermore, we jointly optimize the latency/performance and visual quality, while mutually bridging human perception and neural scene synthesis, to achieve perceptually high-quality immersive interaction. Both objective analysis and subjective study demonstrate the effectiveness of our approach in significantly reducing local storage volume and synthesis latency (up to 99% reduction in both data size and computational time), while simultaneously presenting high-fidelity rendering, with perceptual quality identical to that of fully locally stored and rendered high-quality imagery.},
  YEAR = {2021},
  URL = {http://arxiv.org/abs/2103.16365v1},
  FILE = {2103.16365v1.pdf}
 }","Performance (rendering), Compression","Sampling, Representation",,,,,,,,,,ARXIV 2021,,,,"Nianchen Deng, Zhenyi He, Jiannan Ye, Praneeth Chakravarthula, Xubo Yang, Qi Sun",deng2021foveated,00000092,"Traditional high-quality 3D graphics requires large volumes of fine-detailed scene data for rendering. This demand compromises computational efficiency and local storage resources. Specifically, it becomes more concerning for future wearable and portable virtual and augmented reality (VR/AR) displays. Recent approaches to combat this problem include remote rendering/streaming and neural representations of 3D assets. These approaches have redefined the traditional local storage-rendering pipeline by distributed computing or compression of large data. However, these methods typically suffer from high latency or low quality for practical visualization of large immersive virtual scenes, notably with extra high resolution and refresh rate requirements for VR applications such as gaming and design. Tailored for the future portable, low-storage, and energy-efficient VR platforms, we present the first gaze-contingent 3D neural representation and view synthesis method. We incorporate the human psychophysics of visual- and stereo-acuity into an egocentric neural representation of 3D scenery. Furthermore, we jointly optimize the latency/performance and visual quality, while mutually bridging human perception and neural scene synthesis, to achieve perceptually high-quality immersive interaction. Both objective analysis and subjective study demonstrate the effectiveness of our approach in significantly reducing local storage volume and synthesis latency (up to 99% reduction in both data size and computational time), while simultaneously presenting high-fidelity rendering, with perceptual quality identical to that of fully locally stored and rendered high-quality imagery.",0,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnucFfDlUBieCXztSVteti5w5Nm_Pl-_FvPtBeGUQxKbAiZYkURFZ9tFNxDP9KgJHMts
5/23/2021 18:43:20,CAMPARI: Camera-Aware Decomposed Generative Neural Radiance Fields,CAMPARI,3/31/2021,https://arxiv.org/pdf/2103.17269.pdf,,,,,,,"@article{niemeyer2021campari,
  JOURNAL = {arXiv preprint arXiv:2103.17269},
  BOOKTITLE = {ArXiv Pre-print},
  ID = {niemeyer2021campari},
  ENTRYTYPE = {article},
  AUTHOR = {Michael Niemeyer and Andreas Geiger},
  TITLE = {CAMPARI: Camera-Aware Decomposed Generative Neural Radiance Fields},
  EPRINT = {2103.17269v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Tremendous progress in deep generative models has led to photorealistic image synthesis. While achieving compelling results, most approaches operate in the two-dimensional image domain, ignoring the three-dimensional nature of our world. Several recent works therefore propose generative models which are 3D-aware, i.e., scenes are modeled in 3D and then rendered differentiably to the image plane. This leads to impressive 3D consistency, but incorporating such a bias comes at a price: the camera needs to be modeled as well. Current approaches assume fixed intrinsics and a predefined prior over camera pose ranges. As a result, parameter tuning is typically required for real-world data, and results degrade if the data distribution is not matched. Our key hypothesis is that learning a camera generator jointly with the image generator leads to a more principled approach to 3D-aware image synthesis. Further, we propose to decompose the scene into a background and foreground model, leading to more efficient and disentangled scene representations. While training from raw, unposed image collections, we learn a 3D- and camera-aware generative model which faithfully recovers not only the image but also the camera data distribution. At test time, our model generates images with explicit control over the camera as well as the shape and appearance of the scene.},
  YEAR = {2021},
  URL = {http://arxiv.org/abs/2103.17269v1},
  FILE = {2103.17269v1.pdf}
 }",Image,"Generative/adversarial formulation, Conditional neural field, Lifting 2D features to 3D, Volume partitioning, Data-driven",,,,,,,,,,ARXIV 2021,,,,"Michael Niemeyer, Andreas Geiger",niemeyer2021campari,00000093,"Tremendous progress in deep generative models has led to photorealistic image synthesis. While achieving compelling results, most approaches operate in the two-dimensional image domain, ignoring the three-dimensional nature of our world. Several recent works therefore propose generative models which are 3D-aware, i.e., scenes are modeled in 3D and then rendered differentiably to the image plane. This leads to impressive 3D consistency, but incorporating such a bias comes at a price: the camera needs to be modeled as well. Current approaches assume fixed intrinsics and a predefined prior over camera pose ranges. As a result, parameter tuning is typically required for real-world data, and results degrade if the data distribution is not matched. Our key hypothesis is that learning a camera generator jointly with the image generator leads to a more principled approach to 3D-aware image synthesis. Further, we propose to decompose the scene into a background and foreground model, leading to more efficient and disentangled scene representations. While training from raw, unposed image collections, we learn a 3D- and camera-aware generative model which faithfully recovers not only the image but also the camera data distribution. At test time, our model generates images with explicit control over the camera as well as the shape and appearance of the scene.",2,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnuewjJrUlO5ufAa76jpu9zQcP9dANhUsP0jZ7TGXS9psvo1ZYusKtoEjPBpPSLg9UoE
5/23/2021 18:37:12,NPMs: Neural Parametric Models for 3D Deformable Shapes,NPMs,4/1/2021,https://arxiv.org/pdf/2104.00702.pdf,,,,,,,"@inproceedings{palafox2021npms,
  BOOKTITLE = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
  ID = {palafox2021npms},
  ENTRYTYPE = {inproceedings},
  AUTHOR = {Pablo Palafox and Aljaz Bozic and Justus Thies and Matthias Niessner and Angela Dai},
  TITLE = {NPMs: Neural Parametric Models for 3D Deformable Shapes},
  EPRINT = {2104.00702v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Parametric 3D models have enabled a wide variety of tasks in computer graphics and vision, such as modeling human bodies, faces, and hands. However, the construction of these parametric models is often tedious, as it requires heavy manual tweaking, and they struggle to represent additional complexity and details such as wrinkles or clothing. To this end, we propose Neural Parametric Models (NPMs), a novel, learned alternative to traditional, parametric 3D models, which does not require hand-crafted, object-specific constraints. In particular, we learn to disentangle 4D dynamics into latent-space representations of shape and pose, leveraging the flexibility of recent developments in learned implicit functions. Crucially, once learned, our neural parametric models of shape and pose enable optimization over the learned spaces to fit to new observations, similar to the fitting of a traditional parametric model, e.g., SMPL. This enables NPMs to achieve a significantly more accurate and detailed representation of observed deformable sequences. We show that NPMs improve notably over both parametric and non-parametric state of the art in reconstruction and tracking of monocular depth sequences of clothed humans and hands. Latent-space interpolation as well as shape/pose transfer experiments further demonstrate the usefulness of NPMs. Code is publicly available at https://pablopalafox.github.io/npms.},
  YEAR = {2021},
  URL = {http://arxiv.org/abs/2104.00702v2},
  FILE = {2104.00702v2.pdf}
 }",Human body,"Conditional neural field, Warping field/Flow field",,SDF,,,,,,,,ICCV 2021,,,,"Pablo Palafox, Aljaž Božič, Justus Thies, Matthias Nießner, Angela Dai",palafox2021npms,00000094,"Parametric 3D models have enabled a wide variety of tasks in computer graphics and vision, such as modeling human bodies, faces, and hands. However, the construction of these parametric models is often tedious, as it requires heavy manual tweaking, and they struggle to represent additional complexity and details such as wrinkles or clothing. To this end, we propose Neural Parametric Models (NPMs), a novel, learned alternative to traditional, parametric 3D models, which does not require hand-crafted, object-specific constraints. In particular, we learn to disentangle 4D dynamics into latent-space representations of shape and pose, leveraging the flexibility of recent developments in learned implicit functions. Crucially, once learned, our neural parametric models of shape and pose enable optimization over the learned spaces to fit to new observations, similar to the fitting of a traditional parametric model, e.g., SMPL. This enables NPMs to achieve a significantly more accurate and detailed representation of observed deformable sequences. We show that NPMs improve notably over both parametric and non-parametric state of the art in reconstruction and tracking of monocular depth sequences of clothed humans and hands. Latent-space interpolation as well as shape/pose transfer experiments further demonstrate the usefulness of NPMs. Code is publicly available at https://pablopalafox.github.io/npms.",2,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnufBg_lPvwXTxdJBlp3fWFnD-1pRfBvNIzv1y97a0G9eJXboSouwehufNAEww64CVh8
5/23/2021 18:39:26,RGB-D Local Implicit Function for Depth Completion of Transparent Objects,,4/1/2021,https://arxiv.org/pdf/2104.00622.pdf,https://research.nvidia.com/publication/2021-03_RGB-D-Local-Implicit,Coming soon,,,,,"@inproceedings{zhu2021rgbd,
  BOOKTITLE = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  ID = {zhu2021rgbd},
  ENTRYTYPE = {inproceedings},
  AUTHOR = {Luyang Zhu and Arsalan Mousavian and Yu Xiang and Hammad Mazhar and Jozef van Eenbergen and Shoubhik Debnath and Dieter Fox},
  TITLE = {RGB-D Local Implicit Function for Depth Completion of Transparent Objects},
  EPRINT = {2104.00622v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Majority of the perception methods in robotics require depth information provided by RGB-D cameras. However, standard 3D sensors fail to capture depth of transparent objects due to refraction and absorption of light. In this paper, we introduce a new approach for depth completion of transparent objects from a single RGB-D image. Key to our approach is a local implicit neural representation built on ray-voxel pairs that allows our method to generalize to unseen objects and achieve fast inference speed. Based on this representation, we present a novel framework that can complete missing depth given noisy RGB-D input. We further improve the depth estimation iteratively using a self-correcting refinement model. To train the whole pipeline, we build a large scale synthetic dataset with transparent objects. Experiments demonstrate that our method performs significantly better than the current state-of-the-art methods on both synthetic and real world data. In addition, our approach improves the inference speed by a factor of 20 compared to the previous best method, ClearGrasp. Code and dataset will be released at https://research.nvidia.com/publication/2021-03_RGB-D-Local-Implicit.},
  YEAR = {2021},
  URL = {http://arxiv.org/abs/2104.00622v1},
  FILE = {2104.00622v1.pdf}
 }","Challenging materials (fur, hair, transparency), Beyond graphics",Voxelization,,,,,,,,,,CVPR 2021,,,,"Luyang Zhu, Arsalan Mousavian, Yu Xiang, Hammad Mazhar, Jozef van Eenbergen, Shoubhik Debnath, Dieter Fox",zhu2021rgbd,00000095,"Majority of the perception methods in robotics require depth information provided by RGB-D cameras. However, standard 3D sensors fail to capture depth of transparent objects due to refraction and absorption of light. In this paper, we introduce a new approach for depth completion of transparent objects from a single RGB-D image. Key to our approach is a local implicit neural representation built on ray-voxel pairs that allows our method to generalize to unseen objects and achieve fast inference speed. Based on this representation, we present a novel framework that can complete missing depth given noisy RGB-D input. We further improve the depth estimation iteratively using a self-correcting refinement model. To train the whole pipeline, we build a large scale synthetic dataset with transparent objects. Experiments demonstrate that our method performs significantly better than the current state-of-the-art methods on both synthetic and real world data. In addition, our approach improves the inference speed by a factor of 20 compared to the previous best method, ClearGrasp. Code and dataset will be released at https://research.nvidia.com/publication/2021-03_RGB-D-Local-Implicit.",1,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnuc2W9jUM1S8IXOAVQPfg4wMNCmtenNV3UFX73-_zBSpILVq2Ciolwvo8fZC5L7hdl8
5/25/2021 14:56:49,PhySG: Inverse Rendering with Spherical Gaussians for Physics-based Material Editing and Relighting,PhySG,4/1/2021,https://arxiv.org/pdf/2104.00674.pdf,https://kai-46.github.io/PhySG-website/,Coming soon,Coming soon,,,,"@inproceedings{zhang2021physg,
  BOOKTITLE = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  ID = {zhang2021physg},
  ENTRYTYPE = {inproceedings},
  AUTHOR = {Kai Zhang and Fujun Luan and Qianqian Wang and Kavita Bala and Noah Snavely},
  TITLE = {PhySG: Inverse Rendering with Spherical Gaussians for Physics-based Material Editing and Relighting},
  EPRINT = {2104.00674v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We present PhySG, an end-to-end inverse rendering pipeline that includes a fully differentiable renderer and can reconstruct geometry, materials, and illumination from scratch from a set of RGB input images. Our framework represents specular BRDFs and environmental illumination using mixtures of spherical Gaussians, and represents geometry as a signed distance function parameterized as a Multi-Layer Perceptron. The use of spherical Gaussians allows us to efficiently solve for approximate light transport, and our method works on scenes with challenging non-Lambertian reflectance captured under natural, static illumination. We demonstrate, with both synthetic and real data, that our reconstructions not only enable rendering of novel viewpoints, but also physics-based appearance editing of materials and illumination.},
  YEAR = {2021},
  URL = {http://arxiv.org/abs/2104.00674v1},
  FILE = {2104.00674v1.pdf}
 }",Material/lighting estimation,,,SDF,,,,,,,,CVPR 2021,,,,"Kai Zhang, Fujun Luan, Qianqian Wang, Kavita Bala, Noah Snavely",zhang2021physg,00000096,"We present PhySG, an end-to-end inverse rendering pipeline that includes a fully differentiable renderer and can reconstruct geometry, materials, and illumination from scratch from a set of RGB input images. Our framework represents specular BRDFs and environmental illumination using mixtures of spherical Gaussians, and represents geometry as a signed distance function parameterized as a Multi-Layer Perceptron. The use of spherical Gaussians allows us to efficiently solve for approximate light transport, and our method works on scenes with challenging non-Lambertian reflectance captured under natural, static illumination. We demonstrate, with both synthetic and real data, that our reconstructions not only enable rendering of novel viewpoints, but also physics-based appearance editing of materials and illumination.",2,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnufNW5UI-hqKb1XR6wOsa9dFkSgTelnmb1A3JB_hbC4uCS5fK3z4ufQGt142vqFRoyA
5/23/2021 18:39:59,NeRF-VAE: A Geometry Aware 3D Scene Generative Model,NeRF-VAE,4/1/2021,https://arxiv.org/pdf/2104.00587.pdf,,,https://www.youtube.com/watch?v=f-T3BLVuXkY,,,,"@article{kosiorek2021nerfvae,
  JOURNAL = {arXiv preprint arXiv:2104.00587},
  BOOKTITLE = {ArXiv Pre-print},
  ID = {kosiorek2021nerfvae},
  ENTRYTYPE = {article},
  AUTHOR = {Adam R. Kosiorek and Heiko Strathmann and Daniel Zoran and Pol Moreno and Rosalia Schneider and Sona Mokra and Danilo J. Rezende},
  TITLE = {NeRF-VAE: A Geometry Aware 3D Scene Generative Model},
  EPRINT = {2104.00587v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {stat.ML},
  ABSTRACT = {We propose NeRF-VAE, a 3D scene generative model that incorporates geometric structure via NeRF and differentiable volume rendering. In contrast to NeRF, our model takes into account shared structure across scenes, and is able to infer the structure of a novel scene -- without the need to re-train -- using amortized inference. NeRF-VAE's explicit 3D rendering process further contrasts previous generative models with convolution-based rendering which lacks geometric structure. Our model is a VAE that learns a distribution over radiance fields by conditioning them on a latent scene representation. We show that, once trained, NeRF-VAE is able to infer and render geometrically-consistent scenes from previously unseen 3D environments using very few input images. We further demonstrate that NeRF-VAE generalizes well to out-of-distribution cameras, while convolutional models do not. Finally, we introduce and study an attention-based conditioning mechanism of NeRF-VAE's decoder, which improves model performance.},
  YEAR = {2021},
  URL = {http://arxiv.org/abs/2104.00587v1},
  FILE = {2104.00587v1.pdf}
 }","Few-shot reconstruction, Generalization, Image","Generative/adversarial formulation, Conditional neural field",,,,,,,,,,ARXIV 2021,,,,"Adam R. Kosiorek, Heiko Strathmann, Daniel Zoran, Pol Moreno, Rosalia Schneider, Soňa Mokrá, Danilo J. Rezende",kosiorek2021nerfvae,00000097,"We propose NeRF-VAE, a 3D scene generative model that incorporates geometric structure via NeRF and differentiable volume rendering. In contrast to NeRF, our model takes into account shared structure across scenes, and is able to infer the structure of a novel scene -- without the need to re-train -- using amortized inference. NeRF-VAE's explicit 3D rendering process further contrasts previous generative models with convolution-based rendering which lacks geometric structure. Our model is a VAE that learns a distribution over radiance fields by conditioning them on a latent scene representation. We show that, once trained, NeRF-VAE is able to infer and render geometrically-consistent scenes from previously unseen 3D environments using very few input images. We further demonstrate that NeRF-VAE generalizes well to out-of-distribution cameras, while convolutional models do not. Finally, we introduce and study an attention-based conditioning mechanism of NeRF-VAE's decoder, which improves model performance.",6,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnufZFu2M2l8Ty0_r7hzzy6d6igN-xH6xsN9l1Nc1bWYcR_2l61rIsubU5aN2QaZl56g
5/23/2021 18:40:23,Unconstrained Scene Generation with Locally Conditioned Radiance Fields,,4/1/2021,https://arxiv.org/pdf/2104.00670.pdf,https://apple.github.io/ml-gsn/,https://github.com/apple/ml-gsn,,,,,"@inproceedings{devries2021unconstrained,
  BOOKTITLE = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
  ID = {devries2021unconstrained},
  ENTRYTYPE = {inproceedings},
  AUTHOR = {Terrance DeVries and Miguel Angel Bautista and Nitish Srivastava and Graham W. Taylor and Joshua M. Susskind},
  TITLE = {Unconstrained Scene Generation with Locally Conditioned Radiance Fields},
  EPRINT = {2104.00670v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We tackle the challenge of learning a distribution over complex, realistic, indoor scenes. In this paper, we introduce Generative Scene Networks (GSN), which learns to decompose scenes into a collection of many local radiance fields that can be rendered from a free moving camera. Our model can be used as a prior to generate new scenes, or to complete a scene given only sparse 2D observations. Recent work has shown that generative models of radiance fields can capture properties such as multi-view consistency and view-dependent lighting. However, these models are specialized for constrained viewing of single objects, such as cars or faces. Due to the size and complexity of realistic indoor environments, existing models lack the representational capacity to adequately capture them. Our decomposition scheme scales to larger and more complex scenes while preserving details and diversity, and the learned prior enables high-quality rendering from viewpoints that are significantly different from observed viewpoints. When compared to existing models, GSN produces quantitatively higher-quality scene renderings across several different scene datasets.},
  YEAR = {2021},
  URL = {http://arxiv.org/abs/2104.00670v1},
  FILE = {2104.00670v1.pdf}
 }",,"Generative/adversarial formulation, Conditional neural field, Feature volume",,,,,,,,,,ICCV 2021,,,,"Terrance DeVries, Miguel Angel Bautista, Nitish Srivastava, Graham W. Taylor, Joshua M. Susskind",devries2021unconstrained,00000098,"We tackle the challenge of learning a distribution over complex, realistic, indoor scenes. In this paper, we introduce Generative Scene Networks (GSN), which learns to decompose scenes into a collection of many local radiance fields that can be rendered from a free moving camera. Our model can be used as a prior to generate new scenes, or to complete a scene given only sparse 2D observations. Recent work has shown that generative models of radiance fields can capture properties such as multi-view consistency and view-dependent lighting. However, these models are specialized for constrained viewing of single objects, such as cars or faces. Due to the size and complexity of realistic indoor environments, existing models lack the representational capacity to adequately capture them. Our decomposition scheme scales to larger and more complex scenes while preserving details and diversity, and the learned prior enables high-quality rendering from viewpoints that are significantly different from observed viewpoints. When compared to existing models, GSN produces quantitatively higher-quality scene renderings across several different scene datasets.",0,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnueHcye4R-QH9p5_pvlLrH25NK8AkChUlfkrqmYRkO9j4EEovKQlAHez8fQaPoG1AJY
5/23/2021 18:41:00,Putting NeRF on a Diet: Semantically Consistent Few-Shot View Synthesis,DietNeRF,4/1/2021,https://arxiv.org/pdf/2104.00677.pdf,https://www.ajayj.com/dietnerf,"Coming soon, https://github.com/codestella/putting-nerf-on-a-diet",https://www.youtube.com/watch?v=RF_3hsNizqw,,,,"@article{jain2021dietnerf,
  JOURNAL = {arXiv preprint arXiv:2104.00677},
  BOOKTITLE = {ArXiv Pre-print},
  ID = {jain2021dietnerf},
  ENTRYTYPE = {article},
  AUTHOR = {Ajay Jain and Matthew Tancik and Pieter Abbeel},
  TITLE = {Putting NeRF on a Diet: Semantically Consistent Few-Shot View Synthesis},
  EPRINT = {2104.00677v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We present DietNeRF, a 3D neural scene representation estimated from a few images. Neural Radiance Fields (NeRF) learn a continuous volumetric representation of a scene through multi-view consistency, and can be rendered from novel viewpoints by ray casting. While NeRF has an impressive ability to reconstruct geometry and fine details given many images, up to 100 for challenging 360{\deg} scenes, it often finds a degenerate solution to its image reconstruction objective when only a few input views are available. To improve few-shot quality, we propose DietNeRF. We introduce an auxiliary semantic consistency loss that encourages realistic renderings at novel poses. DietNeRF is trained on individual scenes to (1) correctly render given input views from the same pose, and (2) match high-level semantic attributes across different, random poses. Our semantic loss allows us to supervise DietNeRF from arbitrary poses. We extract these semantics using a pre-trained visual encoder such as CLIP, a Vision Transformer trained on hundreds of millions of diverse single-view, 2D photographs mined from the web with natural language supervision. In experiments, DietNeRF improves the perceptual quality of few-shot view synthesis when learned from scratch, can render novel views with as few as one observed image when pre-trained on a multi-view dataset, and produces plausible completions of completely unobserved regions.},
  YEAR = {2021},
  URL = {http://arxiv.org/abs/2104.00677v1},
  FILE = {2104.00677v1.pdf}
 }",Few-shot reconstruction,"Lifting 2D features to 3D, Data-driven",,,,,,,,,,ARXIV 2021,,,,"Ajay Jain, Matthew Tancik, Pieter Abbeel",jain2021dietnerf,00000099,"We present DietNeRF, a 3D neural scene representation estimated from a few images. Neural Radiance Fields (NeRF) learn a continuous volumetric representation of a scene through multi-view consistency, and can be rendered from novel viewpoints by ray casting. While NeRF has an impressive ability to reconstruct geometry and fine details given many images, up to 100 for challenging 360{\deg} scenes, it often finds a degenerate solution to its image reconstruction objective when only a few input views are available. To improve few-shot quality, we propose DietNeRF. We introduce an auxiliary semantic consistency loss that encourages realistic renderings at novel poses. DietNeRF is trained on individual scenes to (1) correctly render given input views from the same pose, and (2) match high-level semantic attributes across different, random poses. Our semantic loss allows us to supervise DietNeRF from arbitrary poses. We extract these semantics using a pre-trained visual encoder such as CLIP, a Vision Transformer trained on hundreds of millions of diverse single-view, 2D photographs mined from the web with natural language supervision. In experiments, DietNeRF improves the perceptual quality of few-shot view synthesis when learned from scratch, can render novel views with as few as one observed image when pre-trained on a multi-view dataset, and produces plausible completions of completely unobserved regions.",0,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnuf9bV4EhH_BpR0snxpYou2lFMdXYIxbZsm6v9GnOWT4ZYFVemQPUpWK2aI0fBc-d5o
7/19/2021 21:26:43,Multi-scene Representation Learning with Neural Radiance Fields,,4/1/2021,https://iopscience.iop.org/article/10.1088/1742-6596/1880/1/012034,,,,,,,"@article{fu2021multiscene,
  ID = {fu2021multiscene},
  ENTRYTYPE = {article},
  DOI = {10.1088/1742-6596/1880/1/012034},
  URL = {https://doi.org/10.1088/1742-6596/1880/1/012034},
  YEAR = {2021},
  PUBLISHER = {{IOP} Publishing},
  VOLUME = {1880},
  NUMBER = {1},
  PAGES = {012034},
  AUTHOR = {Bofeng Fu and Zheng Wang},
  TITLE = {Multi-scene Representation Learning with Neural Radiance Fields},
  JOURNAL = {Journal of Physics: Conference Series},
  ABSTRACT = {Getting representations of multiple objects or scenes is a raising research topic in Machine Learning (ML) community. Here, we propose a multi-scene representation model that can learn the representation of complex scenes and reconstruct them in high resolution given novel viewing directions. Our method represents a single scene with fully-connected layers. Each set of fully-connected layers are controlled by hyper-networks for multiple scenes modeling. For each scene, we take 3D coordinates (x, y, z) and 2D view-point orientations (I,, E,) as inputs. A set of fully-connected layers output volume density and RGB values at given 3D spatial positions. Then, we render the output volume density and RGB values along the camera rays into images using volume density rendering techniques. During training process, we optimize a continuous volume scene function with a small amount of input viewing directions. By designing versatile embedding module and multi-scene representation networks, our model can render photographic images with novel viewing directions for different complex scenes. Experiment results demonstrate the neural rendering and multi-scene representation abilities of our model. Several thorough experiments show that our method outperforms previous model on both reconstruction precision and scenes generation ability from novel viewing directions.}
 }",Generalization,,NeRF,Density,,,,,,,,Journal of Physics: Conference Series 2021,,No,Direct,"Bofeng Fu, Zheng Wang",fu2021multiscene,00000100,"Getting representations of multiple objects or scenes is a raising research topic in Machine Learning (ML) community. Here, we propose a multi-scene representation model that can learn the representation of complex scenes and reconstruct them in high resolution given novel viewing directions. Our method represents a single scene with fully-connected layers. Each set of fully-connected layers are controlled by hyper-networks for multiple scenes modeling. For each scene, we take 3D coordinates (x, y, z) and 2D view-point orientations (I,, E,) as inputs. A set of fully-connected layers output volume density and RGB values at given 3D spatial positions. Then, we render the output volume density and RGB values along the camera rays into images using volume density rendering techniques. During training process, we optimize a continuous volume scene function with a small amount of input viewing directions. By designing versatile embedding module and multi-scene representation networks, our model can render photographic images with novel viewing directions for different complex scenes. Experiment results demonstrate the neural rendering and multi-scene representation abilities of our model. Several thorough experiments show that our method outperforms previous model on both reconstruction precision and scenes generation ability from novel viewing directions.",0,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnucu9xHWkIXpEh4VGdR-1xqOAzS2iZigXAhZ7in2kHougqMb6_HoRxNSg2B8DjN4ZTc
5/23/2021 18:37:37,Decomposing 3D Scenes into Objects via Unsupervised Volume Segmentation,OBSuRF,4/2/2021,https://arxiv.org/pdf/2104.01148.pdf,https://stelzner.github.io/obsurf/,Coming soon,,,,,"@article{stelzner2021obsurf,
  JOURNAL = {arXiv preprint arXiv:2104.01148},
  BOOKTITLE = {ArXiv Pre-print},
  ID = {stelzner2021obsurf},
  ENTRYTYPE = {article},
  AUTHOR = {Karl Stelzner and Kristian Kersting and Adam R. Kosiorek},
  TITLE = {Decomposing 3D Scenes into Objects via Unsupervised Volume Segmentation},
  EPRINT = {2104.01148v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We present ObSuRF, a method which turns a single image of a scene into a 3D model represented as a set of Neural Radiance Fields (NeRFs), with each NeRF corresponding to a different object. A single forward pass of an encoder network outputs a set of latent vectors describing the objects in the scene. These vectors are used independently to condition a NeRF decoder, defining the geometry and appearance of each object. We make learning more computationally efficient by deriving a novel loss, which allows training NeRFs on RGB-D inputs without explicit ray marching. After confirming that the model performs equal or better than state of the art on three 2D image segmentation benchmarks, we apply it to two multi-object 3D datasets: A multiview version of CLEVR, and a novel dataset in which scenes are populated by ShapeNet models. We find that after training ObSuRF on RGB-D views of training scenes, it is capable of not only recovering the 3D geometry of a scene depicted in a single input image, but also to segment it into objects, despite receiving no supervision in that regard.},
  YEAR = {2021},
  URL = {http://arxiv.org/abs/2104.01148v1},
  FILE = {2104.01148v1.pdf}
 }",Segmentation/composition,"Conditional neural field, Volume partitioning, Object-centric representation",,,,,,,,,,ARXIV 2021,Coming soon,,,"Karl Stelzner, Kristian Kersting, Adam R. Kosiorek",stelzner2021obsurf,00000101,"We present ObSuRF, a method which turns a single image of a scene into a 3D model represented as a set of Neural Radiance Fields (NeRFs), with each NeRF corresponding to a different object. A single forward pass of an encoder network outputs a set of latent vectors describing the objects in the scene. These vectors are used independently to condition a NeRF decoder, defining the geometry and appearance of each object. We make learning more computationally efficient by deriving a novel loss, which allows training NeRFs on RGB-D inputs without explicit ray marching. After confirming that the model performs equal or better than state of the art on three 2D image segmentation benchmarks, we apply it to two multi-object 3D datasets: A multiview version of CLEVR, and a novel dataset in which scenes are populated by ShapeNet models. We find that after training ObSuRF on RGB-D views of training scenes, it is capable of not only recovering the 3D geometry of a scene depicted in a single input image, but also to segment it into objects, despite receiving no supervision in that regard.",1,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnufwTgfSTzHUXiHcwsfPmGXn3_oJmEf3RAfVdd4_0BDZMiV9hf_Vl6zQbXtaF7qi4Ik
5/23/2021 18:38:52,Synergies Between Affordance and Geometry: 6-DoF Grasp Detection via Implicit Representations,GIGA,4/4/2021,https://arxiv.org/pdf/2104.01542.pdf,https://sites.google.com/view/rpl-giga2021,https://github.com/UT-Austin-RPL/GIGA,,,,,"@inproceedings{jiang2021giga,
  BOOKTITLE = {Proceedings of Robotics: Science and Systems},
  ID = {jiang2021giga},
  ENTRYTYPE = {inproceedings},
  AUTHOR = {Zhenyu Jiang and Yifeng Zhu and Maxwell Svetlik and Kuan Fang and Yuke Zhu},
  TITLE = {Synergies Between Affordance and Geometry: 6-DoF Grasp Detection via Implicit Representations},
  EPRINT = {2104.01542v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.RO},
  ABSTRACT = {Grasp detection in clutter requires the robot to reason about the 3D scene from incomplete and noisy perception. In this work, we draw insight that 3D reconstruction and grasp learning are two intimately connected tasks, both of which require a fine-grained understanding of local geometry details. We thus propose to utilize the synergies between grasp affordance and 3D reconstruction through multi-task learning of a shared representation. Our model takes advantage of deep implicit functions, a continuous and memory-efficient representation, to enable differentiable training of both tasks. We train the model on self-supervised grasp trials data in simulation. Evaluation is conducted on a clutter removal task, where the robot clears cluttered objects by grasping them one at a time. The experimental results in simulation and on the real robot have demonstrated that the use of implicit neural representations and joint learning of grasp affordance and 3D reconstruction have led to state-of-the-art grasping results. Our method outperforms baselines by over 10% in terms of grasp success rate. Additional results and videos can be found at https://sites.google.com/view/rpl-giga2021},
  YEAR = {2021},
  URL = {http://arxiv.org/abs/2104.01542v2},
  FILE = {2104.01542v2.pdf}
 }","Generalization, Beyond graphics, Science and engineering, Robotics","Voxelization, Feature volume, Data-driven",,,,,,,,,,RSS 2021,,,,"Zhenyu Jiang, Yifeng Zhu, Maxwell Svetlik, Kuan Fang, Yuke Zhu",jiang2021giga,00000102,"Grasp detection in clutter requires the robot to reason about the 3D scene from incomplete and noisy perception. In this work, we draw insight that 3D reconstruction and grasp learning are two intimately connected tasks, both of which require a fine-grained understanding of local geometry details. We thus propose to utilize the synergies between grasp affordance and 3D reconstruction through multi-task learning of a shared representation. Our model takes advantage of deep implicit functions, a continuous and memory-efficient representation, to enable differentiable training of both tasks. We train the model on self-supervised grasp trials data in simulation. Evaluation is conducted on a clutter removal task, where the robot clears cluttered objects by grasping them one at a time. The experimental results in simulation and on the real robot have demonstrated that the use of implicit neural representations and joint learning of grasp affordance and 3D reconstruction have led to state-of-the-art grasping results. Our method outperforms baselines by over 10% in terms of grasp success rate. Additional results and videos can be found at https://sites.google.com/view/rpl-giga2021",1,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnuciscMB2BnQ3IwmONuEWTrWooUtw-w9rFyXIpRhpomlrhUij3cMK7xurOt4OjcvM10
5/23/2021 18:18:26,pi-GAN: Periodic Implicit Generative Adversarial Networks for 3D-Aware Image Synthesis,pi-GAN,4/5/2021,https://arxiv.org/pdf/2012.00926.pdf,https://marcoamonteiro.github.io/pi-GAN-website/,Coming soon,https://www.youtube.com/watch?v=0HCdof9BGtw,,,,"@inproceedings{chan2021pigan,
  BOOKTITLE = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  ID = {chan2021pigan},
  ENTRYTYPE = {inproceedings},
  AUTHOR = {Eric R. Chan and Marco Monteiro and Petr Kellnhofer and Jiajun Wu and Gordon Wetzstein},
  TITLE = {pi-GAN: Periodic Implicit Generative Adversarial Networks for 3D-Aware Image Synthesis},
  EPRINT = {2012.00926v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We have witnessed rapid progress on 3D-aware image synthesis, leveraging recent advances in generative visual models and neural rendering. Existing approaches however fall short in two ways: first, they may lack an underlying 3D representation or rely on view-inconsistent rendering, hence synthesizing images that are not multi-view consistent; second, they often depend upon representation network architectures that are not expressive enough, and their results thus lack in image quality. We propose a novel generative model, named Periodic Implicit Generative Adversarial Networks ($\pi$-GAN or pi-GAN), for high-quality 3D-aware image synthesis. $\pi$-GAN leverages neural representations with periodic activation functions and volumetric rendering to represent scenes as view-consistent 3D representations with fine detail. The proposed approach obtains state-of-the-art results for 3D-aware image synthesis with multiple real and synthetic datasets.},
  YEAR = {2021},
  URL = {http://arxiv.org/abs/2012.00926v2},
  FILE = {2012.00926v2.pdf}
 }",Generalization,"Generative/adversarial formulation, Conditional neural field",SIREN,,,,,,,,,CVPR 2021,,,,"Eric R. Chan, Marco Monteiro, Petr Kellnhofer, Jiajun Wu, Gordon Wetzstein",chan2021pigan,00000103,"We have witnessed rapid progress on 3D-aware image synthesis, leveraging recent advances in generative visual models and neural rendering. Existing approaches however fall short in two ways: first, they may lack an underlying 3D representation or rely on view-inconsistent rendering, hence synthesizing images that are not multi-view consistent; second, they often depend upon representation network architectures that are not expressive enough, and their results thus lack in image quality. We propose a novel generative model, named Periodic Implicit Generative Adversarial Networks ($\pi$-GAN or pi-GAN), for high-quality 3D-aware image synthesis. $\pi$-GAN leverages neural representations with periodic activation functions and volumetric rendering to represent scenes as view-consistent 3D representations with fine detail. The proposed approach obtains state-of-the-art results for 3D-aware image synthesis with multiple real and synthetic datasets.",20,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnuefyKLYDFPTgEe9z6g59cIDu2YqjbwgjXgH-j5aRzVtk2fOHbnbCLnE0mtvwqr0fBs
5/23/2021 18:35:55,Convolutional Neural Opacity Radiance Fields,,4/5/2021,https://arxiv.org/pdf/2104.01772.pdf,,,,,,,"@article{luo2021convolutional,
  ID = {luo2021convolutional},
  ENTRYTYPE = {article},
  AUTHOR = {Haimin Luo and Anpei Chen and Qixuan Zhang and Bai Pang and Minye Wu and Lan Xu and Jingyi Yu},
  TITLE = {Convolutional Neural Opacity Radiance Fields},
  EPRINT = {2104.01772v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Photo-realistic modeling and rendering of fuzzy objects with complex opacity are critical for numerous immersive VR/AR applications, but it suffers from strong view-dependent brightness, color. In this paper, we propose a novel scheme to generate opacity radiance fields with a convolutional neural renderer for fuzzy objects, which is the first to combine both explicit opacity supervision and convolutional mechanism into the neural radiance field framework so as to enable high-quality appearance and global consistent alpha mattes generation in arbitrary novel views. More specifically, we propose an efficient sampling strategy along with both the camera rays and image plane, which enables efficient radiance field sampling and learning in a patch-wise manner, as well as a novel volumetric feature integration scheme that generates per-patch hybrid feature embeddings to reconstruct the view-consistent fine-detailed appearance and opacity output. We further adopt a patch-wise adversarial training scheme to preserve both high-frequency appearance and opacity details in a self-supervised framework. We also introduce an effective multi-view image capture system to capture high-quality color and alpha maps for challenging fuzzy objects. Extensive experiments on existing and our new challenging fuzzy object dataset demonstrate that our method achieves photo-realistic, globally consistent, and fined detailed appearance and opacity free-viewpoint rendering for various fuzzy objects.},
  YEAR = {2021},
  MONTH = {Apr},
  URL = {http://arxiv.org/abs/2104.01772v1},
  FILE = {2104.01772v1.pdf}
 }","Performance (training), Performance (rendering), Challenging materials (fur, hair, transparency)","Generative/adversarial formulation, Sampling, Feature volume",,,,,,,,,,ICCP 2021,,,,"Haimin Luo, Anpei Chen, Qixuan Zhang, Bai Pang, Minye Wu, Lan Xu, Jingyi Yu",luo2021convolutional,00000104,"Photo-realistic modeling and rendering of fuzzy objects with complex opacity are critical for numerous immersive VR/AR applications, but it suffers from strong view-dependent brightness, color. In this paper, we propose a novel scheme to generate opacity radiance fields with a convolutional neural renderer for fuzzy objects, which is the first to combine both explicit opacity supervision and convolutional mechanism into the neural radiance field framework so as to enable high-quality appearance and global consistent alpha mattes generation in arbitrary novel views. More specifically, we propose an efficient sampling strategy along with both the camera rays and image plane, which enables efficient radiance field sampling and learning in a patch-wise manner, as well as a novel volumetric feature integration scheme that generates per-patch hybrid feature embeddings to reconstruct the view-consistent fine-detailed appearance and opacity output. We further adopt a patch-wise adversarial training scheme to preserve both high-frequency appearance and opacity details in a self-supervised framework. We also introduce an effective multi-view image capture system to capture high-quality color and alpha maps for challenging fuzzy objects. Extensive experiments on existing and our new challenging fuzzy object dataset demonstrate that our method achieves photo-realistic, globally consistent, and fined detailed appearance and opacity free-viewpoint rendering for various fuzzy objects.",2,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnuegXH4CBWNzgGPhWNq-iLOXKT9w-QM6mPH_iUfkE6YbuF8xpEbTQ74Kg3LmaxWWIxg
5/23/2021 18:36:18,MirrorNeRF: One-shot Neural Portrait Radiance Field from Multi-mirror Catadioptric Imaging,MirrorNeRF,4/6/2021,https://arxiv.org/pdf/2104.02607.pdf,,,,,,,"@article{wang2021mirrornerf,
  JOURNAL = {arXiv preprint arXiv:2104.02607},
  BOOKTITLE = {ArXiv Pre-print},
  ID = {wang2021mirrornerf},
  ENTRYTYPE = {article},
  AUTHOR = {Ziyu Wang and Liao Wang and Fuqiang Zhao and Minye Wu and Lan Xu and Jingyi Yu},
  TITLE = {MirrorNeRF: One-shot Neural Portrait Radiance Field from Multi-mirror Catadioptric Imaging},
  EPRINT = {2104.02607v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Photo-realistic neural reconstruction and rendering of the human portrait are critical for numerous VR/AR applications. Still, existing solutions inherently rely on multi-view capture settings, and the one-shot solution to get rid of the tedious multi-view synchronization and calibration remains extremely challenging. In this paper, we propose MirrorNeRF - a one-shot neural portrait free-viewpoint rendering approach using a catadioptric imaging system with multiple sphere mirrors and a single high-resolution digital camera, which is the first to combine neural radiance field with catadioptric imaging so as to enable one-shot photo-realistic human portrait reconstruction and rendering, in a low-cost and casual capture setting. More specifically, we propose a light-weight catadioptric system design with a sphere mirror array to enable diverse ray sampling in the continuous 3D space as well as an effective online calibration for the camera and the mirror array. Our catadioptric imaging system can be easily deployed with a low budget and the casual capture ability for convenient daily usages. We introduce a novel neural warping radiance field representation to learn a continuous displacement field that implicitly compensates for the misalignment due to our flexible system setting. We further propose a density regularization scheme to leverage the inherent geometry information from the catadioptric data in a self-supervision manner, which not only improves the training efficiency but also provides more effective density supervision for higher rendering quality. Extensive experiments demonstrate the effectiveness and robustness of our scheme to achieve one-shot photo-realistic and high-quality appearance free-viewpoint rendering for human portrait scenes.},
  YEAR = {2021},
  URL = {http://arxiv.org/abs/2104.02607v2},
  FILE = {2104.02607v2.pdf}
 }",Few-shot reconstruction,Warping field/Flow field,,,,,,,,,,ARXIV 2021,,,,"Ziyu Wang, Liao Wang, Fuqiang Zhao, Minye Wu, Lan Xu, Jingyi Yu",wang2021mirrornerf,00000105,"Photo-realistic neural reconstruction and rendering of the human portrait are critical for numerous VR/AR applications. Still, existing solutions inherently rely on multi-view capture settings, and the one-shot solution to get rid of the tedious multi-view synchronization and calibration remains extremely challenging. In this paper, we propose MirrorNeRF - a one-shot neural portrait free-viewpoint rendering approach using a catadioptric imaging system with multiple sphere mirrors and a single high-resolution digital camera, which is the first to combine neural radiance field with catadioptric imaging so as to enable one-shot photo-realistic human portrait reconstruction and rendering, in a low-cost and casual capture setting. More specifically, we propose a light-weight catadioptric system design with a sphere mirror array to enable diverse ray sampling in the continuous 3D space as well as an effective online calibration for the camera and the mirror array. Our catadioptric imaging system can be easily deployed with a low budget and the casual capture ability for convenient daily usages. We introduce a novel neural warping radiance field representation to learn a continuous displacement field that implicitly compensates for the misalignment due to our flexible system setting. We further propose a density regularization scheme to leverage the inherent geometry information from the catadioptric data in a self-supervision manner, which not only improves the training efficiency but also provides more effective density supervision for higher rendering quality. Extensive experiments demonstrate the effectiveness and robustness of our scheme to achieve one-shot photo-realistic and high-quality appearance free-viewpoint rendering for human portrait scenes.",1,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnuf_EzpTaZxugyLXym6xQQU5A5fAAoZCyRoIG6kODIGoiPYZpqHC6joLPQo6-H-4q8o
5/23/2021 18:34:59,Neural Articulated Radiance Field,NARF,4/7/2021,https://arxiv.org/pdf/2104.03110.pdf,,https://arxiv.org/pdf/2104.03110.pdf,,,,,"@inproceedings{noguchi2021narf,
  BOOKTITLE = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
  ID = {noguchi2021narf},
  ENTRYTYPE = {inproceedings},
  AUTHOR = {Atsuhiro Noguchi and Xiao Sun and Stephen Lin and Tatsuya Harada},
  TITLE = {Neural Articulated Radiance Field},
  EPRINT = {2104.03110v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We present Neural Articulated Radiance Field (NARF), a novel deformable 3D representation for articulated objects learned from images. While recent advances in 3D implicit representation have made it possible to learn models of complex objects, learning pose-controllable representations of articulated objects remains a challenge, as current methods require 3D shape supervision and are unable to render appearance. In formulating an implicit representation of 3D articulated objects, our method considers only the rigid transformation of the most relevant object part in solving for the radiance field at each 3D location. In this way, the proposed method represents pose-dependent changes without significantly increasing the computational complexity. NARF is fully differentiable and can be trained from images with pose annotations. Moreover, through the use of an autoencoder, it can learn appearance variations over multiple instances of an object class. Experiments show that the proposed method is efficient and can generalize well to novel poses. The code is available for research purposes at https://github.com/nogu-atsu/NARF},
  YEAR = {2021},
  URL = {http://arxiv.org/abs/2104.03110v2},
  FILE = {2104.03110v2.pdf}
 }",Human body,"Volume partitioning, Articulated",,,,,,,,,,ICCV 2021,,,,"Atsuhiro Noguchi, Xiao Sun, Stephen Lin, Tatsuya Harada",noguchi2021narf,00000106,"We present Neural Articulated Radiance Field (NARF), a novel deformable 3D representation for articulated objects learned from images. While recent advances in 3D implicit representation have made it possible to learn models of complex objects, learning pose-controllable representations of articulated objects remains a challenge, as current methods require 3D shape supervision and are unable to render appearance. In formulating an implicit representation of 3D articulated objects, our method considers only the rigid transformation of the most relevant object part in solving for the radiance field at each 3D location. In this way, the proposed method represents pose-dependent changes without significantly increasing the computational complexity. NARF is fully differentiable and can be trained from images with pose annotations. Moreover, through the use of an autoencoder, it can learn appearance variations over multiple instances of an object class. Experiments show that the proposed method is efficient and can generalize well to novel poses. The code is available for research purposes at https://github.com/nogu-atsu/NARF",2,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnufJXCAm0adjUHR5b5cFRiV_BK7clNsyykOqxtTUOcO3f4y0UCshGMQptgiAE6oVRt4
7/19/2021 22:03:42,SCANimate: Weakly Supervised Learning of Skinned Clothed Avatar Networks,SCANimate,4/7/2021,https://arxiv.org/pdf/2104.03313.pdf,https://scanimate.is.tue.mpg.de,https://github.com/shunsukesaito/SCANimate,https://www.youtube.com/watch?v=ohavL55Oznw,,,,"@inproceedings{saito2021scanimate,
  BOOKTITLE = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  ID = {saito2021scanimate},
  ENTRYTYPE = {inproceedings},
  AUTHOR = {Shunsuke Saito and Jinlong Yang and Qianli Ma and Michael J. Black},
  TITLE = {SCANimate: Weakly Supervised Learning of Skinned Clothed Avatar Networks},
  EPRINT = {2104.03313v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We present SCANimate, an end-to-end trainable framework that takes raw 3D scans of a clothed human and turns them into an animatable avatar. These avatars are driven by pose parameters and have realistic clothing that moves and deforms naturally. SCANimate does not rely on a customized mesh template or surface mesh registration. We observe that fitting a parametric 3D body model, like SMPL, to a clothed human scan is tractable while surface registration of the body topology to the scan is often not, because clothing can deviate significantly from the body shape. We also observe that articulated transformations are invertible, resulting in geometric cycle consistency in the posed and unposed shapes. These observations lead us to a weakly supervised learning method that aligns scans into a canonical pose by disentangling articulated deformations without template-based surface registration. Furthermore, to complete missing regions in the aligned scans while modeling pose-dependent deformations, we introduce a locally pose-aware implicit function that learns to complete and model geometry with learned pose correctives. In contrast to commonly used global pose embeddings, our local pose conditioning significantly reduces long-range spurious correlations and improves generalization to unseen poses, especially when training data is limited. Our method can be applied to pose-aware appearance modeling to generate a fully textured avatar. We demonstrate our approach on various clothing types with different amounts of training data, outperforming existing solutions and other variants in terms of fidelity and generality in every setting. The code is available at https://scanimate.is.tue.mpg.de.},
  YEAR = {2021},
  URL = {http://arxiv.org/abs/2104.03313v2},
  FILE = {2104.03313v2.pdf}
 }","Dynamic, Human body",Warping field/Flow field,,SDF,Category-level,,,,,,,CVPR 2021,,No,Direct,"Shunsuke Saito, Jinlong Yang, Qianli Ma, Michael J. Black",saito2021scanimate,00000107,"We present SCANimate, an end-to-end trainable framework that takes raw 3D scans of a clothed human and turns them into an animatable avatar. These avatars are driven by pose parameters and have realistic clothing that moves and deforms naturally. SCANimate does not rely on a customized mesh template or surface mesh registration. We observe that fitting a parametric 3D body model, like SMPL, to a clothed human scan is tractable while surface registration of the body topology to the scan is often not, because clothing can deviate significantly from the body shape. We also observe that articulated transformations are invertible, resulting in geometric cycle consistency in the posed and unposed shapes. These observations lead us to a weakly supervised learning method that aligns scans into a canonical pose by disentangling articulated deformations without template-based surface registration. Furthermore, to complete missing regions in the aligned scans while modeling pose-dependent deformations, we introduce a locally pose-aware implicit function that learns to complete and model geometry with learned pose correctives. In contrast to commonly used global pose embeddings, our local pose conditioning significantly reduces long-range spurious correlations and improves generalization to unseen poses, especially when training data is limited. Our method can be applied to pose-aware appearance modeling to generate a fully textured avatar. We demonstrate our approach on various clothing types with different amounts of training data, outperforming existing solutions and other variants in terms of fidelity and generality in every setting. The code is available at https://scanimate.is.tue.mpg.de.",7,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnuf6FHUIMGX2zGoEdRbaVh_jLkkewLeC4BOT4AYi2ut7vVZAuCBdiU7wVfgMWP_x7nw
5/23/2021 18:33:32,Direct-PoseNet: Absolute Pose Regression with Photometric Consistency,Direct-PoseNet,4/8/2021,https://arxiv.org/pdf/2104.04073.pdf,,,,,,,"@article{chen2021directposenet,
  JOURNAL = {arXiv preprint arXiv:2104.04073},
  BOOKTITLE = {ArXiv Pre-print},
  ID = {chen2021directposenet},
  ENTRYTYPE = {article},
  AUTHOR = {Shuai Chen and Zirui Wang and Victor Prisacariu},
  TITLE = {Direct-PoseNet: Absolute Pose Regression with Photometric Consistency},
  EPRINT = {2104.04073v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We present a relocalization pipeline, which combines an absolute pose regression (APR) network with a novel view synthesis based direct matching module, offering superior accuracy while maintaining low inference time. Our contribution is twofold: i) we design a direct matching module that supplies a photometric supervision signal to refine the pose regression network via differentiable rendering; ii) we modify the rotation representation from the classical quaternion to SO(3) in pose regression, removing the need for balancing rotation and translation loss terms. As a result, our network Direct-PoseNet achieves state-of-the-art performance among all other single-image APR methods on the 7-Scenes benchmark and the LLFF dataset.},
  YEAR = {2021},
  URL = {http://arxiv.org/abs/2104.04073v1},
  FILE = {2104.04073v1.pdf}
 }",Camera parameter estimation,Coarse-to-fine,,,,,,,,,,ARXIV 2021,,,,"Shuai Chen, Zirui Wang, Victor Prisacariu",chen2021directposenet,00000108,"We present a relocalization pipeline, which combines an absolute pose regression (APR) network with a novel view synthesis based direct matching module, offering superior accuracy while maintaining low inference time. Our contribution is twofold: i) we design a direct matching module that supplies a photometric supervision signal to refine the pose regression network via differentiable rendering; ii) we modify the rotation representation from the classical quaternion to SO(3) in pose regression, removing the need for balancing rotation and translation loss terms. As a result, our network Direct-PoseNet achieves state-of-the-art performance among all other single-image APR methods on the 7-Scenes benchmark and the LLFF dataset.",0,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnucGkYBJgD2btAQRyl7FtpVlnEX02OGft3JXp9_-ZoP_WHpxYgLpwcOEAcFelwJiLnQ
5/23/2021 18:35:19,SNARF: Differentiable Forward Skinning for Animating Non-Rigid Neural Implicit Shapes,SNARF,4/8/2021,https://arxiv.org/pdf/2104.03953.pdf,,,,,,,"@inproceedings{chen2021snarf,
  BOOKTITLE = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
  ID = {chen2021snarf},
  ENTRYTYPE = {inproceedings},
  AUTHOR = {Xu Chen and Yufeng Zheng and Michael J. Black and Otmar Hilliges and Andreas Geiger},
  TITLE = {SNARF: Differentiable Forward Skinning for Animating Non-Rigid Neural Implicit Shapes},
  EPRINT = {2104.03953v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Neural implicit surface representations have emerged as a promising paradigm to capture 3D shapes in a continuous and resolution-independent manner. However, adapting them to articulated shapes is non-trivial. Existing approaches learn a backward warp field that maps deformed to canonical points. However, this is problematic since the backward warp field is pose dependent and thus requires large amounts of data to learn. To address this, we introduce SNARF, which combines the advantages of linear blend skinning (LBS) for polygonal meshes with those of neural implicit surfaces by learning a forward deformation field without direct supervision. This deformation field is defined in canonical, pose-independent space, allowing for generalization to unseen poses. Learning the deformation field from posed meshes alone is challenging since the correspondences of deformed points are defined implicitly and may not be unique under changes of topology. We propose a forward skinning model that finds all canonical correspondences of any deformed point using iterative root finding. We derive analytical gradients via implicit differentiation, enabling end-to-end training from 3D meshes with bone transformations. Compared to state-of-the-art neural implicit representations, our approach generalizes better to unseen poses while preserving accuracy. We demonstrate our method in challenging scenarios on (clothed) 3D humans in diverse and unseen poses.},
  YEAR = {2021},
  URL = {http://arxiv.org/abs/2104.03953v1},
  FILE = {2104.03953v1.pdf}
 }",Human body,"Articulated, Warping field/Flow field",,,,,,,,,,ICCV 2021,,,,"Xu Chen, Yufeng Zheng, Michael J. Black, Otmar Hilliges, Andreas Geiger",chen2021snarf,00000109,"Neural implicit surface representations have emerged as a promising paradigm to capture 3D shapes in a continuous and resolution-independent manner. However, adapting them to articulated shapes is non-trivial. Existing approaches learn a backward warp field that maps deformed to canonical points. However, this is problematic since the backward warp field is pose dependent and thus requires large amounts of data to learn. To address this, we introduce SNARF, which combines the advantages of linear blend skinning (LBS) for polygonal meshes with those of neural implicit surfaces by learning a forward deformation field without direct supervision. This deformation field is defined in canonical, pose-independent space, allowing for generalization to unseen poses. Learning the deformation field from posed meshes alone is challenging since the correspondences of deformed points are defined implicitly and may not be unique under changes of topology. We propose a forward skinning model that finds all canonical correspondences of any deformed point using iterative root finding. We derive analytical gradients via implicit differentiation, enabling end-to-end training from 3D meshes with bone transformations. Compared to state-of-the-art neural implicit representations, our approach generalizes better to unseen poses while preserving accuracy. We demonstrate our method in challenging scenarios on (clothed) 3D humans in diverse and unseen poses.",1,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnueGP67Mvo9TzTIqQLIWZlIw7VvsVvs-kJDyxRzPsor0-QbEiglD183zR2sKT5M_O5s
5/23/2021 18:35:38,Modulated Periodic Activations for Generalizable Local Functional Representations,,4/8/2021,https://arxiv.org/pdf/2104.03960.pdf,,,,,,,"@inproceedings{mehta2021modulated,
  BOOKTITLE = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
  ID = {mehta2021modulated},
  ENTRYTYPE = {inproceedings},
  AUTHOR = {Ishit Mehta and Michael Gharbi and Connelly Barnes and Eli Shechtman and Ravi Ramamoorthi and Manmohan Chandraker},
  TITLE = {Modulated Periodic Activations for Generalizable Local Functional Representations},
  EPRINT = {2104.03960v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Multi-Layer Perceptrons (MLPs) make powerful functional representations for sampling and reconstruction problems involving low-dimensional signals like images,shapes and light fields. Recent works have significantly improved their ability to represent high-frequency content by using periodic activations or positional encodings. This often came at the expense of generalization: modern methods are typically optimized for a single signal. We present a new representation that generalizes to multiple instances and achieves state-of-the-art fidelity. We use a dual-MLP architecture to encode the signals. A synthesis network creates a functional mapping from a low-dimensional input (e.g. pixel-position) to the output domain (e.g. RGB color). A modulation network maps a latent code corresponding to the target signal to parameters that modulate the periodic activations of the synthesis network. We also propose a local-functional representation which enables generalization. The signal's domain is partitioned into a regular grid,with each tile represented by a latent code. At test time, the signal is encoded with high-fidelity by inferring (or directly optimizing) the latent code-book. Our approach produces generalizable functional representations of images, videos and shapes, and achieves higher reconstruction quality than prior works that are optimized for a single signal.},
  YEAR = {2021},
  URL = {http://arxiv.org/abs/2104.03960v1},
  FILE = {2104.03960v1.pdf}
 }","Generalization, Compression, Fundamentals","Conditional neural field, Hypernetwork",Other,,,,,,,,,ICCV 2021,,,,"Ishit Mehta, Michaël Gharbi, Connelly Barnes, Eli Shechtman, Ravi Ramamoorthi, Manmohan Chandraker",mehta2021modulated,00000110,"Multi-Layer Perceptrons (MLPs) make powerful functional representations for sampling and reconstruction problems involving low-dimensional signals like images,shapes and light fields. Recent works have significantly improved their ability to represent high-frequency content by using periodic activations or positional encodings. This often came at the expense of generalization: modern methods are typically optimized for a single signal. We present a new representation that generalizes to multiple instances and achieves state-of-the-art fidelity. We use a dual-MLP architecture to encode the signals. A synthesis network creates a functional mapping from a low-dimensional input (e.g. pixel-position) to the output domain (e.g. RGB color). A modulation network maps a latent code corresponding to the target signal to parameters that modulate the periodic activations of the synthesis network. We also propose a local-functional representation which enables generalization. The signal's domain is partitioned into a regular grid,with each tile represented by a latent code. At test time, the signal is encoded with high-fidelity by inferring (or directly optimizing) the latent code-book. Our approach produces generalizable functional representations of images, videos and shapes, and achieves higher reconstruction quality than prior works that are optimized for a single signal.",1,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnuej0-noezBbvYqSep5kkbTiTzqPdYZpJ2ulBWNenotI3zyvhSv89-YIDqRoX6Qawzk
5/23/2021 18:34:02,Neural RGB-D Surface Reconstruction,,4/9/2021,https://arxiv.org/pdf/2104.04532.pdf,,,,,,,"@article{azinovic2021neural,
  JOURNAL = {arXiv preprint arXiv:2104.04532},
  BOOKTITLE = {ArXiv Pre-print},
  ID = {azinovic2021neural},
  ENTRYTYPE = {article},
  AUTHOR = {Dejan Azinovic and Ricardo Martin-Brualla and Dan B Goldman and Matthias Niessner and Justus Thies},
  TITLE = {Neural RGB-D Surface Reconstruction},
  EPRINT = {2104.04532v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {In this work, we explore how to leverage the success of implicit novel view synthesis methods for surface reconstruction. Methods which learn a neural radiance field have shown amazing image synthesis results, but the underlying geometry representation is only a coarse approximation of the real geometry. We demonstrate how depth measurements can be incorporated into the radiance field formulation to produce more detailed and complete reconstruction results than using methods based on either color or depth data alone. In contrast to a density field as the underlying geometry representation, we propose to learn a deep neural network which stores a truncated signed distance field. Using this representation, we show that one can still leverage differentiable volume rendering to estimate color values of the observed images during training to compute a reconstruction loss. This is beneficial for learning the signed distance field in regions with missing depth measurements. Furthermore, we correct misalignment errors of the camera, improving the overall reconstruction quality. In several experiments, we showcase our method and compare to existing works on classical RGB-D fusion and learned representations.},
  YEAR = {2021},
  URL = {http://arxiv.org/abs/2104.04532v1},
  FILE = {2104.04532v1.pdf}
 }",Camera parameter estimation,Conditional neural field,,,,,,,,,,ARXIV 2021,,,,"Dejan Azinović, Ricardo Martin-Brualla, Dan B Goldman, Matthias Nießner, Justus Thies",azinovic2021neural,00000111,"In this work, we explore how to leverage the success of implicit novel view synthesis methods for surface reconstruction. Methods which learn a neural radiance field have shown amazing image synthesis results, but the underlying geometry representation is only a coarse approximation of the real geometry. We demonstrate how depth measurements can be incorporated into the radiance field formulation to produce more detailed and complete reconstruction results than using methods based on either color or depth data alone. In contrast to a density field as the underlying geometry representation, we propose to learn a deep neural network which stores a truncated signed distance field. Using this representation, we show that one can still leverage differentiable volume rendering to estimate color values of the observed images during training to compute a reconstruction loss. This is beneficial for learning the signed distance field in regions with missing depth measurements. Furthermore, we correct misalignment errors of the camera, improving the overall reconstruction quality. In several experiments, we showcase our method and compare to existing works on classical RGB-D fusion and learned representations.",1,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnudWb1Zvxf4nelXco5DMwNyKC4si49QcNwZVlPRvB8L1Q28g5gyTAopzBbVRB4ASSBk
5/23/2021 18:33:47,Compressive Neural Representations of Volumetric Scalar Fields,,4/11/2021,https://arxiv.org/pdf/2104.04523.pdf,,,,,,,"@article{lu2021compressive,
  ID = {lu2021compressive},
  ENTRYTYPE = {article},
  AUTHOR = {Yuzhe Lu and Kairong Jiang and Joshua A. Levine and Matthew Berger},
  TITLE = {Compressive Neural Representations of Volumetric Scalar Fields},
  EPRINT = {2104.04523v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.LG},
  ABSTRACT = {We present an approach for compressing volumetric scalar fields using implicit neural representations. Our approach represents a scalar field as a learned function, wherein a neural network maps a point in the domain to an output scalar value. By setting the number of weights of the neural network to be smaller than the input size, we achieve compressed representations of scalar fields, thus framing compression as a type of function approximation. Combined with carefully quantizing network weights, we show that this approach yields highly compact representations that outperform state-of-the-art volume compression approaches. The conceptual simplicity of our approach enables a number of benefits, such as support for time-varying scalar fields, optimizing to preserve spatial gradients, and random-access field evaluation. We study the impact of network design choices on compression performance, highlighting how simple network architectures are effective for a broad range of volumes.},
  YEAR = {2021},
  MONTH = {Apr},
  URL = {http://arxiv.org/abs/2104.04523v1},
  FILE = {2104.04523v1.pdf}
 }","Dynamic, Compression, Beyond graphics, Fundamentals",,SIREN,,,,,,,,,EuroVis 2021,,,,"Yuzhe Lu, Kairong Jiang, Joshua A. Levine, Matthew Berger",lu2021compressive,00000112,"We present an approach for compressing volumetric scalar fields using implicit neural representations. Our approach represents a scalar field as a learned function, wherein a neural network maps a point in the domain to an output scalar value. By setting the number of weights of the neural network to be smaller than the input size, we achieve compressed representations of scalar fields, thus framing compression as a type of function approximation. Combined with carefully quantizing network weights, we show that this approach yields highly compact representations that outperform state-of-the-art volume compression approaches. The conceptual simplicity of our approach enables a number of benefits, such as support for time-varying scalar fields, optimizing to preserve spatial gradients, and random-access field evaluation. We study the impact of network design choices on compression performance, highlighting how simple network architectures are effective for a broad range of volumes.",1,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnuecHAkjoX655sRj0eSjXWAb4bulPmfscKWXmYbG1F52QG--j3d7he_ZR90zgfJgpRc
9/17/2021 14:04:23,StereoPIFu: Depth Aware Clothed Human Digitization via Stereo Vision,StereoPIFu,4/12/2021,https://arxiv.org/pdf/2104.05289.pdf,https://hy1995.top/StereoPIFuProject/,https://github.com/CrisHY1995/StereoPIFu_Code,,,,,"@inproceedings{hong2021stereopifu,
  BOOKTITLE = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  ID = {hong2021stereopifu},
  ENTRYTYPE = {inproceedings},
  AUTHOR = {Yang Hong and Juyong Zhang and Boyi Jiang and Yudong Guo and Ligang Liu and Hujun Bao},
  TITLE = {StereoPIFu: Depth Aware Clothed Human Digitization via Stereo Vision},
  EPRINT = {2104.05289v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {In this paper, we propose StereoPIFu, which integrates the geometric constraints of stereo vision with implicit function representation of PIFu, to recover the 3D shape of the clothed human from a pair of low-cost rectified images. First, we introduce the effective voxel-aligned features from a stereo vision-based network to enable depth-aware reconstruction. Moreover, the novel relative z-offset is employed to associate predicted high-fidelity human depth and occupancy inference, which helps restore fine-level surface details. Second, a network structure that fully utilizes the geometry information from the stereo images is designed to improve the human body reconstruction quality. Consequently, our StereoPIFu can naturally infer the human body's spatial location in camera space and maintain the correct relative position of different parts of the human body, which enables our method to capture human performance. Compared with previous works, our StereoPIFu significantly improves the robustness, completeness, and accuracy of the clothed human reconstruction, which is demonstrated by extensive experimental results.},
  YEAR = {2021},
  URL = {http://arxiv.org/abs/2104.05289v2},
  FILE = {2104.05289v2.pdf}
 }",Human body,"Conditional neural field, Lifting 2D features to 3D, Feature volume, Data-driven",,Occupancy,Category-level,,,,,,,CVPR 2021,,Yes,,"Yang Hong, Juyong Zhang, Boyi Jiang, Yudong Guo, Ligang Liu, Hujun Bao",hong2021stereopifu,00000204,"In this paper, we propose StereoPIFu, which integrates the geometric constraints of stereo vision with implicit function representation of PIFu, to recover the 3D shape of the clothed human from a pair of low-cost rectified images. First, we introduce the effective voxel-aligned features from a stereo vision-based network to enable depth-aware reconstruction. Moreover, the novel relative z-offset is employed to associate predicted high-fidelity human depth and occupancy inference, which helps restore fine-level surface details. Second, a network structure that fully utilizes the geometry information from the stereo images is designed to improve the human body reconstruction quality. Consequently, our StereoPIFu can naturally infer the human body's spatial location in camera space and maintain the correct relative position of different parts of the human body, which enables our method to capture human performance. Compared with previous works, our StereoPIFu significantly improves the robustness, completeness, and accuracy of the clothed human reconstruction, which is demonstrated by extensive experimental results.",,Yes,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnudTwuF-ycMxqvKOJjqB_7G0cL5KFYYkhSTpg2iOyeIqfDkj739ai5zCHByysGgk8G0
5/23/2021 18:34:37,BARF: Bundle-Adjusting Neural Radiance Fields,BARF,4/13/2021,https://arxiv.org/pdf/2104.06405.pdf,https://chenhsuanlin.bitbucket.io/bundle-adjusting-NeRF/,Coming soon,,,,,"@inproceedings{lin2021barf,
  BOOKTITLE = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
  ID = {lin2021barf},
  ENTRYTYPE = {inproceedings},
  AUTHOR = {Chen-Hsuan Lin and Wei-Chiu Ma and Antonio Torralba and Simon Lucey},
  TITLE = {BARF: Bundle-Adjusting Neural Radiance Fields},
  EPRINT = {2104.06405v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Neural Radiance Fields (NeRF) have recently gained a surge of interest within the computer vision community for its power to synthesize photorealistic novel views of real-world scenes. One limitation of NeRF, however, is its requirement of accurate camera poses to learn the scene representations. In this paper, we propose Bundle-Adjusting Neural Radiance Fields (BARF) for training NeRF from imperfect (or even unknown) camera poses -- the joint problem of learning neural 3D representations and registering camera frames. We establish a theoretical connection to classical image alignment and show that coarse-to-fine registration is also applicable to NeRF. Furthermore, we show that na\""ively applying positional encoding in NeRF has a negative impact on registration with a synthesis-based objective. Experiments on synthetic and real-world data show that BARF can effectively optimize the neural scene representations and resolve large camera pose misalignment at the same time. This enables view synthesis and localization of video sequences from unknown camera poses, opening up new avenues for visual localization systems (e.g. SLAM) and potential applications for dense 3D mapping and reconstruction.},
  YEAR = {2021},
  URL = {http://arxiv.org/abs/2104.06405v2},
  FILE = {2104.06405v2.pdf}
 }",Camera parameter estimation,Coarse-to-fine,,,,,,,,,,ICCV 2021 (Oral),,,,"Chen-Hsuan Lin, Wei-Chiu Ma, Antonio Torralba, Simon Lucey",lin2021barf,00000113,"Neural Radiance Fields (NeRF) have recently gained a surge of interest within the computer vision community for its power to synthesize photorealistic novel views of real-world scenes. One limitation of NeRF, however, is its requirement of accurate camera poses to learn the scene representations. In this paper, we propose Bundle-Adjusting Neural Radiance Fields (BARF) for training NeRF from imperfect (or even unknown) camera poses -- the joint problem of learning neural 3D representations and registering camera frames. We establish a theoretical connection to classical image alignment and show that coarse-to-fine registration is also applicable to NeRF. Furthermore, we show that na\""ively applying positional encoding in NeRF has a negative impact on registration with a synthesis-based objective. Experiments on synthetic and real-world data show that BARF can effectively optimize the neural scene representations and resolve large camera pose misalignment at the same time. This enables view synthesis and localization of video sequences from unknown camera poses, opening up new avenues for visual localization systems (e.g. SLAM) and potential applications for dense 3D mapping and reconstruction.",2,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnudrIYHcoUWJp9qbSFgbFepJU6VwimdVU1K5YsqoSrvcbmacbhXTaQQgqP5C4C1gYNc
5/23/2021 18:36:57,Stereo Radiance Fields (SRF): Learning View Synthesis for Sparse Views of Novel Scenes,SRF,4/14/2021,https://arxiv.org/pdf/2104.06935.pdf,https://virtualhumans.mpi-inf.mpg.de/srf/,,,https://arxiv.org/pdf/2104.06935.pdf,,,"@inproceedings{chibane2021srf,
  BOOKTITLE = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  ID = {chibane2021srf},
  ENTRYTYPE = {inproceedings},
  AUTHOR = {Julian Chibane and Aayush Bansal and Verica Lazova and Gerard Pons-Moll},
  TITLE = {Stereo Radiance Fields (SRF): Learning View Synthesis for Sparse Views of Novel Scenes},
  EPRINT = {2104.06935v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Recent neural view synthesis methods have achieved impressive quality and realism, surpassing classical pipelines which rely on multi-view reconstruction. State-of-the-Art methods, such as NeRF, are designed to learn a single scene with a neural network and require dense multi-view inputs. Testing on a new scene requires re-training from scratch, which takes 2-3 days. In this work, we introduce Stereo Radiance Fields (SRF), a neural view synthesis approach that is trained end-to-end, generalizes to new scenes, and requires only sparse views at test time. The core idea is a neural architecture inspired by classical multi-view stereo methods, which estimates surface points by finding similar image regions in stereo images. In SRF, we predict color and density for each 3D point given an encoding of its stereo correspondence in the input images. The encoding is implicitly learned by an ensemble of pair-wise similarities -- emulating classical stereo. Experiments show that SRF learns structure instead of overfitting on a scene. We train on multiple scenes of the DTU dataset and generalize to new ones without re-training, requiring only 10 sparse and spread-out views as input. We show that 10-15 minutes of fine-tuning further improve the results, achieving significantly sharper, more detailed results than scene-specific models. The code, model, and videos are available at https://virtualhumans.mpi-inf.mpg.de/srf/.},
  YEAR = {2021},
  NOTE = {IEEE Conference on Computer Vision and Pattern Recognition (CVPR) 2021},
  URL = {http://arxiv.org/abs/2104.06935v1},
  FILE = {2104.06935v1.pdf}
 }","Performance (training), Few-shot reconstruction, Generalization","Lifting 2D features to 3D, Image-based rendering",,,Universal,,,,,,,CVPR 2021,,,,"Julian Chibane, Aayush Bansal, Verica Lazova, Gerard Pons-Moll",chibane2021srf,00000114,"Recent neural view synthesis methods have achieved impressive quality and realism, surpassing classical pipelines which rely on multi-view reconstruction. State-of-the-Art methods, such as NeRF, are designed to learn a single scene with a neural network and require dense multi-view inputs. Testing on a new scene requires re-training from scratch, which takes 2-3 days. In this work, we introduce Stereo Radiance Fields (SRF), a neural view synthesis approach that is trained end-to-end, generalizes to new scenes, and requires only sparse views at test time. The core idea is a neural architecture inspired by classical multi-view stereo methods, which estimates surface points by finding similar image regions in stereo images. In SRF, we predict color and density for each 3D point given an encoding of its stereo correspondence in the input images. The encoding is implicitly learned by an ensemble of pair-wise similarities -- emulating classical stereo. Experiments show that SRF learns structure instead of overfitting on a scene. We train on multiple scenes of the DTU dataset and generalize to new ones without re-training, requiring only 10 sparse and spread-out views as input. We show that 10-15 minutes of fine-tuning further improve the results, achieving significantly sharper, more detailed results than scene-specific models. The code, model, and videos are available at https://virtualhumans.mpi-inf.mpg.de/srf/.",4,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnueeLT2w5w2u_Gs7GBsTUdnbi1AZONivCmQPtcDjbZdGLCE_jhbzYLmoCMOfoLds4m0
9/17/2021 11:50:12,LEAP: Learning Articulated Occupancy of People,LEAP,4/14/2021,https://arxiv.org/pdf/2104.06849.pdf,https://neuralbodies.github.io/LEAP/,https://github.com/neuralbodies/leap,https://www.youtube.com/watch?v=UVB8A_T5e3c,,,,"@inproceedings{mihajlovic2021leap,
  BOOKTITLE = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  ID = {mihajlovic2021leap},
  ENTRYTYPE = {inproceedings},
  AUTHOR = {Marko Mihajlovic and Yan Zhang and Michael J. Black and Siyu Tang},
  TITLE = {LEAP: Learning Articulated Occupancy of People},
  EPRINT = {2104.06849v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Substantial progress has been made on modeling rigid 3D objects using deep implicit representations. Yet, extending these methods to learn neural models of human shape is still in its infancy. Human bodies are complex and the key challenge is to learn a representation that generalizes such that it can express body shape deformations for unseen subjects in unseen, highly-articulated, poses. To address this challenge, we introduce LEAP (LEarning Articulated occupancy of People), a novel neural occupancy representation of the human body. Given a set of bone transformations (i.e. joint locations and rotations) and a query point in space, LEAP first maps the query point to a canonical space via learned linear blend skinning (LBS) functions and then efficiently queries the occupancy value via an occupancy network that models accurate identity- and pose-dependent deformations in the canonical space. Experiments show that our canonicalized occupancy estimation with the learned LBS functions greatly improves the generalization capability of the learned occupancy representation across various human shapes and poses, outperforming existing solutions in all settings.},
  YEAR = {2021},
  URL = {http://arxiv.org/abs/2104.06849v1},
  FILE = {2104.06849v1.pdf}
 }","Human body, Editable","Conditional neural field, Articulated, Warping field/Flow field, Data-driven",,Occupancy,Category-level,,,,,,,CVPR 2021,,Yes,Direct,"Marko Mihajlovic, Yan Zhang, Michael J. Black, Siyu Tang",mihajlovic2021leap,00000198,"Substantial progress has been made on modeling rigid 3D objects using deep implicit representations. Yet, extending these methods to learn neural models of human shape is still in its infancy. Human bodies are complex and the key challenge is to learn a representation that generalizes such that it can express body shape deformations for unseen subjects in unseen, highly-articulated, poses. To address this challenge, we introduce LEAP (LEarning Articulated occupancy of People), a novel neural occupancy representation of the human body. Given a set of bone transformations (i.e. joint locations and rotations) and a query point in space, LEAP first maps the query point to a canonical space via learned linear blend skinning (LBS) functions and then efficiently queries the occupancy value via an occupancy network that models accurate identity- and pose-dependent deformations in the canonical space. Experiments show that our canonicalized occupancy estimation with the learned LBS functions greatly improves the generalization capability of the learned occupancy representation across various human shapes and poses, outperforming existing solutions in all settings.",,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnufEHrZwA0Yi2K2FbnIRcDURoAq8An4EYleuQfu0oY4cAfYRQ0pxb123FFDj0_erSlk
5/23/2021 18:32:45,GANcraft: Unsupervised 3D Neural Rendering of Minecraft Worlds,GANcraft,4/15/2021,https://arxiv.org/pdf/2104.07659.pdf,https://nvlabs.github.io/GANcraft/,https://github.com/NVlabs/imaginaire,https://www.youtube.com/watch?v=1Hky092CGFQ,,,,"@inproceedings{hao2021gancraft,
  BOOKTITLE = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
  ID = {hao2021gancraft},
  ENTRYTYPE = {inproceedings},
  AUTHOR = {Zekun Hao and Arun Mallya and Serge Belongie and Ming-Yu Liu},
  TITLE = {GANcraft: Unsupervised 3D Neural Rendering of Minecraft Worlds},
  EPRINT = {2104.07659v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We present GANcraft, an unsupervised neural rendering framework for generating photorealistic images of large 3D block worlds such as those created in Minecraft. Our method takes a semantic block world as input, where each block is assigned a semantic label such as dirt, grass, or water. We represent the world as a continuous volumetric function and train our model to render view-consistent photorealistic images for a user-controlled camera. In the absence of paired ground truth real images for the block world, we devise a training technique based on pseudo-ground truth and adversarial training. This stands in contrast to prior work on neural rendering for view synthesis, which requires ground truth images to estimate scene geometry and view-dependent appearance. In addition to camera trajectory, GANcraft allows user control over both scene semantics and output style. Experimental results with comparison to strong baselines show the effectiveness of GANcraft on this novel task of photorealistic 3D block world synthesis. The project website is available at https://nvlabs.github.io/GANcraft/ .},
  YEAR = {2021},
  URL = {http://arxiv.org/abs/2104.07659v1},
  FILE = {2104.07659v1.pdf}
 }",Generalization,"Generative/adversarial formulation, Voxelization, Feature volume, Data-driven",,,,,,,,,,ICCV 2021 (Oral),,,,"Zekun Hao, Arun Mallya, Serge Belongie, Ming-Yu Liu",hao2021gancraft,00000115,"We present GANcraft, an unsupervised neural rendering framework for generating photorealistic images of large 3D block worlds such as those created in Minecraft. Our method takes a semantic block world as input, where each block is assigned a semantic label such as dirt, grass, or water. We represent the world as a continuous volumetric function and train our model to render view-consistent photorealistic images for a user-controlled camera. In the absence of paired ground truth real images for the block world, we devise a training technique based on pseudo-ground truth and adversarial training. This stands in contrast to prior work on neural rendering for view synthesis, which requires ground truth images to estimate scene geometry and view-dependent appearance. In addition to camera trajectory, GANcraft allows user control over both scene semantics and output style. Experimental results with comparison to strong baselines show the effectiveness of GANcraft on this novel task of photorealistic 3D block world synthesis. The project website is available at https://nvlabs.github.io/GANcraft/ .",2,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnuf44fRZZtJp13rVFeB74xSbXlNORSFtB6UVeo2n2Kc6IPbr3E7ld8bYoqPI0Z8QJAw
5/23/2021 18:33:15,A-SDF: Learning Disentangled Signed Distance Functions for Articulated Shape Representation,A-SDF,4/15/2021,https://arxiv.org/pdf/2104.07645.pdf,https://jitengmu.github.io/A-SDF/,Coming soon,https://www.youtube.com/watch?v=P5WTcaXzC7A,,,,"@article{mu2021asdf,
  JOURNAL = {arXiv preprint arXiv:2104.07645},
  BOOKTITLE = {ArXiv Pre-print},
  ID = {mu2021asdf},
  ENTRYTYPE = {article},
  AUTHOR = {Jiteng Mu and Weichao Qiu and Adam Kortylewski and Alan Yuille and Nuno Vasconcelos and Xiaolong Wang},
  TITLE = {A-SDF: Learning Disentangled Signed Distance Functions for Articulated Shape Representation},
  EPRINT = {2104.07645v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Recent work has made significant progress on using implicit functions, as a continuous representation for 3D rigid object shape reconstruction. However, much less effort has been devoted to modeling general articulated objects. Compared to rigid objects, articulated objects have higher degrees of freedom, which makes it hard to generalize to unseen shapes. To deal with the large shape variance, we introduce Articulated Signed Distance Functions (A-SDF) to represent articulated shapes with a disentangled latent space, where we have separate codes for encoding shape and articulation. We assume no prior knowledge on part geometry, articulation status, joint type, joint axis, and joint location. With this disentangled continuous representation, we demonstrate that we can control the articulation input and animate unseen instances with unseen joint angles. Furthermore, we propose a Test-Time Adaptation inference algorithm to adjust our model during inference. We demonstrate our model generalize well to out-of-distribution and unseen data, e.g., partial point clouds and real-world depth images.},
  YEAR = {2021},
  URL = {http://arxiv.org/abs/2104.07645v1},
  FILE = {2104.07645v1.pdf}
 }",Editable,"Conditional neural field, Articulated",,,,,,,,,,ARXIV 2021,,,,"Jiteng Mu, Weichao Qiu, Adam Kortylewski, Alan Yuille, Nuno Vasconcelos, Xiaolong Wang",mu2021asdf,00000116,"Recent work has made significant progress on using implicit functions, as a continuous representation for 3D rigid object shape reconstruction. However, much less effort has been devoted to modeling general articulated objects. Compared to rigid objects, articulated objects have higher degrees of freedom, which makes it hard to generalize to unseen shapes. To deal with the large shape variance, we introduce Articulated Signed Distance Functions (A-SDF) to represent articulated shapes with a disentangled latent space, where we have separate codes for encoding shape and articulation. We assume no prior knowledge on part geometry, articulation status, joint type, joint axis, and joint location. With this disentangled continuous representation, we demonstrate that we can control the articulation input and animate unseen instances with unseen joint angles. Furthermore, we propose a Test-Time Adaptation inference algorithm to adjust our model during inference. We demonstrate our model generalize well to out-of-distribution and unseen data, e.g., partial point clouds and real-world depth images.",2,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnufFh4I8ScaSvsBqexoHbIv8ObGQxbwD9QCGwO9D80NXwsjObz6ksNCv7a2PalT44HU
5/23/2021 18:47:27,FastNeRF: High-Fidelity Neural Rendering at 200FPS,FastNeRF,4/15/2021,https://arxiv.org/pdf/2103.10380.pdf,https://microsoft.github.io/FastNeRF/,,,,,,"@article{garbin2021fastnerf,
  JOURNAL = {arXiv preprint arXiv:2103.10380},
  BOOKTITLE = {ArXiv Pre-print},
  ID = {garbin2021fastnerf},
  ENTRYTYPE = {article},
  AUTHOR = {Stephan J. Garbin and Marek Kowalski and Matthew Johnson and Jamie Shotton and Julien Valentin},
  TITLE = {FastNeRF: High-Fidelity Neural Rendering at 200FPS},
  EPRINT = {2103.10380v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Recent work on Neural Radiance Fields (NeRF) showed how neural networks can be used to encode complex 3D environments that can be rendered photorealistically from novel viewpoints. Rendering these images is very computationally demanding and recent improvements are still a long way from enabling interactive rates, even on high-end hardware. Motivated by scenarios on mobile and mixed reality devices, we propose FastNeRF, the first NeRF-based system capable of rendering high fidelity photorealistic images at 200Hz on a high-end consumer GPU. The core of our method is a graphics-inspired factorization that allows for (i) compactly caching a deep radiance map at each position in space, (ii) efficiently querying that map using ray directions to estimate the pixel values in the rendered image. Extensive experiments show that the proposed method is 3000 times faster than the original NeRF algorithm and at least an order of magnitude faster than existing work on accelerating NeRF, while maintaining visual quality and extensibility.},
  YEAR = {2021},
  URL = {http://arxiv.org/abs/2103.10380v2},
  FILE = {2103.10380v2.pdf}
 }",Performance (rendering),Caching,,,,,,,,,,ARXIV 2021,,,,"Stephan J. Garbin, Marek Kowalski, Matthew Johnson, Jamie Shotton, Julien Valentin",garbin2021fastnerf,00000117,"Recent work on Neural Radiance Fields (NeRF) showed how neural networks can be used to encode complex 3D environments that can be rendered photorealistically from novel viewpoints. Rendering these images is very computationally demanding and recent improvements are still a long way from enabling interactive rates, even on high-end hardware. Motivated by scenarios on mobile and mixed reality devices, we propose FastNeRF, the first NeRF-based system capable of rendering high fidelity photorealistic images at 200Hz on a high-end consumer GPU. The core of our method is a graphics-inspired factorization that allows for (i) compactly caching a deep radiance map at each position in space, (ii) efficiently querying that map using ray directions to estimate the pixel values in the rendered image. Extensive experiments show that the proposed method is 3000 times faster than the original NeRF algorithm and at least an order of magnitude faster than existing work on accelerating NeRF, while maintaining visual quality and extensibility.",10,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnudd11n0kXe-lH9my_lAaE4-ozTXOx1HXWTOntE5WyoZrKNWfUTVnrdYEr-rnND5C6E
9/17/2021 11:58:40,Locally Aware Piecewise Transformation Fields for 3D Human Mesh Registration,,4/16/2021,https://arxiv.org/pdf/2104.08160.pdf,https://taconite.github.io/PTF/website/PTF.html,https://github.com/taconite/PTF,https://www.youtube.com/watch?v=TvLoGLVF70k,,,,"@inproceedings{wang2021locally,
  BOOKTITLE = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  ID = {wang2021locally},
  ENTRYTYPE = {inproceedings},
  AUTHOR = {Shaofei Wang and Andreas Geiger and Siyu Tang},
  TITLE = {Locally Aware Piecewise Transformation Fields for 3D Human Mesh Registration},
  EPRINT = {2104.08160v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Registering point clouds of dressed humans to parametric human models is a challenging task in computer vision. Traditional approaches often rely on heavily engineered pipelines that require accurate manual initialization of human poses and tedious post-processing. More recently, learning-based methods are proposed in hope to automate this process. We observe that pose initialization is key to accurate registration but existing methods often fail to provide accurate pose initialization. One major obstacle is that, regressing joint rotations from point clouds or images of humans is still very challenging. To this end, we propose novel piecewise transformation fields (PTF), a set of functions that learn 3D translation vectors to map any query point in posed space to its correspond position in rest-pose space. We combine PTF with multi-class occupancy networks, obtaining a novel learning-based framework that learns to simultaneously predict shape and per-point correspondences between the posed space and the canonical space for clothed human. Our key insight is that the translation vector for each query point can be effectively estimated using the point-aligned local features; consequently, rigid per bone transformations and joint rotations can be obtained efficiently via a least-square fitting given the estimated point correspondences, circumventing the challenging task of directly regressing joint rotations from neural networks. Furthermore, the proposed PTF facilitate canonicalized occupancy estimation, which greatly improves generalization capability and results in more accurate surface reconstruction with only half of the parameters compared with the state-of-the-art. Both qualitative and quantitative studies show that fitting parametric models with poses initialized by our network results in much better registration quality, especially for extreme poses.},
  YEAR = {2021},
  URL = {http://arxiv.org/abs/2104.08160v1},
  FILE = {2104.08160v1.pdf}
 }",Human body,"Voxelization, Feature volume, Volume partitioning, Warping field/Flow field, Data-driven",,Occupancy,Category-level,,,,,,,CVPR 2021,,Yes,Direct,"Shaofei Wang, Andreas Geiger, Siyu Tang",wang2021locally,00000200,"Registering point clouds of dressed humans to parametric human models is a challenging task in computer vision. Traditional approaches often rely on heavily engineered pipelines that require accurate manual initialization of human poses and tedious post-processing. More recently, learning-based methods are proposed in hope to automate this process. We observe that pose initialization is key to accurate registration but existing methods often fail to provide accurate pose initialization. One major obstacle is that, regressing joint rotations from point clouds or images of humans is still very challenging. To this end, we propose novel piecewise transformation fields (PTF), a set of functions that learn 3D translation vectors to map any query point in posed space to its correspond position in rest-pose space. We combine PTF with multi-class occupancy networks, obtaining a novel learning-based framework that learns to simultaneously predict shape and per-point correspondences between the posed space and the canonical space for clothed human. Our key insight is that the translation vector for each query point can be effectively estimated using the point-aligned local features; consequently, rigid per bone transformations and joint rotations can be obtained efficiently via a least-square fitting given the estimated point correspondences, circumventing the challenging task of directly regressing joint rotations from neural networks. Furthermore, the proposed PTF facilitate canonicalized occupancy estimation, which greatly improves generalization capability and results in more accurate surface reconstruction with only half of the parameters compared with the state-of-the-art. Both qualitative and quantitative studies show that fitting parametric models with poses initialized by our network results in much better registration quality, especially for extreme poses.",,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnud735hb1_brvWCIYh9WEybeut7gZimQCCINOraM39l8LjBHMD6UTC0cBtbNVoBkzAM
5/23/2021 18:20:16,FiG-NeRF: Figure-Ground Neural Radiance Fields for 3D Object Category Modelling,FiG-NeRF,4/17/2021,https://arxiv.org/pdf/2104.08418.pdf,https://fig-nerf.github.io/,,https://www.youtube.com/watch?v=WtZxuv_hkic,,,,"@article{xie2021fignerf,
  JOURNAL = {arXiv preprint arXiv:2104.08418},
  BOOKTITLE = {ArXiv Pre-print},
  ID = {xie2021fignerf},
  ENTRYTYPE = {article},
  AUTHOR = {Christopher Xie and Keunhong Park and Ricardo Martin-Brualla and Matthew Brown},
  TITLE = {FiG-NeRF: Figure-Ground Neural Radiance Fields for 3D Object Category Modelling},
  EPRINT = {2104.08418v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We investigate the use of Neural Radiance Fields (NeRF) to learn high quality 3D object category models from collections of input images. In contrast to previous work, we are able to do this whilst simultaneously separating foreground objects from their varying backgrounds. We achieve this via a 2-component NeRF model, FiG-NeRF, that prefers explanation of the scene as a geometrically constant background and a deformable foreground that represents the object category. We show that this method can learn accurate 3D object category models using only photometric supervision and casually captured images of the objects. Additionally, our 2-part decomposition allows the model to perform accurate and crisp amodal segmentation. We quantitatively evaluate our method with view synthesis and image fidelity metrics, using synthetic, lab-captured, and in-the-wild data. Our results demonstrate convincing 3D object category modelling that exceed the performance of existing methods.},
  YEAR = {2021},
  URL = {http://arxiv.org/abs/2104.08418v1},
  FILE = {2104.08418v1.pdf}
 }","Generalization, Segmentation/composition, Fundamentals",Conditional neural field,,,Category-level,,,,,,,ARXIV 2021,,,,"Christopher Xie, Keunhong Park, Ricardo Martin-Brualla, Matthew Brown",xie2021fignerf,00000118,"We investigate the use of Neural Radiance Fields (NeRF) to learn high quality 3D object category models from collections of input images. In contrast to previous work, we are able to do this whilst simultaneously separating foreground objects from their varying backgrounds. We achieve this via a 2-component NeRF model, FiG-NeRF, that prefers explanation of the scene as a geometrically constant background and a deformable foreground that represents the object category. We show that this method can learn accurate 3D object category models using only photometric supervision and casually captured images of the objects. Additionally, our 2-part decomposition allows the model to perform accurate and crisp amodal segmentation. We quantitatively evaluate our method with view synthesis and image fidelity metrics, using synthetic, lab-captured, and in-the-wild data. Our results demonstrate convincing 3D object category modelling that exceed the performance of existing methods.",1,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnueGSyESER0aFFTZ0F2Qmuk6zEPafOunzpbKNCIz8OLecQhPf6qLRrM3kP40gIMAevY
7/19/2021 21:59:14,SAPE: Spatially-Adaptive Progressive Encoding for Neural Optimization,SAPE,4/19/2021,https://arxiv.org/pdf/2104.09125.pdf,,,,,,,"@article{hertz2021sape,
  JOURNAL = {arXiv preprint arXiv:2104.09125},
  BOOKTITLE = {ArXiv Pre-print},
  ID = {hertz2021sape},
  ENTRYTYPE = {article},
  AUTHOR = {Amir Hertz and Or Perel and Raja Giryes and Olga Sorkine-Hornung and Daniel Cohen-Or},
  TITLE = {SAPE: Spatially-Adaptive Progressive Encoding for Neural Optimization},
  EPRINT = {2104.09125v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.LG},
  ABSTRACT = {Multilayer-perceptrons (MLP) are known to struggle with learning functions of high-frequencies, and in particular cases with wide frequency bands. We present a spatially adaptive progressive encoding (SAPE) scheme for input signals of MLP networks, which enables them to better fit a wide range of frequencies without sacrificing training stability or requiring any domain specific preprocessing. SAPE gradually unmasks signal components with increasing frequencies as a function of time and space. The progressive exposure of frequencies is monitored by a feedback loop throughout the neural optimization process, allowing changes to propagate at different rates among local spatial portions of the signal space. We demonstrate the advantage of SAPE on a variety of domains and applications, including regression of low dimensional signals and images, representation learning of occupancy networks, and a geometric task of mesh transfer between 3D shapes.},
  YEAR = {2021},
  URL = {http://arxiv.org/abs/2104.09125v2},
  FILE = {2104.09125v2.pdf}
 }",Fundamentals,Coarse-to-fine,Other,,,,,,,,,ARXIV 2021,,,,"Amir Hertz, Or Perel, Raja Giryes, Olga Sorkine-Hornung, Daniel Cohen-Or",hertz2021sape,00000119,"Multilayer-perceptrons (MLP) are known to struggle with learning functions of high-frequencies, and in particular cases with wide frequency bands. We present a spatially adaptive progressive encoding (SAPE) scheme for input signals of MLP networks, which enables them to better fit a wide range of frequencies without sacrificing training stability or requiring any domain specific preprocessing. SAPE gradually unmasks signal components with increasing frequencies as a function of time and space. The progressive exposure of frequencies is monitored by a feedback loop throughout the neural optimization process, allowing changes to propagate at different rates among local spatial portions of the signal space. We demonstrate the advantage of SAPE on a variety of domains and applications, including regression of low dimensional signals and images, representation learning of occupancy networks, and a geometric task of mesh transfer between 3D shapes.",1,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnueLMX_U0Dg_kyK06UDa5Ql_-UPWhjrk9_JJaSNXV2g-zXVLtR7a_bRzVnBgkAxCQGc
7/20/2021 15:00:44,Shadow Neural Radiance Fields for Multi-view Satellite Photogrammetry,S-NeRF,4/20/2021,https://arxiv.org/pdf/2104.09877.pdf,,,,,,,"@inproceedings{derksen2021snerf,
  BOOKTITLE = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  ID = {derksen2021snerf},
  ENTRYTYPE = {inproceedings},
  AUTHOR = {Dawa Derksen and Dario Izzo},
  TITLE = {Shadow Neural Radiance Fields for Multi-view Satellite Photogrammetry},
  EPRINT = {2104.09877v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We present a new generic method for shadow-aware multi-view satellite photogrammetry of Earth Observation scenes. Our proposed method, the Shadow Neural Radiance Field (S-NeRF) follows recent advances in implicit volumetric representation learning. For each scene, we train S-NeRF using very high spatial resolution optical images taken from known viewing angles. The learning requires no labels or shape priors: it is self-supervised by an image reconstruction loss. To accommodate for changing light source conditions both from a directional light source (the Sun) and a diffuse light source (the sky), we extend the NeRF approach in two ways. First, direct illumination from the Sun is modeled via a local light source visibility field. Second, indirect illumination from a diffuse light source is learned as a non-local color field as a function of the position of the Sun. Quantitatively, the combination of these factors reduces the altitude and color errors in shaded areas, compared to NeRF. The S-NeRF methodology not only performs novel view synthesis and full 3D shape estimation, it also enables shadow detection, albedo synthesis, and transient object filtering, without any explicit shape supervision.},
  YEAR = {2021},
  URL = {http://arxiv.org/abs/2104.09877v1},
  FILE = {2104.09877v1.pdf}
 }",Material/lighting estimation,Sampling,,,,,,,,,,CVPR 2021,,,,"Dawa Derksen, Dario Izzo",derksen2021snerf,00000120,"We present a new generic method for shadow-aware multi-view satellite photogrammetry of Earth Observation scenes. Our proposed method, the Shadow Neural Radiance Field (S-NeRF) follows recent advances in implicit volumetric representation learning. For each scene, we train S-NeRF using very high spatial resolution optical images taken from known viewing angles. The learning requires no labels or shape priors: it is self-supervised by an image reconstruction loss. To accommodate for changing light source conditions both from a directional light source (the Sun) and a diffuse light source (the sky), we extend the NeRF approach in two ways. First, direct illumination from the Sun is modeled via a local light source visibility field. Second, indirect illumination from a diffuse light source is learned as a non-local color field as a function of the position of the Sun. Quantitatively, the combination of these factors reduces the altitude and color errors in shaded areas, compared to NeRF. The S-NeRF methodology not only performs novel view synthesis and full 3D shape estimation, it also enables shadow detection, albedo synthesis, and transient object filtering, without any explicit shape supervision.",0,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnuf6QkJKvTXvKqOil8vo2uRBN3TJ6yEoWYOIphGvmWwyLUnwmAEYoToBy_DNgMv8LPA
5/23/2021 18:32:09,UNISURF: Unifying Neural Implicit Surfaces and Radiance Fields for Multi-View Reconstruction,UNISURF,4/20/2021,https://arxiv.org/pdf/2104.10078.pdf,https://arxiv.org/pdf/2104.10078.pdf,,,,,,"@inproceedings{oechsle2021unisurf,
  BOOKTITLE = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
  ID = {oechsle2021unisurf},
  ENTRYTYPE = {inproceedings},
  AUTHOR = {Michael Oechsle and Songyou Peng and Andreas Geiger},
  TITLE = {UNISURF: Unifying Neural Implicit Surfaces and Radiance Fields for Multi-View Reconstruction},
  EPRINT = {2104.10078v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Neural implicit 3D representations have emerged as a powerful paradigm for reconstructing surfaces from multi-view images and synthesizing novel views. Unfortunately, existing methods such as DVR or IDR require accurate per-pixel object masks as supervision. At the same time, neural radiance fields have revolutionized novel view synthesis. However, NeRF's estimated volume density does not admit accurate surface reconstruction. Our key insight is that implicit surface models and radiance fields can be formulated in a unified way, enabling both surface and volume rendering using the same model. This unified perspective enables novel, more efficient sampling procedures and the ability to reconstruct accurate surfaces without input masks. We compare our method on the DTU, BlendedMVS, and a synthetic indoor dataset. Our experiments demonstrate that we outperform NeRF in terms of reconstruction quality while performing on par with IDR without requiring masks.},
  YEAR = {2021},
  URL = {http://arxiv.org/abs/2104.10078v1},
  FILE = {2104.10078v1.pdf}
 }",Fundamentals,Representation,,Occupancy,,,,,,,,ICCV 2021,,No,,"Michael Oechsle, Songyou Peng, Andreas Geiger",oechsle2021unisurf,00000121,"Neural implicit 3D representations have emerged as a powerful paradigm for reconstructing surfaces from multi-view images and synthesizing novel views. Unfortunately, existing methods such as DVR or IDR require accurate per-pixel object masks as supervision. At the same time, neural radiance fields have revolutionized novel view synthesis. However, NeRF's estimated volume density does not admit accurate surface reconstruction. Our key insight is that implicit surface models and radiance fields can be formulated in a unified way, enabling both surface and volume rendering using the same model. This unified perspective enables novel, more efficient sampling procedures and the ability to reconstruct accurate surfaces without input masks. We compare our method on the DTU, BlendedMVS, and a synthetic indoor dataset. Our experiments demonstrate that we outperform NeRF in terms of reconstruction quality while performing on par with IDR without requiring masks.",7,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnucGp9idfZQdOyqa5rPex2jBkzaRljSYAtt8ZBSu1LyPdtv29tjUkTuYXvRPO3OL1vg
5/23/2021 18:21:02,Dynamic CT Reconstruction from Limited Views with Implicit Neural Representations and Parametric Motion Fields,INR,4/23/2021,https://arxiv.org/pdf/2104.11745.pdf,,,,,,,"@article{reed2021inr,
  JOURNAL = {arXiv preprint arXiv:2104.11745},
  BOOKTITLE = {ArXiv Pre-print},
  ID = {reed2021inr},
  ENTRYTYPE = {article},
  AUTHOR = {Albert W. Reed and Hyojin Kim and Rushil Anirudh and K. Aditya Mohan and Kyle Champley and Jingu Kang and Suren Jayasuriya},
  TITLE = {Dynamic CT Reconstruction from Limited Views with Implicit Neural Representations and Parametric Motion Fields},
  EPRINT = {2104.11745v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {eess.IV},
  ABSTRACT = {Reconstructing dynamic, time-varying scenes with computed tomography (4D-CT) is a challenging and ill-posed problem common to industrial and medical settings. Existing 4D-CT reconstructions are designed for sparse sampling schemes that require fast CT scanners to capture multiple, rapid revolutions around the scene in order to generate high quality results. However, if the scene is moving too fast, then the sampling occurs along a limited view and is difficult to reconstruct due to spatiotemporal ambiguities. In this work, we design a reconstruction pipeline using implicit neural representations coupled with a novel parametric motion field warping to perform limited view 4D-CT reconstruction of rapidly deforming scenes. Importantly, we utilize a differentiable analysis-by-synthesis approach to compare with captured x-ray sinogram data in a self-supervised fashion. Thus, our resulting optimization method requires no training data to reconstruct the scene. We demonstrate that our proposed system robustly reconstructs scenes containing deformable and periodic motion and validate against state-of-the-art baselines. Further, we demonstrate an ability to reconstruct continuous spatiotemporal representations of our scenes and upsample them to arbitrary volumes and frame rates post-optimization. This research opens a new avenue for implicit neural representations in computed tomography reconstruction in general.},
  YEAR = {2021},
  URL = {http://arxiv.org/abs/2104.11745v1},
  FILE = {2104.11745v1.pdf}
 }","Dynamic, Beyond graphics, Science and engineering",Warping field/Flow field,,,,,,,,,,ARXIV 2021,,,,"Albert W. Reed, Hyojin Kim, Rushil Anirudh, K. Aditya Mohan, Kyle Champley, Jingu Kang, Suren Jayasuriya",reed2021inr,00000122,"Reconstructing dynamic, time-varying scenes with computed tomography (4D-CT) is a challenging and ill-posed problem common to industrial and medical settings. Existing 4D-CT reconstructions are designed for sparse sampling schemes that require fast CT scanners to capture multiple, rapid revolutions around the scene in order to generate high quality results. However, if the scene is moving too fast, then the sampling occurs along a limited view and is difficult to reconstruct due to spatiotemporal ambiguities. In this work, we design a reconstruction pipeline using implicit neural representations coupled with a novel parametric motion field warping to perform limited view 4D-CT reconstruction of rapidly deforming scenes. Importantly, we utilize a differentiable analysis-by-synthesis approach to compare with captured x-ray sinogram data in a self-supervised fashion. Thus, our resulting optimization method requires no training data to reconstruct the scene. We demonstrate that our proposed system robustly reconstructs scenes containing deformable and periodic motion and validate against state-of-the-art baselines. Further, we demonstrate an ability to reconstruct continuous spatiotemporal representations of our scenes and upsample them to arbitrary volumes and frame rates post-optimization. This research opens a new avenue for implicit neural representations in computed tomography reconstruction in general.",1,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnueqisWqLF-xm-eBwYXf8i5YkWwy_trQEBO-qkxmHaSihSIo4F51EwSrht_cAnxoqZc
8/29/2021 16:25:16,Vector Neurons: A General Framework for SO(3)-Equivariant Networks,Vector Neurons,4/25/2021,https://arxiv.org/pdf/2104.12229.pdf,https://cs.stanford.edu/~congyue/vnn/,"https://github.com/FlyingGiraffe/vnn, https://github.com/FlyingGiraffe/vnn-neural-implicits/",Coming soon,,,,"@article{deng2021vectorneurons,
  JOURNAL = {arXiv preprint arXiv:2104.12229},
  BOOKTITLE = {ArXiv Pre-print},
  ID = {deng2021vectorneurons},
  ENTRYTYPE = {article},
  AUTHOR = {Congyue Deng and Or Litany and Yueqi Duan and Adrien Poulenard and Andrea Tagliasacchi and Leonidas Guibas},
  TITLE = {Vector Neurons: A General Framework for SO(3)-Equivariant Networks},
  EPRINT = {2104.12229v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Invariance and equivariance to the rotation group have been widely discussed in the 3D deep learning community for pointclouds. Yet most proposed methods either use complex mathematical tools that may limit their accessibility, or are tied to specific input data types and network architectures. In this paper, we introduce a general framework built on top of what we call Vector Neuron representations for creating SO(3)-equivariant neural networks for pointcloud processing. Extending neurons from 1D scalars to 3D vectors, our vector neurons enable a simple mapping of SO(3) actions to latent spaces thereby providing a framework for building equivariance in common neural operations -- including linear layers, non-linearities, pooling, and normalizations. Due to their simplicity, vector neurons are versatile and, as we demonstrate, can be incorporated into diverse network architecture backbones, allowing them to process geometry inputs in arbitrary poses. Despite its simplicity, our method performs comparably well in accuracy and generalization with other more complex and specialized state-of-the-art methods on classification and segmentation tasks. We also show for the first time a rotation equivariant reconstruction network.},
  YEAR = {2021},
  URL = {http://arxiv.org/abs/2104.12229v1},
  FILE = {2104.12229v1.pdf}
 }","Generalization, Fundamentals","Data-driven, Symmetry",,Occupancy,Category-level,,,,,,,ARXIV 2021,,Yes,Direct,"Congyue Deng, Or Litany, Yueqi Duan, Adrien Poulenard, Andrea Tagliasacchi, Leonidas Guibas",deng2021vectorneurons,00000164,"Invariance and equivariance to the rotation group have been widely discussed in the 3D deep learning community for pointclouds. Yet most proposed methods either use complex mathematical tools that may limit their accessibility, or are tied to specific input data types and network architectures. In this paper, we introduce a general framework built on top of what we call Vector Neuron representations for creating SO(3)-equivariant neural networks for pointcloud processing. Extending neurons from 1D scalars to 3D vectors, our vector neurons enable a simple mapping of SO(3) actions to latent spaces thereby providing a framework for building equivariance in common neural operations -- including linear layers, non-linearities, pooling, and normalizations. Due to their simplicity, vector neurons are versatile and, as we demonstrate, can be incorporated into diverse network architecture backbones, allowing them to process geometry inputs in arbitrary poses. Despite its simplicity, our method performs comparably well in accuracy and generalization with other more complex and specialized state-of-the-art methods on classification and segmentation tasks. We also show for the first time a rotation equivariant reconstruction network.",1,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnudRwmBQ2J1-lOyhMDfmHqEewMnlyjfkKuN-XDbvEsLJfttbjlmxr5yXARgrUWW9k1w
5/23/2021 18:19:17,Neural Ray-Tracing: Learning Surfaces and Reflectance for Relighting and View Synthesis,Neural Ray-Tracing,4/28/2021,https://arxiv.org/pdf/2104.13562.pdf,,https://github.com/princeton-computational-imaging/neural_raytracing,,,,,"@article{knodt2021neuralraytracing,
  JOURNAL = {arXiv preprint arXiv:2104.13562},
  BOOKTITLE = {ArXiv Pre-print},
  ID = {knodt2021neuralraytracing},
  ENTRYTYPE = {article},
  AUTHOR = {Julian Knodt and Seung-Hwan Baek and Felix Heide},
  TITLE = {Neural Ray-Tracing: Learning Surfaces and Reflectance for Relighting and View Synthesis},
  EPRINT = {2104.13562v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Recent neural rendering methods have demonstrated accurate view interpolation by predicting volumetric density and color with a neural network. Although such volumetric representations can be supervised on static and dynamic scenes, existing methods implicitly bake the complete scene light transport into a single neural network for a given scene, including surface modeling, bidirectional scattering distribution functions, and indirect lighting effects. In contrast to traditional rendering pipelines, this prohibits changing surface reflectance, illumination, or composing other objects in the scene. In this work, we explicitly model the light transport between scene surfaces and we rely on traditional integration schemes and the rendering equation to reconstruct a scene. The proposed method allows BSDF recovery with unknown light conditions and classic light transports such as pathtracing. By learning decomposed transport with surface representations established in conventional rendering methods, the method naturally facilitates editing shape, reflectance, lighting and scene composition. The method outperforms NeRV for relighting under known lighting conditions, and produces realistic reconstructions for relit and edited scenes. We validate the proposed approach for scene editing, relighting and reflectance estimation learned from synthetic and captured views on a subset of NeRV's datasets.},
  YEAR = {2021},
  URL = {http://arxiv.org/abs/2104.13562v1},
  FILE = {2104.13562v1.pdf}
 }","Editable, Material/lighting estimation","Learning residual, Representation",,SDF,Per-scene,,,,,,,ARXIV 2021,,No,Direct,"Julian Knodt, Seung-Hwan Baek, Felix Heide",knodt2021neuralraytracing,00000123,"Recent neural rendering methods have demonstrated accurate view interpolation by predicting volumetric density and color with a neural network. Although such volumetric representations can be supervised on static and dynamic scenes, existing methods implicitly bake the complete scene light transport into a single neural network for a given scene, including surface modeling, bidirectional scattering distribution functions, and indirect lighting effects. In contrast to traditional rendering pipelines, this prohibits changing surface reflectance, illumination, or composing other objects in the scene. In this work, we explicitly model the light transport between scene surfaces and we rely on traditional integration schemes and the rendering equation to reconstruct a scene. The proposed method allows BSDF recovery with unknown light conditions and classic light transports such as pathtracing. By learning decomposed transport with surface representations established in conventional rendering methods, the method naturally facilitates editing shape, reflectance, lighting and scene composition. The method outperforms NeRV for relighting under known lighting conditions, and produces realistic reconstructions for relit and edited scenes. We validate the proposed approach for scene editing, relighting and reflectance estimation learned from synthetic and captured views on a subset of NeRV's datasets.",0,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnufPpWAcLchhyZ0sQ3I4df3qkPJgfX6aa-lZuBRy6gpyIFcgTAK5NFKObcL-zQjAviY
9/30/2021 10:56:12,STORM: An Integrated Framework for Fast Joint-Space Model-Predictive Control for Reactive Manipulation,STORM,4/28/2021,https://arxiv.org/pdf/2104.13542.pdf,https://sites.google.com/view/manipulation-mpc,,https://vimeo.com/526772348,,,,"@inproceedings{bhardwaj2021storm,
  PUBLISHER = {Curran Associates, Inc.},
  BOOKTITLE = {Advances in Neural Information Processing Systems (NeurIPS)},
  ID = {bhardwaj2021storm},
  ENTRYTYPE = {inproceedings},
  AUTHOR = {Mohak Bhardwaj and Balakumar Sundaralingam and Arsalan Mousavian and Nathan Ratliff and Dieter Fox and Fabio Ramos and Byron Boots},
  TITLE = {STORM: An Integrated Framework for Fast Joint-Space Model-Predictive Control for Reactive Manipulation},
  EPRINT = {2104.13542v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.RO},
  ABSTRACT = {Sampling-based model-predictive control (MPC) is a promising tool for feedback control of robots with complex, non-smooth dynamics, and cost functions. However, the computationally demanding nature of sampling-based MPC algorithms has been a key bottleneck in their application to high-dimensional robotic manipulation problems in the real world. Previous methods have addressed this issue by running MPC in the task space while relying on a low-level operational space controller for joint control. However, by not using the joint space of the robot in the MPC formulation, existing methods cannot directly account for non-task space related constraints such as avoiding joint limits, singular configurations, and link collisions. In this paper, we develop a system for fast, joint space sampling-based MPC for manipulators that is efficiently parallelized using GPUs. Our approach can handle task and joint space constraints while taking less than 8ms~(125Hz) to compute the next control command. Further, our method can tightly integrate perception into the control problem by utilizing learned cost functions from raw sensor data. We validate our approach by deploying it on a Franka Panda robot for a variety of dynamic manipulation tasks. We study the effect of different cost formulations and MPC parameters on the synthesized behavior and provide key insights that pave the way for the application of sampling-based MPC for manipulators in a principled manner. We also provide highly optimized, open-source code to be used by the wider robot learning and control community. Videos of experiments can be found at: https://sites.google.com/view/manipulation-mpc},
  YEAR = {2021},
  URL = {http://arxiv.org/abs/2104.13542v2},
  FILE = {2104.13542v2.pdf}
 }","Beyond graphics, Robotics",,NeRF,,,,,,,,,NeurIPS 2021,,,,"Mohak Bhardwaj, Balakumar Sundaralingam, Arsalan Mousavian, Nathan Ratliff, Dieter Fox, Fabio Ramos, Byron Boots",bhardwaj2021storm,00000225,"Sampling-based model-predictive control (MPC) is a promising tool for feedback control of robots with complex, non-smooth dynamics, and cost functions. However, the computationally demanding nature of sampling-based MPC algorithms has been a key bottleneck in their application to high-dimensional robotic manipulation problems in the real world. Previous methods have addressed this issue by running MPC in the task space while relying on a low-level operational space controller for joint control. However, by not using the joint space of the robot in the MPC formulation, existing methods cannot directly account for non-task space related constraints such as avoiding joint limits, singular configurations, and link collisions. In this paper, we develop a system for fast, joint space sampling-based MPC for manipulators that is efficiently parallelized using GPUs. Our approach can handle task and joint space constraints while taking less than 8ms~(125Hz) to compute the next control command. Further, our method can tightly integrate perception into the control problem by utilizing learned cost functions from raw sensor data. We validate our approach by deploying it on a Franka Panda robot for a variety of dynamic manipulation tasks. We study the effect of different cost formulations and MPC parameters on the synthesized behavior and provide key insights that pave the way for the application of sampling-based MPC for manipulators in a principled manner. We also provide highly optimized, open-source code to be used by the wider robot learning and control community. Videos of experiments can be found at: https://sites.google.com/view/manipulation-mpc",,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnufYc8FH5fZQDTInASf-YyfIKZTCYyUaTy0blknEPO3d5A6Kuf_45446YCcBQHGr25I
5/23/2021 18:18:54,Editable Free-Viewpoint Video using a Layered Neural Representation,ST-NeRF,4/30/2021,https://arxiv.org/pdf/2104.14786.pdf,,,https://www.youtube.com/watch?v=Wp4HfOwFGP4&feature=emb_logo,,,,"@article{zhang2021stnerf,
  PUBLISHER = {Association for Computing Machinery},
  JOURNAL = {ACM Transactions on Graphics (TOG)},
  ID = {zhang2021stnerf},
  ENTRYTYPE = {article},
  AUTHOR = {Jiakai Zhang and Xinhang Liu and Xinyi Ye and Fuqiang Zhao and Yanshun Zhang and Minye Wu and Yingliang Zhang and Lan Xu and Jingyi Yu},
  TITLE = {Editable Free-viewpoint Video Using a Layered Neural Representation},
  EPRINT = {2104.14786v1},
  DOI = {10.1145/3450626.3459756},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Generating free-viewpoint videos is critical for immersive VR/AR experience but recent neural advances still lack the editing ability to manipulate the visual perception for large dynamic scenes. To fill this gap, in this paper we propose the first approach for editable photo-realistic free-viewpoint video generation for large-scale dynamic scenes using only sparse 16 cameras. The core of our approach is a new layered neural representation, where each dynamic entity including the environment itself is formulated into a space-time coherent neural layered radiance representation called ST-NeRF. Such layered representation supports fully perception and realistic manipulation of the dynamic scene whilst still supporting a free viewing experience in a wide range. In our ST-NeRF, the dynamic entity/layer is represented as continuous functions, which achieves the disentanglement of location, deformation as well as the appearance of the dynamic entity in a continuous and self-supervised manner. We propose a scene parsing 4D label map tracking to disentangle the spatial information explicitly, and a continuous deform module to disentangle the temporal motion implicitly. An object-aware volume rendering scheme is further introduced for the re-assembling of all the neural layers. We adopt a novel layered loss and motion-aware ray sampling strategy to enable efficient training for a large dynamic scene with multiple performers, Our framework further enables a variety of editing functions, i.e., manipulating the scale and location, duplicating or retiming individual neural layers to create numerous visual effects while preserving high realism. Extensive experiments demonstrate the effectiveness of our approach to achieve high-quality, photo-realistic, and editable free-viewpoint video generation for dynamic scenes.},
  YEAR = {2021},
  URL = {http://arxiv.org/abs/2104.14786v1},
  FILE = {2104.14786v1.pdf}
 }","Dynamic, Editable, Segmentation/composition","Volume partitioning, Warping field/Flow field, Object-centric representation",,,,,,,,,,SIGGRAPH 2021,,,,"Jiakai Zhang, Xinhang Liu, Xinyi Ye, Fuqiang Zhao, Yanshun Zhang, Minye Wu, Yingliang Zhang, Lan Xu, Jingyi Yu",zhang2021stnerf,00000124,"Generating free-viewpoint videos is critical for immersive VR/AR experience but recent neural advances still lack the editing ability to manipulate the visual perception for large dynamic scenes. To fill this gap, in this paper we propose the first approach for editable photo-realistic free-viewpoint video generation for large-scale dynamic scenes using only sparse 16 cameras. The core of our approach is a new layered neural representation, where each dynamic entity including the environment itself is formulated into a space-time coherent neural layered radiance representation called ST-NeRF. Such layered representation supports fully perception and realistic manipulation of the dynamic scene whilst still supporting a free viewing experience in a wide range. In our ST-NeRF, the dynamic entity/layer is represented as continuous functions, which achieves the disentanglement of location, deformation as well as the appearance of the dynamic entity in a continuous and self-supervised manner. We propose a scene parsing 4D label map tracking to disentangle the spatial information explicitly, and a continuous deform module to disentangle the temporal motion implicitly. An object-aware volume rendering scheme is further introduced for the re-assembling of all the neural layers. We adopt a novel layered loss and motion-aware ray sampling strategy to enable efficient training for a large dynamic scene with multiple performers, Our framework further enables a variety of editing functions, i.e., manipulating the scale and location, duplicating or retiming individual neural layers to create numerous visual effects while preserving high realism. Extensive experiments demonstrate the effectiveness of our approach to achieve high-quality, photo-realistic, and editable free-viewpoint video generation for dynamic scenes.",1,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnudu3kBKWr1o7mc6tCAIMd9sNjjokGEwcP3jlsYrRI3hRBRzYJwr6TwDBvPyFGMMrVM
5/23/2021 18:54:15,3D Scene Compression through Entropy Penalized Neural Representation Functions,cNeRF,5/3/2021,https://arxiv.org/pdf/2104.12456.pdf,,,,,,,"@article{bird2021cnerf,
  JOURNAL = {arXiv preprint arXiv:2104.12456},
  BOOKTITLE = {ArXiv Pre-print},
  ID = {bird2021cnerf},
  ENTRYTYPE = {article},
  AUTHOR = {Thomas Bird and Johannes Balle and Saurabh Singh and Philip A. Chou},
  TITLE = {3D Scene Compression through Entropy Penalized Neural Representation Functions},
  EPRINT = {2104.12456v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Some forms of novel visual media enable the viewer to explore a 3D scene from arbitrary viewpoints, by interpolating between a discrete set of original views. Compared to 2D imagery, these types of applications require much larger amounts of storage space, which we seek to reduce. Existing approaches for compressing 3D scenes are based on a separation of compression and rendering: each of the original views is compressed using traditional 2D image formats; the receiver decompresses the views and then performs the rendering. We unify these steps by directly compressing an implicit representation of the scene, a function that maps spatial coordinates to a radiance vector field, which can then be queried to render arbitrary viewpoints. The function is implemented as a neural network and jointly trained for reconstruction as well as compressibility, in an end-to-end manner, with the use of an entropy penalty on the parameters. Our method significantly outperforms a state-of-the-art conventional approach for scene compression, achieving simultaneously higher quality reconstructions and lower bitrates. Furthermore, we show that the performance at lower bitrates can be improved by jointly representing multiple scenes using a soft form of parameter sharing.},
  YEAR = {2021},
  URL = {http://arxiv.org/abs/2104.12456v1},
  FILE = {2104.12456v1.pdf}
 }",Compression,,,,,,,,,,,ARXIV 2021,,,,"Thomas Bird, Johannes Ballé, Saurabh Singh, Philip A. Chou",bird2021cnerf,00000125,"Some forms of novel visual media enable the viewer to explore a 3D scene from arbitrary viewpoints, by interpolating between a discrete set of original views. Compared to 2D imagery, these types of applications require much larger amounts of storage space, which we seek to reduce. Existing approaches for compressing 3D scenes are based on a separation of compression and rendering: each of the original views is compressed using traditional 2D image formats; the receiver decompresses the views and then performs the rendering. We unify these steps by directly compressing an implicit representation of the scene, a function that maps spatial coordinates to a radiance vector field, which can then be queried to render arbitrary viewpoints. The function is implemented as a neural network and jointly trained for reconstruction as well as compressibility, in an end-to-end manner, with the use of an entropy penalty on the parameters. Our method significantly outperforms a state-of-the-art conventional approach for scene compression, achieving simultaneously higher quality reconstructions and lower bitrates. Furthermore, we show that the performance at lower bitrates can be improved by jointly representing multiple scenes using a soft form of parameter sharing.",0,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnue4tvrjIGefCS4jWCxYrMmJyiOm54qTzHBOTBhjeKmJgRv9KX2k1j_dXUZFEjR1_Rw
5/23/2021 18:17:15,Animatable Neural Radiance Fields for Human Body Modeling,,5/6/2021,https://arxiv.org/pdf/2105.02872.pdf,,https://arxiv.org/pdf/2105.02872.pdf,https://www.youtube.com/watch?v=eWOSWbmfJo4,,,,"@inproceedings{peng2021animatable,
  BOOKTITLE = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
  ID = {peng2021animatable},
  ENTRYTYPE = {inproceedings},
  AUTHOR = {Sida Peng and Junting Dong and Qianqian Wang and Shangzhan Zhang and Qing Shuai and Hujun Bao and Xiaowei Zhou},
  TITLE = {Animatable Neural Radiance Fields for Human Body Modeling},
  EPRINT = {2105.02872v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {This paper addresses the challenge of reconstructing an animatable human model from a multi-view video. Some recent works have proposed to decompose a dynamic scene into a canonical neural radiance field and a set of deformation fields that map observation-space points to the canonical space, thereby enabling them to learn the dynamic scene from images. However, they represent the deformation field as translational vector field or SE(3) field, which makes the optimization highly under-constrained. Moreover, these representations cannot be explicitly controlled by input motions. Instead, we introduce neural blend weight fields to produce the deformation fields. Based on the skeleton-driven deformation, blend weight fields are used with 3D human skeletons to generate observation-to-canonical and canonical-to-observation correspondences. Since 3D human skeletons are more observable, they can regularize the learning of deformation fields. Moreover, the learned blend weight fields can be combined with input skeletal motions to generate new deformation fields to animate the human model. Experiments show that our approach significantly outperforms recent human synthesis methods. The code will be available at https://zju3dv.github.io/animatable_nerf/.},
  YEAR = {2021},
  URL = {http://arxiv.org/abs/2105.02872v1},
  FILE = {2105.02872v1.pdf}
 }","Dynamic, Human body",Articulated,,,,,,,,,,ICCV 2021,,,,"Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Hujun Bao, Xiaowei Zhou",peng2021animatable,00000126,"This paper addresses the challenge of reconstructing an animatable human model from a multi-view video. Some recent works have proposed to decompose a dynamic scene into a canonical neural radiance field and a set of deformation fields that map observation-space points to the canonical space, thereby enabling them to learn the dynamic scene from images. However, they represent the deformation field as translational vector field or SE(3) field, which makes the optimization highly under-constrained. Moreover, these representations cannot be explicitly controlled by input motions. Instead, we introduce neural blend weight fields to produce the deformation fields. Based on the skeleton-driven deformation, blend weight fields are used with 3D human skeletons to generate observation-to-canonical and canonical-to-observation correspondences. Since 3D human skeletons are more observable, they can regularize the learning of deformation fields. Moreover, the learned blend weight fields can be combined with input skeletal motions to generate new deformation fields to animate the human model. Experiments show that our approach significantly outperforms recent human synthesis methods. The code will be available at https://zju3dv.github.io/animatable_nerf/.",1,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnud83yRHhP7QeUGBGKT_VcJgJ5dWdokbVWmQK4oHEmgEScgTNCKdUeHfNJjd7aQznIM
6/29/2021 16:36:45,acorn: Adaptive Coordinate Networks for Neural Scene Representation,ACORN,5/6/2021,https://arxiv.org/pdf/2105.02788.pdf,,,,,,,"@article{martel2021acorn,
  PUBLISHER = {Association for Computing Machinery},
  JOURNAL = {ACM Transactions on Graphics (TOG)},
  ID = {martel2021acorn},
  ENTRYTYPE = {article},
  AUTHOR = {Julien N. P. Martel and David B. Lindell and Connor Z. Lin and Eric R. Chan and Marco Monteiro and Gordon Wetzstein},
  TITLE = {ACORN: Adaptive Coordinate Networks for Neural Scene Representation},
  EPRINT = {2105.02788v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Neural representations have emerged as a new paradigm for applications in rendering, imaging, geometric modeling, and simulation. Compared to traditional representations such as meshes, point clouds, or volumes they can be flexibly incorporated into differentiable learning-based pipelines. While recent improvements to neural representations now make it possible to represent signals with fine details at moderate resolutions (e.g., for images and 3D shapes), adequately representing large-scale or complex scenes has proven a challenge. Current neural representations fail to accurately represent images at resolutions greater than a megapixel or 3D scenes with more than a few hundred thousand polygons. Here, we introduce a new hybrid implicit-explicit network architecture and training strategy that adaptively allocates resources during training and inference based on the local complexity of a signal of interest. Our approach uses a multiscale block-coordinate decomposition, similar to a quadtree or octree, that is optimized during training. The network architecture operates in two stages: using the bulk of the network parameters, a coordinate encoder generates a feature grid in a single forward pass. Then, hundreds or thousands of samples within each block can be efficiently evaluated using a lightweight feature decoder. With this hybrid implicit-explicit network architecture, we demonstrate the first experiments that fit gigapixel images to nearly 40 dB peak signal-to-noise ratio. Notably this represents an increase in scale of over 1000x compared to the resolution of previously demonstrated image-fitting experiments. Moreover, our approach is able to represent 3D shapes significantly faster and better than previous techniques; it reduces training times from days to hours or minutes and memory requirements by over an order of magnitude.},
  YEAR = {2021},
  URL = {http://arxiv.org/abs/2105.02788v1},
  FILE = {2105.02788v1.pdf}
 }","Performance (training), Performance (rendering)","Conditional neural field, Coarse-to-fine, Sampling, Voxelization, Feature volume, Representation",NeRF,Occupancy,Per-scene,,,,,,,SIGGRAPH 2021,,Yes,,"Julien N. P. Martel, David B. Lindell, Connor Z. Lin, Eric R. Chan, Marco Monteiro, Gordon Wetzstein",martel2021acorn,00000127,"Neural representations have emerged as a new paradigm for applications in rendering, imaging, geometric modeling, and simulation. Compared to traditional representations such as meshes, point clouds, or volumes they can be flexibly incorporated into differentiable learning-based pipelines. While recent improvements to neural representations now make it possible to represent signals with fine details at moderate resolutions (e.g., for images and 3D shapes), adequately representing large-scale or complex scenes has proven a challenge. Current neural representations fail to accurately represent images at resolutions greater than a megapixel or 3D scenes with more than a few hundred thousand polygons. Here, we introduce a new hybrid implicit-explicit network architecture and training strategy that adaptively allocates resources during training and inference based on the local complexity of a signal of interest. Our approach uses a multiscale block-coordinate decomposition, similar to a quadtree or octree, that is optimized during training. The network architecture operates in two stages: using the bulk of the network parameters, a coordinate encoder generates a feature grid in a single forward pass. Then, hundreds or thousands of samples within each block can be efficiently evaluated using a lightweight feature decoder. With this hybrid implicit-explicit network architecture, we demonstrate the first experiments that fit gigapixel images to nearly 40 dB peak signal-to-noise ratio. Notably this represents an increase in scale of over 1000x compared to the resolution of previously demonstrated image-fitting experiments. Moreover, our approach is able to represent 3D shapes significantly faster and better than previous techniques; it reduces training times from days to hours or minutes and memory requirements by over an order of magnitude.",2,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnuf0_BGyB3qsBHX8bwcbgxs_vOsWALqULd-oM2LM6liA3BHZu_D35CL9iN85aK408yc
5/23/2021 18:17:41,Neural 3D Scene Compression via Model Compression,,5/7/2021,https://arxiv.org/pdf/2105.03120.pdf,,,,,,,"@article{isik2021neural,
  JOURNAL = {arXiv preprint arXiv:2105.03120},
  BOOKTITLE = {ArXiv Pre-print},
  ID = {isik2021neural},
  ENTRYTYPE = {article},
  AUTHOR = {Berivan Isik},
  TITLE = {Neural 3D Scene Compression via Model Compression},
  EPRINT = {2105.03120v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Rendering 3D scenes requires access to arbitrary viewpoints from the scene. Storage of such a 3D scene can be done in two ways; (1) storing 2D images taken from the 3D scene that can reconstruct the scene back through interpolations, or (2) storing a representation of the 3D scene itself that already encodes views from all directions. So far, traditional 3D compression methods have focused on the first type of storage and compressed the original 2D images with image compression techniques. With this approach, the user first decodes the stored 2D images and then renders the 3D scene. However, this separated procedure is inefficient since a large amount of 2D images have to be stored. In this work, we take a different approach and compress a functional representation of 3D scenes. In particular, we introduce a method to compress 3D scenes by compressing the neural networks that represent the scenes as neural radiance fields. Our method provides more efficient storage of 3D scenes since it does not store 2D images -- which are redundant when we render the scene from the neural functional representation.},
  YEAR = {2021},
  URL = {http://arxiv.org/abs/2105.03120v1},
  FILE = {2105.03120v1.pdf}
 }",Compression,,,,,,,,,,,ARXIV 2021,,,,Berivan Isik,isik2021neural,00000128,"Rendering 3D scenes requires access to arbitrary viewpoints from the scene. Storage of such a 3D scene can be done in two ways; (1) storing 2D images taken from the 3D scene that can reconstruct the scene back through interpolations, or (2) storing a representation of the 3D scene itself that already encodes views from all directions. So far, traditional 3D compression methods have focused on the first type of storage and compressed the original 2D images with image compression techniques. With this approach, the user first decodes the stored 2D images and then renders the 3D scene. However, this separated procedure is inefficient since a large amount of 2D images have to be stored. In this work, we take a different approach and compress a functional representation of 3D scenes. In particular, we introduce a method to compress 3D scenes by compressing the neural networks that represent the scenes as neural radiance fields. Our method provides more efficient storage of 3D scenes since it does not store 2D images -- which are redundant when we render the scene from the neural functional representation.",0,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnufIUvxeB2qjL0Wehm4wF7D-4mTzYhTg8UOLRz7xHKNVH73Lyqa3Oqeb66xBxWj0HEA
5/23/2021 18:12:40,Vision-based Neural Scene Representations for Spacecraft,,5/11/2021,https://arxiv.org/pdf/2105.06405.pdf,,,,,,,"@inproceedings{mergy2021visionbased,
  BOOKTITLE = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  ID = {mergy2021visionbased},
  ENTRYTYPE = {inproceedings},
  AUTHOR = {Anne Mergy and Gurvan Lecuyer and Dawa Derksen and Dario Izzo},
  TITLE = {Vision-based Neural Scene Representations for Spacecraft},
  EPRINT = {2105.06405v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {In advanced mission concepts with high levels of autonomy, spacecraft need to internally model the pose and shape of nearby orbiting objects. Recent works in neural scene representations show promising results for inferring generic three-dimensional scenes from optical images. Neural Radiance Fields (NeRF) have shown success in rendering highly specular surfaces using a large number of images and their pose. More recently, Generative Radiance Fields (GRAF) achieved full volumetric reconstruction of a scene from unposed images only, thanks to the use of an adversarial framework to train a NeRF. In this paper, we compare and evaluate the potential of NeRF and GRAF to render novel views and extract the 3D shape of two different spacecraft, the Soil Moisture and Ocean Salinity satellite of ESA's Living Planet Programme and a generic cube sat. Considering the best performances of both models, we observe that NeRF has the ability to render more accurate images regarding the material specularity of the spacecraft and its pose. For its part, GRAF generates precise novel views with accurate details even when parts of the satellites are shadowed while having the significant advantage of not needing any information about the relative pose.},
  YEAR = {2021},
  URL = {http://arxiv.org/abs/2105.06405v1},
  FILE = {2105.06405v1.pdf}
 }","Few-shot reconstruction, Science and engineering",Lifting 2D features to 3D,,,,,,,,,,CVPR 2021,,,,"Anne Mergy, Gurvan Lecuyer, Dawa Derksen, Dario Izzo",mergy2021visionbased,00000129,"In advanced mission concepts with high levels of autonomy, spacecraft need to internally model the pose and shape of nearby orbiting objects. Recent works in neural scene representations show promising results for inferring generic three-dimensional scenes from optical images. Neural Radiance Fields (NeRF) have shown success in rendering highly specular surfaces using a large number of images and their pose. More recently, Generative Radiance Fields (GRAF) achieved full volumetric reconstruction of a scene from unposed images only, thanks to the use of an adversarial framework to train a NeRF. In this paper, we compare and evaluate the potential of NeRF and GRAF to render novel views and extract the 3D shape of two different spacecraft, the Soil Moisture and Ocean Salinity satellite of ESA's Living Planet Programme and a generic cube sat. Considering the best performances of both models, we observe that NeRF has the ability to render more accurate images regarding the material specularity of the spacecraft and its pose. For its part, GRAF generates precise novel views with accurate details even when parts of the satellites are shadowed while having the significant advantage of not needing any information about the relative pose.",0,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnuc73tCCFtFwesBXRuJhAFCnpqrR85wo6C6-wGpAXRcPGd3QOxeovLelZmcuz8HS3U8
5/19/2021 18:01:42,Electrocardio Panorama: Synthesizing New ECG Views with Self-supervision,Nef-Net,5/12/2021,https://arxiv.org/pdf/2105.06293.pdf,,,,,,,"@inproceedings{chen2021nefnet,
  PUBLISHER = {International Joint Conferences on Artificial Intelligence Organization},
  BOOKTITLE = {Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence (IJCAI)},
  ID = {chen2021nefnet},
  ENTRYTYPE = {inproceedings},
  AUTHOR = {Jintai Chen and Xiangshang Zheng and Hongyun Yu and Danny Z. Chen and Jian Wu},
  TITLE = {Electrocardio Panorama: Synthesizing New ECG Views with Self-supervision},
  EPRINT = {2105.06293v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {eess.SP},
  ABSTRACT = {Multi-lead electrocardiogram (ECG) provides clinical information of heartbeats from several fixed viewpoints determined by the lead positioning. However, it is often not satisfactory to visualize ECG signals in these fixed and limited views, as some clinically useful information is represented only from a few specific ECG viewpoints. For the first time, we propose a new concept, Electrocardio Panorama, which allows visualizing ECG signals from any queried viewpoints. To build Electrocardio Panorama, we assume that an underlying electrocardio field exists, representing locations, magnitudes, and directions of ECG signals. We present a Neural electrocardio field Network (Nef-Net), which first predicts the electrocardio field representation by using a sparse set of one or few input ECG views and then synthesizes Electrocardio Panorama based on the predicted representations. Specially, to better disentangle electrocardio field information from viewpoint biases, a new Angular Encoding is proposed to process viewpoint angles. Also, we propose a self-supervised learning approach called Standin Learning, which helps model the electrocardio field without direct supervision. Further, with very few modifications, Nef-Net can also synthesize ECG signals from scratch. Experiments verify that our Nef-Net performs well on Electrocardio Panorama synthesis, and outperforms the previous work on the auxiliary tasks (ECG view transformation and ECG synthesis from scratch). The codes and the division labels of cardiac cycles and ECG deflections on Tianchi ECG and PTB datasets are available at https://github.com/WhatAShot/Electrocardio-Panorama.},
  YEAR = {2021},
  NOTE = {the 30th International Joint Conference on Artificial Intelligence (2021)},
  URL = {http://arxiv.org/abs/2105.06293v1},
  FILE = {2105.06293v1.pdf}
 }","Beyond graphics, Science and engineering",,,,,,,,,,,IJCAI 2021,,,,"Jintai Chen, Xiangshang Zheng, Hongyun Yu, Danny Z. Chen, Jian Wu",chen2021nefnet,00000130,"Multi-lead electrocardiogram (ECG) provides clinical information of heartbeats from several fixed viewpoints determined by the lead positioning. However, it is often not satisfactory to visualize ECG signals in these fixed and limited views, as some clinically useful information is represented only from a few specific ECG viewpoints. For the first time, we propose a new concept, Electrocardio Panorama, which allows visualizing ECG signals from any queried viewpoints. To build Electrocardio Panorama, we assume that an underlying electrocardio field exists, representing locations, magnitudes, and directions of ECG signals. We present a Neural electrocardio field Network (Nef-Net), which first predicts the electrocardio field representation by using a sparse set of one or few input ECG views and then synthesizes Electrocardio Panorama based on the predicted representations. Specially, to better disentangle electrocardio field information from viewpoint biases, a new Angular Encoding is proposed to process viewpoint angles. Also, we propose a self-supervised learning approach called Standin Learning, which helps model the electrocardio field without direct supervision. Further, with very few modifications, Nef-Net can also synthesize ECG signals from scratch. Experiments verify that our Nef-Net performs well on Electrocardio Panorama synthesis, and outperforms the previous work on the auxiliary tasks (ECG view transformation and ECG synthesis from scratch). The codes and the division labels of cardiac cycles and ECG deflections on Tianchi ECG and PTB datasets are available at https://github.com/WhatAShot/Electrocardio-Panorama.",0,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnufYh4swePWp6tbdEQxdxfGg0lv-eigMm07OSY0hYpEPRtM_vFyXsoTDXT1vAIHs6f8
5/23/2021 18:13:26,Neural Trajectory Fields for Dynamic Novel View Synthesis,DCT-NeRF,5/12/2021,https://arxiv.org/pdf/2105.05994.pdf,,,,,,,"@article{wang2021dctnerf,
  JOURNAL = {arXiv preprint arXiv:2105.05994},
  BOOKTITLE = {ArXiv Pre-print},
  ID = {wang2021dctnerf},
  ENTRYTYPE = {article},
  AUTHOR = {Chaoyang Wang and Ben Eckart and Simon Lucey and Orazio Gallo},
  TITLE = {Neural Trajectory Fields for Dynamic Novel View Synthesis},
  EPRINT = {2105.05994v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Recent approaches to render photorealistic views from a limited set of photographs have pushed the boundaries of our interactions with pictures of static scenes. The ability to recreate moments, that is, time-varying sequences, is perhaps an even more interesting scenario, but it remains largely unsolved. We introduce DCT-NeRF, a coordinatebased neural representation for dynamic scenes. DCTNeRF learns smooth and stable trajectories over the input sequence for each point in space. This allows us to enforce consistency between any two frames in the sequence, which results in high quality reconstruction, particularly in dynamic regions.},
  YEAR = {2021},
  URL = {http://arxiv.org/abs/2105.05994v1},
  FILE = {2105.05994v1.pdf}
 }",Dynamic,Warping field/Flow field,,,,,,,,,,ARXIV 2021,,,,"Chaoyang Wang, Ben Eckart, Simon Lucey, Orazio Gallo",wang2021dctnerf,00000131,"Recent approaches to render photorealistic views from a limited set of photographs have pushed the boundaries of our interactions with pictures of static scenes. The ability to recreate moments, that is, time-varying sequences, is perhaps an even more interesting scenario, but it remains largely unsolved. We introduce DCT-NeRF, a coordinatebased neural representation for dynamic scenes. DCTNeRF learns smooth and stable trajectories over the input sequence for each point in space. This allows us to enforce consistency between any two frames in the sequence, which results in high quality reconstruction, particularly in dynamic regions.",0,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnudJYgJYWiVFQuLgvckrRKeqQcvXk_3xAliR1xTi-PawUofWcpSVHW7yUxSMiJw9C1A
5/23/2021 18:11:30,Editing Conditional Radiance Fields,,5/13/2021,https://arxiv.org/pdf/2105.06466.pdf,http://editnerf.csail.mit.edu/,https://github.com/stevliu/editnerf,https://www.youtube.com/watch?v=9qwRD4ejOpw,,,,"@article{liu2021editing,
  JOURNAL = {arXiv preprint arXiv:2105.06466},
  BOOKTITLE = {ArXiv Pre-print},
  ID = {liu2021editing},
  ENTRYTYPE = {article},
  AUTHOR = {Steven Liu and Xiuming Zhang and Zhoutong Zhang and Richard Zhang and Jun-Yan Zhu and Bryan Russell},
  TITLE = {Editing Conditional Radiance Fields},
  EPRINT = {2105.06466v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {A neural radiance field (NeRF) is a scene model supporting high-quality view synthesis, optimized per scene. In this paper, we explore enabling user editing of a category-level NeRF - also known as a conditional radiance field - trained on a shape category. Specifically, we introduce a method for propagating coarse 2D user scribbles to the 3D space, to modify the color or shape of a local region. First, we propose a conditional radiance field that incorporates new modular network components, including a shape branch that is shared across object instances. Observing multiple instances of the same category, our model learns underlying part semantics without any supervision, thereby allowing the propagation of coarse 2D user scribbles to the entire 3D region (e.g., chair seat). Next, we propose a hybrid network update strategy that targets specific network components, which balances efficiency and accuracy. During user interaction, we formulate an optimization problem that both satisfies the user's constraints and preserves the original object structure. We demonstrate our approach on various editing tasks over three shape datasets and show that it outperforms prior neural editing approaches. Finally, we edit the appearance and shape of a real photograph and show that the edit propagates to extrapolated novel views.},
  YEAR = {2021},
  URL = {http://arxiv.org/abs/2105.06466v2},
  FILE = {2105.06466v2.pdf}
 }","Generalization, Editable",Per-instance fine-tuning,,,,,,,,,,ARXIV 2021,,,,"Steven Liu, Xiuming Zhang, Zhoutong Zhang, Richard Zhang, Jun-Yan Zhu, Bryan Russell",liu2021editing,00000132,"A neural radiance field (NeRF) is a scene model supporting high-quality view synthesis, optimized per scene. In this paper, we explore enabling user editing of a category-level NeRF - also known as a conditional radiance field - trained on a shape category. Specifically, we introduce a method for propagating coarse 2D user scribbles to the 3D space, to modify the color or shape of a local region. First, we propose a conditional radiance field that incorporates new modular network components, including a shape branch that is shared across object instances. Observing multiple instances of the same category, our model learns underlying part semantics without any supervision, thereby allowing the propagation of coarse 2D user scribbles to the entire 3D region (e.g., chair seat). Next, we propose a hybrid network update strategy that targets specific network components, which balances efficiency and accuracy. During user interaction, we formulate an optimization problem that both satisfies the user's constraints and preserves the original object structure. We demonstrate our approach on various editing tasks over three shape datasets and show that it outperforms prior neural editing approaches. Finally, we edit the appearance and shape of a real photograph and show that the edit propagates to extrapolated novel views.",0,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnueLKQga6dt5aN5NvITovoCuZmfvd2ZtZw-maSfcLSiGYESGaY9-9Nc4kAtIb_Ju26k
5/23/2021 18:13:01,Dynamic View Synthesis from Dynamic Monocular Video,,5/13/2021,https://arxiv.org/pdf/2105.06468.pdf,,,,,,,"@article{gao2021dynamic,
  JOURNAL = {arXiv preprint arXiv:2105.06468},
  BOOKTITLE = {ArXiv Pre-print},
  ID = {gao2021dynamic},
  ENTRYTYPE = {article},
  AUTHOR = {Chen Gao and Ayush Saraf and Johannes Kopf and Jia-Bin Huang},
  TITLE = {Dynamic View Synthesis from Dynamic Monocular Video},
  EPRINT = {2105.06468v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We present an algorithm for generating novel views at arbitrary viewpoints and any input time step given a monocular video of a dynamic scene. Our work builds upon recent advances in neural implicit representation and uses continuous and differentiable functions for modeling the time-varying structure and the appearance of the scene. We jointly train a time-invariant static NeRF and a time-varying dynamic NeRF, and learn how to blend the results in an unsupervised manner. However, learning this implicit function from a single video is highly ill-posed (with infinitely many solutions that match the input video). To resolve the ambiguity, we introduce regularization losses to encourage a more physically plausible solution. We show extensive quantitative and qualitative results of dynamic view synthesis from casually captured videos.},
  YEAR = {2021},
  URL = {http://arxiv.org/abs/2105.06468v1},
  FILE = {2105.06468v1.pdf}
 }",Dynamic,Warping field/Flow field,,,,,,,,,,ARXIV 2021,,,,"Chen Gao, Ayush Saraf, Johannes Kopf, Jia-Bin Huang",gao2021dynamic,00000133,"We present an algorithm for generating novel views at arbitrary viewpoints and any input time step given a monocular video of a dynamic scene. Our work builds upon recent advances in neural implicit representation and uses continuous and differentiable functions for modeling the time-varying structure and the appearance of the scene. We jointly train a time-invariant static NeRF and a time-varying dynamic NeRF, and learn how to blend the results in an unsupervised manner. However, learning this implicit function from a single video is highly ill-posed (with infinitely many solutions that match the input video). To resolve the ambiguity, we introduce regularization losses to encourage a more physically plausible solution. We show extensive quantitative and qualitative results of dynamic view synthesis from casually captured videos.",0,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnueSCpKXSGJnUWXZxooz7yc5nw0oU4FO8wLmQE6y5MGdcyZ2ck6uyFX-njj8I5B2N0E
5/23/2021 11:24:49,NeuLF: Efficient Novel View Synthesis with Neural 4D Light Field,NeuLF,5/15/2021,https://arxiv.org/pdf/2105.07112.pdf,,,,,,,"@article{liu2021neulf,
  JOURNAL = {arXiv preprint arXiv:2105.07112},
  BOOKTITLE = {ArXiv Pre-print},
  ID = {liu2021neulf},
  ENTRYTYPE = {article},
  AUTHOR = {Celong Liu and Zhong Li and Junsong Yuan and Yi Xu},
  TITLE = {NeuLF: Efficient Novel View Synthesis with Neural 4D Light Field},
  EPRINT = {2105.07112v4},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {In this paper, we present an efficient and robust deep learning solution for novel view synthesis of complex scenes. In our approach, a 3D scene is represented as a light field, i.e., a set of rays, each of which has a corresponding color when reaching the image plane. For efficient novel view rendering, we adopt a 4D parameterization of the light field, where each ray is characterized by a 4D parameter. We then formulate the light field as a 4D function that maps 4D coordinates to corresponding color values. We train a deep fully connected network to optimize this implicit function and memorize the 3D scene. Then, the scene-specific model is used to synthesize novel views. Different from previous light field approaches which require dense view sampling to reliably render novel views, our method can render novel views by sampling rays and querying the color for each ray from the network directly, thus enabling high-quality light field rendering with a sparser set of training images. Our method achieves state-of-the-art novel view synthesis results while maintaining an interactive frame rate.},
  YEAR = {2021},
  URL = {http://arxiv.org/abs/2105.07112v4},
  FILE = {2105.07112v4.pdf}
 }",Performance (rendering),Representation,NeRF,Light Field,,,,,,,,ARXIV 2021,,,,"Celong Liu, Zhong Li, Junsong Yuan, Yi Xu",liu2021neulf,00000134,"In this paper, we present an efficient and robust deep learning solution for novel view synthesis of complex scenes. In our approach, a 3D scene is represented as a light field, i.e., a set of rays, each of which has a corresponding color when reaching the image plane. For efficient novel view rendering, we adopt a 4D parameterization of the light field, where each ray is characterized by a 4D parameter. We then formulate the light field as a 4D function that maps 4D coordinates to corresponding color values. We train a deep fully connected network to optimize this implicit function and memorize the 3D scene. Then, the scene-specific model is used to synthesize novel views. Different from previous light field approaches which require dense view sampling to reliably render novel views, our method can render novel views by sampling rays and querying the color for each ray from the network directly, thus enabling high-quality light field rendering with a sparser set of training images. Our method achieves state-of-the-art novel view synthesis results while maintaining an interactive frame rate.",,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnucrCgLBrBGlExmB-ytxNT75DOTreyjRdKkme8xZGHrFGApTL-c1T_eq5Vs5IoWBORs
5/23/2021 12:29:18,Recursive-NeRF: An Efficient and Dynamically Growing NeRF,Recursive-NeRF,5/19/2021,https://arxiv.org/pdf/2105.09103.pdf,,https://github.com/Gword/Recursive-NeRF,,,,,"@article{yang2021recursivenerf,
  JOURNAL = {arXiv preprint arXiv:2105.09103},
  BOOKTITLE = {ArXiv Pre-print},
  ID = {yang2021recursivenerf},
  ENTRYTYPE = {article},
  AUTHOR = {Guo-Wei Yang and Wen-Yang Zhou and Hao-Yang Peng and Dun Liang and Tai-Jiang Mu and Shi-Min Hu},
  TITLE = {Recursive-NeRF: An Efficient and Dynamically Growing NeRF},
  EPRINT = {2105.09103v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {View synthesis methods using implicit continuous shape representations learned from a set of images, such as the Neural Radiance Field (NeRF) method, have gained increasing attention due to their high quality imagery and scalability to high resolution. However, the heavy computation required by its volumetric approach prevents NeRF from being useful in practice; minutes are taken to render a single image of a few megapixels. Now, an image of a scene can be rendered in a level-of-detail manner, so we posit that a complicated region of the scene should be represented by a large neural network while a small neural network is capable of encoding a simple region, enabling a balance between efficiency and quality. Recursive-NeRF is our embodiment of this idea, providing an efficient and adaptive rendering and training approach for NeRF. The core of Recursive-NeRF learns uncertainties for query coordinates, representing the quality of the predicted color and volumetric intensity at each level. Only query coordinates with high uncertainties are forwarded to the next level to a bigger neural network with a more powerful representational capability. The final rendered image is a composition of results from neural networks of all levels. Our evaluation on three public datasets shows that Recursive-NeRF is more efficient than NeRF while providing state-of-the-art quality. The code will be available at https://github.com/Gword/Recursive-NeRF.},
  YEAR = {2021},
  URL = {http://arxiv.org/abs/2105.09103v1},
  FILE = {2105.09103v1.pdf}
 }",,"Coarse-to-fine, Sampling, Volume partitioning",NeRF,Density,Per-scene,,,,,,,ARXIV 2021,,,,"Guo-Wei Yang, Wen-Yang Zhou, Hao-Yang Peng, Dun Liang, Tai-Jiang Mu, Shi-Min Hu",yang2021recursivenerf,00000135,"View synthesis methods using implicit continuous shape representations learned from a set of images, such as the Neural Radiance Field (NeRF) method, have gained increasing attention due to their high quality imagery and scalability to high resolution. However, the heavy computation required by its volumetric approach prevents NeRF from being useful in practice; minutes are taken to render a single image of a few megapixels. Now, an image of a scene can be rendered in a level-of-detail manner, so we posit that a complicated region of the scene should be represented by a large neural network while a small neural network is capable of encoding a simple region, enabling a balance between efficiency and quality. Recursive-NeRF is our embodiment of this idea, providing an efficient and adaptive rendering and training approach for NeRF. The core of Recursive-NeRF learns uncertainties for query coordinates, representing the quality of the predicted color and volumetric intensity at each level. Only query coordinates with high uncertainties are forwarded to the next level to a bigger neural network with a more powerful representational capability. The final rendered image is a composition of results from neural networks of all levels. Our evaluation on three public datasets shows that Recursive-NeRF is more efficient than NeRF while providing state-of-the-art quality. The code will be available at https://github.com/Gword/Recursive-NeRF.",,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnudMef7jzAVPv3-q0Nma7WzxkO5b06e6zwo3es8L_lS1MUnmnD_yiBGx-adUq1ovcxk
7/19/2021 21:36:40,Neural Radiosity,,5/26/2021,https://arxiv.org/pdf/2105.12319.pdf,,,,,,,"@article{hadadan2021neural,
  JOURNAL = {arXiv preprint arXiv:2105.12319},
  BOOKTITLE = {ArXiv Pre-print},
  ID = {hadadan2021neural},
  ENTRYTYPE = {article},
  AUTHOR = {Saeed Hadadan and Shuhong Chen and Matthias Zwicker},
  TITLE = {Neural Radiosity},
  EPRINT = {2105.12319v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.GR},
  ABSTRACT = {We introduce Neural Radiosity, an algorithm to solve the rendering equation by minimizing the norm of its residual similar as in traditional radiosity techniques. Traditional basis functions used in radiosity techniques, such as piecewise polynomials or meshless basis functions are typically limited to representing isotropic scattering from diffuse surfaces. Instead, we propose to leverage neural networks to represent the full four-dimensional radiance distribution, directly optimizing network parameters to minimize the norm of the residual. Our approach decouples solving the rendering equation from rendering (perspective) images similar as in traditional radiosity techniques, and allows us to efficiently synthesize arbitrary views of a scene. In addition, we propose a network architecture using geometric learnable features that improves convergence of our solver compared to previous techniques. Our approach leads to an algorithm that is simple to implement, and we demonstrate its effectiveness on a variety of scenes with non-diffuse surfaces.},
  YEAR = {2021},
  URL = {http://arxiv.org/abs/2105.12319v1},
  FILE = {2105.12319v1.pdf}
 }",Material/lighting estimation,"Voxelization, Feature volume",,,,,,,,,,ARXIV 2021,,,,"Saeed Hadadan, Shuhong Chen, Matthias Zwicker",hadadan2021neural,00000136,"We introduce Neural Radiosity, an algorithm to solve the rendering equation by minimizing the norm of its residual similar as in traditional radiosity techniques. Traditional basis functions used in radiosity techniques, such as piecewise polynomials or meshless basis functions are typically limited to representing isotropic scattering from diffuse surfaces. Instead, we propose to leverage neural networks to represent the full four-dimensional radiance distribution, directly optimizing network parameters to minimize the norm of the residual. Our approach decouples solving the rendering equation from rendering (perspective) images similar as in traditional radiosity techniques, and allows us to efficiently synthesize arbitrary views of a scene. In addition, we propose a network architecture using geometric learnable features that improves convergence of our solver compared to previous techniques. Our approach leads to an algorithm that is simple to implement, and we demonstrate its effectiveness on a variety of scenes with non-diffuse surfaces.",,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnuesbgOGO4eh-GYQVqDDDstsdt71omMid_g52zdAUllfdP7kWkO8iYW-zXomOSN3H64
7/19/2021 21:34:55,Stylizing 3D Scene via Implicit Representation and HyperNetwork,,5/27/2021,https://arxiv.org/pdf/2105.13016.pdf,,,https://www.youtube.com/watch?v=MJqcI40sXhk,,,,"@article{chiang2021stylizing,
  JOURNAL = {arXiv preprint arXiv:2105.13016},
  BOOKTITLE = {ArXiv Pre-print},
  ID = {chiang2021stylizing},
  ENTRYTYPE = {article},
  AUTHOR = {Pei-Ze Chiang and Meng-Shiun Tsai and Hung-Yu Tseng and Wei-sheng Lai and Wei-Chen Chiu},
  TITLE = {Stylizing 3D Scene via Implicit Representation and HyperNetwork},
  EPRINT = {2105.13016v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {In this work, we aim to address the 3D scene stylization problem - generating stylized images of the scene at arbitrary novel view angles. A straightforward solution is to combine existing novel view synthesis and image/video style transfer approaches, which often leads to blurry results or inconsistent appearance. Inspired by the high quality results of the neural radiance fields (NeRF) method, we propose a joint framework to directly render novel views with the desired style. Our framework consists of two components: an implicit representation of the 3D scene with the neural radiance field model, and a hypernetwork to transfer the style information into the scene representation. In particular, our implicit representation model disentangles the scene into the geometry and appearance branches, and the hypernetwork learns to predict the parameters of the appearance branch from the reference style image. To alleviate the training difficulties and memory burden, we propose a two-stage training procedure and a patch sub-sampling approach to optimize the style and content losses with the neural radiance field model. After optimization, our model is able to render consistent novel views at arbitrary view angles with arbitrary style. Both quantitative evaluation and human subject study have demonstrated that the proposed method generates faithful stylization results with consistent appearance across different views.},
  YEAR = {2021},
  URL = {http://arxiv.org/abs/2105.13016v2},
  FILE = {2105.13016v2.pdf}
 }",Editable,"Hypernetwork, Data-driven",NeRF,Density,,,,,,,,ARXIV 2021,,No,Direct,"Pei-Ze Chiang, Meng-Shiun Tsai, Hung-Yu Tseng, Wei-sheng Lai, Wei-Chen Chiu",chiang2021stylizing,00000137,"In this work, we aim to address the 3D scene stylization problem - generating stylized images of the scene at arbitrary novel view angles. A straightforward solution is to combine existing novel view synthesis and image/video style transfer approaches, which often leads to blurry results or inconsistent appearance. Inspired by the high quality results of the neural radiance fields (NeRF) method, we propose a joint framework to directly render novel views with the desired style. Our framework consists of two components: an implicit representation of the 3D scene with the neural radiance field model, and a hypernetwork to transfer the style information into the scene representation. In particular, our implicit representation model disentangles the scene into the geometry and appearance branches, and the hypernetwork learns to predict the parameters of the appearance branch from the reference style image. To alleviate the training difficulties and memory burden, we propose a two-stage training procedure and a patch sub-sampling approach to optimize the style and content losses with the neural radiance field model. After optimization, our model is able to render consistent novel views at arbitrary view angles with arbitrary style. Both quantitative evaluation and human subject study have demonstrated that the proposed method generates faithful stylization results with consistent appearance across different views.",,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnufAk-V0gSiqeqgPNhBtOkFwCSfRwbMCPI3y5b6PmTQbAmi_AIaRHF5sv0iTaCIHpF4
7/19/2021 21:41:31,Geodesy of irregular small bodies via neural density fields: geodesyNets,geodesyNets,5/27/2021,https://arxiv.org/pdf/2105.13031.pdf,https://github.com/darioizzo/geodesynets,,,,,,"@article{izzo2021geodesynets,
  JOURNAL = {arXiv preprint arXiv:2105.13031},
  BOOKTITLE = {ArXiv Pre-print},
  ID = {izzo2021geodesynets},
  ENTRYTYPE = {article},
  AUTHOR = {Dario Izzo and Pablo Gomez},
  TITLE = {Geodesy of irregular small bodies via neural density fields: geodesyNets},
  EPRINT = {2105.13031v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {astro-ph.EP},
  ABSTRACT = {We present a novel approach based on artificial neural networks, so-called geodesyNets, and present compelling evidence of their ability to serve as accurate geodetic models of highly irregular bodies using minimal prior information on the body. The approach does not rely on the body shape information but, if available, can harness it. GeodesyNets learn a three-dimensional, differentiable, function representing the body density, which we call neural density field. The body shape, as well as other geodetic properties, can easily be recovered. We investigate six different shapes including the bodies 101955 Bennu, 67P Churyumov-Gerasimenko, 433 Eros and 25143 Itokawa for which shape models developed during close proximity surveys are available. Both heterogeneous and homogeneous mass distributions are considered. The gravitational acceleration computed from the trained geodesyNets models, as well as the inferred body shape, show great accuracy in all cases with a relative error on the predicted acceleration smaller than 1\% even close to the asteroid surface. When the body shape information is available, geodesyNets can seamlessly exploit it and be trained to represent a high-fidelity neural density field able to give insights into the internal structure of the body. This work introduces a new unexplored approach to geodesy, adding a powerful tool to consolidated ones based on spherical harmonics, mascon models and polyhedral gravity.},
  YEAR = {2021},
  URL = {http://arxiv.org/abs/2105.13031v1},
  FILE = {2105.13031v1.pdf}
 }","Beyond graphics, Alternative imaging, Science and engineering",,,,,,,,,,,ARXIV 2021,,,,"Dario Izzo, Pablo Gómez",izzo2021geodesynets,00000138,"We present a novel approach based on artificial neural networks, so-called geodesyNets, and present compelling evidence of their ability to serve as accurate geodetic models of highly irregular bodies using minimal prior information on the body. The approach does not rely on the body shape information but, if available, can harness it. GeodesyNets learn a three-dimensional, differentiable, function representing the body density, which we call neural density field. The body shape, as well as other geodetic properties, can easily be recovered. We investigate six different shapes including the bodies 101955 Bennu, 67P Churyumov-Gerasimenko, 433 Eros and 25143 Itokawa for which shape models developed during close proximity surveys are available. Both heterogeneous and homogeneous mass distributions are considered. The gravitational acceleration computed from the trained geodesyNets models, as well as the inferred body shape, show great accuracy in all cases with a relative error on the predicted acceleration smaller than 1\% even close to the asteroid surface. When the body shape information is available, geodesyNets can seamlessly exploit it and be trained to represent a high-fidelity neural density field able to give insights into the internal structure of the body. This work introduces a new unexplored approach to geodesy, adding a powerful tool to consolidated ones based on spherical harmonics, mascon models and polyhedral gravity.",,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnuePnu28a0HG_KMRDz5GKRd6VVLFDQTkq_r4UEtn_QiEX1JL-uQV18QQ81hUNYDF37A
7/19/2021 21:33:42,NeRFactor: Neural Factorization of Shape and Reflectance Under an Unknown Illumination,NeRFactor,6/3/2021,https://arxiv.org/pdf/2106.01970.pdf,https://people.csail.mit.edu/xiuming/projects/nerfactor/,https://github.com/google/nerfactor,https://www.youtube.com/watch?v=UUVSPJlwhPg,,,,"@article{zhang2021nerfactor,
  JOURNAL = {arXiv preprint arXiv:2106.01970},
  BOOKTITLE = {ArXiv Pre-print},
  ID = {zhang2021nerfactor},
  ENTRYTYPE = {article},
  AUTHOR = {Xiuming Zhang and Pratul P. Srinivasan and Boyang Deng and Paul Debevec and William T. Freeman and Jonathan T. Barron},
  TITLE = {NeRFactor: Neural Factorization of Shape and Reflectance Under an Unknown Illumination},
  EPRINT = {2106.01970v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We address the problem of recovering the shape and spatially-varying reflectance of an object from posed multi-view images of the object illuminated by one unknown lighting condition. This enables the rendering of novel views of the object under arbitrary environment lighting and editing of the object's material properties. The key to our approach, which we call Neural Radiance Factorization (NeRFactor), is to distill the volumetric geometry of a Neural Radiance Field (NeRF) [Mildenhall et al. 2020] representation of the object into a surface representation and then jointly refine the geometry while solving for the spatially-varying reflectance and the environment lighting. Specifically, NeRFactor recovers 3D neural fields of surface normals, light visibility, albedo, and Bidirectional Reflectance Distribution Functions (BRDFs) without any supervision, using only a re-rendering loss, simple smoothness priors, and a data-driven BRDF prior learned from real-world BRDF measurements. By explicitly modeling light visibility, NeRFactor is able to separate shadows from albedo and synthesize realistic soft or hard shadows under arbitrary lighting conditions. NeRFactor is able to recover convincing 3D models for free-viewpoint relighting in this challenging and underconstrained capture setup for both synthetic and real scenes. Qualitative and quantitative experiments show that NeRFactor outperforms classic and deep learning-based state of the art across various tasks. Our code and data are available at people.csail.mit.edu/xiuming/projects/nerfactor/.},
  YEAR = {2021},
  URL = {http://arxiv.org/abs/2106.01970v1},
  FILE = {2106.01970v1.pdf}
 }","Editable, Material/lighting estimation",Data-driven,NeRF,Density,Per-scene,,,,,,,ARXIV 2021,,No,Direct,"Xiuming Zhang, Pratul P. Srinivasan, Boyang Deng, Paul Debevec, William T. Freeman, Jonathan T. Barron",zhang2021nerfactor,00000139,"We address the problem of recovering the shape and spatially-varying reflectance of an object from posed multi-view images of the object illuminated by one unknown lighting condition. This enables the rendering of novel views of the object under arbitrary environment lighting and editing of the object's material properties. The key to our approach, which we call Neural Radiance Factorization (NeRFactor), is to distill the volumetric geometry of a Neural Radiance Field (NeRF) [Mildenhall et al. 2020] representation of the object into a surface representation and then jointly refine the geometry while solving for the spatially-varying reflectance and the environment lighting. Specifically, NeRFactor recovers 3D neural fields of surface normals, light visibility, albedo, and Bidirectional Reflectance Distribution Functions (BRDFs) without any supervision, using only a re-rendering loss, simple smoothness priors, and a data-driven BRDF prior learned from real-world BRDF measurements. By explicitly modeling light visibility, NeRFactor is able to separate shadows from albedo and synthesize realistic soft or hard shadows under arbitrary lighting conditions. NeRFactor is able to recover convincing 3D models for free-viewpoint relighting in this challenging and underconstrained capture setup for both synthetic and real scenes. Qualitative and quantitative experiments show that NeRFactor outperforms classic and deep learning-based state of the art across various tasks. Our code and data are available at people.csail.mit.edu/xiuming/projects/nerfactor/.",,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnuey_yD2CxWj90AgMr9BfenY_JXOfq5rvzJ6AG4EgQlzaH2WYlG_TYb7AJ_FZ-D8e5I
7/19/2021 21:42:23,Spline Positional Encoding for Learning 3D Implicit Signed Distance Fields,,6/3/2021,https://arxiv.org/pdf/2106.01553.pdf,,,,,,,"@inproceedings{wang2021spline,
  PUBLISHER = {International Joint Conferences on Artificial Intelligence Organization},
  BOOKTITLE = {Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence (IJCAI)},
  ID = {wang2021spline},
  ENTRYTYPE = {inproceedings},
  AUTHOR = {Peng-Shuai Wang and Yang Liu and Yu-Qi Yang and Xin Tong},
  TITLE = {Spline Positional Encoding for Learning 3D Implicit Signed Distance Fields},
  EPRINT = {2106.01553v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Multilayer perceptrons (MLPs) have been successfully used to represent 3D shapes implicitly and compactly, by mapping 3D coordinates to the corresponding signed distance values or occupancy values. In this paper, we propose a novel positional encoding scheme, called Spline Positional Encoding, to map the input coordinates to a high dimensional space before passing them to MLPs, for helping to recover 3D signed distance fields with fine-scale geometric details from unorganized 3D point clouds. We verified the superiority of our approach over other positional encoding schemes on tasks of 3D shape reconstruction from input point clouds and shape space learning. The efficacy of our approach extended to image reconstruction is also demonstrated and evaluated.},
  YEAR = {2021},
  URL = {http://arxiv.org/abs/2106.01553v1},
  FILE = {2106.01553v1.pdf}
 }",Fundamentals,,Other,SDF,,,,,,,,IJCAI 2021,,Yes,Direct,"Peng-Shuai Wang, Yang Liu, Yu-Qi Yang, Xin Tong",wang2021spline,00000140,"Multilayer perceptrons (MLPs) have been successfully used to represent 3D shapes implicitly and compactly, by mapping 3D coordinates to the corresponding signed distance values or occupancy values. In this paper, we propose a novel positional encoding scheme, called Spline Positional Encoding, to map the input coordinates to a high dimensional space before passing them to MLPs, for helping to recover 3D signed distance fields with fine-scale geometric details from unorganized 3D point clouds. We verified the superiority of our approach over other positional encoding schemes on tasks of 3D shape reconstruction from input point clouds and shape space learning. The efficacy of our approach extended to image reconstruction is also demonstrated and evaluated.",,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnucvVDfS7O1Has7s1iDoVu5TB5mQD45h8ctX6Y3DppmR8LDNp0RU54gCyaznUdBsmB8
7/19/2021 21:46:34,Neural Actor: Neural Free-view Synthesis of Human Actors with Pose Control,NA,6/3/2021,https://arxiv.org/pdf/2106.02019.pdf,http://gvv.mpi-inf.mpg.de/projects/NeuralActor/,Coming soon,http://gvv.mpi-inf.mpg.de/projects/NeuralActor/mp4/main_video_arxiv3.mp4,,,,"@article{liu2021na,
  PUBLISHER = {Association for Computing Machinery},
  JOURNAL = {ACM Transactions on Graphics (TOG)},
  ID = {liu2021na},
  ENTRYTYPE = {article},
  AUTHOR = {Lingjie Liu and Marc Habermann and Viktor Rudnev and Kripasindhu Sarkar and Jiatao Gu and Christian Theobalt},
  TITLE = {Neural Actor: Neural Free-view Synthesis of Human Actors with Pose Control},
  EPRINT = {2106.02019v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We propose Neural Actor (NA), a new method for high-quality synthesis of humans from arbitrary viewpoints and under arbitrary controllable poses. Our method is built upon recent neural scene representation and rendering works which learn representations of geometry and appearance from only 2D images. While existing works demonstrated compelling rendering of static scenes and playback of dynamic scenes, photo-realistic reconstruction and rendering of humans with neural implicit methods, in particular under user-controlled novel poses, is still difficult. To address this problem, we utilize a coarse body model as the proxy to unwarp the surrounding 3D space into a canonical pose. A neural radiance field learns pose-dependent geometric deformations and pose- and view-dependent appearance effects in the canonical space from multi-view video input. To synthesize novel views of high fidelity dynamic geometry and appearance, we leverage 2D texture maps defined on the body model as latent variables for predicting residual deformations and the dynamic appearance. Experiments demonstrate that our method achieves better quality than the state-of-the-arts on playback as well as novel pose synthesis, and can even generalize well to new poses that starkly differ from the training poses. Furthermore, our method also supports body shape control of the synthesized results.},
  YEAR = {2021},
  URL = {http://arxiv.org/abs/2106.02019v1},
  FILE = {2106.02019v1.pdf}
 }","Human body, Editable",Lifting 2D features to 3D,,Density,,,,,,,,SIGGRAPH 2021,Coming soon,No,Direct,"Lingjie Liu, Marc Habermann, Viktor Rudnev, Kripasindhu Sarkar, Jiatao Gu, Christian Theobalt",liu2021na,00000141,"We propose Neural Actor (NA), a new method for high-quality synthesis of humans from arbitrary viewpoints and under arbitrary controllable poses. Our method is built upon recent neural scene representation and rendering works which learn representations of geometry and appearance from only 2D images. While existing works demonstrated compelling rendering of static scenes and playback of dynamic scenes, photo-realistic reconstruction and rendering of humans with neural implicit methods, in particular under user-controlled novel poses, is still difficult. To address this problem, we utilize a coarse body model as the proxy to unwarp the surrounding 3D space into a canonical pose. A neural radiance field learns pose-dependent geometric deformations and pose- and view-dependent appearance effects in the canonical space from multi-view video input. To synthesize novel views of high fidelity dynamic geometry and appearance, we leverage 2D texture maps defined on the body model as latent variables for predicting residual deformations and the dynamic appearance. Experiments demonstrate that our method achieves better quality than the state-of-the-arts on playback as well as novel pose synthesis, and can even generalize well to new poses that starkly differ from the training poses. Furthermore, our method also supports body shape control of the synthesized results.",,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnucdX5uorgMWX0SjmK6AfGv7ax9P7CLWBk9MSWT6paSEWg3OmvRciID60QYk__4cVXQ
7/19/2021 21:38:52,Light Field Networks: Neural Scene Representations with Single-Evaluation Rendering,LFNs,6/4/2021,https://arxiv.org/pdf/2106.02634.pdf,https://vsitzmann.github.io/lfns/,Coming soon,https://www.youtube.com/watch?v=x3sSreTNFw4,,,,"@inproceedings{sitzmann2021lfns,
  PUBLISHER = {International Joint Conferences on Artificial Intelligence Organization},
  BOOKTITLE = {Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence (IJCAI)},
  ID = {sitzmann2021lfns},
  ENTRYTYPE = {inproceedings},
  AUTHOR = {Vincent Sitzmann and Semon Rezchikov and William T. Freeman and Joshua B. Tenenbaum and Fredo Durand},
  TITLE = {Light Field Networks: Neural Scene Representations with Single-Evaluation Rendering},
  EPRINT = {2106.02634v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Inferring representations of 3D scenes from 2D observations is a fundamental problem of computer graphics, computer vision, and artificial intelligence. Emerging 3D-structured neural scene representations are a promising approach to 3D scene understanding. In this work, we propose a novel neural scene representation, Light Field Networks or LFNs, which represent both geometry and appearance of the underlying 3D scene in a 360-degree, four-dimensional light field parameterized via a neural implicit representation. Rendering a ray from an LFN requires only a *single* network evaluation, as opposed to hundreds of evaluations per ray for ray-marching or volumetric based renderers in 3D-structured neural scene representations. In the setting of simple scenes, we leverage meta-learning to learn a prior over LFNs that enables multi-view consistent light field reconstruction from as little as a single image observation. This results in dramatic reductions in time and memory complexity, and enables real-time rendering. The cost of storing a 360-degree light field via an LFN is two orders of magnitude lower than conventional methods such as the Lumigraph. Utilizing the analytical differentiability of neural implicit representations and a novel parameterization of light space, we further demonstrate the extraction of sparse depth maps from LFNs.},
  YEAR = {2021},
  URL = {http://arxiv.org/abs/2106.02634v1},
  FILE = {2106.02634v1.pdf}
 }","Performance (training), Performance (rendering), Generalization","Conditional neural field, Hypernetwork, Representation, Sampling",,,,,,,,,,IJCAI 2021,Coming soon,,,"Vincent Sitzmann, Semon Rezchikov, William T. Freeman, Joshua B. Tenenbaum, Fredo Durand",sitzmann2021lfns,00000142,"Inferring representations of 3D scenes from 2D observations is a fundamental problem of computer graphics, computer vision, and artificial intelligence. Emerging 3D-structured neural scene representations are a promising approach to 3D scene understanding. In this work, we propose a novel neural scene representation, Light Field Networks or LFNs, which represent both geometry and appearance of the underlying 3D scene in a 360-degree, four-dimensional light field parameterized via a neural implicit representation. Rendering a ray from an LFN requires only a *single* network evaluation, as opposed to hundreds of evaluations per ray for ray-marching or volumetric based renderers in 3D-structured neural scene representations. In the setting of simple scenes, we leverage meta-learning to learn a prior over LFNs that enables multi-view consistent light field reconstruction from as little as a single image observation. This results in dramatic reductions in time and memory complexity, and enables real-time rendering. The cost of storing a 360-degree light field via an LFN is two orders of magnitude lower than conventional methods such as the Lumigraph. Utilizing the analytical differentiability of neural implicit representations and a novel parameterization of light space, we further demonstrate the extraction of sparse depth maps from LFNs.",,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnufvBHIey_OALmznYDZ7Yq-uGOflgc6bBah2hhmZw8KnPj-hPyivF8A_-2PAbfzr2Vw
6/29/2021 15:23:25,Deep Medial Fields,DMF,6/7/2021,https://arxiv.org/pdf/2106.03804.pdf,,,,,,,"@article{rebain2021dmf,
  JOURNAL = {arXiv preprint arXiv:2106.03804},
  BOOKTITLE = {ArXiv Pre-print},
  ID = {rebain2021dmf},
  ENTRYTYPE = {article},
  AUTHOR = {Daniel Rebain and Ke Li and Vincent Sitzmann and Soroosh Yazdani and Kwang Moo Yi and Andrea Tagliasacchi},
  TITLE = {Deep Medial Fields},
  EPRINT = {2106.03804v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.GR},
  ABSTRACT = {Implicit representations of geometry, such as occupancy fields or signed distance fields (SDF), have recently re-gained popularity in encoding 3D solid shape in a functional form. In this work, we introduce medial fields: a field function derived from the medial axis transform (MAT) that makes available information about the underlying 3D geometry that is immediately useful for a number of downstream tasks. In particular, the medial field encodes the local thickness of a 3D shape, and enables O(1) projection of a query point onto the medial axis. To construct the medial field we require nothing but the SDF of the shape itself, thus allowing its straightforward incorporation in any application that relies on signed distance fields. Working in unison with the O(1) surface projection supported by the SDF, the medial field opens the door for an entirely new set of efficient, shape-aware operations on implicit representations. We present three such applications, including a modification to sphere tracing that renders implicit representations with better convergence properties, a fast construction method for memory-efficient rigid-body collision proxies, and an efficient approximation of ambient occlusion that remains stable with respect to viewpoint variations.},
  YEAR = {2021},
  URL = {http://arxiv.org/abs/2106.03804v1},
  FILE = {2106.03804v1.pdf}
 }",Fundamentals,Representation,NeRF,Medial Field,Per-scene,,,,,,,ARXIV 2021,,Yes,,"Daniel Rebain, Ke Li, Vincent Sitzmann, Soroosh Yazdani, Kwang Moo Yi, Andrea Tagliasacchi",rebain2021dmf,00000143,"Implicit representations of geometry, such as occupancy fields or signed distance fields (SDF), have recently re-gained popularity in encoding 3D solid shape in a functional form. In this work, we introduce medial fields: a field function derived from the medial axis transform (MAT) that makes available information about the underlying 3D geometry that is immediately useful for a number of downstream tasks. In particular, the medial field encodes the local thickness of a 3D shape, and enables O(1) projection of a query point onto the medial axis. To construct the medial field we require nothing but the SDF of the shape itself, thus allowing its straightforward incorporation in any application that relies on signed distance fields. Working in unison with the O(1) surface projection supported by the SDF, the medial field opens the door for an entirely new set of efficient, shape-aware operations on implicit representations. We present three such applications, including a modification to sphere tracing that renders implicit representations with better convergence properties, a fast construction method for memory-efficient rigid-body collision proxies, and an efficient approximation of ambient occlusion that remains stable with respect to viewpoint variations.",,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnucrNcYA5XBlaiBDBpYN8sfWZdTN9WOP0MiZmrILhvvGwL-3uOv5AvUEMxswG2FXEOc
7/19/2021 21:51:54,MoCo-Flow: Neural Motion Consensus Flow for Dynamic Humans in Stationary Monocular Cameras,MoCo-Flow,6/8/2021,https://arxiv.org/pdf/2106.04477.pdf,https://wyysf-98.github.io/MoCo_Flow/,Coming soon,,,,,"@article{chen2021mocoflow,
  JOURNAL = {arXiv preprint arXiv:2106.04477},
  BOOKTITLE = {ArXiv Pre-print},
  ID = {chen2021mocoflow},
  ENTRYTYPE = {article},
  AUTHOR = {Xuelin Chen and Weiyu Li and Daniel Cohen-Or and Niloy J. Mitra and Baoquan Chen},
  TITLE = {MoCo-Flow: Neural Motion Consensus Flow for Dynamic Humans in Stationary Monocular Cameras},
  EPRINT = {2106.04477v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Synthesizing novel views of dynamic humans from stationary monocular cameras is a popular scenario. This is particularly attractive as it does not require static scenes, controlled environments, or specialized hardware. In contrast to techniques that exploit multi-view observations to constrain the modeling, given a single fixed viewpoint only, the problem of modeling the dynamic scene is significantly more under-constrained and ill-posed. In this paper, we introduce Neural Motion Consensus Flow (MoCo-Flow), a representation that models the dynamic scene using a 4D continuous time-variant function. The proposed representation is learned by an optimization which models a dynamic scene that minimizes the error of rendering all observation images. At the heart of our work lies a novel optimization formulation, which is constrained by a motion consensus regularization on the motion flow. We extensively evaluate MoCo-Flow on several datasets that contain human motions of varying complexity, and compare, both qualitatively and quantitatively, to several baseline methods and variants of our methods. Pretrained model, code, and data will be released for research purposes upon paper acceptance.},
  YEAR = {2021},
  URL = {http://arxiv.org/abs/2106.04477v1},
  FILE = {2106.04477v1.pdf}
 }",Human body,"Coarse-to-fine, Warping field/Flow field",NeRF,Density,Category-level,,,,,,,ARXIV 2021,Coming soon,No,Direct,"Xuelin Chen, Weiyu Li, Daniel Cohen-Or, Niloy J. Mitra, Baoquan Chen",chen2021mocoflow,00000144,"Synthesizing novel views of dynamic humans from stationary monocular cameras is a popular scenario. This is particularly attractive as it does not require static scenes, controlled environments, or specialized hardware. In contrast to techniques that exploit multi-view observations to constrain the modeling, given a single fixed viewpoint only, the problem of modeling the dynamic scene is significantly more under-constrained and ill-posed. In this paper, we introduce Neural Motion Consensus Flow (MoCo-Flow), a representation that models the dynamic scene using a 4D continuous time-variant function. The proposed representation is learned by an optimization which models a dynamic scene that minimizes the error of rendering all observation images. At the heart of our work lies a novel optimization formulation, which is constrained by a motion consensus regularization on the motion flow. We extensively evaluate MoCo-Flow on several datasets that contain human motions of varying complexity, and compare, both qualitatively and quantitatively, to several baseline methods and variants of our methods. Pretrained model, code, and data will be released for research purposes upon paper acceptance.",,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnudxwEnTCiBZza5h6qsq8f_sCBmR9XVYA_qvo4dmWBr3rYEOs8Snd5VPhZdNwaYHZhg
7/28/2021 23:02:16,DoubleField: Bridging the Neural Surface and Radiance Fields for High-fidelity Human Rendering,DoubleField,6/8/2021,https://arxiv.org/pdf/2106.03798.pdf,http://www.liuyebin.com/dbfield/dbfield.html,Coming soon,,,http://www.liuyebin.com/dbfield/assets/supp2.mp4,,"@article{shao2021doublefield,
  JOURNAL = {arXiv preprint arXiv:2106.03798},
  BOOKTITLE = {ArXiv Pre-print},
  ID = {shao2021doublefield},
  ENTRYTYPE = {article},
  AUTHOR = {Ruizhi Shao and Hongwen Zhang and He Zhang and Yanpei Cao and Tao Yu and Yebin Liu},
  TITLE = {DoubleField: Bridging the Neural Surface and Radiance Fields for High-fidelity Human Rendering},
  EPRINT = {2106.03798v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We introduce DoubleField, a novel representation combining the merits of both surface field and radiance field for high-fidelity human rendering. Within DoubleField, the surface field and radiance field are associated together by a shared feature embedding and a surface-guided sampling strategy. In this way, DoubleField has a continuous but disentangled learning space for geometry and appearance modeling, which supports fast training, inference, and finetuning. To achieve high-fidelity free-viewpoint rendering, DoubleField is further augmented to leverage ultra-high-resolution inputs, where a view-to-view transformer and a transfer learning scheme are introduced for more efficient learning and finetuning from sparse-view inputs at original resolutions. The efficacy of DoubleField is validated by the quantitative evaluations on several datasets and the qualitative results in a real-world sparse multi-view system, showing its superior capability for photo-realistic free-viewpoint human rendering. For code and demo video, please refer to our project page: http://www.liuyebin.com/dbfield/dbfield.html.},
  YEAR = {2021},
  URL = {http://arxiv.org/abs/2106.03798v2},
  FILE = {2106.03798v2.pdf}
 }","Performance (training), Human body, Few-shot reconstruction",Lifting 2D features to 3D,NeRF,Density,Per-scene,,,,,,,ARXIV 2021,,No,Direct,"Ruizhi Shao, Hongwen Zhang, He Zhang, Yanpei Cao, Tao Yu, Yebin Liu",shao2021doublefield,00000145,"We introduce DoubleField, a novel representation combining the merits of both surface field and radiance field for high-fidelity human rendering. Within DoubleField, the surface field and radiance field are associated together by a shared feature embedding and a surface-guided sampling strategy. In this way, DoubleField has a continuous but disentangled learning space for geometry and appearance modeling, which supports fast training, inference, and finetuning. To achieve high-fidelity free-viewpoint rendering, DoubleField is further augmented to leverage ultra-high-resolution inputs, where a view-to-view transformer and a transfer learning scheme are introduced for more efficient learning and finetuning from sparse-view inputs at original resolutions. The efficacy of DoubleField is validated by the quantitative evaluations on several datasets and the qualitative results in a real-world sparse multi-view system, showing its superior capability for photo-realistic free-viewpoint human rendering. For code and demo video, please refer to our project page: http://www.liuyebin.com/dbfield/dbfield.html.",,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnueNbV87PwYqzVLrPoGEvNb35a2aapHayncS1mU4AoUCJm_4TV8RDmNaqWoKnPmc_Ac
6/29/2021 15:35:04,Geometry-Consistent Neural Shape Representation with Implicit Displacement Fields,IDF,6/9/2021,https://arxiv.org/pdf/2106.05187.pdf,https://yifita.github.io/publication/idf/,Coming soon,https://www.youtube.com/watch?v=fl4Rje8HM3I,,,,"@inproceedings{yifan2021idf,
  PUBLISHER = {International Joint Conferences on Artificial Intelligence Organization},
  BOOKTITLE = {Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence (IJCAI)},
  ID = {yifan2021idf},
  ENTRYTYPE = {inproceedings},
  AUTHOR = {Wang Yifan and Lukas Rahmann and Olga Sorkine-Hornung},
  TITLE = {Geometry-Consistent Neural Shape Representation with Implicit Displacement Fields},
  EPRINT = {2106.05187v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We present implicit displacement fields, a novel representation for detailed 3D geometry. Inspired by a classic surface deformation technique, displacement mapping, our method represents a complex surface as a smooth base surface plus a displacement along the base's normal directions, resulting in a frequency-based shape decomposition, where the high frequency signal is constrained geometrically by the low frequency signal. Importantly, this disentanglement is unsupervised thanks to a tailored architectural design that has an innate frequency hierarchy by construction. We explore implicit displacement field surface reconstruction and detail transfer and demonstrate superior representational power, training stability and generalizability.},
  YEAR = {2021},
  URL = {http://arxiv.org/abs/2106.05187v2},
  FILE = {2106.05187v2.pdf}
 }",Fundamentals,"Coarse-to-fine, Representation, Volume partitioning, Data-driven",SIREN,SDF,Per-scene,,,,,,,IJCAI 2021,,Yes,,"Wang Yifan, Lukas Rahmann, Olga Sorkine-Hornung",yifan2021idf,00000146,"We present implicit displacement fields, a novel representation for detailed 3D geometry. Inspired by a classic surface deformation technique, displacement mapping, our method represents a complex surface as a smooth base surface plus a displacement along the base's normal directions, resulting in a frequency-based shape decomposition, where the high frequency signal is constrained geometrically by the low frequency signal. Importantly, this disentanglement is unsupervised thanks to a tailored architectural design that has an innate frequency hierarchy by construction. We explore implicit displacement field surface reconstruction and detail transfer and demonstrate superior representational power, training stability and generalizability.",,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnucK82bcfeskdcji2RqhQ0rOBeQ8uCk4pb0QQQy5RXe8NDk-xyWLsGVlKo1aS6d6jwQ
9/18/2021 11:01:55,Implicit-PDF: Non-Parametric Representation of Probability Distributions on the Rotation Manifold,Implicit-PDF,6/10/2021,https://arxiv.org/pdf/2106.05965.pdf,https://implicit-pdf.github.io/,https://github.com/google-research/google-research/tree/master/implicit_pdf,https://www.youtube.com/watch?v=Y-MlRRy0xJA,,,,"@inproceedings{murphy2021implicitpdf,
  PUBLISHER = {PMLR},
  BOOKTITLE = {International Conference on Machine Learning (ICML)},
  ID = {murphy2021implicitpdf},
  ENTRYTYPE = {inproceedings},
  AUTHOR = {Kieran Murphy and Carlos Esteves and Varun Jampani and Srikumar Ramalingam and Ameesh Makadia},
  TITLE = {Implicit-PDF: Non-Parametric Representation of Probability Distributions on the Rotation Manifold},
  EPRINT = {2106.05965v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Single image pose estimation is a fundamental problem in many vision and robotics tasks, and existing deep learning approaches suffer by not completely modeling and handling: i) uncertainty about the predictions, and ii) symmetric objects with multiple (sometimes infinite) correct poses. To this end, we introduce a method to estimate arbitrary, non-parametric distributions on SO(3). Our key idea is to represent the distributions implicitly, with a neural network that estimates the probability given the input image and a candidate pose. Grid sampling or gradient ascent can be used to find the most likely pose, but it is also possible to evaluate the probability at any pose, enabling reasoning about symmetries and uncertainty. This is the most general way of representing distributions on manifolds, and to showcase the rich expressive power, we introduce a dataset of challenging symmetric and nearly-symmetric objects. We require no supervision on pose uncertainty -- the model trains only with a single pose per example. Nonetheless, our implicit model is highly expressive to handle complex distributions over 3D poses, while still obtaining accurate pose estimation on standard non-ambiguous environments, achieving state-of-the-art performance on Pascal3D+ and ModelNet10-SO(3) benchmarks.},
  YEAR = {2021},
  URL = {http://arxiv.org/abs/2106.05965v1},
  FILE = {2106.05965v1.pdf}
 }","Camera parameter estimation, Beyond graphics, Fundamentals",,,,,,,,,,,ICML 2021,https://www.tensorflow.org/datasets/catalog/symmetric_solids,,,"Kieran Murphy, Carlos Esteves, Varun Jampani, Srikumar Ramalingam, Ameesh Makadia",murphy2021implicitpdf,00000226,"Single image pose estimation is a fundamental problem in many vision and robotics tasks, and existing deep learning approaches suffer by not completely modeling and handling: i) uncertainty about the predictions, and ii) symmetric objects with multiple (sometimes infinite) correct poses. To this end, we introduce a method to estimate arbitrary, non-parametric distributions on SO(3). Our key idea is to represent the distributions implicitly, with a neural network that estimates the probability given the input image and a candidate pose. Grid sampling or gradient ascent can be used to find the most likely pose, but it is also possible to evaluate the probability at any pose, enabling reasoning about symmetries and uncertainty. This is the most general way of representing distributions on manifolds, and to showcase the rich expressive power, we introduce a dataset of challenging symmetric and nearly-symmetric objects. We require no supervision on pose uncertainty -- the model trains only with a single pose per example. Nonetheless, our implicit model is highly expressive to handle complex distributions over 3D poses, while still obtaining accurate pose estimation on standard non-ambiguous environments, achieving state-of-the-art performance on Pascal3D+ and ModelNet10-SO(3) benchmarks.",,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnufvbwDjnFb8SmgwXg-GSGNt7z4Q6TbDBipgPwQgaaS6w_N-bYfUTtyfpi5t9hoI71s
7/19/2021 22:06:59,Unsupervised Video Prediction from a Single Frame by Estimating 3D Dynamic Scene Structure,,6/16/2021,https://arxiv.org/pdf/2106.09051.pdf,http://pmh47.net/vipl4s/,,,,,,"@article{henderson2021unsupervised,
  JOURNAL = {arXiv preprint arXiv:2106.09051},
  BOOKTITLE = {ArXiv Pre-print},
  ID = {henderson2021unsupervised},
  ENTRYTYPE = {article},
  AUTHOR = {Paul Henderson and Christoph H. Lampert and Bernd Bickel},
  TITLE = {Unsupervised Video Prediction from a Single Frame by Estimating 3D Dynamic Scene Structure},
  EPRINT = {2106.09051v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Our goal in this work is to generate realistic videos given just one initial frame as input. Existing unsupervised approaches to this task do not consider the fact that a video typically shows a 3D environment, and that this should remain coherent from frame to frame even as the camera and objects move. We address this by developing a model that first estimates the latent 3D structure of the scene, including the segmentation of any moving objects. It then predicts future frames by simulating the object and camera dynamics, and rendering the resulting views. Importantly, it is trained end-to-end using only the unsupervised objective of predicting future frames, without any 3D information nor segmentation annotations. Experiments on two challenging datasets of natural videos show that our model can estimate 3D structure and motion segmentation from a single frame, and hence generate plausible and varied predictions.},
  YEAR = {2021},
  URL = {http://arxiv.org/abs/2106.09051v1},
  FILE = {2106.09051v1.pdf}
 }","Dynamic, Beyond graphics","Conditional neural field, Volume partitioning",,,,,,,,,,ARXIV 2021,,,,"Paul Henderson, Christoph H. Lampert, Bernd Bickel",henderson2021unsupervised,00000147,"Our goal in this work is to generate realistic videos given just one initial frame as input. Existing unsupervised approaches to this task do not consider the fact that a video typically shows a 3D environment, and that this should remain coherent from frame to frame even as the camera and objects move. We address this by developing a model that first estimates the latent 3D structure of the scene, including the segmentation of any moving objects. It then predicts future frames by simulating the object and camera dynamics, and rendering the resulting views. Importantly, it is trained end-to-end using only the unsupervised objective of predicting future frames, without any 3D information nor segmentation annotations. Experiments on two challenging datasets of natural videos show that our model can estimate 3D structure and motion segmentation from a single frame, and hence generate plausible and varied predictions.",,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnufuoYKfAArHXuZ0nNg49hpZZCMNJF2EjsI1jO5CNH9la01PX7008azL8-5Ak2VWLsg
7/19/2021 21:16:24,NeuS: Learning Neural Implicit Surfaces by Volume Rendering for Multi-view Reconstruction,NeuS,6/20/2021,https://arxiv.org/pdf/2106.10689.pdf,https://lingjie0206.github.io/papers/NeuS/index.htm,Coming soon,,,,,"@inproceedings{wang2021neus,
  PUBLISHER = {International Joint Conferences on Artificial Intelligence Organization},
  BOOKTITLE = {Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence (IJCAI)},
  ID = {wang2021neus},
  ENTRYTYPE = {inproceedings},
  AUTHOR = {Peng Wang and Lingjie Liu and Yuan Liu and Christian Theobalt and Taku Komura and Wenping Wang},
  TITLE = {NeuS: Learning Neural Implicit Surfaces by Volume Rendering for Multi-view Reconstruction},
  EPRINT = {2106.10689v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We present a novel neural surface reconstruction method, called NeuS, for reconstructing objects and scenes with high fidelity from 2D image inputs. Existing neural surface reconstruction approaches, such as DVR and IDR, require foreground mask as supervision, easily get trapped in local minima, and therefore struggle with the reconstruction of objects with severe self-occlusion or thin structures. Meanwhile, recent neural methods for novel view synthesis, such as NeRF and its variants, use volume rendering to produce a neural scene representation with robustness of optimization, even for highly complex objects. However, extracting high-quality surfaces from this learned implicit representation is difficult because there are not sufficient surface constraints in the representation. In NeuS, we propose to represent a surface as the zero-level set of a signed distance function (SDF) and develop a new volume rendering method to train a neural SDF representation. We observe that the conventional volume rendering method causes inherent geometric errors (i.e. bias) for surface reconstruction, and therefore propose a new formulation that is free of bias in the first order of approximation, thus leading to more accurate surface reconstruction even without the mask supervision. Experiments on the DTU dataset and the BlendedMVS dataset show that NeuS outperforms the state-of-the-arts in high-quality surface reconstruction, especially for objects and scenes with complex structures and self-occlusion.},
  YEAR = {2021},
  URL = {http://arxiv.org/abs/2106.10689v1},
  FILE = {2106.10689v1.pdf}
 }",,"Representation, Sampling",,Other,,,,,,,,IJCAI 2021,Coming soon,,,"Peng Wang, Lingjie Liu, Yuan Liu, Christian Theobalt, Taku Komura, Wenping Wang",wang2021neus,00000148,"We present a novel neural surface reconstruction method, called NeuS, for reconstructing objects and scenes with high fidelity from 2D image inputs. Existing neural surface reconstruction approaches, such as DVR and IDR, require foreground mask as supervision, easily get trapped in local minima, and therefore struggle with the reconstruction of objects with severe self-occlusion or thin structures. Meanwhile, recent neural methods for novel view synthesis, such as NeRF and its variants, use volume rendering to produce a neural scene representation with robustness of optimization, even for highly complex objects. However, extracting high-quality surfaces from this learned implicit representation is difficult because there are not sufficient surface constraints in the representation. In NeuS, we propose to represent a surface as the zero-level set of a signed distance function (SDF) and develop a new volume rendering method to train a neural SDF representation. We observe that the conventional volume rendering method causes inherent geometric errors (i.e. bias) for surface reconstruction, and therefore propose a new formulation that is free of bias in the first order of approximation, thus leading to more accurate surface reconstruction even without the mask supervision. Experiments on the DTU dataset and the BlendedMVS dataset show that NeuS outperforms the state-of-the-arts in high-quality surface reconstruction, especially for objects and scenes with complex structures and self-occlusion.",,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnudpar5L2k_n7x-FVH0PXyTbKNLpMt8XxQgGpRpL10bwAvy7rYXC46_ZJSZFggIcL6g
7/19/2021 21:19:05,Moving in a 360 World: Synthesizing Panoramic Parallaxes from a Single Panorama,OmniNeRF,6/21/2021,https://arxiv.org/pdf/2106.10859.pdf,,,,,,,"@article{hsu2021omninerf,
  JOURNAL = {arXiv preprint arXiv:2106.10859},
  BOOKTITLE = {ArXiv Pre-print},
  ID = {hsu2021omninerf},
  ENTRYTYPE = {article},
  AUTHOR = {Ching-Yu Hsu and Cheng Sun and Hwann-Tzong Chen},
  TITLE = {Moving in a 360 World: Synthesizing Panoramic Parallaxes from a Single Panorama},
  EPRINT = {2106.10859v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We present Omnidirectional Neural Radiance Fields (OmniNeRF), the first method to the application of parallax-enabled novel panoramic view synthesis. Recent works for novel view synthesis focus on perspective images with limited field-of-view and require sufficient pictures captured in a specific condition. Conversely, OmniNeRF can generate panorama images for unknown viewpoints given a single equirectangular image as training data. To this end, we propose to augment the single RGB-D panorama by projecting back and forth between a 3D world and different 2D panoramic coordinates at different virtual camera positions. By doing so, we are able to optimize an Omnidirectional Neural Radiance Field with visible pixels collecting from omnidirectional viewing angles at a fixed center for the estimation of new viewing angles from varying camera positions. As a result, the proposed OmniNeRF achieves convincing renderings of novel panoramic views that exhibit the parallax effect. We showcase the effectiveness of each of our proposals on both synthetic and real-world datasets.},
  YEAR = {2021},
  URL = {http://arxiv.org/abs/2106.10859v1},
  FILE = {2106.10859v1.pdf}
 }",,Representation,NeRF,Density,Per-scene,,,,,,,ARXIV 2021,,No,Direct,"Ching-Yu Hsu, Cheng Sun, Hwann-Tzong Chen",hsu2021omninerf,00000149,"We present Omnidirectional Neural Radiance Fields (OmniNeRF), the first method to the application of parallax-enabled novel panoramic view synthesis. Recent works for novel view synthesis focus on perspective images with limited field-of-view and require sufficient pictures captured in a specific condition. Conversely, OmniNeRF can generate panorama images for unknown viewpoints given a single equirectangular image as training data. To this end, we propose to augment the single RGB-D panorama by projecting back and forth between a 3D world and different 2D panoramic coordinates at different virtual camera positions. By doing so, we are able to optimize an Omnidirectional Neural Radiance Field with visible pixels collecting from omnidirectional viewing angles at a fixed center for the estimation of new viewing angles from varying camera positions. As a result, the proposed OmniNeRF achieves convincing renderings of novel panoramic views that exhibit the parallax effect. We showcase the effectiveness of each of our proposals on both synthetic and real-world datasets.",,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnudBVLOJWffC9pdvdU-wE5Bwa-F-wdWT6MLTijR596cm0T2A_t9bC8sgJJC4GMRxGnY
6/29/2021 17:02:24,Volume Rendering of Neural Implicit Surfaces,VolSDF,6/22/2021,https://arxiv.org/pdf/2106.12052.pdf,,,,,,,"@inproceedings{yariv2021volsdf,
  PUBLISHER = {Curran Associates, Inc.},
  BOOKTITLE = {Advances in Neural Information Processing Systems (NeurIPS)},
  ID = {yariv2021volsdf},
  ENTRYTYPE = {inproceedings},
  AUTHOR = {Lior Yariv and Jiatao Gu and Yoni Kasten and Yaron Lipman},
  TITLE = {Volume Rendering of Neural Implicit Surfaces},
  EPRINT = {2106.12052v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Neural volume rendering became increasingly popular recently due to its success in synthesizing novel views of a scene from a sparse set of input images. So far, the geometry learned by neural volume rendering techniques was modeled using a generic density function. Furthermore, the geometry itself was extracted using an arbitrary level set of the density function leading to a noisy, often low fidelity reconstruction. The goal of this paper is to improve geometry representation and reconstruction in neural volume rendering. We achieve that by modeling the volume density as a function of the geometry. This is in contrast to previous work modeling the geometry as a function of the volume density. In more detail, we define the volume density function as Laplace's cumulative distribution function (CDF) applied to a signed distance function (SDF) representation. This simple density representation has three benefits: (i) it provides a useful inductive bias to the geometry learned in the neural volume rendering process; (ii) it facilitates a bound on the opacity approximation error, leading to an accurate sampling of the viewing ray. Accurate sampling is important to provide a precise coupling of geometry and radiance; and (iii) it allows efficient unsupervised disentanglement of shape and appearance in volume rendering. Applying this new density representation to challenging scene multiview datasets produced high quality geometry reconstructions, outperforming relevant baselines. Furthermore, switching shape and appearance between scenes is possible due to the disentanglement of the two.},
  YEAR = {2021},
  URL = {http://arxiv.org/abs/2106.12052v1},
  FILE = {2106.12052v1.pdf}
 }",Fundamentals,Representation,None,SDF/Density Hybrid,Per-scene,,,,,,,NeurIPS 2021 (Spotlight),,No,,"Lior Yariv, Jiatao Gu, Yoni Kasten, Yaron Lipman",yariv2021volsdf,00000150,"Neural volume rendering became increasingly popular recently due to its success in synthesizing novel views of a scene from a sparse set of input images. So far, the geometry learned by neural volume rendering techniques was modeled using a generic density function. Furthermore, the geometry itself was extracted using an arbitrary level set of the density function leading to a noisy, often low fidelity reconstruction. The goal of this paper is to improve geometry representation and reconstruction in neural volume rendering. We achieve that by modeling the volume density as a function of the geometry. This is in contrast to previous work modeling the geometry as a function of the volume density. In more detail, we define the volume density function as Laplace's cumulative distribution function (CDF) applied to a signed distance function (SDF) representation. This simple density representation has three benefits: (i) it provides a useful inductive bias to the geometry learned in the neural volume rendering process; (ii) it facilitates a bound on the opacity approximation error, leading to an accurate sampling of the viewing ray. Accurate sampling is important to provide a precise coupling of geometry and radiance; and (iii) it allows efficient unsupervised disentanglement of shape and appearance in volume rendering. Applying this new density representation to challenging scene multiview datasets produced high quality geometry reconstructions, outperforming relevant baselines. Furthermore, switching shape and appearance between scenes is possible due to the disentanglement of the two.",,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnudz5cSlZKYU5WMK1KHEvVOBRxFcnMMcuZbdOH-iw1kBsO2easrankLugoDubHI6S_8
7/19/2021 21:50:34,MetaAvatar: Learning Animatable Clothed Human Models from Few Depth Images,MetaAvatar,6/22/2021,https://arxiv.org/pdf/2106.11944.pdf,https://neuralbodies.github.io/metavatar/,,,,"https://www.youtube.com/watch?v=SXv1sBRwm4U, https://www.youtube.com/watch?v=eLZH-h1VOm8, https://www.youtube.com/watch?v=MMQStRgWJUE",,"@inproceedings{wang2021metaavatar,
  PUBLISHER = {International Joint Conferences on Artificial Intelligence Organization},
  BOOKTITLE = {Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence (IJCAI)},
  ID = {wang2021metaavatar},
  ENTRYTYPE = {inproceedings},
  AUTHOR = {Shaofei Wang and Marko Mihajlovic and Qianli Ma and Andreas Geiger and Siyu Tang},
  TITLE = {MetaAvatar: Learning Animatable Clothed Human Models from Few Depth Images},
  EPRINT = {2106.11944v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {In this paper, we aim to create generalizable and controllable neural signed distance fields (SDFs) that represent clothed humans from monocular depth observations. Recent advances in deep learning, especially neural implicit representations, have enabled human shape reconstruction and controllable avatar generation from different sensor inputs. However, to generate realistic cloth deformations from novel input poses, watertight meshes or dense full-body scans are usually needed as inputs. Furthermore, due to the difficulty of effectively modeling pose-dependent cloth deformations for diverse body shapes and cloth types, existing approaches resort to per-subject/cloth-type optimization from scratch, which is computationally expensive. In contrast, we propose an approach that can quickly generate realistic clothed human avatars, represented as controllable neural SDFs, given only monocular depth images. We achieve this by using meta-learning to learn an initialization of a hypernetwork that predicts the parameters of neural SDFs. The hypernetwork is conditioned on human poses and represents a clothed neural avatar that deforms non-rigidly according to the input poses. Meanwhile, it is meta-learned to effectively incorporate priors of diverse body shapes and cloth types and thus can be much faster to fine-tune, compared to models trained from scratch. We qualitatively and quantitatively show that our approach outperforms state-of-the-art approaches that require complete meshes as inputs while our approach requires only depth frames as inputs and runs orders of magnitudes faster. Furthermore, we demonstrate that our meta-learned hypernetwork is very robust, being the first to generate avatars with realistic dynamic cloth deformations given as few as 8 monocular depth frames.},
  YEAR = {2021},
  URL = {http://arxiv.org/abs/2106.11944v1},
  FILE = {2106.11944v1.pdf}
 }",Human body,"Hypernetwork, Warping field/Flow field",SIREN,SDF,Category-level,,,,,,,IJCAI 2021,,Yes,Direct,"Shaofei Wang, Marko Mihajlovic, Qianli Ma, Andreas Geiger, Siyu Tang",wang2021metaavatar,00000151,"In this paper, we aim to create generalizable and controllable neural signed distance fields (SDFs) that represent clothed humans from monocular depth observations. Recent advances in deep learning, especially neural implicit representations, have enabled human shape reconstruction and controllable avatar generation from different sensor inputs. However, to generate realistic cloth deformations from novel input poses, watertight meshes or dense full-body scans are usually needed as inputs. Furthermore, due to the difficulty of effectively modeling pose-dependent cloth deformations for diverse body shapes and cloth types, existing approaches resort to per-subject/cloth-type optimization from scratch, which is computationally expensive. In contrast, we propose an approach that can quickly generate realistic clothed human avatars, represented as controllable neural SDFs, given only monocular depth images. We achieve this by using meta-learning to learn an initialization of a hypernetwork that predicts the parameters of neural SDFs. The hypernetwork is conditioned on human poses and represents a clothed neural avatar that deforms non-rigidly according to the input poses. Meanwhile, it is meta-learned to effectively incorporate priors of diverse body shapes and cloth types and thus can be much faster to fine-tune, compared to models trained from scratch. We qualitatively and quantitatively show that our approach outperforms state-of-the-art approaches that require complete meshes as inputs while our approach requires only depth frames as inputs and runs orders of magnitudes faster. Furthermore, we demonstrate that our meta-learned hypernetwork is very robust, being the first to generate avatars with realistic dynamic cloth deformations given as few as 8 monocular depth frames.",,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnuf_A4imA6Eltgo1yBBFAPH-mLBTjiWFt9OVGFElazLdX03PwXnQH10SjhO84GaYDqU
10/2/2021 10:56:58,NeRF-Tex: Neural Reflectance Field Textures,NeRF-Tex,6/22/2021,https://d1qx31qr3h6wln.cloudfront.net/publications/NeRFTex.pdf,,,https://d1qx31qr3h6wln.cloudfront.net/publications/NeRFTex_video.mp4,,,,"@article{baatz2021nerftex,
  JOURNAL = {Computer Graphics Forum},
  ID = {baatz2021nerftex},
  ENTRYTYPE = {article},
  TITLE = {NeRF-Tex: Neural Reflectance Field Textures},
  AUTHOR = {Hendrik Baatz and Jonathan Granskog and Marios Papas and Fabrice Rousselle and Jan Nov{\'a}k},
  YEAR = {2021},
  PUBLISHER = {The Eurographics Association and John Wiley & Sons Ltd.}
 }","Material/lighting estimation, Challenging materials (fur, hair, transparency)","Conditional neural field, Representation",NeRF,Density,,,,,,,,EGSR 2021,,,,"Hendrik Baatz, Jonathan Granskog, Marios Papas, Fabrice Rousselle, Jan Nov{\'a}k",baatz2021nerftex,00000229,"We investigate the use of neural fields for modeling diverse mesoscale structures, such as fur, fabric, and grass. Instead of using classical graphics primitives to model the structure, we propose to employ a versatile volumetric primitive represented by a neural reflectance field (NeRF-Tex), which jointly models the geometry of the material and its response to lighting. The NeRF-Tex primitive can be instantiated over a base mesh to''texture''it with the desired meso and microscale appearance. We condition the reflectance field on user-defined",,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnucjxjM8lBFVPyplSuNbQplqhf8C7Q4yHQJLp8JYQoLfnm0w7wwxwakhLv5qaFxE8rM
7/19/2021 17:55:31,Real-time Neural Radiance Caching for Path Tracing,,6/23/2021,https://arxiv.org/pdf/2106.12372.pdf,https://tom94.net/,,,,,,"@article{muller2021realtime,
  PUBLISHER = {Association for Computing Machinery},
  JOURNAL = {ACM Transactions on Graphics (TOG)},
  ID = {muller2021realtime},
  ENTRYTYPE = {article},
  AUTHOR = {Thomas Muller and Fabrice Rousselle and Jan Novak and Alexander Keller},
  TITLE = {Real-time Neural Radiance Caching for Path Tracing},
  EPRINT = {2106.12372v2},
  DOI = {10.1145/3450626.3459812},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.GR},
  ABSTRACT = {We present a real-time neural radiance caching method for path-traced global illumination. Our system is designed to handle fully dynamic scenes, and makes no assumptions about the lighting, geometry, and materials. The data-driven nature of our approach sidesteps many difficulties of caching algorithms, such as locating, interpolating, and updating cache points. Since pretraining neural networks to handle novel, dynamic scenes is a formidable generalization challenge, we do away with pretraining and instead achieve generalization via adaptation, i.e. we opt for training the radiance cache while rendering. We employ self-training to provide low-noise training targets and simulate infinite-bounce transport by merely iterating few-bounce training updates. The updates and cache queries incur a mild overhead -- about 2.6ms on full HD resolution -- thanks to a streaming implementation of the neural network that fully exploits modern hardware. We demonstrate significant noise reduction at the cost of little induced bias, and report state-of-the-art, real-time performance on a number of challenging scenarios.},
  YEAR = {2021},
  URL = {http://arxiv.org/abs/2106.12372v2},
  FILE = {2106.12372v2.pdf}
 }","Performance (rendering), Material/lighting estimation",Caching,,,,,,,,,,SIGGRAPH 2021,,,,"Thomas Müller, Fabrice Rousselle, Jan Novák, Alexander Keller",muller2021realtime,00000152,"We present a real-time neural radiance caching method for path-traced global illumination. Our system is designed to handle fully dynamic scenes, and makes no assumptions about the lighting, geometry, and materials. The data-driven nature of our approach sidesteps many difficulties of caching algorithms, such as locating, interpolating, and updating cache points. Since pretraining neural networks to handle novel, dynamic scenes is a formidable generalization challenge, we do away with pretraining and instead achieve generalization via adaptation, i.e. we opt for training the radiance cache while rendering. We employ self-training to provide low-noise training targets and simulate infinite-bounce transport by merely iterating few-bounce training updates. The updates and cache queries incur a mild overhead -- about 2.6ms on full HD resolution -- thanks to a streaming implementation of the neural network that fully exploits modern hardware. We demonstrate significant noise reduction at the cost of little induced bias, and report state-of-the-art, real-time performance on a number of challenging scenarios.",,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnucEB7FXF4qBtvN2fvOUXz9YV2yTGFz0QFa-9O5SxVuvwPDU4fUkVroyf9QPP2yrYOA
7/19/2021 22:00:47,HyperNeRF: A Higher-Dimensional Representation for Topologically Varying Neural Radiance Fields,HyperNeRF,6/24/2021,https://arxiv.org/pdf/2106.13228.pdf,,,,,,,"@article{park2021hypernerf,
  JOURNAL = {arXiv preprint arXiv:2106.13228},
  BOOKTITLE = {ArXiv Pre-print},
  ID = {park2021hypernerf},
  ENTRYTYPE = {article},
  AUTHOR = {Keunhong Park and Utkarsh Sinha and Peter Hedman and Jonathan T. Barron and Sofien Bouaziz and Dan B Goldman and Ricardo Martin-Brualla and Steven M. Seitz},
  TITLE = {HyperNeRF: A Higher-Dimensional Representation for Topologically Varying Neural Radiance Fields},
  EPRINT = {2106.13228v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Neural Radiance Fields (NeRF) are able to reconstruct scenes with unprecedented fidelity, and various recent works have extended NeRF to handle dynamic scenes. A common approach to reconstruct such non-rigid scenes is through the use of a learned deformation field mapping from coordinates in each input image into a canonical template coordinate space. However, these deformation-based approaches struggle to model changes in topology, as topological changes require a discontinuity in the deformation field, but these deformation fields are necessarily continuous. We address this limitation by lifting NeRFs into a higher dimensional space, and by representing the 5D radiance field corresponding to each individual input image as a slice through this ""hyper-space"". Our method is inspired by level set methods, which model the evolution of surfaces as slices through a higher dimensional surface. We evaluate our method on two tasks: (i) interpolating smoothly between ""moments"", i.e., configurations of the scene, seen in the input images while maintaining visual plausibility, and (ii) novel-view synthesis at fixed moments. We show that our method, which we dub HyperNeRF, outperforms existing methods on both tasks. Compared to Nerfies, HyperNeRF reduces average error rates by 4.1% for interpolation and 8.6% for novel-view synthesis, as measured by LPIPS. Additional videos, results, and visualizations are available at https://hypernerf.github.io.},
  YEAR = {2021},
  URL = {http://arxiv.org/abs/2106.13228v2},
  FILE = {2106.13228v2.pdf}
 }","Dynamic, Fundamentals",Conditional neural field,NeRF,Density,Per-scene,,,,,,,ARXIV 2021,,No,Direct,"Keunhong Park, Utkarsh Sinha, Peter Hedman, Jonathan T. Barron, Sofien Bouaziz, Dan B Goldman, Ricardo Martin-Brualla, Steven M. Seitz",park2021hypernerf,00000153,"Neural Radiance Fields (NeRF) are able to reconstruct scenes with unprecedented fidelity, and various recent works have extended NeRF to handle dynamic scenes. A common approach to reconstruct such non-rigid scenes is through the use of a learned deformation field mapping from coordinates in each input image into a canonical template coordinate space. However, these deformation-based approaches struggle to model changes in topology, as topological changes require a discontinuity in the deformation field, but these deformation fields are necessarily continuous. We address this limitation by lifting NeRFs into a higher dimensional space, and by representing the 5D radiance field corresponding to each individual input image as a slice through this ""hyper-space"". Our method is inspired by level set methods, which model the evolution of surfaces as slices through a higher dimensional surface. We evaluate our method on two tasks: (i) interpolating smoothly between ""moments"", i.e., configurations of the scene, seen in the input images while maintaining visual plausibility, and (ii) novel-view synthesis at fixed moments. We show that our method, which we dub HyperNeRF, outperforms existing methods on both tasks by significant margins. Compared to Nerfies, HyperNeRF reduces average error rates by 8.6% for interpolation and 8.8% for novel-view synthesis, as measured by LPIPS.",,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnucOyoH3EWkc7oYofQ4az6X2UeWPRQgXON1ySIXkGxR-neipgQtRwMfeV73kzx0fR5o
7/19/2021 21:14:02,Animatable Neural Radiance Fields from Monocular RGB Video,,6/25/2021,https://arxiv.org/pdf/2106.13629.pdf,,,,,,,"@inproceedings{chen2021animatable,
  BOOKTITLE = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
  ID = {chen2021animatable},
  ENTRYTYPE = {inproceedings},
  AUTHOR = {Jianchuan Chen and Ying Zhang and Di Kang and Xuefei Zhe and Linchao Bao and Xu Jia and Huchuan Lu},
  TITLE = {Animatable Neural Radiance Fields from Monocular RGB Videos},
  EPRINT = {2106.13629v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We present animatable neural radiance fields (animatable NeRF) for detailed human avatar creation from monocular videos. Our approach extends neural radiance fields (NeRF) to the dynamic scenes with human movements via introducing explicit pose-guided deformation while learning the scene representation network. In particular, we estimate the human pose for each frame and learn a constant canonical space for the detailed human template, which enables natural shape deformation from the observation space to the canonical space under the explicit control of the pose parameters. To compensate for inaccurate pose estimation, we introduce the pose refinement strategy that updates the initial pose during the learning process, which not only helps to learn more accurate human reconstruction but also accelerates the convergence. In experiments we show that the proposed approach achieves 1) implicit human geometry and appearance reconstruction with high-quality details, 2) photo-realistic rendering of the human from novel views, and 3) animation of the human with novel poses.},
  YEAR = {2021},
  URL = {http://arxiv.org/abs/2106.13629v2},
  FILE = {2106.13629v2.pdf}
 }","Dynamic, Human body",,,,,,,,,,,ICCV 2021,,,,"Jianchuan Chen, Ying Zhang, Di Kang, Xuefei Zhe, Linchao Bao, Huchuan Lu",chen2021animatable,00000154,"We present animatable neural radiance fields for detailed human avatar creation from monocular videos. Our approach extends neural radiance fields (NeRF) to the dynamic scenes with human movements via introducing explicit pose-guided deformation while learning the scene representation network. In particular, we estimate the human pose for each frame and learn a constant canonical space for the detailed human template, which enables natural shape deformation from the observation space to the canonical space under the explicit control of the pose parameters. To compensate for inaccurate pose estimation, we introduce the pose refinement strategy that updates the initial pose during the learning process, which not only helps to learn more accurate human reconstruction but also accelerates the convergence. In experiments we show that the proposed approach achieves 1) implicit human geometry and appearance reconstruction with high-quality details, 2) photo-realistic rendering of the human from arbitrary views, and 3) animation of the human with arbitrary poses.",,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnufCHNY5kRDNR3sX7Q1GLACcPxU7q1fIFNvtWnnk3-p4gWlMVe6Pe3iKWmFRjaTGLhQ
7/19/2021 21:56:40,Fast Training of Neural Lumigraph Representations using Meta Learning,MetaNLR++,6/28/2021,https://arxiv.org/pdf/2106.14942.pdf,http://www.computationalimaging.org/publications/metanlr/,,https://www.youtube.com/watch?v=5pBFwyUyW6o,,,,"@article{bergman2021metanlr++,
  JOURNAL = {arXiv preprint arXiv:2106.14942},
  BOOKTITLE = {ArXiv Pre-print},
  ID = {bergman2021metanlr++},
  ENTRYTYPE = {article},
  AUTHOR = {Alexander W. Bergman and Petr Kellnhofer and Gordon Wetzstein},
  TITLE = {Fast Training of Neural Lumigraph Representations using Meta Learning},
  EPRINT = {2106.14942v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Novel view synthesis is a long-standing problem in machine learning and computer vision. Significant progress has recently been made in developing neural scene representations and rendering techniques that synthesize photorealistic images from arbitrary views. These representations, however, are extremely slow to train and often also slow to render. Inspired by neural variants of image-based rendering, we develop a new neural rendering approach with the goal of quickly learning a high-quality representation which can also be rendered in real-time. Our approach, MetaNLR++, accomplishes this by using a unique combination of a neural shape representation and 2D CNN-based image feature extraction, aggregation, and re-projection. To push representation convergence times down to minutes, we leverage meta learning to learn neural shape and image feature priors which accelerate training. The optimized shape and image features can then be extracted using traditional graphics techniques and rendered in real time. We show that MetaNLR++ achieves similar or better novel view synthesis results in a fraction of the time that competing methods require.},
  YEAR = {2021},
  URL = {http://arxiv.org/abs/2106.14942v1},
  FILE = {2106.14942v1.pdf}
 }","Performance (training), Performance (rendering), Generalization","Hypernetwork, Lifting 2D features to 3D, Image-based rendering",SIREN,SDF,Universal,,,,,,,ARXIV 2021,,No,Direct,"Alexander W. Bergman, Petr Kellnhofer, Gordon Wetzstein",bergman2021metanlr++,00000155,"Novel view synthesis is a long-standing problem in machine learning and computer vision. Significant progress has recently been made in developing neural scene representations and rendering techniques that synthesize photorealistic images from arbitrary views. These representations, however, are extremely slow to train and often also slow to render. Inspired by neural variants of image-based rendering, we develop a new neural rendering approach with the goal of quickly learning a high-quality representation which can also be rendered in real-time. Our approach, MetaNLR++, accomplishes this by using a unique combination of a neural shape representation and 2D CNN-based image feature extraction, aggregation, and re-projection. To push representation convergence times down to minutes, we leverage meta learning to learn neural shape and image feature priors which accelerate training. The optimized shape and image features can then be extracted using traditional graphics techniques and rendered in real time. We show that MetaNLR++ achieves similar or better novel view synthesis results in a fraction of the time that competing methods require.",,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnue_-wuaulufbyBJreOxl2pbL4HZgwNtOQOz2-1Arn-r4qdL3YpsVw3tgt0k2Hkxbjo
7/19/2021 21:15:17,IREM: High-Resolution Magnetic Resonance Image Reconstruction via Implicit Neural Representation,IREM,6/29/2021,https://arxiv.org/pdf/2106.15097.pdf,,,,,,,"@article{wu2021irem,
  ID = {wu2021irem},
  ENTRYTYPE = {article},
  AUTHOR = {Qing Wu and Yuwei Li and Lan Xu and Ruiming Feng and Hongjiang Wei and Qing Yang and Boliang Yu and Xiaozhao Liu and Jingyi Yu and Yuyao Zhang},
  TITLE = {IREM: High-Resolution Magnetic Resonance (MR) Image Reconstruction via Implicit Neural Representation},
  EPRINT = {2106.15097v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {eess.IV},
  ABSTRACT = {For collecting high-quality high-resolution (HR) MR image, we propose a novel image reconstruction network named IREM, which is trained on multiple low-resolution (LR) MR images and achieve an arbitrary up-sampling rate for HR image reconstruction. In this work, we suppose the desired HR image as an implicit continuous function of the 3D image spatial coordinate and the thick-slice LR images as several sparse discrete samplings of this function. Then the super-resolution (SR) task is to learn the continuous volumetric function from a limited observations using an fully-connected neural network combined with Fourier feature positional encoding. By simply minimizing the error between the network prediction and the acquired LR image intensity across each imaging plane, IREM is trained to represent a continuous model of the observed tissue anatomy. Experimental results indicate that IREM succeeds in representing high frequency image feature, and in real scene data collection, IREM reduces scan time and achieves high-quality high-resolution MR imaging in terms of SNR and local image detail.},
  YEAR = {2021},
  MONTH = {Jun},
  URL = {http://arxiv.org/abs/2106.15097v1},
  FILE = {2106.15097v1.pdf}
 }",,,,,,,,,,,,MICCAI 2021,,,,"Qing Wu, Yuwei Li, Lan Xu, Ruiming Feng, Hongjiang Wei, Qing Yang, Boliang Yu, Xiaozhao Liu, Jingyi Yu, Yuyao Zhang",wu2021irem,00000156,"For collecting high-quality high-resolution (HR) MR image, we propose a novel image reconstruction network named IREM, which is trained on multiple low-resolution (LR) MR images and achieve an arbitrary up-sampling rate for HR image reconstruction. In this work, we suppose the desired HR image as an implicit continuous function of the 3D image spatial coordinate and the thick-slice LR images as several sparse discrete samplings of this function. Then the super-resolution (SR) task is to learn the continuous volumetric function from a limited observations using an fully-connected neural network combined with Fourier feature positional encoding. By simply minimizing the error between the network prediction and the acquired LR image intensity across each imaging plane, IREM is trained to represent a continuous model of the observed tissue anatomy. Experimental results indicate that IREM succeeds in representing high frequency image feature, and in real scene data collection, IREM reduces scan time and achieves high-quality high-resolution MR imaging in terms of SNR and local image detail.",,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnudUtwYaIxnfgBqAh4b72PLnjMIVWRN-NcBg25xyQ74kDnckbNe8ic9NXAV1UXUYYGE
7/19/2021 21:20:02,Rethinking positional encoding,,7/6/2021,https://arxiv.org/pdf/2107.02561.pdf,,https://github.com/osiriszjq/Rethinking-positional-encoding,,,,,"@article{zheng2021rethinking,
  JOURNAL = {arXiv preprint arXiv:2107.02561},
  BOOKTITLE = {ArXiv Pre-print},
  ID = {zheng2021rethinking},
  ENTRYTYPE = {article},
  AUTHOR = {Jianqiao Zheng and Sameera Ramasinghe and Simon Lucey},
  TITLE = {Rethinking Positional Encoding},
  EPRINT = {2107.02561v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.LG},
  ABSTRACT = {It is well noted that coordinate based MLPs benefit greatly -- in terms of preserving high-frequency information -- through the encoding of coordinate positions as an array of Fourier features. Hitherto, the rationale for the effectiveness of these positional encodings has been solely studied through a Fourier lens. In this paper, we strive to broaden this understanding by showing that alternative non-Fourier embedding functions can indeed be used for positional encoding. Moreover, we show that their performance is entirely determined by a trade-off between the stable rank of the embedded matrix and the distance preservation between embedded coordinates. We further establish that the now ubiquitous Fourier feature mapping of position is a special case that fulfills these conditions. Consequently, we present a more general theory to analyze positional encoding in terms of shifted basis functions. To this end, we develop the necessary theoretical formulae and empirically verify that our theoretical claims hold in practice. Codes available at https://github.com/osiriszjq/Rethinking-positional-encoding.},
  YEAR = {2021},
  URL = {http://arxiv.org/abs/2107.02561v2},
  FILE = {2107.02561v2.pdf}
 }",Fundamentals,,,,,,,,,,,ARXIV 2021,,,,"Jianqiao Zheng, Sameera Ramasinghe, Simon Lucey",zheng2021rethinking,00000157,"It is well noted that coordinate based MLPs benefit greatly -- in terms of preserving high-frequency information -- through the encoding of coordinate positions as an array of Fourier features. Hitherto, the rationale for the effectiveness of these positional encodings has been solely studied through a Fourier lens. In this paper, we strive to broaden this understanding by showing that alternative non-Fourier embedding functions can indeed be used for positional encoding. Moreover, we show that their performance is entirely determined by a trade-off between the stable rank of the embedded matrix and the distance preservation between embedded coordinates. We further establish that the now ubiquitous Fourier feature mapping of position is a special case that fulfills these conditions. Consequently, we present a more general theory to analyze positional encoding in terms of shifted basis functions. To this end, we develop the necessary theoretical formulae and empirically verify that our theoretical claims hold in practice. Codes available at https://github.com/osiriszjq/Rethinking-positional-encoding.",,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnuf7IT1U5c1-YxmrQ3knUF7D1IO7TxK0vUkSzBGQ8_VGkpKi1hstD2BJ0Tb-5gbeMLY
7/19/2021 21:49:10,Depth-supervised NeRF: Fewer Views and Faster Training for Free,DS-NeRF,7/6/2021,https://arxiv.org/pdf/2107.02791.pdf,https://www.cs.cmu.edu/~dsnerf/,https://github.com/dunbar12138/DSNeRF,https://www.youtube.com/watch?v=84LFxCo7ogk,,,,"@article{deng2021dsnerf,
  JOURNAL = {arXiv preprint arXiv:2107.02791},
  BOOKTITLE = {ArXiv Pre-print},
  ID = {deng2021dsnerf},
  ENTRYTYPE = {article},
  AUTHOR = {Kangle Deng and Andrew Liu and Jun-Yan Zhu and Deva Ramanan},
  TITLE = {Depth-supervised NeRF: Fewer Views and Faster Training for Free},
  EPRINT = {2107.02791v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {One common failure mode of Neural Radiance Field (NeRF) models is fitting incorrect geometries when given an insufficient number of input views. We propose DS-NeRF (Depth-supervised Neural Radiance Fields), a loss for learning neural radiance fields that takes advantage of readily-available depth supervision. Our key insight is that sparse depth supervision can be used to regularize the learned geometry, a crucial component for effectively rendering novel views using NeRF. We exploit the fact that current NeRF pipelines require images with known camera poses that are typically estimated by running structure-from-motion (SFM). Crucially, SFM also produces sparse 3D points that can be used as ``free"" depth supervision during training: we simply add a loss to ensure that depth rendered along rays that intersect these 3D points is close to the observed depth. We find that DS-NeRF can render more accurate images given fewer training views while training 2-6x faster. With only two training views on real-world images, DS-NeRF significantly outperforms NeRF as well as other sparse-view variants. We show that our loss is compatible with these NeRF models, demonstrating that depth is a cheap and easily digestible supervisory signal. Finally, we show that DS-NeRF supports other types of depth supervision such as scanned depth sensors and RGBD reconstruction outputs.},
  YEAR = {2021},
  URL = {http://arxiv.org/abs/2107.02791v1},
  FILE = {2107.02791v1.pdf}
 }","Performance (training), Few-shot reconstruction",,NeRF,Density,Per-scene,,,,,,,ARXIV 2021,,No,Direct,"Kangle Deng, Andrew Liu, Jun-Yan Zhu, Deva Ramanan",deng2021dsnerf,00000158,"One common failure mode of Neural Radiance Field (NeRF) models is fitting incorrect geometries when given an insufficient number of input views. We propose DS-NeRF (Depth-supervised Neural Radiance Fields), a loss for learning neural radiance fields that takes advantage of readily-available depth supervision. Our key insight is that sparse depth supervision can be used to regularize the learned geometry, a crucial component for effectively rendering novel views using NeRF. We exploit the fact that current NeRF pipelines require images with known camera poses that are typically estimated by running structure-from-motion (SFM). Crucially, SFM also produces sparse 3D points that can be used as ``free"" depth supervision during training: we simply add a loss to ensure that depth rendered along rays that intersect these 3D points is close to the observed depth. We find that DS-NeRF can render more accurate images given fewer training views while training 2-6x faster. With only two training views on real-world images, DS-NeRF significantly outperforms NeRF as well as other sparse-view variants. We show that our loss is compatible with these NeRF models, demonstrating that depth is a cheap and easily digestible supervisory signal. Finally, we show that DS-NeRF supports other types of depth supervision such as scanned depth sensors and RGBD reconstruction outputs.",,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnueqtBZ36XDFKHEwuOgdqrBHWWSwQM5LTCjsQw8EGp626EU98CBeiB4MFG9dnnC7nSA
7/19/2021 21:44:49,3D Neural Scene Representations for Visuomotor Control,,7/8/2021,https://arxiv.org/pdf/2107.04004.pdf,https://3d-representation-learning.github.io/nerf-dy/,,,,"https://www.youtube.com/watch?v=boKF-q6qofQ, https://www.youtube.com/watch?v=GFkb1x6Oxgo, https://www.youtube.com/watch?v=2fSkcTOvl5M, https://www.youtube.com/watch?v=nckvx1S7-cw",,"@inproceedings{li20213d,
  BOOKTITLE = {Proceedings of Robotics: Science and Systems},
  ID = {li20213d},
  ENTRYTYPE = {inproceedings},
  AUTHOR = {Yunzhu Li and Shuang Li and Vincent Sitzmann and Pulkit Agrawal and Antonio Torralba},
  TITLE = {3D Neural Scene Representations for Visuomotor Control},
  EPRINT = {2107.04004v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.RO},
  ABSTRACT = {Humans have a strong intuitive understanding of the 3D environment around us. The mental model of the physics in our brain applies to objects of different materials and enables us to perform a wide range of manipulation tasks that are far beyond the reach of current robots. In this work, we desire to learn models for dynamic 3D scenes purely from 2D visual observations. Our model combines Neural Radiance Fields (NeRF) and time contrastive learning with an autoencoding framework, which learns viewpoint-invariant 3D-aware scene representations. We show that a dynamics model, constructed over the learned representation space, enables visuomotor control for challenging manipulation tasks involving both rigid bodies and fluids, where the target is specified in a viewpoint different from what the robot operates on. When coupled with an auto-decoding framework, it can even support goal specification from camera viewpoints that are outside the training distribution. We further demonstrate the richness of the learned 3D dynamics model by performing future prediction and novel view synthesis. Finally, we provide detailed ablation studies regarding different system designs and qualitative analysis of the learned representations.},
  YEAR = {2021},
  URL = {http://arxiv.org/abs/2107.04004v1},
  FILE = {2107.04004v1.pdf}
 }","Beyond graphics, Science and engineering, Robotics",,,,,,,,,,,RSS 2021,,,,"Yunzhu Li, Shuang Li, Vincent Sitzmann, Pulkit Agrawal, Antonio Torralba",li20213d,00000159,"Humans have a strong intuitive understanding of the 3D environment around us. The mental model of the physics in our brain applies to objects of different materials and enables us to perform a wide range of manipulation tasks that are far beyond the reach of current robots. In this work, we desire to learn models for dynamic 3D scenes purely from 2D visual observations. Our model combines Neural Radiance Fields (NeRF) and time contrastive learning with an autoencoding framework, which learns viewpoint-invariant 3D-aware scene representations. We show that a dynamics model, constructed over the learned representation space, enables visuomotor control for challenging manipulation tasks involving both rigid bodies and fluids, where the target is specified in a viewpoint different from what the robot operates on. When coupled with an auto-decoding framework, it can even support goal specification from camera viewpoints that are outside the training distribution. We further demonstrate the richness of the learned 3D dynamics model by performing future prediction and novel view synthesis. Finally, we provide detailed ablation studies regarding different system designs and qualitative analysis of the learned representations.",,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnudAVY4LwBVOrydc0wcfc2QBTNmXCiuk6RygsC0YPG_Gw0B1Tx9o8x3ZESDKYf82JrE
7/19/2021 21:43:10,Adaptive weight matrix and phantom intensity learning for computed tomography of chemiluminescence,,7/12/2021,https://www.osapublishing.org/oe/fulltext.cfm?uri=oe-29-15-23682&id=453213,,,,,,,"@article{pan2021adaptive,
  ID = {pan2021adaptive},
  ENTRYTYPE = {article},
  AUTHOR = {Hujie Pan and Di Xiao and Fuhao Zhang and Xuesong Li and Min Xu},
  JOURNAL = {Opt. Express},
  KEYWORDS = {Computational imaging; Computed tomography; Light fields; Light propagation; Neural networks; Propagation methods},
  NUMBER = {15},
  PAGES = {23682--23700},
  PUBLISHER = {OSA},
  TITLE = {Adaptive weight matrix and phantom intensity learning for computed tomography of chemiluminescence},
  VOLUME = {29},
  MONTH = {Jul},
  YEAR = {2021},
  URL = {http://www.opticsexpress.org/abstract.cfm?URI=oe-29-15-23682},
  DOI = {10.1364/OE.427459},
  ABSTRACT = {Classic algebraic reconstruction technique (ART) for computed tomography requires pre-determined weights of the voxels for the projected pixel values to build the equations. However, such weights cannot be accurately obtained in the application of chemiluminescence measurements due to the high physical complexity and computation resources required. Moreover, streaks arise in the results from ART method especially with imperfect projections. In this study, we propose a semi-case-wise learning-based method named Weight Encode Reconstruction Network (WERNet) to co-learn the target phantom intensities and the adaptive weight matrix of the case without labeling the target voxel set and thus offers a more applicable solution for computed tomography problems. Both numerical and experimental validations were conducted to evaluate the algorithm. In the numerical test, with the help of gradient normalization, the WERNet reconstructed voxel set with a high accuracy and showed a higher capability of denoising compared to the classic ART methods. In the experimental test, WERNet produces comparable results to the ART method while having a better performance in avoiding the streaks. Furthermore, with the adaptive weight matrix, WERNet is not sensitive to the ensemble intensity of the projection which shows much better robustness than ART method.}
 }","Beyond graphics, Alternative imaging, Science and engineering",,,,,,,,,,,Optics Express 2021,,,,"Hujie Pan, Di Xiao, Fuhao Zhang, Xuesong Li, Min Xu",pan2021adaptive,00000160,"Classic algebraic reconstruction technique (ART) for computed tomography requires pre-determined weights of the voxels for the projected pixel values to build the equations. However, such weights cannot be accurately obtained in the application of chemiluminescence measurements due to the high physical complexity and computation resources required. Moreover, streaks arise in the results from ART method especially with imperfect projections. In this study, we propose a semi-case-wise learning-based method named Weight Encode Reconstruction Network (WERNet) to co-learn the target phantom intensities and the adaptive weight matrix of the case without labeling the target voxel set and thus offers a more applicable solution for computed tomography problems. Both numerical and experimental validations were conducted to evaluate the algorithm. In the numerical test, with the help of gradient normalization, the WERNet reconstructed voxel set with a high accuracy and showed a higher capability of denoising compared to the classic ART methods. In the experimental test, WERNet produces comparable results to the ART method while having a better performance in avoiding the streaks. Furthermore, with the adaptive weight matrix, WERNet is not sensitive to the ensemble intensity of the projection which shows much better robustness than ART method.",,,Yes,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnudgQ4m9eA_NzjPwCi-m3I1T-x3lmk9XJ2cWgIJ1i62xWTmawwMX5y7bRYWwcpVKa4Q
8/29/2021 20:31:22,Unsupervised Discovery of Object Radiance Fields,uORF,7/16/2021,https://arxiv.org/pdf/2107.07905.pdf,https://kovenyu.com/uorf/,https://github.com/KovenYu/uORF,https://www.youtube.com/watch?v=6J9OpvT4dCA,https://kovenyu.com/uorf/static/uORF_supp.pdf,,,"@article{yu2021uorf,
  JOURNAL = {arXiv preprint arXiv:2107.07905},
  BOOKTITLE = {ArXiv Pre-print},
  ID = {yu2021uorf},
  ENTRYTYPE = {article},
  AUTHOR = {Hong-Xing Yu and Leonidas J. Guibas and Jiajun Wu},
  TITLE = {Unsupervised Discovery of Object Radiance Fields},
  EPRINT = {2107.07905v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We study the problem of inferring an object-centric scene representation from a single image, aiming to derive a representation that explains the image formation process, captures the scene's 3D nature, and is learned without supervision. Most existing methods on scene decomposition lack one or more of these characteristics, due to the fundamental challenge in integrating the complex 3D-to-2D image formation process into powerful inference schemes like deep networks. In this paper, we propose unsupervised discovery of Object Radiance Fields (uORF), integrating recent progresses in neural 3D scene representations and rendering with deep inference networks for unsupervised 3D scene decomposition. Trained on multi-view RGB images without annotations, uORF learns to decompose complex scenes with diverse, textured background from a single image. We show that uORF performs well on unsupervised 3D scene segmentation, novel view synthesis, and scene editing on three datasets.},
  YEAR = {2021},
  URL = {http://arxiv.org/abs/2107.07905v1},
  FILE = {2107.07905v1.pdf}
 }","Generalization, Editable, Segmentation/composition","Conditional neural field, Lifting 2D features to 3D, Volume partitioning, Object-centric representation, Segmentation, Data-driven",NeRF,Density,Category-level,,,,,,,ARXIV 2021,,No,Direct,"Hong-Xing Yu, Leonidas J. Guibas, Jiajun Wu",yu2021uorf,00000178,"We study the problem of inferring an object-centric scene representation from a single image, aiming to derive a representation that explains the image formation process, captures the scene's 3D nature, and is learned without supervision. Most existing methods on scene decomposition lack one or more of these characteristics, due to the fundamental challenge in integrating the complex 3D-to-2D image formation process into powerful inference schemes like deep networks. In this paper, we propose unsupervised discovery of Object Radiance Fields (uORF), integrating recent progresses in neural 3D scene representations and rendering with deep inference networks for unsupervised 3D scene decomposition. Trained on multi-view RGB images without annotations, uORF learns to decompose complex scenes with diverse, textured background from a single image. We show that uORF performs well on unsupervised 3D scene segmentation, novel view synthesis, and scene editing on three datasets.",,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnucPWiPZxCTzeTAXDv6o47rcF6AIN0LoQ_CFIyqpQPOw4fHX7VIvoMyObMZex6Lc3SM
8/29/2021 19:19:51,A Deep Signed Directional Distance Function for Object Shape Representation,,7/23/2021,https://arxiv.org/pdf/2107.11024.pdf,,,,,,,"@article{zobeidi2021a,
  JOURNAL = {arXiv preprint arXiv:2107.11024},
  BOOKTITLE = {ArXiv Pre-print},
  ID = {zobeidi2021a},
  ENTRYTYPE = {article},
  AUTHOR = {Ehsan Zobeidi and Nikolay Atanasov},
  TITLE = {A Deep Signed Directional Distance Function for Object Shape Representation},
  EPRINT = {2107.11024v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Neural networks that map 3D coordinates to signed distance function (SDF) or occupancy values have enabled high-fidelity implicit representations of object shape. This paper develops a new shape model that allows synthesizing novel distance views by optimizing a continuous signed directional distance function (SDDF). Similar to deep SDF models, our SDDF formulation can represent whole categories of shapes and complete or interpolate across shapes from partial input data. Unlike an SDF, which measures distance to the nearest surface in any direction, an SDDF measures distance in a given direction. This allows training an SDDF model without 3D shape supervision, using only distance measurements, readily available from depth camera or Lidar sensors. Our model also removes post-processing steps like surface extraction or rendering by directly predicting distance at arbitrary locations and viewing directions. Unlike deep view-synthesis techniques, such as Neural Radiance Fields, which train high-capacity black-box models, our model encodes by construction the property that SDDF values decrease linearly along the viewing direction. This structure constraint not only results in dimensionality reduction but also provides analytical confidence about the accuracy of SDDF predictions, regardless of the distance to the object surface.},
  YEAR = {2021},
  URL = {http://arxiv.org/abs/2107.11024v1},
  FILE = {2107.11024v1.pdf}
 }",,,,Signed Directional Distance Field (SDDF),Category-level,,,,,,,ARXIV 2021,,Yes,Direct,"Ehsan Zobeidi, Nikolay Atanasov",zobeidi2021a,00000175,"Neural networks that map 3D coordinates to signed distance function (SDF) or occupancy values have enabled high-fidelity implicit representations of object shape. This paper develops a new shape model that allows synthesizing novel distance views by optimizing a continuous signed directional distance function (SDDF). Similar to deep SDF models, our SDDF formulation can represent whole categories of shapes and complete or interpolate across shapes from partial input data. Unlike an SDF, which measures distance to the nearest surface in any direction, an SDDF measures distance in a given direction. This allows training an SDDF model without 3D shape supervision, using only distance measurements, readily available from depth camera or Lidar sensors. Our model also removes post-processing steps like surface extraction or rendering by directly predicting distance at arbitrary locations and viewing directions. Unlike deep view-synthesis techniques, such as Neural Radiance Fields, which train high-capacity black-box models, our model encodes by construction the property that SDDF values decrease linearly along the viewing direction. This structure constraint not only results in dimensionality reduction but also provides analytical confidence about the accuracy of SDDF predictions, regardless of the distance to the object surface.",,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnuefWEGFNp6sezYTA7JdVGGS2yBnt1wUbAdIahJ8BBtZoeL_302TurXUk-Aqmo-R6VI
8/29/2021 17:44:47,H3D-Net: Few-Shot High-Fidelity 3D Head Reconstruction,H3D-Net,7/26/2021,https://arxiv.org/pdf/2107.12512.pdf,https://crisalixsa.github.io/h3d-net/,https://github.com/CrisalixSA/h3ds,,,,,"@article{ramon2021h3dnet,
  JOURNAL = {arXiv preprint arXiv:2107.12512},
  BOOKTITLE = {ArXiv Pre-print},
  ID = {ramon2021h3dnet},
  ENTRYTYPE = {article},
  AUTHOR = {Eduard Ramon and Gil Triginer and Janna Escur and Albert Pumarola and Jaime Garcia and Xavier Giro-i-Nieto and Francesc Moreno-Noguer},
  TITLE = {H3D-Net: Few-Shot High-Fidelity 3D Head Reconstruction},
  EPRINT = {2107.12512v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Recent learning approaches that implicitly represent surface geometry using coordinate-based neural representations have shown impressive results in the problem of multi-view 3D reconstruction. The effectiveness of these techniques is, however, subject to the availability of a large number (several tens) of input views of the scene, and computationally demanding optimizations. In this paper, we tackle these limitations for the specific problem of few-shot full 3D head reconstruction, by endowing coordinate-based representations with a probabilistic shape prior that enables faster convergence and better generalization when using few input images (down to three). First, we learn a shape model of 3D heads from thousands of incomplete raw scans using implicit representations. At test time, we jointly overfit two coordinate-based neural networks to the scene, one modeling the geometry and another estimating the surface radiance, using implicit differentiable rendering. We devise a two-stage optimization strategy in which the learned prior is used to initialize and constrain the geometry during an initial optimization phase. Then, the prior is unfrozen and fine-tuned to the scene. By doing this, we achieve high-fidelity head reconstructions, including hair and shoulders, and with a high level of detail that consistently outperforms both state-of-the-art 3D Morphable Models methods in the few-shot scenario, and non-parametric methods when large sets of views are available.},
  YEAR = {2021},
  URL = {http://arxiv.org/abs/2107.12512v1},
  FILE = {2107.12512v1.pdf}
 }","Human head, Few-shot reconstruction","Conditional neural field, Per-instance fine-tuning, Data-driven",NeRF,SDF,Category-level,,,,,,,ARXIV 2021,https://github.com/CrisalixSA/h3ds,No,Direct,"Eduard Ramon, Gil Triginer, Janna Escur, Albert Pumarola, Jaime Garcia, Xavier Giro-i-Nieto, Francesc Moreno-Noguer",ramon2021h3dnet,00000174,"Recent learning approaches that implicitly represent surface geometry using coordinate-based neural representations have shown impressive results in the problem of multi-view 3D reconstruction. The effectiveness of these techniques is, however, subject to the availability of a large number (several tens) of input views of the scene, and computationally demanding optimizations. In this paper, we tackle these limitations for the specific problem of few-shot full 3D head reconstruction, by endowing coordinate-based representations with a probabilistic shape prior that enables faster convergence and better generalization when using few input images (down to three). First, we learn a shape model of 3D heads from thousands of incomplete raw scans using implicit representations. At test time, we jointly overfit two coordinate-based neural networks to the scene, one modeling the geometry and another estimating the surface radiance, using implicit differentiable rendering. We devise a two-stage optimization strategy in which the learned prior is used to initialize and constrain the geometry during an initial optimization phase. Then, the prior is unfrozen and fine-tuned to the scene. By doing this, we achieve high-fidelity head reconstructions, including hair and shoulders, and with a high level of detail that consistently outperforms both state-of-the-art 3D Morphable Models methods in the few-shot scenario, and non-parametric methods when large sets of views are available.",,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnudLbcUmfk4XpKlS9cL3pm01cCXhJo38D9fdoEDA3yMAHQEQu9Rm0_BI2NXRCO4WmjM
9/17/2021 19:35:45,NeLF: Neural Light-transport Field for Portrait View Synthesis and Relighting,NeLF,7/26/2021,https://arxiv.org/pdf/2107.12351.pdf,,,,,,,"@article{sun2021nelf,
  PUBLISHER = {The Eurographics Association and John Wiley & Sons Ltd.},
  JOURNAL = {Computer Graphics Forum},
  ID = {sun2021nelf},
  ENTRYTYPE = {article},
  AUTHOR = {Tiancheng Sun and Kai-En Lin and Sai Bi and Zexiang Xu and Ravi Ramamoorthi},
  TITLE = {NeLF: Neural Light-transport Field for Portrait View Synthesis and Relighting},
  EPRINT = {2107.12351v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Human portraits exhibit various appearances when observed from different views under different lighting conditions. We can easily imagine how the face will look like in another setup, but computer algorithms still fail on this problem given limited observations. To this end, we present a system for portrait view synthesis and relighting: given multiple portraits, we use a neural network to predict the light-transport field in 3D space, and from the predicted Neural Light-transport Field (NeLF) produce a portrait from a new camera view under a new environmental lighting. Our system is trained on a large number of synthetic models, and can generalize to different synthetic and real portraits under various lighting conditions. Our method achieves simultaneous view synthesis and relighting given multi-view portraits as the input, and achieves state-of-the-art results.},
  YEAR = {2021},
  URL = {http://arxiv.org/abs/2107.12351v1},
  FILE = {2107.12351v1.pdf}
 }","Human head, Material/lighting estimation","Lifting 2D features to 3D, Feature volume, Data-driven",,,Category-level,,,,,,,EGSR 2021,,No,,"Tiancheng Sun, Kai-En Lin, Sai Bi, Zexiang Xu, Ravi Ramamoorthi",sun2021nelf,00000211,"Human portraits exhibit various appearances when observed from different views under different lighting conditions. We can easily imagine how the face will look like in another setup, but computer algorithms still fail on this problem given limited observations. To this end, we present a system for portrait view synthesis and relighting: given multiple portraits, we use a neural network to predict the light-transport field in 3D space, and from the predicted Neural Light-transport Field (NeLF) produce a portrait from a new camera view under a new environmental lighting. Our system is trained on a large number of synthetic models, and can generalize to different synthetic and real portraits under various lighting conditions. Our method achieves simultaneous view synthesis and relighting given multi-view portraits as the input, and achieves state-of-the-art results.",,Yes,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnueCtXCzkg_yk-PqoXG5OyovlNn_v7tS5DYS41yCJFYxxPG5ERYHKNXkdjXJPEYdJJg
8/29/2021 16:46:20,Neural Image Representations for Multi-Image Fusion and Layer Separation,,8/2/2021,https://arxiv.org/pdf/2108.01199.pdf,,,,,,,"@article{nam2021neural,
  JOURNAL = {arXiv preprint arXiv:2108.01199},
  BOOKTITLE = {ArXiv Pre-print},
  ID = {nam2021neural},
  ENTRYTYPE = {article},
  AUTHOR = {Seonghyeon Nam and Marcus A. Brubaker and Michael S. Brown},
  TITLE = {Neural Image Representations for Multi-Image Fusion and Layer Separation},
  EPRINT = {2108.01199v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We propose a framework for aligning and fusing multiple images into a single coordinate-based neural representations. Our framework targets burst images that have misalignment due to camera ego motion and small changes in the scene. We describe different strategies for alignment depending on the assumption of the scene motion, namely, perspective planar (i.e., homography), optical flow with minimal scene change, and optical flow with notable occlusion and disocclusion. Our framework effectively combines the multiple inputs into a single neural implicit function without the need for selecting one of the images as a reference frame. We demonstrate how to use this multi-frame fusion framework for various layer separation tasks.},
  YEAR = {2021},
  URL = {http://arxiv.org/abs/2108.01199v2},
  FILE = {2108.01199v2.pdf}
 }","Dynamic, Image",Warping field/Flow field,,,Per-scene,,,,,,,ARXIV 2021,,,Direct,"Seonghyeon Nam, Marcus A. Brubaker, Michael S. Brown",nam2021neural,00000171,"We propose a framework for aligning and fusing multiple images into a single coordinate-based neural representations. Our framework targets burst images that have misalignment due to camera ego motion and small changes in the scene. We describe different strategies for alignment depending on the assumption of the scene motion, namely, perspective planar (i.e., homography), optical flow with minimal scene change, and optical flow with notable occlusion and disocclusion. Our framework effectively combines the multiple inputs into a single neural implicit function without the need for selecting one of the images as a reference frame. We demonstrate how to use this multi-frame fusion framework for various layer separation tasks.",,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnuedQu0kRtV8fZhN08ACUoKeOI7AEiu5mwj5l5TV1HGZu25ceWDQQQnM64R_iEDMQac
8/30/2021 15:08:06,View Synthesis In Casually Captured Scenes Using a Cylindrical Neural Radiance Field With Exposure Compensation,,8/9/2021,https://dl.acm.org/doi/pdf/10.1145/3450618.3469147,,,,,,,"@article{khademi2021view,
  JOURNAL = {ACM Transactions on Graphics (TOG)},
  ID = {khademi2021view},
  ENTRYTYPE = {article},
  AUTHOR = {Wesley Khademi and Jonathan Ventura},
  TITLE = {View Synthesis In Casually Captured Scenes Using a Cylindrical Neural Radiance Field With Exposure Compensation},
  YEAR = {2021},
  ISBN = {9781450383714},
  PUBLISHER = {Association for Computing Machinery},
  ADDRESS = {New York, NY, USA},
  URL = {https://doi.org/10.1145/3450618.3469147},
  DOI = {10.1145/3450618.3469147},
  ABSTRACT = { We extend Neural Radiance Fields (NeRF) with a cylindrical parameterization that enables rendering photorealistic novel views of 360deg outward facing scenes. We further introduce a learned exposure compensation parameter to account for the varying exposure in training images that may occur from casually capturing a scene. We evaluate our method on a variety of 360deg casually captured scenes.},
  BOOKTITLE = {ACM SIGGRAPH 2021 Posters},
  ARTICLENO = {28},
  NUMPAGES = {2},
  LOCATION = {Virtual Event, USA},
  SERIES = {SIGGRAPH '21}
 }",,"Representation, Sampling",NeRF,Density,Per-scene,,,,,,,SIGGRAPH 2021,,No,Direct,"Wesley Khademi, Jonathan Ventura",khademi2021view,00000190,"We extend Neural Radiance Fields (NeRF) with a cylindrical parameterization that
 enables rendering photorealistic novel views of 360° outward facing scenes. We further
 introduce a learned exposure compensation parameter to account for the varying exposure
 in training images that may occur from casually capturing a scene. We evaluate our
 method on a variety of 360° casually captured scenes.",,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnucwYw--bnUM0WnsqqAvSYjM9jUra2Y_-Iaysjwq3bchuJB9yVOPTvQH-m-pmlD5ilY
8/30/2021 14:56:53,FLAME-in-NeRF: Neural control of Radiance Fields for Free View Face Animation,FLAME-in-NeRF,8/10/2021,https://arxiv.org/pdf/2108.04913.pdf,,,,,,,"@article{athar2021flameinnerf,
  JOURNAL = {arXiv preprint arXiv:2108.04913},
  BOOKTITLE = {ArXiv Pre-print},
  ID = {athar2021flameinnerf},
  ENTRYTYPE = {article},
  AUTHOR = {ShahRukh Athar and Zhixin Shu and Dimitris Samaras},
  TITLE = {FLAME-in-NeRF : Neural control of Radiance Fields for Free View Face Animation},
  EPRINT = {2108.04913v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {This paper presents a neural rendering method for controllable portrait video synthesis. Recent advances in volumetric neural rendering, such as neural radiance fields (NeRF), has enabled the photorealistic novel view synthesis of static scenes with impressive results. However, modeling dynamic and controllable objects as part of a scene with such scene representations is still challenging. In this work, we design a system that enables both novel view synthesis for portrait video, including the human subject and the scene background, and explicit control of the facial expressions through a low-dimensional expression representation. We leverage the expression space of a 3D morphable face model (3DMM) to represent the distribution of human facial expressions, and use it to condition the NeRF volumetric function. Furthermore, we impose a spatial prior brought by 3DMM fitting to guide the network to learn disentangled control for scene appearance and facial actions. We demonstrate the effectiveness of our method on free view synthesis of portrait videos with expression controls. To train a scene, our method only requires a short video of a subject captured by a mobile device.},
  YEAR = {2021},
  URL = {http://arxiv.org/abs/2108.04913v1},
  FILE = {2108.04913v1.pdf}
 }","Dynamic, Human head, Generalization, Editable","Conditional neural field, Coarse-to-fine, Warping field/Flow field, Data-driven",NeRF,Density,Category-level,,,,,,,ARXIV 2021,,No,Direct,"ShahRukh Athar, Zhixin Shu, Dimitris Samaras",athar2021flameinnerf,00000189,"This paper presents a neural rendering method for controllable portrait video synthesis. Recent advances in volumetric neural rendering, such as neural radiance fields (NeRF), has enabled the photorealistic novel view synthesis of static scenes with impressive results. However, modeling dynamic and controllable objects as part of a scene with such scene representations is still challenging. In this work, we design a system that enables both novel view synthesis for portrait video, including the human subject and the scene background, and explicit control of the facial expressions through a low-dimensional expression representation. We leverage the expression space of a 3D morphable face model (3DMM) to represent the distribution of human facial expressions, and use it to condition the NeRF volumetric function. Furthermore, we impose a spatial prior brought by 3DMM fitting to guide the network to learn disentangled control for scene appearance and facial actions. We demonstrate the effectiveness of our method on free view synthesis of portrait videos with expression controls. To train a scene, our method only requires a short video of a subject captured by a mobile device.",,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnufV6foTllHAlcynWScf0OGMUCkPLKI2h4L-LhWhtMIuZW3kE5_SB_7A-J_RT9EP5Mg
8/30/2021 11:59:11,SIDER: Single-Image Neural Optimization for Facial Geometric Detail Recovery,SIDER,8/11/2021,https://arxiv.org/pdf/2108.05465.pdf,,,,,,,"@article{chatziagapi2021sider,
  JOURNAL = {arXiv preprint arXiv:2108.05465},
  BOOKTITLE = {ArXiv Pre-print},
  ID = {chatziagapi2021sider},
  ENTRYTYPE = {article},
  AUTHOR = {Aggelina Chatziagapi and ShahRukh Athar and Francesc Moreno-Noguer and Dimitris Samaras},
  TITLE = {SIDER: Single-Image Neural Optimization for Facial Geometric Detail Recovery},
  EPRINT = {2108.05465v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We present SIDER(Single-Image neural optimization for facial geometric DEtail Recovery), a novel photometric optimization method that recovers detailed facial geometry from a single image in an unsupervised manner. Inspired by classical techniques of coarse-to-fine optimization and recent advances in implicit neural representations of 3D shape, SIDER combines a geometry prior based on statistical models and Signed Distance Functions (SDFs) to recover facial details from single images. First, it estimates a coarse geometry using a morphable model represented as an SDF. Next, it reconstructs facial geometry details by optimizing a photometric loss with respect to the ground truth image. In contrast to prior work, SIDER does not rely on any dataset priors and does not require additional supervision from multiple views, lighting changes or ground truth 3D shape. Extensive qualitative and quantitative evaluation demonstrates that our method achieves state-of-the-art on facial geometric detail recovery, using only a single in-the-wild image.},
  YEAR = {2021},
  URL = {http://arxiv.org/abs/2108.05465v1},
  FILE = {2108.05465v1.pdf}
 }","Human head, Few-shot reconstruction, Generalization","Coarse-to-fine, Data-driven",,SDF,Category-level,,,,,,,ARXIV 2021,,No,Direct,"Aggelina Chatziagapi, ShahRukh Athar, Francesc Moreno-Noguer, Dimitris Samaras",chatziagapi2021sider,00000187,"We present SIDER(Single-Image neural optimization for facial geometric DEtail Recovery), a novel photometric optimization method that recovers detailed facial geometry from a single image in an unsupervised manner. Inspired by classical techniques of coarse-to-fine optimization and recent advances in implicit neural representations of 3D shape, SIDER combines a geometry prior based on statistical models and Signed Distance Functions (SDFs) to recover facial details from single images. First, it estimates a coarse geometry using a morphable model represented as an SDF. Next, it reconstructs facial geometry details by optimizing a photometric loss with respect to the ground truth image. In contrast to prior work, SIDER does not rely on any dataset priors and does not require additional supervision from multiple views, lighting changes or ground truth 3D shape. Extensive qualitative and quantitative evaluation demonstrates that our method achieves state-of-the-art on facial geometric detail recovery, using only a single in-the-wild image.",,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnueSnWp-C8RFcJKnAfMsIm-DALevHqTx_FYekpXpN1NLz_TtHQ_3-_4rnkADFN-W3hE
8/30/2021 13:31:02,Continual Neural Mapping: Learning An Implicit Scene Representation from Sequential Observations,,8/12/2021,https://arxiv.org/pdf/2108.05851.pdf,,,https://zikeyan.github.io/videos/iccv2021.mp4,,,,"@inproceedings{yan2021continual,
  BOOKTITLE = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
  ID = {yan2021continual},
  ENTRYTYPE = {inproceedings},
  AUTHOR = {Zike Yan and Yuxin Tian and Xuesong Shi and Ping Guo and Peng Wang and Hongbin Zha},
  TITLE = {Continual Neural Mapping: Learning An Implicit Scene Representation from Sequential Observations},
  EPRINT = {2108.05851v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Recent advances have enabled a single neural network to serve as an implicit scene representation, establishing the mapping function between spatial coordinates and scene properties. In this paper, we make a further step towards continual learning of the implicit scene representation directly from sequential observations, namely Continual Neural Mapping. The proposed problem setting bridges the gap between batch-trained implicit neural representations and commonly used streaming data in robotics and vision communities. We introduce an experience replay approach to tackle an exemplary task of continual neural mapping: approximating a continuous signed distance function (SDF) from sequential depth images as a scene geometry representation. We show for the first time that a single network can represent scene geometry over time continually without catastrophic forgetting, while achieving promising trade-offs between accuracy and efficiency.},
  YEAR = {2021},
  URL = {http://arxiv.org/abs/2108.05851v1},
  FILE = {2108.05851v1.pdf}
 }","Robotics, Multi-task/Continual/Transfer learning","Conditional neural field, Volume partitioning",,SDF,,,,,,,,ICCV 2021,,Yes,Direct,"Zike Yan, Yuxin Tian, Xuesong Shi, Ping Guo, Peng Wang, Hongbin Zha",yan2021continual,00000188,"Recent advances have enabled a single neural network to serve as an implicit scene representation, establishing the mapping function between spatial coordinates and scene properties. In this paper, we make a further step towards continual learning of the implicit scene representation directly from sequential observations, namely Continual Neural Mapping. The proposed problem setting bridges the gap between batch-trained implicit neural representations and commonly used streaming data in robotics and vision communities. We introduce an experience replay approach to tackle an exemplary task of continual neural mapping: approximating a continuous signed distance function (SDF) from sequential depth images as a scene geometry representation. We show for the first time that a single network can represent scene geometry over time continually without catastrophic forgetting, while achieving promising trade-offs between accuracy and efficiency.",,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnueL7aPFzJ8YhDl0H59GO33RYdtE9Gc-wUrKUzKQitnE_GUqYXEVxBpRTM4OlsERLkU
9/18/2021 10:30:51,Unsupervised Non-Rigid Image Distortion Removal via Grid Deformation,,8/16/2021,https://ivlab.cse.lsu.edu/pub/iccv_21_distortion_removal.pdf,,https://github.com/Nianyi-Li/unsupervised-NDIR,,,,,"@inproceedings{li2021unsupervised,
  BOOKTITLE = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
  YEAR = {2021},
  ID = {li2021unsupervised},
  ENTRYTYPE = {inproceedings},
  ABSTRACT = {Many computer vision problems face difficulties when imaging through turbulent refractive media (eg, air and water) due to the refraction and scattering of light. These effects cause geometric distortion that requires either handcrafted physical priors or supervised learning methods to remove. In this paper, we present a novel unsupervised network to recover the latent distortion-free image. The key idea is to model non-rigid distortions as deformable grids. Our network consists of a grid deformer that estimates the distortion field and an image},
  AUTHOR = {Nianyi Li and Simron Thapa and Cameron Whyte and Albert Reed and Suren Jayasuriya and Jinwei Ye},
  PUB_YEAR = {NA},
  TITLE = {Unsupervised Non-Rigid Image Distortion Removal via Grid Deformation},
  VENUE = {NA}
 }","Image, Beyond graphics, Alternative imaging",,,,,,,,,,,ICCV 2021,,,"Direct, Indirect","Nianyi Li, Simron Thapa, Cameron Whyte, Albert Reed, Suren Jayasuriya, Jinwei Ye",li2021unsupervised,00000225,"Many computer vision problems face difficulties when imaging through turbulent refractive media (eg, air and water) due to the refraction and scattering of light. These effects cause geometric distortion that requires either handcrafted physical priors or supervised learning methods to remove. In this paper, we present a novel unsupervised network to recover the latent distortion-free image. The key idea is to model non-rigid distortions as deformable grids. Our network consists of a grid deformer that estimates the distortion field and an image",,,Yes,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnufGIbNbqU1lxFQfaYoGO_kQ37_CX7swaM5cE-bkzULODCdFrRi24X_dgfaOGH5E-tw
9/17/2021 14:20:27,ARCH++: Animation-Ready Clothed Human Reconstruction Revisited,ARCH++,8/17/2021,https://arxiv.org/pdf/2108.07845.pdf,,,,,,,"@inproceedings{he2021arch++,
  BOOKTITLE = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
  ID = {he2021arch++},
  ENTRYTYPE = {inproceedings},
  AUTHOR = {Tong He and Yuanlu Xu and Shunsuke Saito and Stefano Soatto and Tony Tung},
  TITLE = {ARCH++: Animation-Ready Clothed Human Reconstruction Revisited},
  EPRINT = {2108.07845v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We present ARCH++, an image-based method to reconstruct 3D avatars with arbitrary clothing styles. Our reconstructed avatars are animation-ready and highly realistic, in both the visible regions from input views and the unseen regions. While prior work shows great promise of reconstructing animatable clothed humans with various topologies, we observe that there exist fundamental limitations resulting in sub-optimal reconstruction quality. In this paper, we revisit the major steps of image-based avatar reconstruction and address the limitations with ARCH++. First, we introduce an end-to-end point based geometry encoder to better describe the semantics of the underlying 3D human body, in replacement of previous hand-crafted features. Second, in order to address the occupancy ambiguity caused by topological changes of clothed humans in the canonical pose, we propose a co-supervising framework with cross-space consistency to jointly estimate the occupancy in both the posed and canonical spaces. Last, we use image-to-image translation networks to further refine detailed geometry and texture on the reconstructed surface, which improves the fidelity and consistency across arbitrary viewpoints. In the experiments, we demonstrate improvements over the state of the art on both public benchmarks and user studies in reconstruction quality and realism.},
  YEAR = {2021},
  URL = {http://arxiv.org/abs/2108.07845v1},
  FILE = {2108.07845v1.pdf}
 }","Human body, Few-shot reconstruction, Editable","Conditional neural field, Lifting 2D features to 3D, Voxelization, Feature volume, Data-driven",,,Category-level,,,,,,,ICCV 2021,,No,,"Tong He, Yuanlu Xu, Shunsuke Saito, Stefano Soatto, Tony Tung",he2021arch++,00000207,"We present ARCH++, an image-based method to reconstruct 3D avatars with arbitrary clothing styles. Our reconstructed avatars are animation-ready and highly realistic, in both the visible regions from input views and the unseen regions. While prior work shows great promise of reconstructing animatable clothed humans with various topologies, we observe that there exist fundamental limitations resulting in sub-optimal reconstruction quality. In this paper, we revisit the major steps of image-based avatar reconstruction and address the limitations with ARCH++. First, we introduce an end-to-end point based geometry encoder to better describe the semantics of the underlying 3D human body, in replacement of previous hand-crafted features. Second, in order to address the occupancy ambiguity caused by topological changes of clothed humans in the canonical pose, we propose a co-supervising framework with cross-space consistency to jointly estimate the occupancy in both the posed and canonical spaces. Last, we use image-to-image translation networks to further refine detailed geometry and texture on the reconstructed surface, which improves the fidelity and consistency across arbitrary viewpoints. In the experiments, we demonstrate improvements over the state of the art on both public benchmarks and user studies in reconstruction quality and realism.",,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnuf9CjswqXq_Hm_PWfP_GCcHairpxscXm98u7fu89kokde-qv3TqUM7UMmUJAVAosdo
8/29/2021 16:11:17,Neural-GIF: Neural Generalized Implicit Functions for Animating People in Clothing,Neural-GIF,8/19/2021,https://arxiv.org/pdf/2108.08807.pdf,,,,,,,"@inproceedings{tiwari2021neuralgif,
  BOOKTITLE = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
  ID = {tiwari2021neuralgif},
  ENTRYTYPE = {inproceedings},
  AUTHOR = {Garvita Tiwari and Nikolaos Sarafianos and Tony Tung and Gerard Pons-Moll},
  TITLE = {Neural-GIF: Neural Generalized Implicit Functions for Animating People in Clothing},
  EPRINT = {2108.08807v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We present Neural Generalized Implicit Functions(Neural-GIF), to animate people in clothing as a function of the body pose. Given a sequence of scans of a subject in various poses, we learn to animate the character for new poses. Existing methods have relied on template-based representations of the human body (or clothing). However such models usually have fixed and limited resolutions, require difficult data pre-processing steps and cannot be used with complex clothing. We draw inspiration from template-based methods, which factorize motion into articulation and non-rigid deformation, but generalize this concept for implicit shape learning to obtain a more flexible model. We learn to map every point in the space to a canonical space, where a learned deformation field is applied to model non-rigid effects, before evaluating the signed distance field. Our formulation allows the learning of complex and non-rigid deformations of clothing and soft tissue, without computing a template registration as it is common with current approaches. Neural-GIF can be trained on raw 3D scans and reconstructs detailed complex surface geometry and deformations. Moreover, the model can generalize to new poses. We evaluate our method on a variety of characters from different public datasets in diverse clothing styles and show significant improvements over baseline methods, quantitatively and qualitatively. We also extend our model to multiple shape setting. To stimulate further research, we will make the model, code and data publicly available at: https://virtualhumans.mpi-inf.mpg.de/neuralgif/},
  YEAR = {2021},
  URL = {http://arxiv.org/abs/2108.08807v2},
  FILE = {2108.08807v2.pdf}
 }",Human body,Warping field/Flow field,,SDF,Category-level,,,,,,,ICCV 2021,,Yes,,"Garvita Tiwari, Nikolaos Sarafianos, Tony Tung, Gerard Pons-Moll",tiwari2021neuralgif,00000161,"We present Neural Generalized Implicit Functions(Neural-GIF), to animate people in clothing as a function of the body pose. Given a sequence of scans of a subject in various poses, we learn to animate the character for new poses. Existing methods have relied on template-based representations of the human body (or clothing). However such models usually have fixed and limited resolutions, require difficult data pre-processing steps and cannot be used with complex clothing. We draw inspiration from template-based methods, which factorize motion into articulation and non-rigid deformation, but generalize this concept for implicit shape learning to obtain a more flexible model. We learn to map every point in the space to a canonical space, where a learned deformation field is applied to model non-rigid effects, before evaluating the signed distance field. Our formulation allows the learning of complex and non-rigid deformations of clothing and soft tissue, without computing a template registration as it is common with current approaches. Neural-GIF can be trained on raw 3D scans and reconstructs detailed complex surface geometry and deformations. Moreover, the model can generalize to new poses. We evaluate our method on a variety of characters from different public datasets in diverse clothing styles and show significant improvements over baseline methods, quantitatively and qualitatively. We also extend our model to multiple shape setting. To stimulate further research, we will make the model, code and data publicly available at: https://virtualhumans.mpi-inf.mpg.de/neuralgif/",,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnucRISTnmm3x_keTkutQUzq6_UCWbD-iDZQI47FJypjhNrqbgNo8tr-mDnf3b4IwceU
8/29/2021 16:44:39,Augmenting Implicit Neural Shape Representations with Explicit Deformation Fields,,8/19/2021,https://arxiv.org/pdf/2108.08931.pdf,,,,,,,"@article{atzmon2021augmenting,
  JOURNAL = {arXiv preprint arXiv:2108.08931},
  BOOKTITLE = {ArXiv Pre-print},
  ID = {atzmon2021augmenting},
  ENTRYTYPE = {article},
  AUTHOR = {Matan Atzmon and David Novotny and Andrea Vedaldi and Yaron Lipman},
  TITLE = {Augmenting Implicit Neural Shape Representations with Explicit Deformation Fields},
  EPRINT = {2108.08931v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Implicit neural representation is a recent approach to learn shape collections as zero level-sets of neural networks, where each shape is represented by a latent code. So far, the focus has been shape reconstruction, while shape generalization was mostly left to generic encoder-decoder or auto-decoder regularization. In this paper we advocate deformation-aware regularization for implicit neural representations, aiming at producing plausible deformations as latent code changes. The challenge is that implicit representations do not capture correspondences between different shapes, which makes it difficult to represent and regularize their deformations. Thus, we propose to pair the implicit representation of the shapes with an explicit, piecewise linear deformation field, learned as an auxiliary function. We demonstrate that, by regularizing these deformation fields, we can encourage the implicit neural representation to induce natural deformations in the learned shape space, such as as-rigid-as-possible deformations.},
  YEAR = {2021},
  URL = {http://arxiv.org/abs/2108.08931v1},
  FILE = {2108.08931v1.pdf}
 }","Dynamic, Human body",Warping field/Flow field,,,,,,,,,,ARXIV 2021,,Yes,,"Matan Atzmon, David Novotny, Andrea Vedaldi, Yaron Lipman",atzmon2021augmenting,00000170,"Implicit neural representation is a recent approach to learn shape collections as zero level-sets of neural networks, where each shape is represented by a latent code. So far, the focus has been shape reconstruction, while shape generalization was mostly left to generic encoder-decoder or auto-decoder regularization. In this paper we advocate deformation-aware regularization for implicit neural representations, aiming at producing plausible deformations as latent code changes. The challenge is that implicit representations do not capture correspondences between different shapes, which makes it difficult to represent and regularize their deformations. Thus, we propose to pair the implicit representation of the shapes with an explicit, piecewise linear deformation field, learned as an auxiliary function. We demonstrate that, by regularizing these deformation fields, we can encourage the implicit neural representation to induce natural deformations in the learned shape space, such as as-rigid-as-possible deformations.",,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnud3SZEcDAPRnTm3B9ey4HytRiEd5CpXK_4ld_s7-J_nn1oyilBfrfljJajLq3tVJAA
8/29/2021 16:33:37,Learning Deeper Non-Monotonic Networks by Softly Transferring Solution Space,,8/20/2021,https://www.ijcai.org/proceedings/2021/0440.pdf,,,,,,,"@inproceedings{wu2021learning,
  PUBLISHER = {International Joint Conferences on Artificial Intelligence Organization},
  BOOKTITLE = {Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence (IJCAI)},
  ID = {wu2021learning},
  ENTRYTYPE = {inproceedings},
  ABSTRACT = {Different from popular neural networks using quasiconvex activations, non-monotonic networks activated by periodic nonlinearities have emerged as a more competitive paradigm, offering revolutionary benefits: 1) compactly characterizing highfrequency patterns; 2) precisely representing highorder derivatives. Nevertheless, they are also wellknown for being hard to train, due to easily overfitting dissonant noise and only allowing for tiny architectures (shallower than 5 layers). The fundamental bottleneck is that the},
  AUTHOR = {Zheng-Fan Wu and Hui Xue and Weimin Bai},
  YEAR = {2021},
  TITLE = {Learning Deeper Non-Monotonic Networks by Softly Transferring Solution Space},
  VENUE = {NA}
 }","Fundamentals, Multi-task/Continual/Transfer learning",,Other,,,,,,,,,IJCAI 2021,,,,"Zheng-Fan Wu, Hui Xue, Weimin Bai",wu2021learning,00000166,"Different from popular neural networks using quasiconvex activations, non-monotonic networks activated by periodic nonlinearities have emerged as a more competitive paradigm, offering revolutionary benefits: 1) compactly characterizing highfrequency patterns; 2) precisely representing highorder derivatives. Nevertheless, they are also wellknown for being hard to train, due to easily overfitting dissonant noise and only allowing for tiny architectures (shallower than 5 layers). The fundamental bottleneck is that the",,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnudzf5QiTMMln7ouKyhqKNeYksT10dq3Z5z57GtwEVHb7b7sIYeU_zeV-7wvGQNEIOc
8/29/2021 16:15:58,Implicit Neural Representations for Deconvolving SAS Images,,8/23/2021,https://web.asu.edu/sites/default/files/imaging-lyceum/files/2021151090.pdf,,,,,,,"@article{reed2021implicit,
  ID = {reed2021implicit},
  ENTRYTYPE = {article},
  ABSTRACT = {Synthetic aperture sonar (SAS) image resolution is constrained by waveform bandwidth and array geometry. Specifically, the waveform bandwidth determines a point spread function (PSF) that blurs the locations of point scatterers in the scene. In theory, deconvolving the reconstructed SAS image with the scene PSF restores the original distribution of scatterers and yields sharper reconstructions. However, deconvolution is an ill-posed operation that is highly sensitive to noise. In this work, we leverage implicit neural representations (INRs)},
  AUTHOR = {Albert Reed and Thomas Blanford and Daniel C Brown and Suren Jayasuriya},
  PUB_YEAR = {NA},
  TITLE = {Implicit Neural Representations for Deconvolving SAS Images},
  VENUE = {NA}
 }","Beyond graphics, Science and engineering, Alternative imaging",,,,,,,,,,,OCEANS 2021,,,Direct,"Albert Reed, Thomas Blanford, Daniel C Brown, Suren Jayasuriya",reed2021implicit,00000162,"Synthetic aperture sonar (SAS) image resolution is constrained by waveform bandwidth and array geometry. Specifically, the waveform bandwidth determines a point spread function (PSF) that blurs the locations of point scatterers in the scene. In theory, deconvolving the reconstructed SAS image with the scene PSF restores the original distribution of scatterers and yields sharper reconstructions. However, deconvolution is an ill-posed operation that is highly sensitive to noise. In this work, we leverage implicit neural representations (INRs)",,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnucT5ZJ2L152EUy6DWBVffWDDsEu6hcudMlTNUqnBFTFut9anoxBW6mMFIi9glyaf8k
8/29/2021 16:40:15,Learning Signed Distance Field for Multi-view Surface Reconstruction,,8/23/2021,https://arxiv.org/pdf/2108.09964.pdf,,,,,,,"@inproceedings{zhang2021learning,
  BOOKTITLE = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
  ID = {zhang2021learning},
  ENTRYTYPE = {inproceedings},
  AUTHOR = {Jingyang Zhang and Yao Yao and Long Quan},
  TITLE = {Learning Signed Distance Field for Multi-view Surface Reconstruction},
  EPRINT = {2108.09964v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Recent works on implicit neural representations have shown promising results for multi-view surface reconstruction. However, most approaches are limited to relatively simple geometries and usually require clean object masks for reconstructing complex and concave objects. In this work, we introduce a novel neural surface reconstruction framework that leverages the knowledge of stereo matching and feature consistency to optimize the implicit surface representation. More specifically, we apply a signed distance field (SDF) and a surface light field to represent the scene geometry and appearance respectively. The SDF is directly supervised by geometry from stereo matching, and is refined by optimizing the multi-view feature consistency and the fidelity of rendered images. Our method is able to improve the robustness of geometry estimation and support reconstruction of complex scene topologies. Extensive experiments have been conducted on DTU, EPFL and Tanks and Temples datasets. Compared to previous state-of-the-art methods, our method achieves better mesh reconstruction in wide open scenes without masks as input.},
  YEAR = {2021},
  URL = {http://arxiv.org/abs/2108.09964v1},
  FILE = {2108.09964v1.pdf}
 }",,Data-driven,,SDF,,,,,,,,ICCV 2021,,No,Direct,"Jingyang Zhang, Yao Yao, Long Quan",zhang2021learning,00000168,"Recent works on implicit neural representations have shown promising results for multi-view surface reconstruction. However, most approaches are limited to relatively simple geometries and usually require clean object masks for reconstructing complex and concave objects. In this work, we introduce a novel neural surface reconstruction framework that leverages the knowledge of stereo matching and feature consistency to optimize the implicit surface representation. More specifically, we apply a signed distance field (SDF) and a surface light field to represent the scene geometry and appearance respectively. The SDF is directly supervised by geometry from stereo matching, and is refined by optimizing the multi-view feature consistency and the fidelity of rendered images. Our method is able to improve the robustness of geometry estimation and support reconstruction of complex scene topologies. Extensive experiments have been conducted on DTU, EPFL and Tanks and Temples datasets. Compared to previous state-of-the-art methods, our method achieves better mesh reconstruction in wide open scenes without masks as input.",,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnud7NQ3u8yr_E5pERXBkMJG3fu5hbjC-G8OJSlCmpYEkSSgaAknDW8s5MBalWQnriQA
8/30/2021 17:53:56,NeRP: Implicit Neural Representation Learning with Prior Embedding for Sparsely Sampled Image Reconstruction,NeRP,8/24/2021,https://arxiv.org/ftp/arxiv/papers/2108/2108.10991.pdf,,,,,,,"@article{shen2021nerp,
  JOURNAL = {arXiv preprint arXiv:ers/2108/2108.10991},
  BOOKTITLE = {ArXiv Pre-print},
  ID = {shen2021nerp},
  ENTRYTYPE = {article},
  AUTHOR = {Liyue Shen and John Pauly and Lei Xing},
  TITLE = {NeRP: Implicit Neural Representation Learning with Prior Embedding for Sparsely Sampled Image Reconstruction},
  EPRINT = {2108.10991v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {eess.IV},
  ABSTRACT = {Image reconstruction is an inverse problem that solves for a computational image based on sampled sensor measurement. Sparsely sampled image reconstruction poses addition challenges due to limited measurements. In this work, we propose an implicit Neural Representation learning methodology with Prior embedding (NeRP) to reconstruct a computational image from sparsely sampled measurements. The method differs fundamentally from previous deep learning-based image reconstruction approaches in that NeRP exploits the internal information in an image prior, and the physics of the sparsely sampled measurements to produce a representation of the unknown subject. No large-scale data is required to train the NeRP except for a prior image and sparsely sampled measurements. In addition, we demonstrate that NeRP is a general methodology that generalizes to different imaging modalities such as CT and MRI. We also show that NeRP can robustly capture the subtle yet significant image changes required for assessing tumor progression.},
  YEAR = {2021},
  URL = {http://arxiv.org/abs/2108.10991v1},
  FILE = {2108.10991v1.pdf}
 }","Generalization, Image, Alternative imaging, Science and engineering","Per-instance fine-tuning, Data-driven",,,Category-level,,,,,,,ARXIV 2021,,,Direct,"Liyue Shen, John Pauly, Lei Xing",shen2021nerp,00000191,"Image reconstruction is an inverse problem that solves for a computational image based on sampled sensor measurement. Sparsely sampled image reconstruction poses addition challenges due to limited measurements. In this work, we propose an implicit Neural Representation learning methodology with Prior embedding (NeRP) to reconstruct a computational image from sparsely sampled measurements. The method differs fundamentally from previous deep learning-based image reconstruction approaches in that NeRP exploits the internal information in an image prior, and the physics of the sparsely sampled measurements to produce a representation of the unknown subject. No large-scale data is required to train the NeRP except for a prior image and sparsely sampled measurements. In addition, we demonstrate that NeRP is a general methodology that generalizes to different imaging modalities such as CT and MRI. We also show that NeRP can robustly capture the subtle yet significant image changes required for assessing tumor progression.",,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnucUZFMmfu2bppryAI-AkEnvR_KGcgljUFwipJwj9-xW-gD3E2V3ztmGQ_CHFKpmvfs
9/17/2021 11:34:22,imGHUM: Implicit Generative Models of 3D Human Shape and Articulated Pose,imGHUM,8/24/2021,https://arxiv.org/pdf/2108.10842.pdf,https://research.google/pubs/pub50642/,,,,,,"@inproceedings{alldieck2021imghum,
  BOOKTITLE = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  ID = {alldieck2021imghum},
  ENTRYTYPE = {inproceedings},
  AUTHOR = {Thiemo Alldieck and Hongyi Xu and Cristian Sminchisescu},
  TITLE = {imGHUM: Implicit Generative Models of 3D Human Shape and Articulated Pose},
  EPRINT = {2108.10842v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We present imGHUM, the first holistic generative model of 3D human shape and articulated pose, represented as a signed distance function. In contrast to prior work, we model the full human body implicitly as a function zero-level-set and without the use of an explicit template mesh. We propose a novel network architecture and a learning paradigm, which make it possible to learn a detailed implicit generative model of human pose, shape, and semantics, on par with state-of-the-art mesh-based models. Our model features desired detail for human models, such as articulated pose including hand motion and facial expressions, a broad spectrum of shape variations, and can be queried at arbitrary resolutions and spatial locations. Additionally, our model has attached spatial semantics making it straightforward to establish correspondences between different shape instances, thus enabling applications that are difficult to tackle using classical implicit representations. In extensive experiments, we demonstrate the model accuracy and its applicability to current research problems.},
  YEAR = {2021},
  URL = {http://arxiv.org/abs/2108.10842v1},
  FILE = {2108.10842v1.pdf}
 }",Human body,"Conditional neural field, Volume partitioning",,SDF,,,,,,,,CVPR 2021,,No,Direct,"Thiemo Alldieck, Hongyi Xu, Cristian Sminchisescu",alldieck2021imghum,00000194,"We present imGHUM, the first holistic generative model of 3D human shape and articulated pose, represented as a signed distance function. In contrast to prior work, we model the full human body implicitly as a function zero-level-set and without the use of an explicit template mesh. We propose a novel network architecture and a learning paradigm, which make it possible to learn a detailed implicit generative model of human pose, shape, and semantics, on par with state-of-the-art mesh-based models. Our model features desired detail for human models, such as articulated pose including hand motion and facial expressions, a broad spectrum of shape variations, and can be queried at arbitrary resolutions and spatial locations. Additionally, our model has attached spatial semantics making it straightforward to establish correspondences between different shape instances, thus enabling applications that are difficult to tackle using classical implicit representations. In extensive experiments, we demonstrate the model accuracy and its applicability to current research problems.",,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnucZoo_zkhaeRTrMV19ilTBO6src3Yu6R1wmj8PkABf5YsB2hGGUyevGHWCOjGqgNb8
8/30/2021 18:01:26,Self-Calibrating Neural Radiance Fields,,8/30/2021,http://jaesik.info/publications/data/21_iccv1.pdf,,https://github.com/POSTECH-CVLab/SCNeRF,https://www.youtube.com/watch?v=wsjx6geduvk,,,,"@inproceedings{jeong2021selfcalibrating,
  BOOKTITLE = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
  YEAR = {2021},
  ID = {jeong2021selfcalibrating},
  ENTRYTYPE = {inproceedings},
  ABSTRACT = {In this work, we propose a camera self-calibration algorithm for generic cameras with arbitrary non-linear distortions. We jointly learn the geometry of the scene and the accurate camera parameters without any calibration objects. Our camera model consists of a pinhole model, a fourth order radial distortion, and a generic noise model that can learn arbitrary non-linear camera distortions. While traditional self-calibration algorithms mostly rely on geometric constraints, we additionally incorporate photometric consistency. This requires},
  AUTHOR = {Yoonwoo Jeong and Seokjun Ahn and Christopher Choy and Animashree Anandkumar and Minsu Cho and Jaesik Park},
  JOURNAL = {arXiv preprint arXiv:2108.13826},
  PUB_YEAR = {2021},
  TITLE = {Self-Calibrating Neural Radiance Fields},
  VENUE = {arXiv preprint arXiv ...}
 }",Camera parameter estimation,Coarse-to-fine,NeRF,Density,Per-scene,,,,,,,ICCV 2021,https://github.com/POSTECH-CVLab/SCNeRF,No,Direct,"Yoonwoo Jeong, Seokjun Ahn, Christopher Choy, Animashree Anandkumar, Minsu Cho, Jaesik Park",jeong2021selfcalibrating,00000192,"In this work, we propose a camera self-calibration algorithm for generic cameras with arbitrary non-linear distortions. We jointly learn the geometry of the scene and the accurate camera parameters without any calibration objects. Our camera model consists of a pinhole model, a fourth order radial distortion, and a generic noise model that can learn arbitrary non-linear camera distortions. While traditional self-calibration algorithms mostly rely on geometric constraints, we additionally incorporate photometric consistency. This requires",,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnudu8-3apyntMu4IdCS8QZ7kTZGHsEETz-wNbhdGdYt68FXwzvBua-YaWK1arweWzXU
9/18/2021 9:42:49,A modified physics-informed neural network with positional encoding,,9/1/2021,https://library.seg.org/doi/pdf/10.1190/segam2021-3584127.1,,,,,,,"@inbook{huang2021a,
  ID = {huang2021a},
  ENTRYTYPE = {inbook},
  ABSTRACT = {Recently developed physics-informed neural network (PINN) for solving for the scattered wavefield in the Helmholtz equation showed large potential in seismic modeling because of its flexibility, low memory requirement, and no limitations on the shape of the solution space. However, the predicted solutions were somewhat smooth and the convergence of the training was slow. Thus, we propose a modified PINN using sinusoidal activation functions and positional encoding, aiming to accelerate the convergence and fit better. We transform},
  AUTHOR = {Xinquan Huang and Tariq Alkhalifah and Chao Song},
  BOOKTITLE = {First International Meeting for Applied Geoscience \& Energy},
  ORGANIZATION = {Society of Exploration Geophysicists},
  PAGES = {2480--2484},
  TITLE = {A modified physics-informed neural network with positional encoding},
  YEAR = {2021},
  DOI = {10.1190/segam2021-3584127.1},
  URL = {https://library.seg.org/doi/abs/10.1190/segam2021-3584127.1}
 }","Beyond graphics, Fundamentals, Science and engineering, PDE",,NeRF,,,,,,,,,IMAGE 2021,,,Direct,"Xinquan Huang, Tariq Alkhalifah, Chao Song",huang2021a,00000220,"Recently developed physics-informed neural network (PINN) for solving for the scattered wavefield in the Helmholtz equation showed large potential in seismic modeling because of its flexibility, low memory requirement, and no limitations on the shape of the solution space. However, the predicted solutions were somewhat smooth and the convergence of the training was slow. Thus, we propose a modified PINN using sinusoidal activation functions and positional encoding, aiming to accelerate the convergence and fit better. We transform",,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnueXrGSBBSg2Q9j7bzwcVEIS60_gKQRrmo25MPkzwZ0uomWEl8hCfWwG2T9kD87LUoY
9/18/2021 10:00:17,Seeing Implicit Neural Representations as Fourier Series,,9/1/2021,https://arxiv.org/pdf/2109.00249.pdf,,,,,,,"@article{benbarka2021seeing,
  JOURNAL = {arXiv preprint arXiv:2109.00249},
  BOOKTITLE = {ArXiv Pre-print},
  ID = {benbarka2021seeing},
  ENTRYTYPE = {article},
  AUTHOR = {Nuri Benbarka and Timon Hofer and Hamd ul-moqeet Riaz and Andreas Zell},
  TITLE = {Seeing Implicit Neural Representations as Fourier Series},
  EPRINT = {2109.00249v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Implicit Neural Representations (INR) use multilayer perceptrons to represent high-frequency functions in low-dimensional problem domains. Recently these representations achieved state-of-the-art results on tasks related to complex 3D objects and scenes. A core problem is the representation of highly detailed signals, which is tackled using networks with periodic activation functions (SIRENs) or applying Fourier mappings to the input. This work analyzes the connection between the two methods and shows that a Fourier mapped perceptron is structurally like one hidden layer SIREN. Furthermore, we identify the relationship between the previously proposed Fourier mapping and the general d-dimensional Fourier series, leading to an integer lattice mapping. Moreover, we modify a progressive training strategy to work on arbitrary Fourier mappings and show that it improves the generalization of the interpolation task. Lastly, we compare the different mappings on the image regression and novel view synthesis tasks. We confirm the previous finding that the main contributor to the mapping performance is the size of the embedding and standard deviation of its elements.},
  YEAR = {2021},
  URL = {http://arxiv.org/abs/2109.00249v1},
  FILE = {2109.00249v1.pdf}
 }","Image, Beyond graphics, Fundamentals",Coarse-to-fine,Other,,,,,,,,,ARXIV 2021,,,,"Nuri Benbarka, Timon Höfer, Hamd ul-moqeet Riaz, Andreas Zell",benbarka2021seeing,00000221,"Implicit Neural Representations (INR) use multilayer perceptrons to represent high-frequency functions in low-dimensional problem domains. Recently these representations achieved state-of-the-art results on tasks related to complex 3D objects and scenes. A core problem is the representation of highly detailed signals, which is tackled using networks with periodic activation functions (SIRENs) or applying Fourier mappings to the input. This work analyzes the connection between the two methods and shows that a Fourier mapped perceptron is structurally like one hidden layer SIREN. Furthermore, we identify the relationship between the previously proposed Fourier mapping and the general d-dimensional Fourier series, leading to an integer lattice mapping. Moreover, we modify a progressive training strategy to work on arbitrary Fourier mappings and show that it improves the generalization of the interpolation task. Lastly, we compare the different mappings on the image regression and novel view synthesis tasks. We confirm the previous finding that the main contributor to the mapping performance is the size of the embedding and standard deviation of its elements.",,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnuc547Fhb1qDXYatJXvyHp4hd0ZNJfez-UE3GEmVCVqJdZ7VJgWtDeU46gpJZUNhyR8
9/28/2021 9:31:58,Common Objects in 3D: Large-Scale Learning and Evaluation of Real-life 3D Category Reconstruction,CO3D,9/1/2021,https://arxiv.org/pdf/2109.00512.pdf,,https://github.com/facebookresearch/co3d,https://www.youtube.com/watch?v=hMx9nzG50xQ,,,,"@inproceedings{reizenstein2021co3d,
  BOOKTITLE = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  ID = {reizenstein2021co3d},
  ENTRYTYPE = {inproceedings},
  AUTHOR = {Jeremy Reizenstein and Roman Shapovalov and Philipp Henzler and Luca Sbordone and Patrick Labatut and David Novotny},
  TITLE = {Common Objects in 3D: Large-Scale Learning and Evaluation of Real-life 3D Category Reconstruction},
  EPRINT = {2109.00512v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Traditional approaches for learning 3D object categories have been predominantly trained and evaluated on synthetic datasets due to the unavailability of real 3D-annotated category-centric data. Our main goal is to facilitate advances in this field by collecting real-world data in a magnitude similar to the existing synthetic counterparts. The principal contribution of this work is thus a large-scale dataset, called Common Objects in 3D, with real multi-view images of object categories annotated with camera poses and ground truth 3D point clouds. The dataset contains a total of 1.5 million frames from nearly 19,000 videos capturing objects from 50 MS-COCO categories and, as such, it is significantly larger than alternatives both in terms of the number of categories and objects. We exploit this new dataset to conduct one of the first large-scale ""in-the-wild"" evaluations of several new-view-synthesis and category-centric 3D reconstruction methods. Finally, we contribute NerFormer - a novel neural rendering method that leverages the powerful Transformer to reconstruct an object given a small number of its views. The CO3D dataset is available at https://github.com/facebookresearch/co3d .},
  YEAR = {2021},
  NOTE = {International Conference on Computer Vision, 2021},
  URL = {http://arxiv.org/abs/2109.00512v1},
  FILE = {2109.00512v1.pdf}
 }","Few-shot reconstruction, Generalization","Conditional neural field, Lifting 2D features to 3D, Transformer",NeRF,Density,Universal,,,,,,,CVPR 2021,https://ai.facebook.com/datasets/co3d-downloads/,No,Direct,"Jeremy Reizenstein, Roman Shapovalov, Philipp Henzler, Luca Sbordone, Patrick Labatut, David Novotny",reizenstein2021co3d,00000223,"Traditional approaches for learning 3D object categories have been predominantly trained and evaluated on synthetic datasets due to the unavailability of real 3D-annotated category-centric data. Our main goal is to facilitate advances in this field by collecting real-world data in a magnitude similar to the existing synthetic counterparts. The principal contribution of this work is thus a large-scale dataset, called Common Objects in 3D, with real multi-view images of object categories annotated with camera poses and ground truth 3D point clouds. The dataset contains a total of 1.5 million frames from nearly 19,000 videos capturing objects from 50 MS-COCO categories and, as such, it is significantly larger than alternatives both in terms of the number of categories and objects. We exploit this new dataset to conduct one of the first large-scale ""in-the-wild"" evaluations of several new-view-synthesis and category-centric 3D reconstruction methods. Finally, we contribute NerFormer - a novel neural rendering method that leverages the powerful Transformer to reconstruct an object given a small number of its views. The CO3D dataset is available at https://github.com/facebookresearch/co3d .",,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnufkbbJ8jAIcaKRywTg6DcWdp7zDAmSs67GEBVQik8_QgjvvodCT-YaUExEPOYnUy3s
9/18/2021 8:55:15,NerfingMVS: Guided Optimization of Neural Radiance Fields for Indoor Multi-view Stereo,NerfingMVS,9/2/2021,https://arxiv.org/pdf/2109.01129.pdf,https://weiyithu.github.io/NerfingMVS/,https://github.com/weiyithu/NerfingMVS,https://www.youtube.com/watch?v=i-b5lPnYipA,,,,"@article{weiOralnerfingmvs,
  ID = {weiOralnerfingmvs},
  ENTRYTYPE = {article},
  AUTHOR = {Yi Wei and Shaohui Liu and Yongming Rao and Wang Zhao and Jiwen Lu and Jie Zhou},
  TITLE = {NerfingMVS: Guided Optimization of Neural Radiance Fields for Indoor Multi-view Stereo},
  EPRINT = {2109.01129v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {In this work, we present a new multi-view depth estimation method that utilizes both conventional SfM reconstruction and learning-based priors over the recently proposed neural radiance fields (NeRF). Unlike existing neural network based optimization method that relies on estimated correspondences, our method directly optimizes over implicit volumes, eliminating the challenging step of matching pixels in indoor scenes. The key to our approach is to utilize the learning-based priors to guide the optimization process of NeRF. Our system firstly adapts a monocular depth network over the target scene by finetuning on its sparse SfM reconstruction. Then, we show that the shape-radiance ambiguity of NeRF still exists in indoor environments and propose to address the issue by employing the adapted depth priors to monitor the sampling process of volume rendering. Finally, a per-pixel confidence map acquired by error computation on the rendered image can be used to further improve the depth quality. Experiments show that our proposed framework significantly outperforms state-of-the-art methods on indoor scenes, with surprising findings presented on the effectiveness of correspondence-based optimization and NeRF-based optimization over the adapted depth priors. In addition, we show that the guided optimization scheme does not sacrifice the original synthesis capability of neural radiance fields, improving the rendering quality on both seen and novel views. Code is available at https://github.com/weiyithu/NerfingMVS.},
  YEAR = {2021},
  MONTH = {Sep},
  URL = {http://arxiv.org/abs/2109.01129v2},
  FILE = {2109.01129v2.pdf}
 }",,"Lifting 2D features to 3D, Sampling, Data-driven",NeRF,Density,,,,,,,,ICCV 2021 Oral,https://drive.google.com/drive/folders/1X_w57Q_MIFlI3lzhRt7Z8C5X9tNS8cg-,No,Direct,"Yi Wei, Shaohui Liu, Yongming Rao, Wang Zhao, Jiwen Lu, Jie Zhou",weioralnerfingmvs,00000218,"In this work, we present a new multi-view depth estimation method that utilizes both conventional SfM reconstruction and learning-based priors over the recently proposed neural radiance fields (NeRF). Unlike existing neural network based optimization method that relies on estimated correspondences, our method directly optimizes over implicit volumes, eliminating the challenging step of matching pixels in indoor scenes. The key to our approach is to utilize the learning-based priors to guide the optimization process of NeRF. Our system firstly adapts a monocular depth network over the target scene by finetuning on its sparse SfM reconstruction. Then, we show that the shape-radiance ambiguity of NeRF still exists in indoor environments and propose to address the issue by employing the adapted depth priors to monitor the sampling process of volume rendering. Finally, a per-pixel confidence map acquired by error computation on the rendered image can be used to further improve the depth quality. Experiments show that our proposed framework significantly outperforms state-of-the-art methods on indoor scenes, with surprising findings presented on the effectiveness of correspondence-based optimization and NeRF-based optimization over the adapted depth priors. In addition, we show that the guided optimization scheme does not sacrifice the original synthesis capability of neural radiance fields, improving the rendering quality on both seen and novel views. Code is available at https://github.com/weiyithu/NerfingMVS.",,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnuee3diLaZ747OhpmsjBJOTLznSphf_bFmIeCcsN2z5VsBQ3BqpLRT-G-9a4MCkvdp0
9/18/2021 10:03:21,IntraTomo: Self-supervised Learning-based Tomography via Sinogram Synthesis and Prediction,IntraTomo,9/2/2021,https://vccimaging.org/Publications/Zang2021IntraTomo/Zang2021IntraTomo.pdf,https://vccimaging.org/Publications/Zang2021IntraTomo/,https://github.com/gmzang/IntraTomo,,https://vccimaging.org/Publications/Zang2021IntraTomo/Zang2021IntraTomo-supp.pdf,,,"@inproceedings{zang2021intratomo,
  BOOKTITLE = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
  YEAR = {2021},
  ID = {zang2021intratomo},
  ENTRYTYPE = {inproceedings},
  ABSTRACT = {We propose IntraTomo, a powerful framework that combines the benefits of learning-based and model-based approaches for solving highly ill-posed inverse problems, in the Computed Tomography (CT) context. IntraTomo is composed of two core modules: a novel sinogram prediction module and a geometry refinement module, which are applied iteratively. In the first module, the unknown density field is represented as a continuous and differentiable function, parameterized by a deep neural network. This network is learned, in},
  AUTHOR = {Guangming Zang and Ramzi Idoughi and Rui Li and Peter Wonka and Wolfgang Heidrich},
  PUB_YEAR = {2021},
  PUBLISHER = {IEEE},
  TITLE = {IntraTomo: Self-supervised Learning-based Tomography via Sinogram Synthesis and Prediction},
  VENUE = {NA}
 }","Beyond graphics, Alternative imaging, Science and engineering",,NeRF,,,,,,,,,ICCV 2021,https://github.com/gmzang/IntraTomo,,Direct,"Guangming Zang, Ramzi Idoughi, Rui Li, Peter Wonka, Wolfgang Heidrich",zang2021intratomo,00000222,"We propose IntraTomo, a powerful framework that combines the benefits of learning-based and model-based approaches for solving highly ill-posed inverse problems, in the Computed Tomography (CT) context. IntraTomo is composed of two core modules: a novel sinogram prediction module and a geometry refinement module, which are applied iteratively. In the first module, the unknown density field is represented as a continuous and differentiable function, parameterized by a deep neural network. This network is learned, in",,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnueuWp61dpYUOM_4iY7WqfLVfTzYYnFAzYwD6bW45HQd5vaYxlIdiL6p7W84DP3DfPE
9/17/2021 22:57:44,CodeNeRF: Disentangled Neural Radiance Fields for Object Categories,CodeNeRF,9/3/2021,https://arxiv.org/pdf/2109.01750.pdf,https://sites.google.com/view/wbjang/home/codenerf,https://github.com/wayne1123/code-nerf,https://user-images.githubusercontent.com/32883157/130004248-0ff74d4e-993e-43f2-91ee-bd25776e65bc.mp4,,,,"@inproceedings{jang2021codenerf,
  BOOKTITLE = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
  ID = {jang2021codenerf},
  ENTRYTYPE = {inproceedings},
  AUTHOR = {Wonbong Jang and Lourdes Agapito},
  TITLE = {CodeNeRF: Disentangled Neural Radiance Fields for Object Categories},
  EPRINT = {2109.01750v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.GR},
  ABSTRACT = {CodeNeRF is an implicit 3D neural representation that learns the variation of object shapes and textures across a category and can be trained, from a set of posed images, to synthesize novel views of unseen objects. Unlike the original NeRF, which is scene specific, CodeNeRF learns to disentangle shape and texture by learning separate embeddings. At test time, given a single unposed image of an unseen object, CodeNeRF jointly estimates camera viewpoint, and shape and appearance codes via optimization. Unseen objects can be reconstructed from a single image, and then rendered from new viewpoints or their shape and texture edited by varying the latent codes. We conduct experiments on the SRN benchmark, which show that CodeNeRF generalises well to unseen objects and achieves on-par performance with methods that require known camera pose at test time. Our results on real-world images demonstrate that CodeNeRF can bridge the sim-to-real gap. Project page: \url{https://github.com/wayne1123/code-nerf}},
  YEAR = {2021},
  URL = {http://arxiv.org/abs/2109.01750v1},
  FILE = {2109.01750v1.pdf}
 }","Generalization, Editable","Conditional neural field, Per-instance fine-tuning, Data-driven",NeRF,Density,Category-level,,,,,,,ICCV 2021,,No,Direct,"Wonbong Jang, Lourdes Agapito",jang2021codenerf,00000214,"CodeNeRF is an implicit 3D neural representation that learns the variation of object shapes and textures across a category and can be trained, from a set of posed images, to synthesize novel views of unseen objects. Unlike the original NeRF, which is scene specific, CodeNeRF learns to disentangle shape and texture by learning separate embeddings. At test time, given a single unposed image of an unseen object, CodeNeRF jointly estimates camera viewpoint, and shape and appearance codes via optimization. Unseen objects can be reconstructed from a single image, and then rendered from new viewpoints or their shape and texture edited by varying the latent codes. We conduct experiments on the SRN benchmark, which show that CodeNeRF generalises well to unseen objects and achieves on-par performance with methods that require known camera pose at test time. Our results on real-world images demonstrate that CodeNeRF can bridge the sim-to-real gap. Project page: \url{https://github.com/wayne1123/code-nerf}",,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnueJ4x3F4fHychMCKn5hQk3ycOjLMx1wOSbB8mTNVUZx0ZaYqjWaxbqOgACqV_dj8vA
9/17/2021 21:31:17,Learning Object-Compositional Neural Radiance Field for Editable Scene Rendering,,9/4/2021,https://arxiv.org/pdf/2109.01847.pdf,https://zju3dv.github.io/object_nerf/,https://github.com/zju3dv/object_nerf,https://www.youtube.com/watch?v=VTEROu-Yz04,http://www.cad.zju.edu.cn/home/gfzhang/papers/object_nerf/object_nerf_supp.pdf,,,"@inproceedings{yang2021learning,
  BOOKTITLE = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
  ID = {yang2021learning},
  ENTRYTYPE = {inproceedings},
  AUTHOR = {Bangbang Yang and Yinda Zhang and Yinghao Xu and Yijin Li and Han Zhou and Hujun Bao and Guofeng Zhang and Zhaopeng Cui},
  TITLE = {Learning Object-Compositional Neural Radiance Field for Editable Scene Rendering},
  EPRINT = {2109.01847v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Implicit neural rendering techniques have shown promising results for novel view synthesis. However, existing methods usually encode the entire scene as a whole, which is generally not aware of the object identity and limits the ability to the high-level editing tasks such as moving or adding furniture. In this paper, we present a novel neural scene rendering system, which learns an object-compositional neural radiance field and produces realistic rendering with editing capability for a clustered and real-world scene. Specifically, we design a novel two-pathway architecture, in which the scene branch encodes the scene geometry and appearance, and the object branch encodes each standalone object conditioned on learnable object activation codes. To survive the training in heavily cluttered scenes, we propose a scene-guided training strategy to solve the 3D space ambiguity in the occluded regions and learn sharp boundaries for each object. Extensive experiments demonstrate that our system not only achieves competitive performance for static scene novel-view synthesis, but also produces realistic rendering for object-level editing.},
  YEAR = {2021},
  URL = {http://arxiv.org/abs/2109.01847v1},
  FILE = {2109.01847v1.pdf}
 }",Editable,"Conditional neural field, Voxelization, Volume partitioning, Object-centric representation, Segmentation",NeRF,Density,,,,,,,,ICCV 2021,,No,Direct,"Bangbang Yang, Yinda Zhang, Yinghao Xu, Yijin Li, Han Zhou, Hujun Bao, Guofeng Zhang, Zhaopeng Cui",yang2021learning,00000212,"Implicit neural rendering techniques have shown promising results for novel view synthesis. However, existing methods usually encode the entire scene as a whole, which is generally not aware of the object identity and limits the ability to the high-level editing tasks such as moving or adding furniture. In this paper, we present a novel neural scene rendering system, which learns an object-compositional neural radiance field and produces realistic rendering with editing capability for a clustered and real-world scene. Specifically, we design a novel two-pathway architecture, in which the scene branch encodes the scene geometry and appearance, and the object branch encodes each standalone object conditioned on learnable object activation codes. To survive the training in heavily cluttered scenes, we propose a scene-guided training strategy to solve the 3D space ambiguity in the occluded regions and learn sharp boundaries for each object. Extensive experiments demonstrate that our system not only achieves competitive performance for static scene novel-view synthesis, but also produces realistic rendering for object-level editing.",,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnudtZ0By4WU9QOx2hDFVJC7jg4FTcOXqkBFO2WWieWjT2YW-IgYoNA2k54FUkp5yg9Y
9/17/2021 17:49:15,Stochastic Neural Radiance Fields: Quantifying Uncertainty in Implicit 3D Representations,S-NeRF,9/5/2021,https://arxiv.org/pdf/2109.02123.pdf,,,,,,,"@article{shen2021snerf,
  JOURNAL = {arXiv preprint arXiv:2109.02123},
  BOOKTITLE = {ArXiv Pre-print},
  ID = {shen2021snerf},
  ENTRYTYPE = {article},
  AUTHOR = {Jianxiong Shen and Adria Ruiz and Antonio Agudo and Francesc Moreno-Noguer},
  TITLE = {Stochastic Neural Radiance Fields: Quantifying Uncertainty in Implicit 3D Representations},
  EPRINT = {2109.02123v3},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Neural Radiance Fields (NeRF) has become a popular framework for learning implicit 3D representations and addressing different tasks such as novel-view synthesis or depth-map estimation. However, in downstream applications where decisions need to be made based on automatic predictions, it is critical to leverage the confidence associated with the model estimations. Whereas uncertainty quantification is a long-standing problem in Machine Learning, it has been largely overlooked in the recent NeRF literature. In this context, we propose Stochastic Neural Radiance Fields (S-NeRF), a generalization of standard NeRF that learns a probability distribution over all the possible radiance fields modeling the scene. This distribution allows to quantify the uncertainty associated with the scene information provided by the model. S-NeRF optimization is posed as a Bayesian learning problem which is efficiently addressed using the Variational Inference framework. Exhaustive experiments over benchmark datasets demonstrate that S-NeRF is able to provide more reliable predictions and confidence values than generic approaches previously proposed for uncertainty estimation in other domains.},
  YEAR = {2021},
  URL = {http://arxiv.org/abs/2109.02123v3},
  FILE = {2109.02123v3.pdf}
 }",Fundamentals,,NeRF,Density,Per-scene,,,,,,,ARXIV 2021,,No,Direct,"Jianxiong Shen, Adria Ruiz, Antonio Agudo, Francesc Moreno",shen2021snerf,00000209,"Neural Radiance Fields (NeRF) has become a popular framework for learning implicit 3D representations and addressing different tasks such as novel-view synthesis or depth-map estimation. However, in downstream applications where decisions need to be made based on automatic predictions, it is critical to leverage the confidence associated with the model estimations. Whereas uncertainty quantification is a long-standing problem in Machine Learning, it has been largely overlooked in the recent NeRF literature. In this context, we propose Stochastic Neural Radiance Fields (S-NeRF), a generalization of standard NeRF that learns a probability distribution over all the possible radiance fields modeling the scene. This distribution allows to quantify the uncertainty associated with the scene information provided by the model. S-NeRF optimization is posed as a Bayesian learning problem which is efficiently addressed using the Variational Inference framework. Exhaustive experiments over benchmark datasets demonstrate that S-NeRF is able to provide more reliable predictions and confidence values than generic approaches previously proposed for uncertainty estimation in other domains.",,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnucZ0vrwUrcA_Pd6srEDC0BcOZLZC9UhZbZv6QdhgyJS48kiVOhf_FbOdscAQt8feF0
9/17/2021 18:10:41,NEAT: Neural Attention Fields for End-to-End Autonomous Driving,NEAT,9/9/2021,https://arxiv.org/pdf/2109.04456.pdf,,https://github.com/autonomousvision/neat,https://www.youtube.com/watch?v=gtO-ghjKkRs,http://www.cvlibs.net/publications/Chitta2021ICCV_supplementary.pdf,,,"@inproceedings{chitta2021neat,
  BOOKTITLE = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
  ID = {chitta2021neat},
  ENTRYTYPE = {inproceedings},
  AUTHOR = {Kashyap Chitta and Aditya Prakash and Andreas Geiger},
  TITLE = {NEAT: Neural Attention Fields for End-to-End Autonomous Driving},
  EPRINT = {2109.04456v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Efficient reasoning about the semantic, spatial, and temporal structure of a scene is a crucial prerequisite for autonomous driving. We present NEural ATtention fields (NEAT), a novel representation that enables such reasoning for end-to-end imitation learning models. NEAT is a continuous function which maps locations in Bird's Eye View (BEV) scene coordinates to waypoints and semantics, using intermediate attention maps to iteratively compress high-dimensional 2D image features into a compact representation. This allows our model to selectively attend to relevant regions in the input while ignoring information irrelevant to the driving task, effectively associating the images with the BEV representation. In a new evaluation setting involving adverse environmental conditions and challenging scenarios, NEAT outperforms several strong baselines and achieves driving scores on par with the privileged CARLA expert used to generate its training data. Furthermore, visualizing the attention maps for models with NEAT intermediate representations provides improved interpretability.},
  YEAR = {2021},
  URL = {http://arxiv.org/abs/2109.04456v1},
  FILE = {2109.04456v1.pdf}
 }","Dynamic, Beyond graphics, Science and engineering",Data-driven,,,,,,,,,,ICCV 2021,,,Direct,"Kashyap Chitta, Aditya Prakash, Andreas Geiger",chitta2021neat,00000210,"Efficient reasoning about the semantic, spatial, and temporal structure of a scene is a crucial prerequisite for autonomous driving. We present NEural ATtention fields (NEAT), a novel representation that enables such reasoning for end-to-end imitation learning models. NEAT is a continuous function which maps locations in Bird's Eye View (BEV) scene coordinates to waypoints and semantics, using intermediate attention maps to iteratively compress high-dimensional 2D image features into a compact representation. This allows our model to selectively attend to relevant regions in the input while ignoring information irrelevant to the driving task, effectively associating the images with the BEV representation. In a new evaluation setting involving adverse environmental conditions and challenging scenarios, NEAT outperforms several strong baselines and achieves driving scores on par with the privileged CARLA expert used to generate its training data. Furthermore, visualizing the attention maps for models with NEAT intermediate representations provides improved interpretability.",,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnueApOF85N3QRL_z9exTWINC9uAnW2ilZaiKe_U2H8bBzN5yrp77n3GBRA2Fd4tgzSM
9/18/2021 8:37:18,Multiresolution Deep Implicit Functions for 3D Shape Representation,MDIF,9/12/2021,https://arxiv.org/pdf/2109.05591.pdf,,,,,,,"@inproceedings{chen2021mdif,
  BOOKTITLE = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
  ID = {chen2021mdif},
  ENTRYTYPE = {inproceedings},
  AUTHOR = {Zhang Chen and Yinda Zhang and Kyle Genova and Sean Fanello and Sofien Bouaziz and Christian Haene and Ruofei Du and Cem Keskin and Thomas Funkhouser and Danhang Tang},
  TITLE = {Multiresolution Deep Implicit Functions for 3D Shape Representation},
  EPRINT = {2109.05591v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We introduce Multiresolution Deep Implicit Functions (MDIF), a hierarchical representation that can recover fine geometry detail, while being able to perform global operations such as shape completion. Our model represents a complex 3D shape with a hierarchy of latent grids, which can be decoded into different levels of detail and also achieve better accuracy. For shape completion, we propose latent grid dropout to simulate partial data in the latent space and therefore defer the completing functionality to the decoder side. This along with our multires design significantly improves the shape completion quality under decoder-only latent optimization. To the best of our knowledge, MDIF is the first deep implicit function model that can at the same time (1) represent different levels of detail and allow progressive decoding; (2) support both encoder-decoder inference and decoder-only latent optimization, and fulfill multiple applications; (3) perform detailed decoder-only shape completion. Experiments demonstrate its superior performance against prior art in various 3D reconstruction tasks.},
  YEAR = {2021},
  URL = {http://arxiv.org/abs/2109.05591v2},
  FILE = {2109.05591v2.pdf}
 }",Surface reconstruction,"Coarse-to-fine, Learning residual, Voxelization, Feature volume",,,,,,,,,,ICCV 2021,,Yes,,"Zhang Chen, Yinda Zhang, Kyle Genova, Sean Fanello, Sofien Bouaziz, Christian Haene, Ruofei Du, Cem Keskin, Thomas Funkhouser, Danhang Tang",chen2021mdif,00000215,"We introduce Multiresolution Deep Implicit Functions (MDIF), a hierarchical representation that can recover fine geometry detail, while being able to perform global operations such as shape completion. Our model represents a complex 3D shape with a hierarchy of latent grids, which can be decoded into different levels of detail and also achieve better accuracy. For shape completion, we propose latent grid dropout to simulate partial data in the latent space and therefore defer the completing functionality to the decoder side. This along with our multires design significantly improves the shape completion quality under decoder-only latent optimization. To the best of our knowledge, MDIF is the first deep implicit function model that can at the same time (1) represent different levels of detail and allow progressive decoding; (2) support both encoder-decoder inference and decoder-only latent optimization, and fulfill multiple applications; (3) perform detailed decoder-only shape completion. Experiments demonstrate its superior performance against prior art in various 3D reconstruction tasks.",,Yes,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnudSNXqiA7JXUB_cYa3iQJej9WTQ-5Oa-7atmVmQuCJ1I6512LywVCwuLyCYAn277Dc
9/18/2021 8:42:31,Pose with Style: Detail-Preserving Pose-Guided Image Synthesis with Conditional StyleGAN,Pose with Style,9/13/2021,https://arxiv.org/pdf/2109.06166.pdf,https://pose-with-style.github.io/,Coming soon,,,https://www.youtube.com/watch?v=d_ETeAVLilw,,"@article{albahar2021posewithstyle,
  PUBLISHER = {Association for Computing Machinery},
  JOURNAL = {ACM Transactions on Graphics (TOG)},
  ID = {albahar2021posewithstyle},
  ENTRYTYPE = {article},
  AUTHOR = {Badour AlBahar and Jingwan Lu and Jimei Yang and Zhixin Shu and Eli Shechtman and Jia-Bin Huang},
  TITLE = {Pose with Style: Detail-Preserving Pose-Guided Image Synthesis with Conditional StyleGAN},
  EPRINT = {2109.06166v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We present an algorithm for re-rendering a person from a single image under arbitrary poses. Existing methods often have difficulties in hallucinating occluded contents photo-realistically while preserving the identity and fine details in the source image. We first learn to inpaint the correspondence field between the body surface texture and the source image with a human body symmetry prior. The inpainted correspondence field allows us to transfer/warp local features extracted from the source to the target view even under large pose changes. Directly mapping the warped local features to an RGB image using a simple CNN decoder often leads to visible artifacts. Thus, we extend the StyleGAN generator so that it takes pose as input (for controlling poses) and introduces a spatially varying modulation for the latent space using the warped local features (for controlling appearances). We show that our method compares favorably against the state-of-the-art algorithms in both quantitative evaluation and visual comparison.},
  YEAR = {2021},
  URL = {http://arxiv.org/abs/2109.06166v1},
  FILE = {2109.06166v1.pdf}
 }","Human body, Image",Generative/adversarial formulation,,,Category-level,,,,,,,SIGGRAPH 2021,,,Indirect,"Badour AlBahar, Jingwan Lu, Jimei Yang, Zhixin Shu, Eli Shechtman, Jia-Bin Huang",albahar2021posewithstyle,00000216,"We present an algorithm for re-rendering a person from a single image under arbitrary poses. Existing methods often have difficulties in hallucinating occluded contents photo-realistically while preserving the identity and fine details in the source image. We first learn to inpaint the correspondence field between the body surface texture and the source image with a human body symmetry prior. The inpainted correspondence field allows us to transfer/warp local features extracted from the source to the target view even under large pose changes. Directly mapping the warped local features to an RGB image using a simple CNN decoder often leads to visible artifacts. Thus, we extend the StyleGAN generator so that it takes pose as input (for controlling poses) and introduces a spatially varying modulation for the latent space using the warped local features (for controlling appearances). We show that our method compares favorably against the state-of-the-art algorithms in both quantitative evaluation and visual comparison.",,,Yes,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnufKNnfS6KzEtQba-fc7Vxa39u2iLsgvjt0vqpDZE-XH1_xugWTdm47hOrHe8mP9rHY
9/26/2021 16:35:20,Neural Human Performer: Learning Generalizable Radiance Fields for Human Performance Rendering,Neural Human Performer,9/15/2021,https://arxiv.org/pdf/2109.07448.pdf,https://youngjoongunc.github.io/nhp/,https://github.com/YoungJoongUNC/Neural_Human_Performer,https://www.youtube.com/watch?v=4b5SPwPOKVo,,,,"@inproceedings{kwon2021neuralhumanperformer,
  PUBLISHER = {Curran Associates, Inc.},
  BOOKTITLE = {Advances in Neural Information Processing Systems (NeurIPS)},
  ID = {kwon2021neuralhumanperformer},
  ENTRYTYPE = {inproceedings},
  AUTHOR = {Youngjoong Kwon and Dahun Kim and Duygu Ceylan and Henry Fuchs},
  TITLE = {Neural Human Performer: Learning Generalizable Radiance Fields for Human Performance Rendering},
  EPRINT = {2109.07448v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {In this paper, we aim at synthesizing a free-viewpoint video of an arbitrary human performance using sparse multi-view cameras. Recently, several works have addressed this problem by learning person-specific neural radiance fields (NeRF) to capture the appearance of a particular human. In parallel, some work proposed to use pixel-aligned features to generalize radiance fields to arbitrary new scenes and objects. Adopting such generalization approaches to humans, however, is highly challenging due to the heavy occlusions and dynamic articulations of body parts. To tackle this, we propose Neural Human Performer, a novel approach that learns generalizable neural radiance fields based on a parametric human body model for robust performance capture. Specifically, we first introduce a temporal transformer that aggregates tracked visual features based on the skeletal body motion over time. Moreover, a multi-view transformer is proposed to perform cross-attention between the temporally-fused features and the pixel-aligned features at each time step to integrate observations on the fly from multiple views. Experiments on the ZJU-MoCap and AIST datasets show that our method significantly outperforms recent generalizable NeRF methods on unseen identities and poses. The video results and code are available at https://youngjoongunc.github.io/nhp.},
  YEAR = {2021},
  URL = {http://arxiv.org/abs/2109.07448v1},
  FILE = {2109.07448v1.pdf}
 }","Dynamic, Human body, Generalization","Lifting 2D features to 3D, Transformer",,,,,,,,,,NeurIPS 2021 (Spotlight),https://github.com/YoungJoongUNC/Neural_Human_Performer,,,"Youngjoong Kwon, Dahun Kim, Duygu Ceylan, Henry Fuchs",kwon2021neuralhumanperformer,00000220,"In this paper, we aim at synthesizing a free-viewpoint video of an arbitrary human performance using sparse multi-view cameras. Recently, several works have addressed this problem by learning person-specific neural radiance fields (NeRF) to capture the appearance of a particular human. In parallel, some work proposed to use pixel-aligned features to generalize radiance fields to arbitrary new scenes and objects. Adopting such generalization approaches to humans, however, is highly challenging due to the heavy occlusions and dynamic articulations of body parts. To tackle this, we propose Neural Human Performer, a novel approach that learns generalizable neural radiance fields based on a parametric human body model for robust performance capture. Specifically, we first introduce a temporal transformer that aggregates tracked visual features based on the skeletal body motion over time. Moreover, a multi-view transformer is proposed to perform cross-attention between the temporally-fused features and the pixel-aligned features at each time step to integrate observations on the fly from multiple views. Experiments on the ZJU-MoCap and AIST datasets show that our method significantly outperforms recent generalizable NeRF methods on unseen identities and poses. The video results and code are available at https://youngjoongunc.github.io/nhp.",,Yes,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnudCnivkGC_rvx4Q-3w3-ZlVfmGC39wXFzDnO0K0YDQe-inuUQoxHAIwyna5b6LEmsc
9/26/2021 16:32:23,"ObjectFolder: A Dataset of Objects with Implicit Visual, Auditory, and Tactile Representations",ObjectFolder,9/16/2021,https://arxiv.org/pdf/2109.07991.pdf,https://ai.stanford.edu/~rhgao/objectfolder/,Coming soon,https://www.youtube.com/watch?v=wQ4o8XeS-X0,https://ai.stanford.edu/~rhgao/objectfolder/ObjectFolder_Supp.pdf,,,"@inproceedings{gao2021objectfolder,
  BOOKTITLE = {Proceedings of the Conference on Robot Learning (CoRL)},
  ID = {gao2021objectfolder},
  ENTRYTYPE = {inproceedings},
  AUTHOR = {Ruohan Gao and Yen-Yu Chang and Shivani Mall and Li Fei-Fei and Jiajun Wu},
  TITLE = {ObjectFolder: A Dataset of Objects with Implicit Visual, Auditory, and Tactile Representations},
  EPRINT = {2109.07991v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.RO},
  ABSTRACT = {Multisensory object-centric perception, reasoning, and interaction have been a key research topic in recent years. However, the progress in these directions is limited by the small set of objects available -- synthetic objects are not realistic enough and are mostly centered around geometry, while real object datasets such as YCB are often practically challenging and unstable to acquire due to international shipping, inventory, and financial cost. We present ObjectFolder, a dataset of 100 virtualized objects that addresses both challenges with two key innovations. First, ObjectFolder encodes the visual, auditory, and tactile sensory data for all objects, enabling a number of multisensory object recognition tasks, beyond existing datasets that focus purely on object geometry. Second, ObjectFolder employs a uniform, object-centric, and implicit representation for each object's visual textures, acoustic simulations, and tactile readings, making the dataset flexible to use and easy to share. We demonstrate the usefulness of our dataset as a testbed for multisensory perception and control by evaluating it on a variety of benchmark tasks, including instance recognition, cross-sensory retrieval, 3D reconstruction, and robotic grasping.},
  YEAR = {2021},
  URL = {http://arxiv.org/abs/2109.07991v2},
  FILE = {2109.07991v2.pdf}
 }","Compression, Beyond graphics, Science and engineering, Robotics, Audio",,,,,,,,,,,CoRL 2021,Coming soon,No,,"Ruohan Gao, Yen-Yu Chang, Shivani Mall, Li Fei-Fei, Jiajun Wu",gao2021objectfolder,00000219,"Multisensory object-centric perception, reasoning, and interaction have been a key research topic in recent years. However, the progress in these directions is limited by the small set of objects available -- synthetic objects are not realistic enough and are mostly centered around geometry, while real object datasets such as YCB are often practically challenging and unstable to acquire due to international shipping, inventory, and financial cost. We present ObjectFolder, a dataset of 100 virtualized objects that addresses both challenges with two key innovations. First, ObjectFolder encodes the visual, auditory, and tactile sensory data for all objects, enabling a number of multisensory object recognition tasks, beyond existing datasets that focus purely on object geometry. Second, ObjectFolder employs a uniform, object-centric, and implicit representation for each object's visual textures, acoustic simulations, and tactile readings, making the dataset flexible to use and easy to share. We demonstrate the usefulness of our dataset as a testbed for multisensory perception and control by evaluating it on a variety of benchmark tasks, including instance recognition, cross-sensory retrieval, 3D reconstruction, and robotic grasping.",,,No,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnuc2WbU-aLYACHIt0A6L6vmHr61sSuBEOpz-tJTy3ACCp4-bFsVEdxcgoojav5fmtuE
9/18/2021 8:47:01,Neural Cameras: Learning Camera Characteristics for Coherent Mixed Reality Rendering,Neural Cameras,9/18/2021,https://www.hci.otago.ac.nz/papers/MandlIEEEISMAR2021.pdf,,,,,,,"@article{mandl2021neuralcameras,
  ID = {mandl2021neuralcameras},
  ENTRYTYPE = {article},
  ABSTRACT = {Coherent rendering is important for generating plausible Mixed Reality presentations of virtual objects within a user's real-world environment. Besides photo-realistic rendering and correct lighting, visual coherence requires simulating the imaging system that is used to capture the real environment. While existing approaches either focus on a specific camera or a specific component of the imaging system, we introduce Neural Cameras, the first approach that jointly simulates all major components of an arbitrary modern camera using},
  AUTHOR = {David Mandl and Peter Mohr and Tobias Langlotz and Christoph Ebner and Shohei Mori and Stefanie Zollmann and Peter M Roth and Denis Kalkofen},
  YEAR = {2021},
  TITLE = {Neural Cameras: Learning Camera Characteristics for Coherent Mixed Reality Rendering},
  BOOKTITLE = {IEEE Symp. on Mixed and Augmented Reality (ISMAR)},
  MONTH = {Oct}
 }","Camera parameter estimation, Beyond graphics",,,,,,,,,,,ISMAR 2021,,,,"David Mandl, Peter Mohr, Tobias Langlotz, Christoph Ebner, Shohei Mori, Stefanie Zollmann, Peter M Roth, Denis Kalkofen",mandl2021neuralcameras,00000217,"Coherent rendering is important for generating plausible Mixed Reality presentations of virtual objects within a user's real-world environment. Besides photo-realistic rendering and correct lighting, visual coherence requires simulating the imaging system that is used to capture the real environment. While existing approaches either focus on a specific camera or a specific component of the imaging system, we introduce Neural Cameras, the first approach that jointly simulates all major components of an arbitrary modern camera using",,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnueW5ORQ_TGinn-7URFGjneV88kI53FkZOl0MURf3K-oDErzpSLFdZmy9njV25Fr8OU
9/27/2021 21:52:06,Implicit Neural Distance Representation for Unsupervised and Supervised Classification of Complex Anatomies,,9/21/2021,https://link.springer.com/content/pdf/10.1007%2F978-3-030-87196-3.pdf,,https://github.com/kristineaajuhl/Implicit-Neural-Distance-Representation-of-Complex-Anatomie,,,,,"@inproceedings{juhl2021implicit,
  ID = {juhl2021implicit},
  ENTRYTYPE = {inproceedings},
  ABSTRACT = {The task of 3D shape classification is closely related to finding a good representation of the shapes. In this study, we focus on surface representations of complex anatomies and on how such representations can be utilized for super-and unsupervised classification. We present a novel Implicit Neural Distance Representation based on unsigned distance fields (UDFs). The UDFs can be embedded into a low-dimensional latent space, which is optimized using only the shape itself. We demonstrate that this self-optimized latent space},
  AUTHOR = {Kristine Aavild Juhl and Xabier Morales and Ole de Backer and Oscar Camara and Rasmus Reinhold Paulsen},
  BOOKTITLE = {International Conference on Medical Image Computing and Computer-Assisted Intervention},
  ORGANIZATION = {Springer},
  PAGES = {405--415},
  PUB_YEAR = {2021},
  TITLE = {Implicit Neural Distance Representation for Unsupervised and Supervised Classification of Complex Anatomies},
  VENUE = {... Conference on Medical ...}
 }","Human body, Human head, Beyond graphics, Science and engineering, Classification",Conditional neural field,,UDF,,,,,,,,MICCAI 2021,,Yes,,"Kristine Aavild Juhl, Xabier Morales, Oscar Camara, Ole de Backer and Rasmus Reinhold Paulsen",juhl2021implicit,00000221,"The task of 3D shape classification is closely related to finding a good representation of the shapes. In this study, we focus on surface representations of complex anatomies and on how such representations can be utilized for super-and unsupervised classification. We present a novel Implicit Neural Distance Representation based on unsigned distance fields (UDFs). The UDFs can be embedded into a low-dimensional latent space, which is optimized using only the shape itself. We demonstrate that this self-optimized latent space",,,No,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnucchUyft5YvRzPsyHZ5ImxD5JFr44FJRzoyVN3AWEvJNRCLtESAt_q3zYDHdawSuh8
9/27/2021 21:57:47,A Skeleton-Driven Neural Occupancy Representation for Articulated Hands,HALO,9/23/2021,https://arxiv.org/pdf/2109.11399.pdf,,,,,,,"@article{karunratanakul2021halo,
  JOURNAL = {arXiv preprint arXiv:2109.11399},
  BOOKTITLE = {ArXiv Pre-print},
  ID = {karunratanakul2021halo},
  ENTRYTYPE = {article},
  AUTHOR = {Korrawe Karunratanakul and Adrian Spurr and Zicong Fan and Otmar Hilliges and Siyu Tang},
  TITLE = {A Skeleton-Driven Neural Occupancy Representation for Articulated Hands},
  EPRINT = {2109.11399v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We present Hand ArticuLated Occupancy (HALO), a novel representation of articulated hands that bridges the advantages of 3D keypoints and neural implicit surfaces and can be used in end-to-end trainable architectures. Unlike existing statistical parametric hand models (e.g.~MANO), HALO directly leverages 3D joint skeleton as input and produces a neural occupancy volume representing the posed hand surface. The key benefits of HALO are (1) it is driven by 3D key points, which have benefits in terms of accuracy and are easier to learn for neural networks than the latent hand-model parameters; (2) it provides a differentiable volumetric occupancy representation of the posed hand; (3) it can be trained end-to-end, allowing the formulation of losses on the hand surface that benefit the learning of 3D keypoints. We demonstrate the applicability of HALO to the task of conditional generation of hands that grasp 3D objects. The differentiable nature of HALO is shown to improve the quality of the synthesized hands both in terms of physical plausibility and user preference.},
  YEAR = {2021},
  URL = {http://arxiv.org/abs/2109.11399v1},
  FILE = {2109.11399v1.pdf}
 }",Human hand,"Conditional neural field, Articulated, Warping field/Flow field",,Occupancy,Category-level,,,,,,,ARXIV 2021,,Yes,,"Korrawe Karunratanakul, Adrian Spurr, Zicong Fan, Otmar Hilliges, Siyu Tang",karunratanakul2021halo,00000222,"We present Hand ArticuLated Occupancy (HALO), a novel representation of articulated hands that bridges the advantages of 3D keypoints and neural implicit surfaces and can be used in end-to-end trainable architectures. Unlike existing statistical parametric hand models (e.g.~MANO), HALO directly leverages 3D joint skeleton as input and produces a neural occupancy volume representing the posed hand surface. The key benefits of HALO are (1) it is driven by 3D key points, which have benefits in terms of accuracy and are easier to learn for neural networks than the latent hand-model parameters; (2) it provides a differentiable volumetric occupancy representation of the posed hand; (3) it can be trained end-to-end, allowing the formulation of losses on the hand surface that benefit the learning of 3D keypoints. We demonstrate the applicability of HALO to the task of conditional generation of hands that grasp 3D objects. The differentiable nature of HALO is shown to improve the quality of the synthesized hands both in terms of physical plausibility and user preference.",,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnucJUsAvepKOsi1dgre3Gisbt9As03wAR3J4i85rlDOwueaICbi5x7K8JHmonkD3ZO8
9/27/2021 22:03:40,Layered Neural Atlases for Consistent Video Editing,,9/23/2021,https://arxiv.org/pdf/2109.11418.pdf,https://layered-neural-atlases.github.io/,Coming soon,https://www.youtube.com/watch?v=aQhakPFC4oQ,https://layered-neural-atlases.github.io/supplementary/index.html,,,"@article{kasten2021layered,
  PUBLISHER = {Association for Computing Machinery},
  JOURNAL = {ACM Transactions on Graphics (TOG)},
  ID = {kasten2021layered},
  ENTRYTYPE = {article},
  AUTHOR = {Yoni Kasten and Dolev Ofri and Oliver Wang and Tali Dekel},
  TITLE = {Layered Neural Atlases for Consistent Video Editing},
  EPRINT = {2109.11418v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We present a method that decomposes, or ""unwraps"", an input video into a set of layered 2D atlases, each providing a unified representation of the appearance of an object (or background) over the video. For each pixel in the video, our method estimates its corresponding 2D coordinate in each of the atlases, giving us a consistent parameterization of the video, along with an associated alpha (opacity) value. Importantly, we design our atlases to be interpretable and semantic, which facilitates easy and intuitive editing in the atlas domain, with minimal manual work required. Edits applied to a single 2D atlas (or input video frame) are automatically and consistently mapped back to the original video frames, while preserving occlusions, deformation, and other complex scene effects such as shadows and reflections. Our method employs a coordinate-based Multilayer Perceptron (MLP) representation for mappings, atlases, and alphas, which are jointly optimized on a per-video basis, using a combination of video reconstruction and regularization losses. By operating purely in 2D, our method does not require any prior 3D knowledge about scene geometry or camera poses, and can handle complex dynamic real world videos. We demonstrate various video editing applications, including texture mapping, video style transfer, image-to-video texture transfer, and segmentation/labeling propagation, all automatically produced by editing a single 2D atlas image.},
  YEAR = {2021},
  URL = {http://arxiv.org/abs/2109.11418v1},
  FILE = {2109.11418v1.pdf}
 }","Dynamic, Image, Editable, Segmentation/composition",,,,,,,,,,,SIGGRAPH 2021,,,,"Yoni Kasten, Dolev Ofri, Oliver Wang, Tali Dekel",kasten2021layered,00000223,"We present a method that decomposes, or ""unwraps"", an input video into a set of layered 2D atlases, each providing a unified representation of the appearance of an object (or background) over the video. For each pixel in the video, our method estimates its corresponding 2D coordinate in each of the atlases, giving us a consistent parameterization of the video, along with an associated alpha (opacity) value. Importantly, we design our atlases to be interpretable and semantic, which facilitates easy and intuitive editing in the atlas domain, with minimal manual work required. Edits applied to a single 2D atlas (or input video frame) are automatically and consistently mapped back to the original video frames, while preserving occlusions, deformation, and other complex scene effects such as shadows and reflections. Our method employs a coordinate-based Multilayer Perceptron (MLP) representation for mappings, atlases, and alphas, which are jointly optimized on a per-video basis, using a combination of video reconstruction and regularization losses. By operating purely in 2D, our method does not require any prior 3D knowledge about scene geometry or camera poses, and can handle complex dynamic real world videos. We demonstrate various video editing applications, including texture mapping, video style transfer, image-to-video texture transfer, and segmentation/labeling propagation, all automatically produced by editing a single 2D atlas image.",,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnufGKxBmYKsVd0adS9yv_GM4604Y9cjycOO-Oeplg5LmxA47MCqwG5aKKU20NRAAPtM
10/2/2021 8:15:20,ImplicitVol: Sensorless 3D Ultrasound Reconstruction with Deep Implicit Representation,ImplicitVol,9/24/2021,https://arxiv.org/pdf/2109.12108.pdf,https://pakheiyeung.github.io/ImplicitVol_wp/,https://github.com/pakheiyeung/ImplicitVol,https://www.youtube.com/watch?v=D4ZCo14mqxs,,,,"@article{yeung2021implicitvol,
  JOURNAL = {arXiv preprint arXiv:2109.12108},
  BOOKTITLE = {ArXiv Pre-print},
  ID = {yeung2021implicitvol},
  ENTRYTYPE = {article},
  AUTHOR = {Pak-Hei Yeung and Linde Hesse and Moska Aliasi and Monique Haak and the INTERGROWTH-21st Consortium and Weidi Xie and Ana I. L. Namburete},
  TITLE = {ImplicitVol: Sensorless 3D Ultrasound Reconstruction with Deep Implicit Representation},
  EPRINT = {2109.12108v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {eess.IV},
  ABSTRACT = {The objective of this work is to achieve sensorless reconstruction of a 3D volume from a set of 2D freehand ultrasound images with deep implicit representation. In contrast to the conventional way that represents a 3D volume as a discrete voxel grid, we do so by parameterizing it as the zero level-set of a continuous function, i.e. implicitly representing the 3D volume as a mapping from the spatial coordinates to the corresponding intensity values. Our proposed model, termed as ImplicitVol, takes a set of 2D scans and their estimated locations in 3D as input, jointly re?fing the estimated 3D locations and learning a full reconstruction of the 3D volume. When testing on real 2D ultrasound images, novel cross-sectional views that are sampled from ImplicitVol show significantly better visual quality than those sampled from existing reconstruction approaches, outperforming them by over 30% (NCC and SSIM), between the output and ground-truth on the 3D volume testing data. The code will be made publicly available.},
  YEAR = {2021},
  URL = {http://arxiv.org/abs/2109.12108v1},
  FILE = {2109.12108v1.pdf}
 }","Beyond graphics, Alternative imaging, Science and engineering",,,,,,,,,,,ARXIV 2021,,,,"Pak-Hei Yeung, Linde Hesse, Moska Aliasi, Monique Haak, the INTERGROWTH-21st Consortium, Weidi Xie, Ana I. L. Namburete",yeung2021implicitvol,00000228,"The objective of this work is to achieve sensorless reconstruction of a 3D volume from a set of 2D freehand ultrasound images with deep implicit representation. In contrast to the conventional way that represents a 3D volume as a discrete voxel grid, we do so by parameterizing it as the zero level-set of a continuous function, i.e. implicitly representing the 3D volume as a mapping from the spatial coordinates to the corresponding intensity values. Our proposed model, termed as ImplicitVol, takes a set of 2D scans and their estimated locations in 3D as input, jointly re?fing the estimated 3D locations and learning a full reconstruction of the 3D volume. When testing on real 2D ultrasound images, novel cross-sectional views that are sampled from ImplicitVol show significantly better visual quality than those sampled from existing reconstruction approaches, outperforming them by over 30% (NCC and SSIM), between the output and ground-truth on the 3D volume testing data. The code will be made publicly available.",,,,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnucZM7G6szTtRCu9HShI1ey3Hg6foiphG6agUh1zmxffNN4q3jA80rHY-T-viQhXi1Y
10/1/2021 13:30:27,TöRF: Time-of-Flight Radiance Fields for Dynamic Scene View Synthesis,TöRF,9/30/2021,https://arxiv.org/pdf/2109.15271.pdf,https://imaging.cs.cmu.edu/torf/,,,,,,"@inproceedings{attal2021torf,
  PUBLISHER = {Curran Associates, Inc.},
  BOOKTITLE = {Advances in Neural Information Processing Systems (NeurIPS)},
  ID = {attal2021torf},
  ENTRYTYPE = {inproceedings},
  AUTHOR = {Benjamin Attal and Eliot Laidlaw and Aaron Gokaslan and Changil Kim and Christian Richardt and James Tompkin and Matthew O'Toole},
  TITLE = {ToRF: Time-of-Flight Radiance Fields for Dynamic Scene View Synthesis},
  EPRINT = {2109.15271v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Neural networks can represent and accurately reconstruct radiance fields for static 3D scenes (e.g., NeRF). Several works extend these to dynamic scenes captured with monocular video, with promising performance. However, the monocular setting is known to be an under-constrained problem, and so methods rely on data-driven priors for reconstructing dynamic content. We replace these priors with measurements from a time-of-flight (ToF) camera, and introduce a neural representation based on an image formation model for continuous-wave ToF cameras. Instead of working with processed depth maps, we model the raw ToF sensor measurements to improve reconstruction quality and avoid issues with low reflectance regions, multi-path interference, and a sensor's limited unambiguous depth range. We show that this approach improves robustness of dynamic scene reconstruction to erroneous calibration and large motions, and discuss the benefits and limitations of integrating RGB+ToF sensors that are now available on modern smartphones.},
  YEAR = {2021},
  URL = {http://arxiv.org/abs/2109.15271v1},
  FILE = {2109.15271v1.pdf}
 }","Dynamic, Alternative imaging",,NeRF,Density,,,,,,,,NeurIPS 2021,,No,Direct,"Benjamin Attal, Eliot Laidlaw, Aaron Gokaslan, Changil Kim, Christian Richardt, James Tompkin, Matthew O'Toole",attal2021torf,00000227,"Neural networks can represent and accurately reconstruct radiance fields for static 3D scenes (e.g., NeRF). Several works extend these to dynamic scenes captured with monocular video, with promising performance. However, the monocular setting is known to be an under-constrained problem, and so methods rely on data-driven priors for reconstructing dynamic content. We replace these priors with measurements from a time-of-flight (ToF) camera, and introduce a neural representation based on an image formation model for continuous-wave ToF cameras. Instead of working with processed depth maps, we model the raw ToF sensor measurements to improve reconstruction quality and avoid issues with low reflectance regions, multi-path interference, and a sensor's limited unambiguous depth range. We show that this approach improves robustness of dynamic scene reconstruction to erroneous calibration and large motions, and discuss the benefits and limitations of integrating RGB+ToF sensors that are now available on modern smartphones.",,No,No,,,https://docs.google.com/forms/d/e/1FAIpQLSccBMzSvvL52m6b2LVmPsOkAyD-Vtj0dl0-K4E5D0S6b04vrA/viewform?edit2=2_ABaOnucCSNypJ3kkbKEtBjm0gbCY2rIQx9bNu1imXPjvoPZ1buY-gM0Qk3OZL1lT1u7E5d8